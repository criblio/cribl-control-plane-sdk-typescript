/*
 * Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT.
 */

import * as z from "zod/v3";
import { safeParse } from "../lib/schemas.js";
import { Result as SafeParseResult } from "../types/fp.js";
import {
  BackpressureBehaviorOptions1,
  BackpressureBehaviorOptions1$inboundSchema,
  BackpressureBehaviorOptions1$outboundSchema,
} from "./backpressurebehavioroptions1.js";
import {
  CompressionLevelOptions,
  CompressionLevelOptions$inboundSchema,
  CompressionLevelOptions$outboundSchema,
} from "./compressionleveloptions.js";
import {
  CompressionOptions2,
  CompressionOptions2$inboundSchema,
  CompressionOptions2$outboundSchema,
} from "./compressionoptions2.js";
import {
  DataFormatOptions,
  DataFormatOptions$inboundSchema,
  DataFormatOptions$outboundSchema,
} from "./dataformatoptions.js";
import {
  DataPageVersionOptions,
  DataPageVersionOptions$inboundSchema,
  DataPageVersionOptions$outboundSchema,
} from "./datapageversionoptions.js";
import {
  DiskSpaceProtectionOptions,
  DiskSpaceProtectionOptions$inboundSchema,
  DiskSpaceProtectionOptions$outboundSchema,
} from "./diskspaceprotectionoptions.js";
import { SDKValidationError } from "./errors/sdkvalidationerror.js";
import {
  ItemsTypeKeyValueMetadata,
  ItemsTypeKeyValueMetadata$inboundSchema,
  ItemsTypeKeyValueMetadata$Outbound,
  ItemsTypeKeyValueMetadata$outboundSchema,
} from "./itemstypekeyvaluemetadata.js";
import {
  ParquetVersionOptions,
  ParquetVersionOptions$inboundSchema,
  ParquetVersionOptions$outboundSchema,
} from "./parquetversionoptions.js";
import {
  ServerSideEncryptionOptions,
  ServerSideEncryptionOptions$inboundSchema,
  ServerSideEncryptionOptions$outboundSchema,
} from "./serversideencryptionoptions.js";
import {
  SignatureVersionOptions5,
  SignatureVersionOptions5$inboundSchema,
  SignatureVersionOptions5$outboundSchema,
} from "./signatureversionoptions5.js";
import {
  StorageClassOptions2,
  StorageClassOptions2$inboundSchema,
  StorageClassOptions2$outboundSchema,
} from "./storageclassoptions2.js";

export type OutputCloudflareR2 = {
  /**
   * Unique ID for this output
   */
  id?: string | undefined;
  type: "cloudflare_r2";
  /**
   * Pipeline to process data before sending out to this output
   */
  pipeline?: string | undefined;
  /**
   * Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
   */
  systemFields?: Array<string> | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Cloudflare R2 service URL (example: https://<ACCOUNT_ID>.r2.cloudflarestorage.com)
   */
  endpoint: string;
  /**
   * Name of the destination R2 bucket. This value can be a constant or a JavaScript expression that can only be evaluated at init time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`
   */
  bucket: string;
  /**
   * AWS authentication method. Choose Auto to use IAM roles.
   */
  awsAuthenticationMethod?: string | undefined;
  /**
   * Secret key. This value can be a constant or a JavaScript expression, such as `${C.env.SOME_SECRET}`).
   */
  awsSecretKey?: string | undefined;
  region?: any | undefined;
  /**
   * Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant stable storage.
   */
  stagePath?: string | undefined;
  /**
   * Add the Output ID value to staging location
   */
  addIdToStagePath?: boolean | undefined;
  /**
   * Root directory to prepend to path before uploading. Enter a constant, or a JavaScript expression enclosed in quotes or backticks.
   */
  destPath?: string | undefined;
  /**
   * Signature version to use for signing MinIO requests
   */
  signatureVersion?: SignatureVersionOptions5 | undefined;
  objectACL?: any | undefined;
  /**
   * Storage class to select for uploaded objects
   */
  storageClass?: StorageClassOptions2 | undefined;
  /**
   * Server-side encryption for uploaded objects
   */
  serverSideEncryption?: ServerSideEncryptionOptions | undefined;
  /**
   * Reuse connections between requests, which can improve performance
   */
  reuseConnections?: boolean | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA, such as self-signed certificates)
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * Disable if you can access files within the bucket but not the bucket itself
   */
  verifyPermissions?: boolean | undefined;
  /**
   * Remove empty staging directories after moving files
   */
  removeEmptyDirs?: boolean | undefined;
  /**
   * JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.
   */
  partitionExpr?: string | undefined;
  /**
   * Format of the output data
   */
  format?: DataFormatOptions | undefined;
  /**
   * JavaScript expression to define the output filename prefix (can be constant)
   */
  baseFileName?: string | undefined;
  /**
   * JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
   */
  fileNameSuffix?: string | undefined;
  /**
   * Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
   */
  maxFileSizeMB?: number | undefined;
  /**
   * Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
   */
  maxOpenFiles?: number | undefined;
  /**
   * If set, this line will be written to the beginning of each output file
   */
  headerLine?: string | undefined;
  /**
   * Buffer size used to write to a file
   */
  writeHighWaterMark?: number | undefined;
  /**
   * How to handle events when all receivers are exerting backpressure
   */
  onBackpressure?: BackpressureBehaviorOptions1 | undefined;
  /**
   * If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
   */
  deadletterEnabled?: boolean | undefined;
  /**
   * How to handle events when disk space is below the global 'Min free disk space' limit
   */
  onDiskFullBackpressure?: DiskSpaceProtectionOptions | undefined;
  /**
   * Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.
   */
  forceCloseOnShutdown?: boolean | undefined;
  /**
   * Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
   */
  maxFileOpenTimeSec?: number | undefined;
  /**
   * Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
   */
  maxFileIdleTimeSec?: number | undefined;
  /**
   * Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.
   */
  maxConcurrentFileParts?: number | undefined;
  description?: string | undefined;
  /**
   * This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)
   */
  awsApiKey?: string | undefined;
  /**
   * Select or create a stored secret that references your access key and secret key
   */
  awsSecret?: string | undefined;
  /**
   * Data compression format to apply to HTTP content before it is delivered
   */
  compress?: CompressionOptions2 | undefined;
  /**
   * Compression level to apply before moving files to final destination
   */
  compressionLevel?: CompressionLevelOptions | undefined;
  /**
   * Automatically calculate the schema based on the events of each Parquet file generated
   */
  automaticSchema?: boolean | undefined;
  /**
   * To add a new schema, navigate to Processing > Knowledge > Parquet Schemas
   */
  parquetSchema?: string | undefined;
  /**
   * Determines which data types are supported and how they are represented
   */
  parquetVersion?: ParquetVersionOptions | undefined;
  /**
   * Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
   */
  parquetDataPageVersion?: DataPageVersionOptions | undefined;
  /**
   * The number of rows that every group will contain. The final group can contain a smaller number of rows.
   */
  parquetRowGroupLength?: number | undefined;
  /**
   * Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
   */
  parquetPageSize?: string | undefined;
  /**
   * Log up to 3 rows that @{product} skips due to data mismatch
   */
  shouldLogInvalidRows?: boolean | undefined;
  /**
   * The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
   */
  keyValueMetadata?: Array<ItemsTypeKeyValueMetadata> | undefined;
  /**
   * Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
   */
  enableStatistics?: boolean | undefined;
  /**
   * One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
   */
  enableWritePageIndex?: boolean | undefined;
  /**
   * Parquet tools can use the checksum of a Parquet page to verify data integrity
   */
  enablePageChecksum?: boolean | undefined;
  /**
   * How frequently, in seconds, to clean up empty directories
   */
  emptyDirCleanupSec?: number | undefined;
  /**
   * Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.
   */
  directoryBatchSize?: number | undefined;
  /**
   * Storage location for files that fail to reach their final destination after maximum retries are exceeded
   */
  deadletterPath?: string | undefined;
  /**
   * The maximum number of times a file will attempt to move to its final destination before being dead-lettered
   */
  maxRetryNum?: number | undefined;
};

/** @internal */
export const OutputCloudflareR2$inboundSchema: z.ZodType<
  OutputCloudflareR2,
  z.ZodTypeDef,
  unknown
> = z.object({
  id: z.string().optional(),
  type: z.literal("cloudflare_r2"),
  pipeline: z.string().optional(),
  systemFields: z.array(z.string()).optional(),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  endpoint: z.string(),
  bucket: z.string(),
  awsAuthenticationMethod: z.string().default("auto"),
  awsSecretKey: z.string().optional(),
  region: z.any().optional(),
  stagePath: z.string().default("$CRIBL_HOME/state/outputs/staging"),
  addIdToStagePath: z.boolean().default(true),
  destPath: z.string().optional(),
  signatureVersion: SignatureVersionOptions5$inboundSchema.default("v4"),
  objectACL: z.any().optional(),
  storageClass: StorageClassOptions2$inboundSchema.optional(),
  serverSideEncryption: ServerSideEncryptionOptions$inboundSchema.optional(),
  reuseConnections: z.boolean().default(true),
  rejectUnauthorized: z.boolean().default(true),
  verifyPermissions: z.boolean().default(true),
  removeEmptyDirs: z.boolean().default(true),
  partitionExpr: z.string().default(
    "C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')",
  ),
  format: DataFormatOptions$inboundSchema.default("json"),
  baseFileName: z.string().default("`CriblOut`"),
  fileNameSuffix: z.string().default(
    "`.${C.env[\"CRIBL_WORKER_ID\"]}.${__format}${__compression === \"gzip\" ? \".gz\" : \"\"}`",
  ),
  maxFileSizeMB: z.number().default(32),
  maxOpenFiles: z.number().default(100),
  headerLine: z.string().default(""),
  writeHighWaterMark: z.number().default(64),
  onBackpressure: BackpressureBehaviorOptions1$inboundSchema.default("block"),
  deadletterEnabled: z.boolean().default(false),
  onDiskFullBackpressure: DiskSpaceProtectionOptions$inboundSchema.default(
    "block",
  ),
  forceCloseOnShutdown: z.boolean().default(false),
  maxFileOpenTimeSec: z.number().default(300),
  maxFileIdleTimeSec: z.number().default(30),
  maxConcurrentFileParts: z.number().default(4),
  description: z.string().optional(),
  awsApiKey: z.string().optional(),
  awsSecret: z.string().optional(),
  compress: CompressionOptions2$inboundSchema.default("gzip"),
  compressionLevel: CompressionLevelOptions$inboundSchema.default("best_speed"),
  automaticSchema: z.boolean().default(false),
  parquetSchema: z.string().optional(),
  parquetVersion: ParquetVersionOptions$inboundSchema.default("PARQUET_2_6"),
  parquetDataPageVersion: DataPageVersionOptions$inboundSchema.default(
    "DATA_PAGE_V2",
  ),
  parquetRowGroupLength: z.number().default(10000),
  parquetPageSize: z.string().default("1MB"),
  shouldLogInvalidRows: z.boolean().optional(),
  keyValueMetadata: z.array(ItemsTypeKeyValueMetadata$inboundSchema).optional(),
  enableStatistics: z.boolean().default(true),
  enableWritePageIndex: z.boolean().default(true),
  enablePageChecksum: z.boolean().default(false),
  emptyDirCleanupSec: z.number().default(300),
  directoryBatchSize: z.number().default(1000),
  deadletterPath: z.string().default("$CRIBL_HOME/state/outputs/dead-letter"),
  maxRetryNum: z.number().default(20),
});
/** @internal */
export type OutputCloudflareR2$Outbound = {
  id?: string | undefined;
  type: "cloudflare_r2";
  pipeline?: string | undefined;
  systemFields?: Array<string> | undefined;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  endpoint: string;
  bucket: string;
  awsAuthenticationMethod: string;
  awsSecretKey?: string | undefined;
  region?: any | undefined;
  stagePath: string;
  addIdToStagePath: boolean;
  destPath?: string | undefined;
  signatureVersion: string;
  objectACL?: any | undefined;
  storageClass?: string | undefined;
  serverSideEncryption?: string | undefined;
  reuseConnections: boolean;
  rejectUnauthorized: boolean;
  verifyPermissions: boolean;
  removeEmptyDirs: boolean;
  partitionExpr: string;
  format: string;
  baseFileName: string;
  fileNameSuffix: string;
  maxFileSizeMB: number;
  maxOpenFiles: number;
  headerLine: string;
  writeHighWaterMark: number;
  onBackpressure: string;
  deadletterEnabled: boolean;
  onDiskFullBackpressure: string;
  forceCloseOnShutdown: boolean;
  maxFileOpenTimeSec: number;
  maxFileIdleTimeSec: number;
  maxConcurrentFileParts: number;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  awsSecret?: string | undefined;
  compress: string;
  compressionLevel: string;
  automaticSchema: boolean;
  parquetSchema?: string | undefined;
  parquetVersion: string;
  parquetDataPageVersion: string;
  parquetRowGroupLength: number;
  parquetPageSize: string;
  shouldLogInvalidRows?: boolean | undefined;
  keyValueMetadata?: Array<ItemsTypeKeyValueMetadata$Outbound> | undefined;
  enableStatistics: boolean;
  enableWritePageIndex: boolean;
  enablePageChecksum: boolean;
  emptyDirCleanupSec: number;
  directoryBatchSize: number;
  deadletterPath: string;
  maxRetryNum: number;
};

/** @internal */
export const OutputCloudflareR2$outboundSchema: z.ZodType<
  OutputCloudflareR2$Outbound,
  z.ZodTypeDef,
  OutputCloudflareR2
> = z.object({
  id: z.string().optional(),
  type: z.literal("cloudflare_r2"),
  pipeline: z.string().optional(),
  systemFields: z.array(z.string()).optional(),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  endpoint: z.string(),
  bucket: z.string(),
  awsAuthenticationMethod: z.string().default("auto"),
  awsSecretKey: z.string().optional(),
  region: z.any().optional(),
  stagePath: z.string().default("$CRIBL_HOME/state/outputs/staging"),
  addIdToStagePath: z.boolean().default(true),
  destPath: z.string().optional(),
  signatureVersion: SignatureVersionOptions5$outboundSchema.default("v4"),
  objectACL: z.any().optional(),
  storageClass: StorageClassOptions2$outboundSchema.optional(),
  serverSideEncryption: ServerSideEncryptionOptions$outboundSchema.optional(),
  reuseConnections: z.boolean().default(true),
  rejectUnauthorized: z.boolean().default(true),
  verifyPermissions: z.boolean().default(true),
  removeEmptyDirs: z.boolean().default(true),
  partitionExpr: z.string().default(
    "C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')",
  ),
  format: DataFormatOptions$outboundSchema.default("json"),
  baseFileName: z.string().default("`CriblOut`"),
  fileNameSuffix: z.string().default(
    "`.${C.env[\"CRIBL_WORKER_ID\"]}.${__format}${__compression === \"gzip\" ? \".gz\" : \"\"}`",
  ),
  maxFileSizeMB: z.number().default(32),
  maxOpenFiles: z.number().default(100),
  headerLine: z.string().default(""),
  writeHighWaterMark: z.number().default(64),
  onBackpressure: BackpressureBehaviorOptions1$outboundSchema.default("block"),
  deadletterEnabled: z.boolean().default(false),
  onDiskFullBackpressure: DiskSpaceProtectionOptions$outboundSchema.default(
    "block",
  ),
  forceCloseOnShutdown: z.boolean().default(false),
  maxFileOpenTimeSec: z.number().default(300),
  maxFileIdleTimeSec: z.number().default(30),
  maxConcurrentFileParts: z.number().default(4),
  description: z.string().optional(),
  awsApiKey: z.string().optional(),
  awsSecret: z.string().optional(),
  compress: CompressionOptions2$outboundSchema.default("gzip"),
  compressionLevel: CompressionLevelOptions$outboundSchema.default(
    "best_speed",
  ),
  automaticSchema: z.boolean().default(false),
  parquetSchema: z.string().optional(),
  parquetVersion: ParquetVersionOptions$outboundSchema.default("PARQUET_2_6"),
  parquetDataPageVersion: DataPageVersionOptions$outboundSchema.default(
    "DATA_PAGE_V2",
  ),
  parquetRowGroupLength: z.number().default(10000),
  parquetPageSize: z.string().default("1MB"),
  shouldLogInvalidRows: z.boolean().optional(),
  keyValueMetadata: z.array(ItemsTypeKeyValueMetadata$outboundSchema)
    .optional(),
  enableStatistics: z.boolean().default(true),
  enableWritePageIndex: z.boolean().default(true),
  enablePageChecksum: z.boolean().default(false),
  emptyDirCleanupSec: z.number().default(300),
  directoryBatchSize: z.number().default(1000),
  deadletterPath: z.string().default("$CRIBL_HOME/state/outputs/dead-letter"),
  maxRetryNum: z.number().default(20),
});

export function outputCloudflareR2ToJSON(
  outputCloudflareR2: OutputCloudflareR2,
): string {
  return JSON.stringify(
    OutputCloudflareR2$outboundSchema.parse(outputCloudflareR2),
  );
}
export function outputCloudflareR2FromJSON(
  jsonString: string,
): SafeParseResult<OutputCloudflareR2, SDKValidationError> {
  return safeParse(
    jsonString,
    (x) => OutputCloudflareR2$inboundSchema.parse(JSON.parse(x)),
    `Failed to parse 'OutputCloudflareR2' from JSON`,
  );
}
