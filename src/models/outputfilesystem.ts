/*
 * Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT.
 */

import * as z from "zod/v3";
import { safeParse } from "../lib/schemas.js";
import { ClosedEnum } from "../types/enums.js";
import { Result as SafeParseResult } from "../types/fp.js";
import {
  CompressionLevelOptions,
  CompressionLevelOptions$inboundSchema,
  CompressionLevelOptions$outboundSchema,
} from "./compressionleveloptions.js";
import { SDKValidationError } from "./errors/sdkvalidationerror.js";
import {
  Format1Options,
  Format1Options$inboundSchema,
  Format1Options$outboundSchema,
} from "./format1options.js";
import {
  ParquetDataPageVersionOptions,
  ParquetDataPageVersionOptions$inboundSchema,
  ParquetDataPageVersionOptions$outboundSchema,
} from "./parquetdatapageversionoptions.js";
import {
  ParquetVersionOptions,
  ParquetVersionOptions$inboundSchema,
  ParquetVersionOptions$outboundSchema,
} from "./parquetversionoptions.js";
import {
  PqCompressOptions,
  PqCompressOptions$inboundSchema,
  PqCompressOptions$outboundSchema,
} from "./pqcompressoptions.js";
import {
  PqOnBackpressureOptions,
  PqOnBackpressureOptions$inboundSchema,
  PqOnBackpressureOptions$outboundSchema,
} from "./pqonbackpressureoptions.js";
import {
  TagsType,
  TagsType$inboundSchema,
  TagsType$Outbound,
  TagsType$outboundSchema,
} from "./tagstype.js";

export const OutputFilesystemType6 = {
  Filesystem: "filesystem",
} as const;
export type OutputFilesystemType6 = ClosedEnum<typeof OutputFilesystemType6>;

export type OutputFilesystemFilesystem6 = {
  /**
   * If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
   */
  deadletterEnabled?: boolean | undefined;
  /**
   * Unique ID for this output
   */
  id?: string | undefined;
  type: OutputFilesystemType6;
  /**
   * Pipeline to process data before sending out to this output
   */
  pipeline?: string | undefined;
  /**
   * Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
   */
  systemFields?: Array<string> | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Final destination for the output files
   */
  destPath: string;
  /**
   * Filesystem location in which to buffer files before compressing and moving to final destination. Use performant, stable storage.
   */
  stagePath?: string | undefined;
  /**
   * Add the Output ID value to staging location
   */
  addIdToStagePath?: boolean | undefined;
  /**
   * Remove empty staging directories after moving files
   */
  removeEmptyDirs?: boolean | undefined;
  /**
   * JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.
   */
  partitionExpr?: string | undefined;
  /**
   * Format of the output data
   */
  format?: Format1Options | undefined;
  /**
   * JavaScript expression to define the output filename prefix (can be constant)
   */
  baseFileName?: string | undefined;
  /**
   * JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
   */
  fileNameSuffix?: string | undefined;
  /**
   * Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
   */
  maxFileSizeMB?: number | undefined;
  /**
   * Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
   */
  maxFileOpenTimeSec?: number | undefined;
  /**
   * Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
   */
  maxFileIdleTimeSec?: number | undefined;
  /**
   * Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
   */
  maxOpenFiles?: number | undefined;
  /**
   * If set, this line will be written to the beginning of each output file
   */
  headerLine?: string | undefined;
  /**
   * Buffer size used to write to a file
   */
  writeHighWaterMark?: number | undefined;
  /**
   * How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
   */
  onBackpressure?: PqOnBackpressureOptions | undefined;
  /**
   * How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
   */
  onDiskFullBackpressure?: PqOnBackpressureOptions | undefined;
  description?: string | undefined;
  /**
   * Codec to use to compress the persisted data
   */
  compress?: PqCompressOptions | undefined;
  /**
   * Compression level to apply before moving files to final destination
   */
  compressionLevel?: CompressionLevelOptions | undefined;
  /**
   * Automatically calculate the schema based on the events of each Parquet file generated
   */
  automaticSchema?: boolean | undefined;
  /**
   * To add a new schema, navigate to Processing > Knowledge > Parquet Schemas
   */
  parquetSchema?: string | undefined;
  /**
   * Determines which data types are supported and how they are represented
   */
  parquetVersion?: ParquetVersionOptions | undefined;
  /**
   * Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
   */
  parquetDataPageVersion?: ParquetDataPageVersionOptions | undefined;
  /**
   * The number of rows that every group will contain. The final group can contain a smaller number of rows.
   */
  parquetRowGroupLength?: number | undefined;
  /**
   * Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
   */
  parquetPageSize?: string | undefined;
  /**
   * Log up to 3 rows that @{product} skips due to data mismatch
   */
  shouldLogInvalidRows?: boolean | undefined;
  /**
   * The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
   */
  keyValueMetadata?: Array<TagsType> | undefined;
  /**
   * Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
   */
  enableStatistics?: boolean | undefined;
  /**
   * One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
   */
  enableWritePageIndex?: boolean | undefined;
  /**
   * Parquet tools can use the checksum of a Parquet page to verify data integrity
   */
  enablePageChecksum?: boolean | undefined;
  /**
   * How frequently, in seconds, to clean up empty directories
   */
  emptyDirCleanupSec?: number | undefined;
  /**
   * Storage location for files that fail to reach their final destination after maximum retries are exceeded
   */
  deadletterPath?: string | undefined;
  /**
   * The maximum number of times a file will attempt to move to its final destination before being dead-lettered
   */
  maxRetryNum?: number | undefined;
};

export const OutputFilesystemType5 = {
  Filesystem: "filesystem",
} as const;
export type OutputFilesystemType5 = ClosedEnum<typeof OutputFilesystemType5>;

export type OutputFilesystemFilesystem5 = {
  /**
   * If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
   */
  deadletterEnabled?: boolean | undefined;
  /**
   * Unique ID for this output
   */
  id?: string | undefined;
  type: OutputFilesystemType5;
  /**
   * Pipeline to process data before sending out to this output
   */
  pipeline?: string | undefined;
  /**
   * Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
   */
  systemFields?: Array<string> | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Final destination for the output files
   */
  destPath: string;
  /**
   * Filesystem location in which to buffer files before compressing and moving to final destination. Use performant, stable storage.
   */
  stagePath?: string | undefined;
  /**
   * Add the Output ID value to staging location
   */
  addIdToStagePath?: boolean | undefined;
  /**
   * Remove empty staging directories after moving files
   */
  removeEmptyDirs?: boolean | undefined;
  /**
   * JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.
   */
  partitionExpr?: string | undefined;
  /**
   * Format of the output data
   */
  format?: Format1Options | undefined;
  /**
   * JavaScript expression to define the output filename prefix (can be constant)
   */
  baseFileName?: string | undefined;
  /**
   * JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
   */
  fileNameSuffix?: string | undefined;
  /**
   * Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
   */
  maxFileSizeMB?: number | undefined;
  /**
   * Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
   */
  maxFileOpenTimeSec?: number | undefined;
  /**
   * Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
   */
  maxFileIdleTimeSec?: number | undefined;
  /**
   * Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
   */
  maxOpenFiles?: number | undefined;
  /**
   * If set, this line will be written to the beginning of each output file
   */
  headerLine?: string | undefined;
  /**
   * Buffer size used to write to a file
   */
  writeHighWaterMark?: number | undefined;
  /**
   * How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
   */
  onBackpressure?: PqOnBackpressureOptions | undefined;
  /**
   * How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
   */
  onDiskFullBackpressure?: PqOnBackpressureOptions | undefined;
  description?: string | undefined;
  /**
   * Codec to use to compress the persisted data
   */
  compress?: PqCompressOptions | undefined;
  /**
   * Compression level to apply before moving files to final destination
   */
  compressionLevel?: CompressionLevelOptions | undefined;
  /**
   * Automatically calculate the schema based on the events of each Parquet file generated
   */
  automaticSchema?: boolean | undefined;
  /**
   * To add a new schema, navigate to Processing > Knowledge > Parquet Schemas
   */
  parquetSchema?: string | undefined;
  /**
   * Determines which data types are supported and how they are represented
   */
  parquetVersion?: ParquetVersionOptions | undefined;
  /**
   * Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
   */
  parquetDataPageVersion?: ParquetDataPageVersionOptions | undefined;
  /**
   * The number of rows that every group will contain. The final group can contain a smaller number of rows.
   */
  parquetRowGroupLength?: number | undefined;
  /**
   * Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
   */
  parquetPageSize?: string | undefined;
  /**
   * Log up to 3 rows that @{product} skips due to data mismatch
   */
  shouldLogInvalidRows?: boolean | undefined;
  /**
   * The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
   */
  keyValueMetadata?: Array<TagsType> | undefined;
  /**
   * Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
   */
  enableStatistics?: boolean | undefined;
  /**
   * One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
   */
  enableWritePageIndex?: boolean | undefined;
  /**
   * Parquet tools can use the checksum of a Parquet page to verify data integrity
   */
  enablePageChecksum?: boolean | undefined;
  /**
   * How frequently, in seconds, to clean up empty directories
   */
  emptyDirCleanupSec?: number | undefined;
  /**
   * Storage location for files that fail to reach their final destination after maximum retries are exceeded
   */
  deadletterPath?: string | undefined;
  /**
   * The maximum number of times a file will attempt to move to its final destination before being dead-lettered
   */
  maxRetryNum?: number | undefined;
};

export const OutputFilesystemType4 = {
  Filesystem: "filesystem",
} as const;
export type OutputFilesystemType4 = ClosedEnum<typeof OutputFilesystemType4>;

export type OutputFilesystemFilesystem4 = {
  /**
   * Remove empty staging directories after moving files
   */
  removeEmptyDirs?: boolean | undefined;
  /**
   * Unique ID for this output
   */
  id?: string | undefined;
  type: OutputFilesystemType4;
  /**
   * Pipeline to process data before sending out to this output
   */
  pipeline?: string | undefined;
  /**
   * Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
   */
  systemFields?: Array<string> | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Final destination for the output files
   */
  destPath: string;
  /**
   * Filesystem location in which to buffer files before compressing and moving to final destination. Use performant, stable storage.
   */
  stagePath?: string | undefined;
  /**
   * Add the Output ID value to staging location
   */
  addIdToStagePath?: boolean | undefined;
  /**
   * JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.
   */
  partitionExpr?: string | undefined;
  /**
   * Format of the output data
   */
  format?: Format1Options | undefined;
  /**
   * JavaScript expression to define the output filename prefix (can be constant)
   */
  baseFileName?: string | undefined;
  /**
   * JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
   */
  fileNameSuffix?: string | undefined;
  /**
   * Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
   */
  maxFileSizeMB?: number | undefined;
  /**
   * Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
   */
  maxFileOpenTimeSec?: number | undefined;
  /**
   * Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
   */
  maxFileIdleTimeSec?: number | undefined;
  /**
   * Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
   */
  maxOpenFiles?: number | undefined;
  /**
   * If set, this line will be written to the beginning of each output file
   */
  headerLine?: string | undefined;
  /**
   * Buffer size used to write to a file
   */
  writeHighWaterMark?: number | undefined;
  /**
   * How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
   */
  onBackpressure?: PqOnBackpressureOptions | undefined;
  /**
   * If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
   */
  deadletterEnabled?: boolean | undefined;
  /**
   * How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
   */
  onDiskFullBackpressure?: PqOnBackpressureOptions | undefined;
  description?: string | undefined;
  /**
   * Codec to use to compress the persisted data
   */
  compress?: PqCompressOptions | undefined;
  /**
   * Compression level to apply before moving files to final destination
   */
  compressionLevel?: CompressionLevelOptions | undefined;
  /**
   * Automatically calculate the schema based on the events of each Parquet file generated
   */
  automaticSchema?: boolean | undefined;
  /**
   * To add a new schema, navigate to Processing > Knowledge > Parquet Schemas
   */
  parquetSchema?: string | undefined;
  /**
   * Determines which data types are supported and how they are represented
   */
  parquetVersion?: ParquetVersionOptions | undefined;
  /**
   * Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
   */
  parquetDataPageVersion?: ParquetDataPageVersionOptions | undefined;
  /**
   * The number of rows that every group will contain. The final group can contain a smaller number of rows.
   */
  parquetRowGroupLength?: number | undefined;
  /**
   * Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
   */
  parquetPageSize?: string | undefined;
  /**
   * Log up to 3 rows that @{product} skips due to data mismatch
   */
  shouldLogInvalidRows?: boolean | undefined;
  /**
   * The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
   */
  keyValueMetadata?: Array<TagsType> | undefined;
  /**
   * Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
   */
  enableStatistics?: boolean | undefined;
  /**
   * One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
   */
  enableWritePageIndex?: boolean | undefined;
  /**
   * Parquet tools can use the checksum of a Parquet page to verify data integrity
   */
  enablePageChecksum?: boolean | undefined;
  /**
   * How frequently, in seconds, to clean up empty directories
   */
  emptyDirCleanupSec?: number | undefined;
  /**
   * Storage location for files that fail to reach their final destination after maximum retries are exceeded
   */
  deadletterPath?: string | undefined;
  /**
   * The maximum number of times a file will attempt to move to its final destination before being dead-lettered
   */
  maxRetryNum?: number | undefined;
};

export const OutputFilesystemType3 = {
  Filesystem: "filesystem",
} as const;
export type OutputFilesystemType3 = ClosedEnum<typeof OutputFilesystemType3>;

export type OutputFilesystemFilesystem3 = {
  /**
   * Remove empty staging directories after moving files
   */
  removeEmptyDirs?: boolean | undefined;
  /**
   * Unique ID for this output
   */
  id?: string | undefined;
  type: OutputFilesystemType3;
  /**
   * Pipeline to process data before sending out to this output
   */
  pipeline?: string | undefined;
  /**
   * Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
   */
  systemFields?: Array<string> | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Final destination for the output files
   */
  destPath: string;
  /**
   * Filesystem location in which to buffer files before compressing and moving to final destination. Use performant, stable storage.
   */
  stagePath?: string | undefined;
  /**
   * Add the Output ID value to staging location
   */
  addIdToStagePath?: boolean | undefined;
  /**
   * JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.
   */
  partitionExpr?: string | undefined;
  /**
   * Format of the output data
   */
  format?: Format1Options | undefined;
  /**
   * JavaScript expression to define the output filename prefix (can be constant)
   */
  baseFileName?: string | undefined;
  /**
   * JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
   */
  fileNameSuffix?: string | undefined;
  /**
   * Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
   */
  maxFileSizeMB?: number | undefined;
  /**
   * Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
   */
  maxFileOpenTimeSec?: number | undefined;
  /**
   * Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
   */
  maxFileIdleTimeSec?: number | undefined;
  /**
   * Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
   */
  maxOpenFiles?: number | undefined;
  /**
   * If set, this line will be written to the beginning of each output file
   */
  headerLine?: string | undefined;
  /**
   * Buffer size used to write to a file
   */
  writeHighWaterMark?: number | undefined;
  /**
   * How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
   */
  onBackpressure?: PqOnBackpressureOptions | undefined;
  /**
   * If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
   */
  deadletterEnabled?: boolean | undefined;
  /**
   * How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
   */
  onDiskFullBackpressure?: PqOnBackpressureOptions | undefined;
  description?: string | undefined;
  /**
   * Codec to use to compress the persisted data
   */
  compress?: PqCompressOptions | undefined;
  /**
   * Compression level to apply before moving files to final destination
   */
  compressionLevel?: CompressionLevelOptions | undefined;
  /**
   * Automatically calculate the schema based on the events of each Parquet file generated
   */
  automaticSchema?: boolean | undefined;
  /**
   * To add a new schema, navigate to Processing > Knowledge > Parquet Schemas
   */
  parquetSchema?: string | undefined;
  /**
   * Determines which data types are supported and how they are represented
   */
  parquetVersion?: ParquetVersionOptions | undefined;
  /**
   * Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
   */
  parquetDataPageVersion?: ParquetDataPageVersionOptions | undefined;
  /**
   * The number of rows that every group will contain. The final group can contain a smaller number of rows.
   */
  parquetRowGroupLength?: number | undefined;
  /**
   * Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
   */
  parquetPageSize?: string | undefined;
  /**
   * Log up to 3 rows that @{product} skips due to data mismatch
   */
  shouldLogInvalidRows?: boolean | undefined;
  /**
   * The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
   */
  keyValueMetadata?: Array<TagsType> | undefined;
  /**
   * Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
   */
  enableStatistics?: boolean | undefined;
  /**
   * One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
   */
  enableWritePageIndex?: boolean | undefined;
  /**
   * Parquet tools can use the checksum of a Parquet page to verify data integrity
   */
  enablePageChecksum?: boolean | undefined;
  /**
   * How frequently, in seconds, to clean up empty directories
   */
  emptyDirCleanupSec?: number | undefined;
  /**
   * Storage location for files that fail to reach their final destination after maximum retries are exceeded
   */
  deadletterPath?: string | undefined;
  /**
   * The maximum number of times a file will attempt to move to its final destination before being dead-lettered
   */
  maxRetryNum?: number | undefined;
};

export const OutputFilesystemType2 = {
  Filesystem: "filesystem",
} as const;
export type OutputFilesystemType2 = ClosedEnum<typeof OutputFilesystemType2>;

export type OutputFilesystemFilesystem2 = {
  /**
   * Format of the output data
   */
  format?: Format1Options | undefined;
  /**
   * Unique ID for this output
   */
  id?: string | undefined;
  type: OutputFilesystemType2;
  /**
   * Pipeline to process data before sending out to this output
   */
  pipeline?: string | undefined;
  /**
   * Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
   */
  systemFields?: Array<string> | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Final destination for the output files
   */
  destPath: string;
  /**
   * Filesystem location in which to buffer files before compressing and moving to final destination. Use performant, stable storage.
   */
  stagePath?: string | undefined;
  /**
   * Add the Output ID value to staging location
   */
  addIdToStagePath?: boolean | undefined;
  /**
   * Remove empty staging directories after moving files
   */
  removeEmptyDirs?: boolean | undefined;
  /**
   * JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.
   */
  partitionExpr?: string | undefined;
  /**
   * JavaScript expression to define the output filename prefix (can be constant)
   */
  baseFileName?: string | undefined;
  /**
   * JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
   */
  fileNameSuffix?: string | undefined;
  /**
   * Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
   */
  maxFileSizeMB?: number | undefined;
  /**
   * Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
   */
  maxFileOpenTimeSec?: number | undefined;
  /**
   * Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
   */
  maxFileIdleTimeSec?: number | undefined;
  /**
   * Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
   */
  maxOpenFiles?: number | undefined;
  /**
   * If set, this line will be written to the beginning of each output file
   */
  headerLine?: string | undefined;
  /**
   * Buffer size used to write to a file
   */
  writeHighWaterMark?: number | undefined;
  /**
   * How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
   */
  onBackpressure?: PqOnBackpressureOptions | undefined;
  /**
   * If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
   */
  deadletterEnabled?: boolean | undefined;
  /**
   * How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
   */
  onDiskFullBackpressure?: PqOnBackpressureOptions | undefined;
  description?: string | undefined;
  /**
   * Codec to use to compress the persisted data
   */
  compress?: PqCompressOptions | undefined;
  /**
   * Compression level to apply before moving files to final destination
   */
  compressionLevel?: CompressionLevelOptions | undefined;
  /**
   * Automatically calculate the schema based on the events of each Parquet file generated
   */
  automaticSchema?: boolean | undefined;
  /**
   * To add a new schema, navigate to Processing > Knowledge > Parquet Schemas
   */
  parquetSchema?: string | undefined;
  /**
   * Determines which data types are supported and how they are represented
   */
  parquetVersion?: ParquetVersionOptions | undefined;
  /**
   * Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
   */
  parquetDataPageVersion?: ParquetDataPageVersionOptions | undefined;
  /**
   * The number of rows that every group will contain. The final group can contain a smaller number of rows.
   */
  parquetRowGroupLength?: number | undefined;
  /**
   * Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
   */
  parquetPageSize?: string | undefined;
  /**
   * Log up to 3 rows that @{product} skips due to data mismatch
   */
  shouldLogInvalidRows: boolean;
  /**
   * The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
   */
  keyValueMetadata: Array<TagsType>;
  /**
   * Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
   */
  enableStatistics?: boolean | undefined;
  /**
   * One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
   */
  enableWritePageIndex?: boolean | undefined;
  /**
   * Parquet tools can use the checksum of a Parquet page to verify data integrity
   */
  enablePageChecksum?: boolean | undefined;
  /**
   * How frequently, in seconds, to clean up empty directories
   */
  emptyDirCleanupSec?: number | undefined;
  /**
   * Storage location for files that fail to reach their final destination after maximum retries are exceeded
   */
  deadletterPath?: string | undefined;
  /**
   * The maximum number of times a file will attempt to move to its final destination before being dead-lettered
   */
  maxRetryNum?: number | undefined;
};

export const OutputFilesystemType1 = {
  Filesystem: "filesystem",
} as const;
export type OutputFilesystemType1 = ClosedEnum<typeof OutputFilesystemType1>;

export type OutputFilesystemFilesystem1 = {
  /**
   * Format of the output data
   */
  format?: Format1Options | undefined;
  /**
   * Unique ID for this output
   */
  id?: string | undefined;
  type: OutputFilesystemType1;
  /**
   * Pipeline to process data before sending out to this output
   */
  pipeline?: string | undefined;
  /**
   * Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
   */
  systemFields?: Array<string> | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Final destination for the output files
   */
  destPath: string;
  /**
   * Filesystem location in which to buffer files before compressing and moving to final destination. Use performant, stable storage.
   */
  stagePath?: string | undefined;
  /**
   * Add the Output ID value to staging location
   */
  addIdToStagePath?: boolean | undefined;
  /**
   * Remove empty staging directories after moving files
   */
  removeEmptyDirs?: boolean | undefined;
  /**
   * JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.
   */
  partitionExpr?: string | undefined;
  /**
   * JavaScript expression to define the output filename prefix (can be constant)
   */
  baseFileName?: string | undefined;
  /**
   * JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
   */
  fileNameSuffix?: string | undefined;
  /**
   * Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
   */
  maxFileSizeMB?: number | undefined;
  /**
   * Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
   */
  maxFileOpenTimeSec?: number | undefined;
  /**
   * Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
   */
  maxFileIdleTimeSec?: number | undefined;
  /**
   * Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
   */
  maxOpenFiles?: number | undefined;
  /**
   * If set, this line will be written to the beginning of each output file
   */
  headerLine?: string | undefined;
  /**
   * Buffer size used to write to a file
   */
  writeHighWaterMark?: number | undefined;
  /**
   * How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
   */
  onBackpressure?: PqOnBackpressureOptions | undefined;
  /**
   * If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
   */
  deadletterEnabled?: boolean | undefined;
  /**
   * How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
   */
  onDiskFullBackpressure?: PqOnBackpressureOptions | undefined;
  description?: string | undefined;
  /**
   * Codec to use to compress the persisted data
   */
  compress?: PqCompressOptions | undefined;
  /**
   * Compression level to apply before moving files to final destination
   */
  compressionLevel?: CompressionLevelOptions | undefined;
  /**
   * Automatically calculate the schema based on the events of each Parquet file generated
   */
  automaticSchema?: boolean | undefined;
  /**
   * To add a new schema, navigate to Processing > Knowledge > Parquet Schemas
   */
  parquetSchema?: string | undefined;
  /**
   * Determines which data types are supported and how they are represented
   */
  parquetVersion?: ParquetVersionOptions | undefined;
  /**
   * Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
   */
  parquetDataPageVersion?: ParquetDataPageVersionOptions | undefined;
  /**
   * The number of rows that every group will contain. The final group can contain a smaller number of rows.
   */
  parquetRowGroupLength?: number | undefined;
  /**
   * Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
   */
  parquetPageSize?: string | undefined;
  /**
   * Log up to 3 rows that @{product} skips due to data mismatch
   */
  shouldLogInvalidRows?: boolean | undefined;
  /**
   * The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
   */
  keyValueMetadata?: Array<TagsType> | undefined;
  /**
   * Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
   */
  enableStatistics?: boolean | undefined;
  /**
   * One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
   */
  enableWritePageIndex?: boolean | undefined;
  /**
   * Parquet tools can use the checksum of a Parquet page to verify data integrity
   */
  enablePageChecksum?: boolean | undefined;
  /**
   * How frequently, in seconds, to clean up empty directories
   */
  emptyDirCleanupSec?: number | undefined;
  /**
   * Storage location for files that fail to reach their final destination after maximum retries are exceeded
   */
  deadletterPath?: string | undefined;
  /**
   * The maximum number of times a file will attempt to move to its final destination before being dead-lettered
   */
  maxRetryNum?: number | undefined;
};

export type OutputFilesystem =
  | OutputFilesystemFilesystem2
  | OutputFilesystemFilesystem1
  | OutputFilesystemFilesystem3
  | OutputFilesystemFilesystem4
  | OutputFilesystemFilesystem5
  | OutputFilesystemFilesystem6;

/** @internal */
export const OutputFilesystemType6$inboundSchema: z.ZodNativeEnum<
  typeof OutputFilesystemType6
> = z.nativeEnum(OutputFilesystemType6);
/** @internal */
export const OutputFilesystemType6$outboundSchema: z.ZodNativeEnum<
  typeof OutputFilesystemType6
> = OutputFilesystemType6$inboundSchema;

/** @internal */
export const OutputFilesystemFilesystem6$inboundSchema: z.ZodType<
  OutputFilesystemFilesystem6,
  z.ZodTypeDef,
  unknown
> = z.object({
  deadletterEnabled: z.boolean().default(false),
  id: z.string().optional(),
  type: OutputFilesystemType6$inboundSchema,
  pipeline: z.string().optional(),
  systemFields: z.array(z.string()).optional(),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  destPath: z.string(),
  stagePath: z.string().optional(),
  addIdToStagePath: z.boolean().default(true),
  removeEmptyDirs: z.boolean().default(true),
  partitionExpr: z.string().default(
    "C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')",
  ),
  format: Format1Options$inboundSchema.default("json"),
  baseFileName: z.string().default("`CriblOut`"),
  fileNameSuffix: z.string().default(
    "`.${C.env[\"CRIBL_WORKER_ID\"]}.${__format}${__compression === \"gzip\" ? \".gz\" : \"\"}`",
  ),
  maxFileSizeMB: z.number().default(32),
  maxFileOpenTimeSec: z.number().default(300),
  maxFileIdleTimeSec: z.number().default(30),
  maxOpenFiles: z.number().default(100),
  headerLine: z.string().default(""),
  writeHighWaterMark: z.number().default(64),
  onBackpressure: PqOnBackpressureOptions$inboundSchema.default("block"),
  onDiskFullBackpressure: PqOnBackpressureOptions$inboundSchema.default(
    "block",
  ),
  description: z.string().optional(),
  compress: PqCompressOptions$inboundSchema.default("none"),
  compressionLevel: CompressionLevelOptions$inboundSchema.default("best_speed"),
  automaticSchema: z.boolean().default(false),
  parquetSchema: z.string().optional(),
  parquetVersion: ParquetVersionOptions$inboundSchema.default("PARQUET_2_6"),
  parquetDataPageVersion: ParquetDataPageVersionOptions$inboundSchema.default(
    "DATA_PAGE_V2",
  ),
  parquetRowGroupLength: z.number().default(10000),
  parquetPageSize: z.string().default("1MB"),
  shouldLogInvalidRows: z.boolean().optional(),
  keyValueMetadata: z.array(TagsType$inboundSchema).optional(),
  enableStatistics: z.boolean().default(true),
  enableWritePageIndex: z.boolean().default(true),
  enablePageChecksum: z.boolean().default(false),
  emptyDirCleanupSec: z.number().default(300),
  deadletterPath: z.string().default("$CRIBL_HOME/state/outputs/dead-letter"),
  maxRetryNum: z.number().default(20),
});
/** @internal */
export type OutputFilesystemFilesystem6$Outbound = {
  deadletterEnabled: boolean;
  id?: string | undefined;
  type: string;
  pipeline?: string | undefined;
  systemFields?: Array<string> | undefined;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  destPath: string;
  stagePath?: string | undefined;
  addIdToStagePath: boolean;
  removeEmptyDirs: boolean;
  partitionExpr: string;
  format: string;
  baseFileName: string;
  fileNameSuffix: string;
  maxFileSizeMB: number;
  maxFileOpenTimeSec: number;
  maxFileIdleTimeSec: number;
  maxOpenFiles: number;
  headerLine: string;
  writeHighWaterMark: number;
  onBackpressure: string;
  onDiskFullBackpressure: string;
  description?: string | undefined;
  compress: string;
  compressionLevel: string;
  automaticSchema: boolean;
  parquetSchema?: string | undefined;
  parquetVersion: string;
  parquetDataPageVersion: string;
  parquetRowGroupLength: number;
  parquetPageSize: string;
  shouldLogInvalidRows?: boolean | undefined;
  keyValueMetadata?: Array<TagsType$Outbound> | undefined;
  enableStatistics: boolean;
  enableWritePageIndex: boolean;
  enablePageChecksum: boolean;
  emptyDirCleanupSec: number;
  deadletterPath: string;
  maxRetryNum: number;
};

/** @internal */
export const OutputFilesystemFilesystem6$outboundSchema: z.ZodType<
  OutputFilesystemFilesystem6$Outbound,
  z.ZodTypeDef,
  OutputFilesystemFilesystem6
> = z.object({
  deadletterEnabled: z.boolean().default(false),
  id: z.string().optional(),
  type: OutputFilesystemType6$outboundSchema,
  pipeline: z.string().optional(),
  systemFields: z.array(z.string()).optional(),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  destPath: z.string(),
  stagePath: z.string().optional(),
  addIdToStagePath: z.boolean().default(true),
  removeEmptyDirs: z.boolean().default(true),
  partitionExpr: z.string().default(
    "C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')",
  ),
  format: Format1Options$outboundSchema.default("json"),
  baseFileName: z.string().default("`CriblOut`"),
  fileNameSuffix: z.string().default(
    "`.${C.env[\"CRIBL_WORKER_ID\"]}.${__format}${__compression === \"gzip\" ? \".gz\" : \"\"}`",
  ),
  maxFileSizeMB: z.number().default(32),
  maxFileOpenTimeSec: z.number().default(300),
  maxFileIdleTimeSec: z.number().default(30),
  maxOpenFiles: z.number().default(100),
  headerLine: z.string().default(""),
  writeHighWaterMark: z.number().default(64),
  onBackpressure: PqOnBackpressureOptions$outboundSchema.default("block"),
  onDiskFullBackpressure: PqOnBackpressureOptions$outboundSchema.default(
    "block",
  ),
  description: z.string().optional(),
  compress: PqCompressOptions$outboundSchema.default("none"),
  compressionLevel: CompressionLevelOptions$outboundSchema.default(
    "best_speed",
  ),
  automaticSchema: z.boolean().default(false),
  parquetSchema: z.string().optional(),
  parquetVersion: ParquetVersionOptions$outboundSchema.default("PARQUET_2_6"),
  parquetDataPageVersion: ParquetDataPageVersionOptions$outboundSchema.default(
    "DATA_PAGE_V2",
  ),
  parquetRowGroupLength: z.number().default(10000),
  parquetPageSize: z.string().default("1MB"),
  shouldLogInvalidRows: z.boolean().optional(),
  keyValueMetadata: z.array(TagsType$outboundSchema).optional(),
  enableStatistics: z.boolean().default(true),
  enableWritePageIndex: z.boolean().default(true),
  enablePageChecksum: z.boolean().default(false),
  emptyDirCleanupSec: z.number().default(300),
  deadletterPath: z.string().default("$CRIBL_HOME/state/outputs/dead-letter"),
  maxRetryNum: z.number().default(20),
});

export function outputFilesystemFilesystem6ToJSON(
  outputFilesystemFilesystem6: OutputFilesystemFilesystem6,
): string {
  return JSON.stringify(
    OutputFilesystemFilesystem6$outboundSchema.parse(
      outputFilesystemFilesystem6,
    ),
  );
}
export function outputFilesystemFilesystem6FromJSON(
  jsonString: string,
): SafeParseResult<OutputFilesystemFilesystem6, SDKValidationError> {
  return safeParse(
    jsonString,
    (x) => OutputFilesystemFilesystem6$inboundSchema.parse(JSON.parse(x)),
    `Failed to parse 'OutputFilesystemFilesystem6' from JSON`,
  );
}

/** @internal */
export const OutputFilesystemType5$inboundSchema: z.ZodNativeEnum<
  typeof OutputFilesystemType5
> = z.nativeEnum(OutputFilesystemType5);
/** @internal */
export const OutputFilesystemType5$outboundSchema: z.ZodNativeEnum<
  typeof OutputFilesystemType5
> = OutputFilesystemType5$inboundSchema;

/** @internal */
export const OutputFilesystemFilesystem5$inboundSchema: z.ZodType<
  OutputFilesystemFilesystem5,
  z.ZodTypeDef,
  unknown
> = z.object({
  deadletterEnabled: z.boolean().default(false),
  id: z.string().optional(),
  type: OutputFilesystemType5$inboundSchema,
  pipeline: z.string().optional(),
  systemFields: z.array(z.string()).optional(),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  destPath: z.string(),
  stagePath: z.string().optional(),
  addIdToStagePath: z.boolean().default(true),
  removeEmptyDirs: z.boolean().default(true),
  partitionExpr: z.string().default(
    "C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')",
  ),
  format: Format1Options$inboundSchema.default("json"),
  baseFileName: z.string().default("`CriblOut`"),
  fileNameSuffix: z.string().default(
    "`.${C.env[\"CRIBL_WORKER_ID\"]}.${__format}${__compression === \"gzip\" ? \".gz\" : \"\"}`",
  ),
  maxFileSizeMB: z.number().default(32),
  maxFileOpenTimeSec: z.number().default(300),
  maxFileIdleTimeSec: z.number().default(30),
  maxOpenFiles: z.number().default(100),
  headerLine: z.string().default(""),
  writeHighWaterMark: z.number().default(64),
  onBackpressure: PqOnBackpressureOptions$inboundSchema.default("block"),
  onDiskFullBackpressure: PqOnBackpressureOptions$inboundSchema.default(
    "block",
  ),
  description: z.string().optional(),
  compress: PqCompressOptions$inboundSchema.default("none"),
  compressionLevel: CompressionLevelOptions$inboundSchema.default("best_speed"),
  automaticSchema: z.boolean().default(false),
  parquetSchema: z.string().optional(),
  parquetVersion: ParquetVersionOptions$inboundSchema.default("PARQUET_2_6"),
  parquetDataPageVersion: ParquetDataPageVersionOptions$inboundSchema.default(
    "DATA_PAGE_V2",
  ),
  parquetRowGroupLength: z.number().default(10000),
  parquetPageSize: z.string().default("1MB"),
  shouldLogInvalidRows: z.boolean().optional(),
  keyValueMetadata: z.array(TagsType$inboundSchema).optional(),
  enableStatistics: z.boolean().default(true),
  enableWritePageIndex: z.boolean().default(true),
  enablePageChecksum: z.boolean().default(false),
  emptyDirCleanupSec: z.number().default(300),
  deadletterPath: z.string().default("$CRIBL_HOME/state/outputs/dead-letter"),
  maxRetryNum: z.number().default(20),
});
/** @internal */
export type OutputFilesystemFilesystem5$Outbound = {
  deadletterEnabled: boolean;
  id?: string | undefined;
  type: string;
  pipeline?: string | undefined;
  systemFields?: Array<string> | undefined;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  destPath: string;
  stagePath?: string | undefined;
  addIdToStagePath: boolean;
  removeEmptyDirs: boolean;
  partitionExpr: string;
  format: string;
  baseFileName: string;
  fileNameSuffix: string;
  maxFileSizeMB: number;
  maxFileOpenTimeSec: number;
  maxFileIdleTimeSec: number;
  maxOpenFiles: number;
  headerLine: string;
  writeHighWaterMark: number;
  onBackpressure: string;
  onDiskFullBackpressure: string;
  description?: string | undefined;
  compress: string;
  compressionLevel: string;
  automaticSchema: boolean;
  parquetSchema?: string | undefined;
  parquetVersion: string;
  parquetDataPageVersion: string;
  parquetRowGroupLength: number;
  parquetPageSize: string;
  shouldLogInvalidRows?: boolean | undefined;
  keyValueMetadata?: Array<TagsType$Outbound> | undefined;
  enableStatistics: boolean;
  enableWritePageIndex: boolean;
  enablePageChecksum: boolean;
  emptyDirCleanupSec: number;
  deadletterPath: string;
  maxRetryNum: number;
};

/** @internal */
export const OutputFilesystemFilesystem5$outboundSchema: z.ZodType<
  OutputFilesystemFilesystem5$Outbound,
  z.ZodTypeDef,
  OutputFilesystemFilesystem5
> = z.object({
  deadletterEnabled: z.boolean().default(false),
  id: z.string().optional(),
  type: OutputFilesystemType5$outboundSchema,
  pipeline: z.string().optional(),
  systemFields: z.array(z.string()).optional(),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  destPath: z.string(),
  stagePath: z.string().optional(),
  addIdToStagePath: z.boolean().default(true),
  removeEmptyDirs: z.boolean().default(true),
  partitionExpr: z.string().default(
    "C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')",
  ),
  format: Format1Options$outboundSchema.default("json"),
  baseFileName: z.string().default("`CriblOut`"),
  fileNameSuffix: z.string().default(
    "`.${C.env[\"CRIBL_WORKER_ID\"]}.${__format}${__compression === \"gzip\" ? \".gz\" : \"\"}`",
  ),
  maxFileSizeMB: z.number().default(32),
  maxFileOpenTimeSec: z.number().default(300),
  maxFileIdleTimeSec: z.number().default(30),
  maxOpenFiles: z.number().default(100),
  headerLine: z.string().default(""),
  writeHighWaterMark: z.number().default(64),
  onBackpressure: PqOnBackpressureOptions$outboundSchema.default("block"),
  onDiskFullBackpressure: PqOnBackpressureOptions$outboundSchema.default(
    "block",
  ),
  description: z.string().optional(),
  compress: PqCompressOptions$outboundSchema.default("none"),
  compressionLevel: CompressionLevelOptions$outboundSchema.default(
    "best_speed",
  ),
  automaticSchema: z.boolean().default(false),
  parquetSchema: z.string().optional(),
  parquetVersion: ParquetVersionOptions$outboundSchema.default("PARQUET_2_6"),
  parquetDataPageVersion: ParquetDataPageVersionOptions$outboundSchema.default(
    "DATA_PAGE_V2",
  ),
  parquetRowGroupLength: z.number().default(10000),
  parquetPageSize: z.string().default("1MB"),
  shouldLogInvalidRows: z.boolean().optional(),
  keyValueMetadata: z.array(TagsType$outboundSchema).optional(),
  enableStatistics: z.boolean().default(true),
  enableWritePageIndex: z.boolean().default(true),
  enablePageChecksum: z.boolean().default(false),
  emptyDirCleanupSec: z.number().default(300),
  deadletterPath: z.string().default("$CRIBL_HOME/state/outputs/dead-letter"),
  maxRetryNum: z.number().default(20),
});

export function outputFilesystemFilesystem5ToJSON(
  outputFilesystemFilesystem5: OutputFilesystemFilesystem5,
): string {
  return JSON.stringify(
    OutputFilesystemFilesystem5$outboundSchema.parse(
      outputFilesystemFilesystem5,
    ),
  );
}
export function outputFilesystemFilesystem5FromJSON(
  jsonString: string,
): SafeParseResult<OutputFilesystemFilesystem5, SDKValidationError> {
  return safeParse(
    jsonString,
    (x) => OutputFilesystemFilesystem5$inboundSchema.parse(JSON.parse(x)),
    `Failed to parse 'OutputFilesystemFilesystem5' from JSON`,
  );
}

/** @internal */
export const OutputFilesystemType4$inboundSchema: z.ZodNativeEnum<
  typeof OutputFilesystemType4
> = z.nativeEnum(OutputFilesystemType4);
/** @internal */
export const OutputFilesystemType4$outboundSchema: z.ZodNativeEnum<
  typeof OutputFilesystemType4
> = OutputFilesystemType4$inboundSchema;

/** @internal */
export const OutputFilesystemFilesystem4$inboundSchema: z.ZodType<
  OutputFilesystemFilesystem4,
  z.ZodTypeDef,
  unknown
> = z.object({
  removeEmptyDirs: z.boolean().default(true),
  id: z.string().optional(),
  type: OutputFilesystemType4$inboundSchema,
  pipeline: z.string().optional(),
  systemFields: z.array(z.string()).optional(),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  destPath: z.string(),
  stagePath: z.string().optional(),
  addIdToStagePath: z.boolean().default(true),
  partitionExpr: z.string().default(
    "C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')",
  ),
  format: Format1Options$inboundSchema.default("json"),
  baseFileName: z.string().default("`CriblOut`"),
  fileNameSuffix: z.string().default(
    "`.${C.env[\"CRIBL_WORKER_ID\"]}.${__format}${__compression === \"gzip\" ? \".gz\" : \"\"}`",
  ),
  maxFileSizeMB: z.number().default(32),
  maxFileOpenTimeSec: z.number().default(300),
  maxFileIdleTimeSec: z.number().default(30),
  maxOpenFiles: z.number().default(100),
  headerLine: z.string().default(""),
  writeHighWaterMark: z.number().default(64),
  onBackpressure: PqOnBackpressureOptions$inboundSchema.default("block"),
  deadletterEnabled: z.boolean().default(false),
  onDiskFullBackpressure: PqOnBackpressureOptions$inboundSchema.default(
    "block",
  ),
  description: z.string().optional(),
  compress: PqCompressOptions$inboundSchema.default("none"),
  compressionLevel: CompressionLevelOptions$inboundSchema.default("best_speed"),
  automaticSchema: z.boolean().default(false),
  parquetSchema: z.string().optional(),
  parquetVersion: ParquetVersionOptions$inboundSchema.default("PARQUET_2_6"),
  parquetDataPageVersion: ParquetDataPageVersionOptions$inboundSchema.default(
    "DATA_PAGE_V2",
  ),
  parquetRowGroupLength: z.number().default(10000),
  parquetPageSize: z.string().default("1MB"),
  shouldLogInvalidRows: z.boolean().optional(),
  keyValueMetadata: z.array(TagsType$inboundSchema).optional(),
  enableStatistics: z.boolean().default(true),
  enableWritePageIndex: z.boolean().default(true),
  enablePageChecksum: z.boolean().default(false),
  emptyDirCleanupSec: z.number().default(300),
  deadletterPath: z.string().default("$CRIBL_HOME/state/outputs/dead-letter"),
  maxRetryNum: z.number().default(20),
});
/** @internal */
export type OutputFilesystemFilesystem4$Outbound = {
  removeEmptyDirs: boolean;
  id?: string | undefined;
  type: string;
  pipeline?: string | undefined;
  systemFields?: Array<string> | undefined;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  destPath: string;
  stagePath?: string | undefined;
  addIdToStagePath: boolean;
  partitionExpr: string;
  format: string;
  baseFileName: string;
  fileNameSuffix: string;
  maxFileSizeMB: number;
  maxFileOpenTimeSec: number;
  maxFileIdleTimeSec: number;
  maxOpenFiles: number;
  headerLine: string;
  writeHighWaterMark: number;
  onBackpressure: string;
  deadletterEnabled: boolean;
  onDiskFullBackpressure: string;
  description?: string | undefined;
  compress: string;
  compressionLevel: string;
  automaticSchema: boolean;
  parquetSchema?: string | undefined;
  parquetVersion: string;
  parquetDataPageVersion: string;
  parquetRowGroupLength: number;
  parquetPageSize: string;
  shouldLogInvalidRows?: boolean | undefined;
  keyValueMetadata?: Array<TagsType$Outbound> | undefined;
  enableStatistics: boolean;
  enableWritePageIndex: boolean;
  enablePageChecksum: boolean;
  emptyDirCleanupSec: number;
  deadletterPath: string;
  maxRetryNum: number;
};

/** @internal */
export const OutputFilesystemFilesystem4$outboundSchema: z.ZodType<
  OutputFilesystemFilesystem4$Outbound,
  z.ZodTypeDef,
  OutputFilesystemFilesystem4
> = z.object({
  removeEmptyDirs: z.boolean().default(true),
  id: z.string().optional(),
  type: OutputFilesystemType4$outboundSchema,
  pipeline: z.string().optional(),
  systemFields: z.array(z.string()).optional(),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  destPath: z.string(),
  stagePath: z.string().optional(),
  addIdToStagePath: z.boolean().default(true),
  partitionExpr: z.string().default(
    "C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')",
  ),
  format: Format1Options$outboundSchema.default("json"),
  baseFileName: z.string().default("`CriblOut`"),
  fileNameSuffix: z.string().default(
    "`.${C.env[\"CRIBL_WORKER_ID\"]}.${__format}${__compression === \"gzip\" ? \".gz\" : \"\"}`",
  ),
  maxFileSizeMB: z.number().default(32),
  maxFileOpenTimeSec: z.number().default(300),
  maxFileIdleTimeSec: z.number().default(30),
  maxOpenFiles: z.number().default(100),
  headerLine: z.string().default(""),
  writeHighWaterMark: z.number().default(64),
  onBackpressure: PqOnBackpressureOptions$outboundSchema.default("block"),
  deadletterEnabled: z.boolean().default(false),
  onDiskFullBackpressure: PqOnBackpressureOptions$outboundSchema.default(
    "block",
  ),
  description: z.string().optional(),
  compress: PqCompressOptions$outboundSchema.default("none"),
  compressionLevel: CompressionLevelOptions$outboundSchema.default(
    "best_speed",
  ),
  automaticSchema: z.boolean().default(false),
  parquetSchema: z.string().optional(),
  parquetVersion: ParquetVersionOptions$outboundSchema.default("PARQUET_2_6"),
  parquetDataPageVersion: ParquetDataPageVersionOptions$outboundSchema.default(
    "DATA_PAGE_V2",
  ),
  parquetRowGroupLength: z.number().default(10000),
  parquetPageSize: z.string().default("1MB"),
  shouldLogInvalidRows: z.boolean().optional(),
  keyValueMetadata: z.array(TagsType$outboundSchema).optional(),
  enableStatistics: z.boolean().default(true),
  enableWritePageIndex: z.boolean().default(true),
  enablePageChecksum: z.boolean().default(false),
  emptyDirCleanupSec: z.number().default(300),
  deadletterPath: z.string().default("$CRIBL_HOME/state/outputs/dead-letter"),
  maxRetryNum: z.number().default(20),
});

export function outputFilesystemFilesystem4ToJSON(
  outputFilesystemFilesystem4: OutputFilesystemFilesystem4,
): string {
  return JSON.stringify(
    OutputFilesystemFilesystem4$outboundSchema.parse(
      outputFilesystemFilesystem4,
    ),
  );
}
export function outputFilesystemFilesystem4FromJSON(
  jsonString: string,
): SafeParseResult<OutputFilesystemFilesystem4, SDKValidationError> {
  return safeParse(
    jsonString,
    (x) => OutputFilesystemFilesystem4$inboundSchema.parse(JSON.parse(x)),
    `Failed to parse 'OutputFilesystemFilesystem4' from JSON`,
  );
}

/** @internal */
export const OutputFilesystemType3$inboundSchema: z.ZodNativeEnum<
  typeof OutputFilesystemType3
> = z.nativeEnum(OutputFilesystemType3);
/** @internal */
export const OutputFilesystemType3$outboundSchema: z.ZodNativeEnum<
  typeof OutputFilesystemType3
> = OutputFilesystemType3$inboundSchema;

/** @internal */
export const OutputFilesystemFilesystem3$inboundSchema: z.ZodType<
  OutputFilesystemFilesystem3,
  z.ZodTypeDef,
  unknown
> = z.object({
  removeEmptyDirs: z.boolean().default(true),
  id: z.string().optional(),
  type: OutputFilesystemType3$inboundSchema,
  pipeline: z.string().optional(),
  systemFields: z.array(z.string()).optional(),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  destPath: z.string(),
  stagePath: z.string().optional(),
  addIdToStagePath: z.boolean().default(true),
  partitionExpr: z.string().default(
    "C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')",
  ),
  format: Format1Options$inboundSchema.default("json"),
  baseFileName: z.string().default("`CriblOut`"),
  fileNameSuffix: z.string().default(
    "`.${C.env[\"CRIBL_WORKER_ID\"]}.${__format}${__compression === \"gzip\" ? \".gz\" : \"\"}`",
  ),
  maxFileSizeMB: z.number().default(32),
  maxFileOpenTimeSec: z.number().default(300),
  maxFileIdleTimeSec: z.number().default(30),
  maxOpenFiles: z.number().default(100),
  headerLine: z.string().default(""),
  writeHighWaterMark: z.number().default(64),
  onBackpressure: PqOnBackpressureOptions$inboundSchema.default("block"),
  deadletterEnabled: z.boolean().default(false),
  onDiskFullBackpressure: PqOnBackpressureOptions$inboundSchema.default(
    "block",
  ),
  description: z.string().optional(),
  compress: PqCompressOptions$inboundSchema.default("none"),
  compressionLevel: CompressionLevelOptions$inboundSchema.default("best_speed"),
  automaticSchema: z.boolean().default(false),
  parquetSchema: z.string().optional(),
  parquetVersion: ParquetVersionOptions$inboundSchema.default("PARQUET_2_6"),
  parquetDataPageVersion: ParquetDataPageVersionOptions$inboundSchema.default(
    "DATA_PAGE_V2",
  ),
  parquetRowGroupLength: z.number().default(10000),
  parquetPageSize: z.string().default("1MB"),
  shouldLogInvalidRows: z.boolean().optional(),
  keyValueMetadata: z.array(TagsType$inboundSchema).optional(),
  enableStatistics: z.boolean().default(true),
  enableWritePageIndex: z.boolean().default(true),
  enablePageChecksum: z.boolean().default(false),
  emptyDirCleanupSec: z.number().default(300),
  deadletterPath: z.string().default("$CRIBL_HOME/state/outputs/dead-letter"),
  maxRetryNum: z.number().default(20),
});
/** @internal */
export type OutputFilesystemFilesystem3$Outbound = {
  removeEmptyDirs: boolean;
  id?: string | undefined;
  type: string;
  pipeline?: string | undefined;
  systemFields?: Array<string> | undefined;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  destPath: string;
  stagePath?: string | undefined;
  addIdToStagePath: boolean;
  partitionExpr: string;
  format: string;
  baseFileName: string;
  fileNameSuffix: string;
  maxFileSizeMB: number;
  maxFileOpenTimeSec: number;
  maxFileIdleTimeSec: number;
  maxOpenFiles: number;
  headerLine: string;
  writeHighWaterMark: number;
  onBackpressure: string;
  deadletterEnabled: boolean;
  onDiskFullBackpressure: string;
  description?: string | undefined;
  compress: string;
  compressionLevel: string;
  automaticSchema: boolean;
  parquetSchema?: string | undefined;
  parquetVersion: string;
  parquetDataPageVersion: string;
  parquetRowGroupLength: number;
  parquetPageSize: string;
  shouldLogInvalidRows?: boolean | undefined;
  keyValueMetadata?: Array<TagsType$Outbound> | undefined;
  enableStatistics: boolean;
  enableWritePageIndex: boolean;
  enablePageChecksum: boolean;
  emptyDirCleanupSec: number;
  deadletterPath: string;
  maxRetryNum: number;
};

/** @internal */
export const OutputFilesystemFilesystem3$outboundSchema: z.ZodType<
  OutputFilesystemFilesystem3$Outbound,
  z.ZodTypeDef,
  OutputFilesystemFilesystem3
> = z.object({
  removeEmptyDirs: z.boolean().default(true),
  id: z.string().optional(),
  type: OutputFilesystemType3$outboundSchema,
  pipeline: z.string().optional(),
  systemFields: z.array(z.string()).optional(),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  destPath: z.string(),
  stagePath: z.string().optional(),
  addIdToStagePath: z.boolean().default(true),
  partitionExpr: z.string().default(
    "C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')",
  ),
  format: Format1Options$outboundSchema.default("json"),
  baseFileName: z.string().default("`CriblOut`"),
  fileNameSuffix: z.string().default(
    "`.${C.env[\"CRIBL_WORKER_ID\"]}.${__format}${__compression === \"gzip\" ? \".gz\" : \"\"}`",
  ),
  maxFileSizeMB: z.number().default(32),
  maxFileOpenTimeSec: z.number().default(300),
  maxFileIdleTimeSec: z.number().default(30),
  maxOpenFiles: z.number().default(100),
  headerLine: z.string().default(""),
  writeHighWaterMark: z.number().default(64),
  onBackpressure: PqOnBackpressureOptions$outboundSchema.default("block"),
  deadletterEnabled: z.boolean().default(false),
  onDiskFullBackpressure: PqOnBackpressureOptions$outboundSchema.default(
    "block",
  ),
  description: z.string().optional(),
  compress: PqCompressOptions$outboundSchema.default("none"),
  compressionLevel: CompressionLevelOptions$outboundSchema.default(
    "best_speed",
  ),
  automaticSchema: z.boolean().default(false),
  parquetSchema: z.string().optional(),
  parquetVersion: ParquetVersionOptions$outboundSchema.default("PARQUET_2_6"),
  parquetDataPageVersion: ParquetDataPageVersionOptions$outboundSchema.default(
    "DATA_PAGE_V2",
  ),
  parquetRowGroupLength: z.number().default(10000),
  parquetPageSize: z.string().default("1MB"),
  shouldLogInvalidRows: z.boolean().optional(),
  keyValueMetadata: z.array(TagsType$outboundSchema).optional(),
  enableStatistics: z.boolean().default(true),
  enableWritePageIndex: z.boolean().default(true),
  enablePageChecksum: z.boolean().default(false),
  emptyDirCleanupSec: z.number().default(300),
  deadletterPath: z.string().default("$CRIBL_HOME/state/outputs/dead-letter"),
  maxRetryNum: z.number().default(20),
});

export function outputFilesystemFilesystem3ToJSON(
  outputFilesystemFilesystem3: OutputFilesystemFilesystem3,
): string {
  return JSON.stringify(
    OutputFilesystemFilesystem3$outboundSchema.parse(
      outputFilesystemFilesystem3,
    ),
  );
}
export function outputFilesystemFilesystem3FromJSON(
  jsonString: string,
): SafeParseResult<OutputFilesystemFilesystem3, SDKValidationError> {
  return safeParse(
    jsonString,
    (x) => OutputFilesystemFilesystem3$inboundSchema.parse(JSON.parse(x)),
    `Failed to parse 'OutputFilesystemFilesystem3' from JSON`,
  );
}

/** @internal */
export const OutputFilesystemType2$inboundSchema: z.ZodNativeEnum<
  typeof OutputFilesystemType2
> = z.nativeEnum(OutputFilesystemType2);
/** @internal */
export const OutputFilesystemType2$outboundSchema: z.ZodNativeEnum<
  typeof OutputFilesystemType2
> = OutputFilesystemType2$inboundSchema;

/** @internal */
export const OutputFilesystemFilesystem2$inboundSchema: z.ZodType<
  OutputFilesystemFilesystem2,
  z.ZodTypeDef,
  unknown
> = z.object({
  format: Format1Options$inboundSchema.default("json"),
  id: z.string().optional(),
  type: OutputFilesystemType2$inboundSchema,
  pipeline: z.string().optional(),
  systemFields: z.array(z.string()).optional(),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  destPath: z.string(),
  stagePath: z.string().optional(),
  addIdToStagePath: z.boolean().default(true),
  removeEmptyDirs: z.boolean().default(true),
  partitionExpr: z.string().default(
    "C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')",
  ),
  baseFileName: z.string().default("`CriblOut`"),
  fileNameSuffix: z.string().default(
    "`.${C.env[\"CRIBL_WORKER_ID\"]}.${__format}${__compression === \"gzip\" ? \".gz\" : \"\"}`",
  ),
  maxFileSizeMB: z.number().default(32),
  maxFileOpenTimeSec: z.number().default(300),
  maxFileIdleTimeSec: z.number().default(30),
  maxOpenFiles: z.number().default(100),
  headerLine: z.string().default(""),
  writeHighWaterMark: z.number().default(64),
  onBackpressure: PqOnBackpressureOptions$inboundSchema.default("block"),
  deadletterEnabled: z.boolean().default(false),
  onDiskFullBackpressure: PqOnBackpressureOptions$inboundSchema.default(
    "block",
  ),
  description: z.string().optional(),
  compress: PqCompressOptions$inboundSchema.default("none"),
  compressionLevel: CompressionLevelOptions$inboundSchema.default("best_speed"),
  automaticSchema: z.boolean().default(false),
  parquetSchema: z.string().optional(),
  parquetVersion: ParquetVersionOptions$inboundSchema.default("PARQUET_2_6"),
  parquetDataPageVersion: ParquetDataPageVersionOptions$inboundSchema.default(
    "DATA_PAGE_V2",
  ),
  parquetRowGroupLength: z.number().default(10000),
  parquetPageSize: z.string().default("1MB"),
  shouldLogInvalidRows: z.boolean(),
  keyValueMetadata: z.array(TagsType$inboundSchema),
  enableStatistics: z.boolean().default(true),
  enableWritePageIndex: z.boolean().default(true),
  enablePageChecksum: z.boolean().default(false),
  emptyDirCleanupSec: z.number().default(300),
  deadletterPath: z.string().default("$CRIBL_HOME/state/outputs/dead-letter"),
  maxRetryNum: z.number().default(20),
});
/** @internal */
export type OutputFilesystemFilesystem2$Outbound = {
  format: string;
  id?: string | undefined;
  type: string;
  pipeline?: string | undefined;
  systemFields?: Array<string> | undefined;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  destPath: string;
  stagePath?: string | undefined;
  addIdToStagePath: boolean;
  removeEmptyDirs: boolean;
  partitionExpr: string;
  baseFileName: string;
  fileNameSuffix: string;
  maxFileSizeMB: number;
  maxFileOpenTimeSec: number;
  maxFileIdleTimeSec: number;
  maxOpenFiles: number;
  headerLine: string;
  writeHighWaterMark: number;
  onBackpressure: string;
  deadletterEnabled: boolean;
  onDiskFullBackpressure: string;
  description?: string | undefined;
  compress: string;
  compressionLevel: string;
  automaticSchema: boolean;
  parquetSchema?: string | undefined;
  parquetVersion: string;
  parquetDataPageVersion: string;
  parquetRowGroupLength: number;
  parquetPageSize: string;
  shouldLogInvalidRows: boolean;
  keyValueMetadata: Array<TagsType$Outbound>;
  enableStatistics: boolean;
  enableWritePageIndex: boolean;
  enablePageChecksum: boolean;
  emptyDirCleanupSec: number;
  deadletterPath: string;
  maxRetryNum: number;
};

/** @internal */
export const OutputFilesystemFilesystem2$outboundSchema: z.ZodType<
  OutputFilesystemFilesystem2$Outbound,
  z.ZodTypeDef,
  OutputFilesystemFilesystem2
> = z.object({
  format: Format1Options$outboundSchema.default("json"),
  id: z.string().optional(),
  type: OutputFilesystemType2$outboundSchema,
  pipeline: z.string().optional(),
  systemFields: z.array(z.string()).optional(),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  destPath: z.string(),
  stagePath: z.string().optional(),
  addIdToStagePath: z.boolean().default(true),
  removeEmptyDirs: z.boolean().default(true),
  partitionExpr: z.string().default(
    "C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')",
  ),
  baseFileName: z.string().default("`CriblOut`"),
  fileNameSuffix: z.string().default(
    "`.${C.env[\"CRIBL_WORKER_ID\"]}.${__format}${__compression === \"gzip\" ? \".gz\" : \"\"}`",
  ),
  maxFileSizeMB: z.number().default(32),
  maxFileOpenTimeSec: z.number().default(300),
  maxFileIdleTimeSec: z.number().default(30),
  maxOpenFiles: z.number().default(100),
  headerLine: z.string().default(""),
  writeHighWaterMark: z.number().default(64),
  onBackpressure: PqOnBackpressureOptions$outboundSchema.default("block"),
  deadletterEnabled: z.boolean().default(false),
  onDiskFullBackpressure: PqOnBackpressureOptions$outboundSchema.default(
    "block",
  ),
  description: z.string().optional(),
  compress: PqCompressOptions$outboundSchema.default("none"),
  compressionLevel: CompressionLevelOptions$outboundSchema.default(
    "best_speed",
  ),
  automaticSchema: z.boolean().default(false),
  parquetSchema: z.string().optional(),
  parquetVersion: ParquetVersionOptions$outboundSchema.default("PARQUET_2_6"),
  parquetDataPageVersion: ParquetDataPageVersionOptions$outboundSchema.default(
    "DATA_PAGE_V2",
  ),
  parquetRowGroupLength: z.number().default(10000),
  parquetPageSize: z.string().default("1MB"),
  shouldLogInvalidRows: z.boolean(),
  keyValueMetadata: z.array(TagsType$outboundSchema),
  enableStatistics: z.boolean().default(true),
  enableWritePageIndex: z.boolean().default(true),
  enablePageChecksum: z.boolean().default(false),
  emptyDirCleanupSec: z.number().default(300),
  deadletterPath: z.string().default("$CRIBL_HOME/state/outputs/dead-letter"),
  maxRetryNum: z.number().default(20),
});

export function outputFilesystemFilesystem2ToJSON(
  outputFilesystemFilesystem2: OutputFilesystemFilesystem2,
): string {
  return JSON.stringify(
    OutputFilesystemFilesystem2$outboundSchema.parse(
      outputFilesystemFilesystem2,
    ),
  );
}
export function outputFilesystemFilesystem2FromJSON(
  jsonString: string,
): SafeParseResult<OutputFilesystemFilesystem2, SDKValidationError> {
  return safeParse(
    jsonString,
    (x) => OutputFilesystemFilesystem2$inboundSchema.parse(JSON.parse(x)),
    `Failed to parse 'OutputFilesystemFilesystem2' from JSON`,
  );
}

/** @internal */
export const OutputFilesystemType1$inboundSchema: z.ZodNativeEnum<
  typeof OutputFilesystemType1
> = z.nativeEnum(OutputFilesystemType1);
/** @internal */
export const OutputFilesystemType1$outboundSchema: z.ZodNativeEnum<
  typeof OutputFilesystemType1
> = OutputFilesystemType1$inboundSchema;

/** @internal */
export const OutputFilesystemFilesystem1$inboundSchema: z.ZodType<
  OutputFilesystemFilesystem1,
  z.ZodTypeDef,
  unknown
> = z.object({
  format: Format1Options$inboundSchema.default("json"),
  id: z.string().optional(),
  type: OutputFilesystemType1$inboundSchema,
  pipeline: z.string().optional(),
  systemFields: z.array(z.string()).optional(),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  destPath: z.string(),
  stagePath: z.string().optional(),
  addIdToStagePath: z.boolean().default(true),
  removeEmptyDirs: z.boolean().default(true),
  partitionExpr: z.string().default(
    "C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')",
  ),
  baseFileName: z.string().default("`CriblOut`"),
  fileNameSuffix: z.string().default(
    "`.${C.env[\"CRIBL_WORKER_ID\"]}.${__format}${__compression === \"gzip\" ? \".gz\" : \"\"}`",
  ),
  maxFileSizeMB: z.number().default(32),
  maxFileOpenTimeSec: z.number().default(300),
  maxFileIdleTimeSec: z.number().default(30),
  maxOpenFiles: z.number().default(100),
  headerLine: z.string().default(""),
  writeHighWaterMark: z.number().default(64),
  onBackpressure: PqOnBackpressureOptions$inboundSchema.default("block"),
  deadletterEnabled: z.boolean().default(false),
  onDiskFullBackpressure: PqOnBackpressureOptions$inboundSchema.default(
    "block",
  ),
  description: z.string().optional(),
  compress: PqCompressOptions$inboundSchema.default("none"),
  compressionLevel: CompressionLevelOptions$inboundSchema.default("best_speed"),
  automaticSchema: z.boolean().default(false),
  parquetSchema: z.string().optional(),
  parquetVersion: ParquetVersionOptions$inboundSchema.default("PARQUET_2_6"),
  parquetDataPageVersion: ParquetDataPageVersionOptions$inboundSchema.default(
    "DATA_PAGE_V2",
  ),
  parquetRowGroupLength: z.number().default(10000),
  parquetPageSize: z.string().default("1MB"),
  shouldLogInvalidRows: z.boolean().optional(),
  keyValueMetadata: z.array(TagsType$inboundSchema).optional(),
  enableStatistics: z.boolean().default(true),
  enableWritePageIndex: z.boolean().default(true),
  enablePageChecksum: z.boolean().default(false),
  emptyDirCleanupSec: z.number().default(300),
  deadletterPath: z.string().default("$CRIBL_HOME/state/outputs/dead-letter"),
  maxRetryNum: z.number().default(20),
});
/** @internal */
export type OutputFilesystemFilesystem1$Outbound = {
  format: string;
  id?: string | undefined;
  type: string;
  pipeline?: string | undefined;
  systemFields?: Array<string> | undefined;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  destPath: string;
  stagePath?: string | undefined;
  addIdToStagePath: boolean;
  removeEmptyDirs: boolean;
  partitionExpr: string;
  baseFileName: string;
  fileNameSuffix: string;
  maxFileSizeMB: number;
  maxFileOpenTimeSec: number;
  maxFileIdleTimeSec: number;
  maxOpenFiles: number;
  headerLine: string;
  writeHighWaterMark: number;
  onBackpressure: string;
  deadletterEnabled: boolean;
  onDiskFullBackpressure: string;
  description?: string | undefined;
  compress: string;
  compressionLevel: string;
  automaticSchema: boolean;
  parquetSchema?: string | undefined;
  parquetVersion: string;
  parquetDataPageVersion: string;
  parquetRowGroupLength: number;
  parquetPageSize: string;
  shouldLogInvalidRows?: boolean | undefined;
  keyValueMetadata?: Array<TagsType$Outbound> | undefined;
  enableStatistics: boolean;
  enableWritePageIndex: boolean;
  enablePageChecksum: boolean;
  emptyDirCleanupSec: number;
  deadletterPath: string;
  maxRetryNum: number;
};

/** @internal */
export const OutputFilesystemFilesystem1$outboundSchema: z.ZodType<
  OutputFilesystemFilesystem1$Outbound,
  z.ZodTypeDef,
  OutputFilesystemFilesystem1
> = z.object({
  format: Format1Options$outboundSchema.default("json"),
  id: z.string().optional(),
  type: OutputFilesystemType1$outboundSchema,
  pipeline: z.string().optional(),
  systemFields: z.array(z.string()).optional(),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  destPath: z.string(),
  stagePath: z.string().optional(),
  addIdToStagePath: z.boolean().default(true),
  removeEmptyDirs: z.boolean().default(true),
  partitionExpr: z.string().default(
    "C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')",
  ),
  baseFileName: z.string().default("`CriblOut`"),
  fileNameSuffix: z.string().default(
    "`.${C.env[\"CRIBL_WORKER_ID\"]}.${__format}${__compression === \"gzip\" ? \".gz\" : \"\"}`",
  ),
  maxFileSizeMB: z.number().default(32),
  maxFileOpenTimeSec: z.number().default(300),
  maxFileIdleTimeSec: z.number().default(30),
  maxOpenFiles: z.number().default(100),
  headerLine: z.string().default(""),
  writeHighWaterMark: z.number().default(64),
  onBackpressure: PqOnBackpressureOptions$outboundSchema.default("block"),
  deadletterEnabled: z.boolean().default(false),
  onDiskFullBackpressure: PqOnBackpressureOptions$outboundSchema.default(
    "block",
  ),
  description: z.string().optional(),
  compress: PqCompressOptions$outboundSchema.default("none"),
  compressionLevel: CompressionLevelOptions$outboundSchema.default(
    "best_speed",
  ),
  automaticSchema: z.boolean().default(false),
  parquetSchema: z.string().optional(),
  parquetVersion: ParquetVersionOptions$outboundSchema.default("PARQUET_2_6"),
  parquetDataPageVersion: ParquetDataPageVersionOptions$outboundSchema.default(
    "DATA_PAGE_V2",
  ),
  parquetRowGroupLength: z.number().default(10000),
  parquetPageSize: z.string().default("1MB"),
  shouldLogInvalidRows: z.boolean().optional(),
  keyValueMetadata: z.array(TagsType$outboundSchema).optional(),
  enableStatistics: z.boolean().default(true),
  enableWritePageIndex: z.boolean().default(true),
  enablePageChecksum: z.boolean().default(false),
  emptyDirCleanupSec: z.number().default(300),
  deadletterPath: z.string().default("$CRIBL_HOME/state/outputs/dead-letter"),
  maxRetryNum: z.number().default(20),
});

export function outputFilesystemFilesystem1ToJSON(
  outputFilesystemFilesystem1: OutputFilesystemFilesystem1,
): string {
  return JSON.stringify(
    OutputFilesystemFilesystem1$outboundSchema.parse(
      outputFilesystemFilesystem1,
    ),
  );
}
export function outputFilesystemFilesystem1FromJSON(
  jsonString: string,
): SafeParseResult<OutputFilesystemFilesystem1, SDKValidationError> {
  return safeParse(
    jsonString,
    (x) => OutputFilesystemFilesystem1$inboundSchema.parse(JSON.parse(x)),
    `Failed to parse 'OutputFilesystemFilesystem1' from JSON`,
  );
}

/** @internal */
export const OutputFilesystem$inboundSchema: z.ZodType<
  OutputFilesystem,
  z.ZodTypeDef,
  unknown
> = z.union([
  z.lazy(() => OutputFilesystemFilesystem2$inboundSchema),
  z.lazy(() => OutputFilesystemFilesystem1$inboundSchema),
  z.lazy(() => OutputFilesystemFilesystem3$inboundSchema),
  z.lazy(() => OutputFilesystemFilesystem4$inboundSchema),
  z.lazy(() => OutputFilesystemFilesystem5$inboundSchema),
  z.lazy(() => OutputFilesystemFilesystem6$inboundSchema),
]);
/** @internal */
export type OutputFilesystem$Outbound =
  | OutputFilesystemFilesystem2$Outbound
  | OutputFilesystemFilesystem1$Outbound
  | OutputFilesystemFilesystem3$Outbound
  | OutputFilesystemFilesystem4$Outbound
  | OutputFilesystemFilesystem5$Outbound
  | OutputFilesystemFilesystem6$Outbound;

/** @internal */
export const OutputFilesystem$outboundSchema: z.ZodType<
  OutputFilesystem$Outbound,
  z.ZodTypeDef,
  OutputFilesystem
> = z.union([
  z.lazy(() => OutputFilesystemFilesystem2$outboundSchema),
  z.lazy(() => OutputFilesystemFilesystem1$outboundSchema),
  z.lazy(() => OutputFilesystemFilesystem3$outboundSchema),
  z.lazy(() => OutputFilesystemFilesystem4$outboundSchema),
  z.lazy(() => OutputFilesystemFilesystem5$outboundSchema),
  z.lazy(() => OutputFilesystemFilesystem6$outboundSchema),
]);

export function outputFilesystemToJSON(
  outputFilesystem: OutputFilesystem,
): string {
  return JSON.stringify(
    OutputFilesystem$outboundSchema.parse(outputFilesystem),
  );
}
export function outputFilesystemFromJSON(
  jsonString: string,
): SafeParseResult<OutputFilesystem, SDKValidationError> {
  return safeParse(
    jsonString,
    (x) => OutputFilesystem$inboundSchema.parse(JSON.parse(x)),
    `Failed to parse 'OutputFilesystem' from JSON`,
  );
}
