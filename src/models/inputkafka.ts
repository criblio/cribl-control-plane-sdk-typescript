/*
 * Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT.
 */

import * as z from "zod/v3";
import { safeParse } from "../lib/schemas.js";
import { Result as SafeParseResult } from "../types/fp.js";
import {
  ConnectionsType,
  ConnectionsType$inboundSchema,
  ConnectionsType$Outbound,
  ConnectionsType$outboundSchema,
} from "./connectionstype.js";
import { SDKValidationError } from "./errors/sdkvalidationerror.js";
import {
  KafkaSchemaRegistryType,
  KafkaSchemaRegistryType$inboundSchema,
  KafkaSchemaRegistryType$Outbound,
  KafkaSchemaRegistryType$outboundSchema,
} from "./kafkaschemaregistrytype.js";
import {
  Metadata1Type,
  Metadata1Type$inboundSchema,
  Metadata1Type$Outbound,
  Metadata1Type$outboundSchema,
} from "./metadata1type.js";
import {
  PqType,
  PqType$inboundSchema,
  PqType$Outbound,
  PqType$outboundSchema,
} from "./pqtype.js";
import {
  SaslType,
  SaslType$inboundSchema,
  SaslType$Outbound,
  SaslType$outboundSchema,
} from "./sasltype.js";
import {
  Tls1Type,
  Tls1Type$inboundSchema,
  Tls1Type$Outbound,
  Tls1Type$outboundSchema,
} from "./tls1type.js";
import {
  TypeKafkaOption,
  TypeKafkaOption$inboundSchema,
  TypeKafkaOption$outboundSchema,
} from "./typekafkaoption.js";

export type InputKafkaKafka4 = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Unique ID for this input
   */
  id?: string | undefined;
  type: TypeKafkaOption;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<ConnectionsType> | undefined;
  pq: PqType;
  /**
   * Enter each Kafka bootstrap server you want to use. Specify the hostname and port (such as mykafkabroker:9092) or just the hostname (in which case @{product} will assign port 9092).
   */
  brokers: Array<string>;
  /**
   * Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only.
   */
  topics: Array<string>;
  /**
   * The consumer group to which this instance belongs. Defaults to 'Cribl'.
   */
  groupId?: string | undefined;
  /**
   * Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message
   */
  fromBeginning?: boolean | undefined;
  kafkaSchemaRegistry?: KafkaSchemaRegistryType | undefined;
  /**
   * Maximum time to wait for a connection to complete successfully
   */
  connectionTimeout?: number | undefined;
  /**
   * Maximum time to wait for Kafka to respond to a request
   */
  requestTimeout?: number | undefined;
  /**
   * If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data
   */
  maxRetries?: number | undefined;
  /**
   * The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).
   */
  maxBackOff?: number | undefined;
  /**
   * Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).
   */
  initialBackoff?: number | undefined;
  /**
   * Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.
   */
  backoffRate?: number | undefined;
  /**
   * Maximum time to wait for Kafka to respond to an authentication request
   */
  authenticationTimeout?: number | undefined;
  /**
   * Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.
   */
  reauthenticationThreshold?: number | undefined;
  /**
   * Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.
   */
  sasl?: SaslType | undefined;
  tls?: Tls1Type | undefined;
  /**
   *       Timeout used to detect client failures when using Kafka's group-management facilities.
   *
   * @remarks
   *       If the client sends no heartbeats to the broker before the timeout expires,
   *       the broker will remove the client from the group and initiate a rebalance.
   *       Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.
   */
  sessionTimeout?: number | undefined;
  /**
   *       Maximum allowed time for each worker to join the group after a rebalance begins.
   *
   * @remarks
   *       If the timeout is exceeded, the coordinator broker will remove the worker from the group.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.
   */
  rebalanceTimeout?: number | undefined;
  /**
   *       Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.
   *
   * @remarks
   *       Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.
   */
  heartbeatInterval?: number | undefined;
  /**
   * How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.
   */
  autoCommitInterval?: number | undefined;
  /**
   * How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.
   */
  autoCommitThreshold?: number | undefined;
  /**
   * Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).
   */
  maxBytesPerPartition?: number | undefined;
  /**
   * Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).
   */
  maxBytes?: number | undefined;
  /**
   * Maximum number of network errors before the consumer re-creates a socket
   */
  maxSocketErrors?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<Metadata1Type> | undefined;
  description?: string | undefined;
};

export type InputKafkaKafka3 = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Unique ID for this input
   */
  id?: string | undefined;
  type: TypeKafkaOption;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<ConnectionsType> | undefined;
  pq?: PqType | undefined;
  /**
   * Enter each Kafka bootstrap server you want to use. Specify the hostname and port (such as mykafkabroker:9092) or just the hostname (in which case @{product} will assign port 9092).
   */
  brokers: Array<string>;
  /**
   * Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only.
   */
  topics: Array<string>;
  /**
   * The consumer group to which this instance belongs. Defaults to 'Cribl'.
   */
  groupId?: string | undefined;
  /**
   * Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message
   */
  fromBeginning?: boolean | undefined;
  kafkaSchemaRegistry?: KafkaSchemaRegistryType | undefined;
  /**
   * Maximum time to wait for a connection to complete successfully
   */
  connectionTimeout?: number | undefined;
  /**
   * Maximum time to wait for Kafka to respond to a request
   */
  requestTimeout?: number | undefined;
  /**
   * If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data
   */
  maxRetries?: number | undefined;
  /**
   * The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).
   */
  maxBackOff?: number | undefined;
  /**
   * Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).
   */
  initialBackoff?: number | undefined;
  /**
   * Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.
   */
  backoffRate?: number | undefined;
  /**
   * Maximum time to wait for Kafka to respond to an authentication request
   */
  authenticationTimeout?: number | undefined;
  /**
   * Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.
   */
  reauthenticationThreshold?: number | undefined;
  /**
   * Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.
   */
  sasl?: SaslType | undefined;
  tls?: Tls1Type | undefined;
  /**
   *       Timeout used to detect client failures when using Kafka's group-management facilities.
   *
   * @remarks
   *       If the client sends no heartbeats to the broker before the timeout expires,
   *       the broker will remove the client from the group and initiate a rebalance.
   *       Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.
   */
  sessionTimeout?: number | undefined;
  /**
   *       Maximum allowed time for each worker to join the group after a rebalance begins.
   *
   * @remarks
   *       If the timeout is exceeded, the coordinator broker will remove the worker from the group.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.
   */
  rebalanceTimeout?: number | undefined;
  /**
   *       Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.
   *
   * @remarks
   *       Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.
   */
  heartbeatInterval?: number | undefined;
  /**
   * How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.
   */
  autoCommitInterval?: number | undefined;
  /**
   * How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.
   */
  autoCommitThreshold?: number | undefined;
  /**
   * Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).
   */
  maxBytesPerPartition?: number | undefined;
  /**
   * Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).
   */
  maxBytes?: number | undefined;
  /**
   * Maximum number of network errors before the consumer re-creates a socket
   */
  maxSocketErrors?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<Metadata1Type> | undefined;
  description?: string | undefined;
};

export type InputKafkaKafka2 = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Unique ID for this input
   */
  id?: string | undefined;
  type: TypeKafkaOption;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections: Array<ConnectionsType>;
  pq?: PqType | undefined;
  /**
   * Enter each Kafka bootstrap server you want to use. Specify the hostname and port (such as mykafkabroker:9092) or just the hostname (in which case @{product} will assign port 9092).
   */
  brokers: Array<string>;
  /**
   * Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only.
   */
  topics: Array<string>;
  /**
   * The consumer group to which this instance belongs. Defaults to 'Cribl'.
   */
  groupId?: string | undefined;
  /**
   * Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message
   */
  fromBeginning?: boolean | undefined;
  kafkaSchemaRegistry?: KafkaSchemaRegistryType | undefined;
  /**
   * Maximum time to wait for a connection to complete successfully
   */
  connectionTimeout?: number | undefined;
  /**
   * Maximum time to wait for Kafka to respond to a request
   */
  requestTimeout?: number | undefined;
  /**
   * If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data
   */
  maxRetries?: number | undefined;
  /**
   * The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).
   */
  maxBackOff?: number | undefined;
  /**
   * Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).
   */
  initialBackoff?: number | undefined;
  /**
   * Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.
   */
  backoffRate?: number | undefined;
  /**
   * Maximum time to wait for Kafka to respond to an authentication request
   */
  authenticationTimeout?: number | undefined;
  /**
   * Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.
   */
  reauthenticationThreshold?: number | undefined;
  /**
   * Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.
   */
  sasl?: SaslType | undefined;
  tls?: Tls1Type | undefined;
  /**
   *       Timeout used to detect client failures when using Kafka's group-management facilities.
   *
   * @remarks
   *       If the client sends no heartbeats to the broker before the timeout expires,
   *       the broker will remove the client from the group and initiate a rebalance.
   *       Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.
   */
  sessionTimeout?: number | undefined;
  /**
   *       Maximum allowed time for each worker to join the group after a rebalance begins.
   *
   * @remarks
   *       If the timeout is exceeded, the coordinator broker will remove the worker from the group.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.
   */
  rebalanceTimeout?: number | undefined;
  /**
   *       Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.
   *
   * @remarks
   *       Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.
   */
  heartbeatInterval?: number | undefined;
  /**
   * How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.
   */
  autoCommitInterval?: number | undefined;
  /**
   * How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.
   */
  autoCommitThreshold?: number | undefined;
  /**
   * Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).
   */
  maxBytesPerPartition?: number | undefined;
  /**
   * Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).
   */
  maxBytes?: number | undefined;
  /**
   * Maximum number of network errors before the consumer re-creates a socket
   */
  maxSocketErrors?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<Metadata1Type> | undefined;
  description?: string | undefined;
};

export type InputKafkaKafka1 = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Unique ID for this input
   */
  id?: string | undefined;
  type: TypeKafkaOption;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<ConnectionsType> | undefined;
  pq?: PqType | undefined;
  /**
   * Enter each Kafka bootstrap server you want to use. Specify the hostname and port (such as mykafkabroker:9092) or just the hostname (in which case @{product} will assign port 9092).
   */
  brokers: Array<string>;
  /**
   * Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only.
   */
  topics: Array<string>;
  /**
   * The consumer group to which this instance belongs. Defaults to 'Cribl'.
   */
  groupId?: string | undefined;
  /**
   * Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message
   */
  fromBeginning?: boolean | undefined;
  kafkaSchemaRegistry?: KafkaSchemaRegistryType | undefined;
  /**
   * Maximum time to wait for a connection to complete successfully
   */
  connectionTimeout?: number | undefined;
  /**
   * Maximum time to wait for Kafka to respond to a request
   */
  requestTimeout?: number | undefined;
  /**
   * If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data
   */
  maxRetries?: number | undefined;
  /**
   * The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).
   */
  maxBackOff?: number | undefined;
  /**
   * Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).
   */
  initialBackoff?: number | undefined;
  /**
   * Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.
   */
  backoffRate?: number | undefined;
  /**
   * Maximum time to wait for Kafka to respond to an authentication request
   */
  authenticationTimeout?: number | undefined;
  /**
   * Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.
   */
  reauthenticationThreshold?: number | undefined;
  /**
   * Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.
   */
  sasl?: SaslType | undefined;
  tls?: Tls1Type | undefined;
  /**
   *       Timeout used to detect client failures when using Kafka's group-management facilities.
   *
   * @remarks
   *       If the client sends no heartbeats to the broker before the timeout expires,
   *       the broker will remove the client from the group and initiate a rebalance.
   *       Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.
   */
  sessionTimeout?: number | undefined;
  /**
   *       Maximum allowed time for each worker to join the group after a rebalance begins.
   *
   * @remarks
   *       If the timeout is exceeded, the coordinator broker will remove the worker from the group.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.
   */
  rebalanceTimeout?: number | undefined;
  /**
   *       Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.
   *
   * @remarks
   *       Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.
   */
  heartbeatInterval?: number | undefined;
  /**
   * How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.
   */
  autoCommitInterval?: number | undefined;
  /**
   * How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.
   */
  autoCommitThreshold?: number | undefined;
  /**
   * Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).
   */
  maxBytesPerPartition?: number | undefined;
  /**
   * Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).
   */
  maxBytes?: number | undefined;
  /**
   * Maximum number of network errors before the consumer re-creates a socket
   */
  maxSocketErrors?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<Metadata1Type> | undefined;
  description?: string | undefined;
};

export type InputKafka =
  | InputKafkaKafka2
  | InputKafkaKafka4
  | InputKafkaKafka1
  | InputKafkaKafka3;

/** @internal */
export const InputKafkaKafka4$inboundSchema: z.ZodType<
  InputKafkaKafka4,
  z.ZodTypeDef,
  unknown
> = z.object({
  pqEnabled: z.boolean().default(false),
  id: z.string().optional(),
  type: TypeKafkaOption$inboundSchema,
  disabled: z.boolean().default(false),
  pipeline: z.string().optional(),
  sendToRoutes: z.boolean().default(true),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  connections: z.array(ConnectionsType$inboundSchema).optional(),
  pq: PqType$inboundSchema,
  brokers: z.array(z.string()),
  topics: z.array(z.string()),
  groupId: z.string().default("Cribl"),
  fromBeginning: z.boolean().default(true),
  kafkaSchemaRegistry: KafkaSchemaRegistryType$inboundSchema.optional(),
  connectionTimeout: z.number().default(10000),
  requestTimeout: z.number().default(60000),
  maxRetries: z.number().default(5),
  maxBackOff: z.number().default(30000),
  initialBackoff: z.number().default(300),
  backoffRate: z.number().default(2),
  authenticationTimeout: z.number().default(10000),
  reauthenticationThreshold: z.number().default(10000),
  sasl: SaslType$inboundSchema.optional(),
  tls: Tls1Type$inboundSchema.optional(),
  sessionTimeout: z.number().default(30000),
  rebalanceTimeout: z.number().default(60000),
  heartbeatInterval: z.number().default(3000),
  autoCommitInterval: z.number().optional(),
  autoCommitThreshold: z.number().optional(),
  maxBytesPerPartition: z.number().default(1048576),
  maxBytes: z.number().default(10485760),
  maxSocketErrors: z.number().default(0),
  metadata: z.array(Metadata1Type$inboundSchema).optional(),
  description: z.string().optional(),
});
/** @internal */
export type InputKafkaKafka4$Outbound = {
  pqEnabled: boolean;
  id?: string | undefined;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<ConnectionsType$Outbound> | undefined;
  pq: PqType$Outbound;
  brokers: Array<string>;
  topics: Array<string>;
  groupId: string;
  fromBeginning: boolean;
  kafkaSchemaRegistry?: KafkaSchemaRegistryType$Outbound | undefined;
  connectionTimeout: number;
  requestTimeout: number;
  maxRetries: number;
  maxBackOff: number;
  initialBackoff: number;
  backoffRate: number;
  authenticationTimeout: number;
  reauthenticationThreshold: number;
  sasl?: SaslType$Outbound | undefined;
  tls?: Tls1Type$Outbound | undefined;
  sessionTimeout: number;
  rebalanceTimeout: number;
  heartbeatInterval: number;
  autoCommitInterval?: number | undefined;
  autoCommitThreshold?: number | undefined;
  maxBytesPerPartition: number;
  maxBytes: number;
  maxSocketErrors: number;
  metadata?: Array<Metadata1Type$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputKafkaKafka4$outboundSchema: z.ZodType<
  InputKafkaKafka4$Outbound,
  z.ZodTypeDef,
  InputKafkaKafka4
> = z.object({
  pqEnabled: z.boolean().default(false),
  id: z.string().optional(),
  type: TypeKafkaOption$outboundSchema,
  disabled: z.boolean().default(false),
  pipeline: z.string().optional(),
  sendToRoutes: z.boolean().default(true),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  connections: z.array(ConnectionsType$outboundSchema).optional(),
  pq: PqType$outboundSchema,
  brokers: z.array(z.string()),
  topics: z.array(z.string()),
  groupId: z.string().default("Cribl"),
  fromBeginning: z.boolean().default(true),
  kafkaSchemaRegistry: KafkaSchemaRegistryType$outboundSchema.optional(),
  connectionTimeout: z.number().default(10000),
  requestTimeout: z.number().default(60000),
  maxRetries: z.number().default(5),
  maxBackOff: z.number().default(30000),
  initialBackoff: z.number().default(300),
  backoffRate: z.number().default(2),
  authenticationTimeout: z.number().default(10000),
  reauthenticationThreshold: z.number().default(10000),
  sasl: SaslType$outboundSchema.optional(),
  tls: Tls1Type$outboundSchema.optional(),
  sessionTimeout: z.number().default(30000),
  rebalanceTimeout: z.number().default(60000),
  heartbeatInterval: z.number().default(3000),
  autoCommitInterval: z.number().optional(),
  autoCommitThreshold: z.number().optional(),
  maxBytesPerPartition: z.number().default(1048576),
  maxBytes: z.number().default(10485760),
  maxSocketErrors: z.number().default(0),
  metadata: z.array(Metadata1Type$outboundSchema).optional(),
  description: z.string().optional(),
});

export function inputKafkaKafka4ToJSON(
  inputKafkaKafka4: InputKafkaKafka4,
): string {
  return JSON.stringify(
    InputKafkaKafka4$outboundSchema.parse(inputKafkaKafka4),
  );
}
export function inputKafkaKafka4FromJSON(
  jsonString: string,
): SafeParseResult<InputKafkaKafka4, SDKValidationError> {
  return safeParse(
    jsonString,
    (x) => InputKafkaKafka4$inboundSchema.parse(JSON.parse(x)),
    `Failed to parse 'InputKafkaKafka4' from JSON`,
  );
}

/** @internal */
export const InputKafkaKafka3$inboundSchema: z.ZodType<
  InputKafkaKafka3,
  z.ZodTypeDef,
  unknown
> = z.object({
  pqEnabled: z.boolean().default(false),
  id: z.string().optional(),
  type: TypeKafkaOption$inboundSchema,
  disabled: z.boolean().default(false),
  pipeline: z.string().optional(),
  sendToRoutes: z.boolean().default(true),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  connections: z.array(ConnectionsType$inboundSchema).optional(),
  pq: PqType$inboundSchema.optional(),
  brokers: z.array(z.string()),
  topics: z.array(z.string()),
  groupId: z.string().default("Cribl"),
  fromBeginning: z.boolean().default(true),
  kafkaSchemaRegistry: KafkaSchemaRegistryType$inboundSchema.optional(),
  connectionTimeout: z.number().default(10000),
  requestTimeout: z.number().default(60000),
  maxRetries: z.number().default(5),
  maxBackOff: z.number().default(30000),
  initialBackoff: z.number().default(300),
  backoffRate: z.number().default(2),
  authenticationTimeout: z.number().default(10000),
  reauthenticationThreshold: z.number().default(10000),
  sasl: SaslType$inboundSchema.optional(),
  tls: Tls1Type$inboundSchema.optional(),
  sessionTimeout: z.number().default(30000),
  rebalanceTimeout: z.number().default(60000),
  heartbeatInterval: z.number().default(3000),
  autoCommitInterval: z.number().optional(),
  autoCommitThreshold: z.number().optional(),
  maxBytesPerPartition: z.number().default(1048576),
  maxBytes: z.number().default(10485760),
  maxSocketErrors: z.number().default(0),
  metadata: z.array(Metadata1Type$inboundSchema).optional(),
  description: z.string().optional(),
});
/** @internal */
export type InputKafkaKafka3$Outbound = {
  pqEnabled: boolean;
  id?: string | undefined;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<ConnectionsType$Outbound> | undefined;
  pq?: PqType$Outbound | undefined;
  brokers: Array<string>;
  topics: Array<string>;
  groupId: string;
  fromBeginning: boolean;
  kafkaSchemaRegistry?: KafkaSchemaRegistryType$Outbound | undefined;
  connectionTimeout: number;
  requestTimeout: number;
  maxRetries: number;
  maxBackOff: number;
  initialBackoff: number;
  backoffRate: number;
  authenticationTimeout: number;
  reauthenticationThreshold: number;
  sasl?: SaslType$Outbound | undefined;
  tls?: Tls1Type$Outbound | undefined;
  sessionTimeout: number;
  rebalanceTimeout: number;
  heartbeatInterval: number;
  autoCommitInterval?: number | undefined;
  autoCommitThreshold?: number | undefined;
  maxBytesPerPartition: number;
  maxBytes: number;
  maxSocketErrors: number;
  metadata?: Array<Metadata1Type$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputKafkaKafka3$outboundSchema: z.ZodType<
  InputKafkaKafka3$Outbound,
  z.ZodTypeDef,
  InputKafkaKafka3
> = z.object({
  pqEnabled: z.boolean().default(false),
  id: z.string().optional(),
  type: TypeKafkaOption$outboundSchema,
  disabled: z.boolean().default(false),
  pipeline: z.string().optional(),
  sendToRoutes: z.boolean().default(true),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  connections: z.array(ConnectionsType$outboundSchema).optional(),
  pq: PqType$outboundSchema.optional(),
  brokers: z.array(z.string()),
  topics: z.array(z.string()),
  groupId: z.string().default("Cribl"),
  fromBeginning: z.boolean().default(true),
  kafkaSchemaRegistry: KafkaSchemaRegistryType$outboundSchema.optional(),
  connectionTimeout: z.number().default(10000),
  requestTimeout: z.number().default(60000),
  maxRetries: z.number().default(5),
  maxBackOff: z.number().default(30000),
  initialBackoff: z.number().default(300),
  backoffRate: z.number().default(2),
  authenticationTimeout: z.number().default(10000),
  reauthenticationThreshold: z.number().default(10000),
  sasl: SaslType$outboundSchema.optional(),
  tls: Tls1Type$outboundSchema.optional(),
  sessionTimeout: z.number().default(30000),
  rebalanceTimeout: z.number().default(60000),
  heartbeatInterval: z.number().default(3000),
  autoCommitInterval: z.number().optional(),
  autoCommitThreshold: z.number().optional(),
  maxBytesPerPartition: z.number().default(1048576),
  maxBytes: z.number().default(10485760),
  maxSocketErrors: z.number().default(0),
  metadata: z.array(Metadata1Type$outboundSchema).optional(),
  description: z.string().optional(),
});

export function inputKafkaKafka3ToJSON(
  inputKafkaKafka3: InputKafkaKafka3,
): string {
  return JSON.stringify(
    InputKafkaKafka3$outboundSchema.parse(inputKafkaKafka3),
  );
}
export function inputKafkaKafka3FromJSON(
  jsonString: string,
): SafeParseResult<InputKafkaKafka3, SDKValidationError> {
  return safeParse(
    jsonString,
    (x) => InputKafkaKafka3$inboundSchema.parse(JSON.parse(x)),
    `Failed to parse 'InputKafkaKafka3' from JSON`,
  );
}

/** @internal */
export const InputKafkaKafka2$inboundSchema: z.ZodType<
  InputKafkaKafka2,
  z.ZodTypeDef,
  unknown
> = z.object({
  sendToRoutes: z.boolean().default(true),
  id: z.string().optional(),
  type: TypeKafkaOption$inboundSchema,
  disabled: z.boolean().default(false),
  pipeline: z.string().optional(),
  environment: z.string().optional(),
  pqEnabled: z.boolean().default(false),
  streamtags: z.array(z.string()).optional(),
  connections: z.array(ConnectionsType$inboundSchema),
  pq: PqType$inboundSchema.optional(),
  brokers: z.array(z.string()),
  topics: z.array(z.string()),
  groupId: z.string().default("Cribl"),
  fromBeginning: z.boolean().default(true),
  kafkaSchemaRegistry: KafkaSchemaRegistryType$inboundSchema.optional(),
  connectionTimeout: z.number().default(10000),
  requestTimeout: z.number().default(60000),
  maxRetries: z.number().default(5),
  maxBackOff: z.number().default(30000),
  initialBackoff: z.number().default(300),
  backoffRate: z.number().default(2),
  authenticationTimeout: z.number().default(10000),
  reauthenticationThreshold: z.number().default(10000),
  sasl: SaslType$inboundSchema.optional(),
  tls: Tls1Type$inboundSchema.optional(),
  sessionTimeout: z.number().default(30000),
  rebalanceTimeout: z.number().default(60000),
  heartbeatInterval: z.number().default(3000),
  autoCommitInterval: z.number().optional(),
  autoCommitThreshold: z.number().optional(),
  maxBytesPerPartition: z.number().default(1048576),
  maxBytes: z.number().default(10485760),
  maxSocketErrors: z.number().default(0),
  metadata: z.array(Metadata1Type$inboundSchema).optional(),
  description: z.string().optional(),
});
/** @internal */
export type InputKafkaKafka2$Outbound = {
  sendToRoutes: boolean;
  id?: string | undefined;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  connections: Array<ConnectionsType$Outbound>;
  pq?: PqType$Outbound | undefined;
  brokers: Array<string>;
  topics: Array<string>;
  groupId: string;
  fromBeginning: boolean;
  kafkaSchemaRegistry?: KafkaSchemaRegistryType$Outbound | undefined;
  connectionTimeout: number;
  requestTimeout: number;
  maxRetries: number;
  maxBackOff: number;
  initialBackoff: number;
  backoffRate: number;
  authenticationTimeout: number;
  reauthenticationThreshold: number;
  sasl?: SaslType$Outbound | undefined;
  tls?: Tls1Type$Outbound | undefined;
  sessionTimeout: number;
  rebalanceTimeout: number;
  heartbeatInterval: number;
  autoCommitInterval?: number | undefined;
  autoCommitThreshold?: number | undefined;
  maxBytesPerPartition: number;
  maxBytes: number;
  maxSocketErrors: number;
  metadata?: Array<Metadata1Type$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputKafkaKafka2$outboundSchema: z.ZodType<
  InputKafkaKafka2$Outbound,
  z.ZodTypeDef,
  InputKafkaKafka2
> = z.object({
  sendToRoutes: z.boolean().default(true),
  id: z.string().optional(),
  type: TypeKafkaOption$outboundSchema,
  disabled: z.boolean().default(false),
  pipeline: z.string().optional(),
  environment: z.string().optional(),
  pqEnabled: z.boolean().default(false),
  streamtags: z.array(z.string()).optional(),
  connections: z.array(ConnectionsType$outboundSchema),
  pq: PqType$outboundSchema.optional(),
  brokers: z.array(z.string()),
  topics: z.array(z.string()),
  groupId: z.string().default("Cribl"),
  fromBeginning: z.boolean().default(true),
  kafkaSchemaRegistry: KafkaSchemaRegistryType$outboundSchema.optional(),
  connectionTimeout: z.number().default(10000),
  requestTimeout: z.number().default(60000),
  maxRetries: z.number().default(5),
  maxBackOff: z.number().default(30000),
  initialBackoff: z.number().default(300),
  backoffRate: z.number().default(2),
  authenticationTimeout: z.number().default(10000),
  reauthenticationThreshold: z.number().default(10000),
  sasl: SaslType$outboundSchema.optional(),
  tls: Tls1Type$outboundSchema.optional(),
  sessionTimeout: z.number().default(30000),
  rebalanceTimeout: z.number().default(60000),
  heartbeatInterval: z.number().default(3000),
  autoCommitInterval: z.number().optional(),
  autoCommitThreshold: z.number().optional(),
  maxBytesPerPartition: z.number().default(1048576),
  maxBytes: z.number().default(10485760),
  maxSocketErrors: z.number().default(0),
  metadata: z.array(Metadata1Type$outboundSchema).optional(),
  description: z.string().optional(),
});

export function inputKafkaKafka2ToJSON(
  inputKafkaKafka2: InputKafkaKafka2,
): string {
  return JSON.stringify(
    InputKafkaKafka2$outboundSchema.parse(inputKafkaKafka2),
  );
}
export function inputKafkaKafka2FromJSON(
  jsonString: string,
): SafeParseResult<InputKafkaKafka2, SDKValidationError> {
  return safeParse(
    jsonString,
    (x) => InputKafkaKafka2$inboundSchema.parse(JSON.parse(x)),
    `Failed to parse 'InputKafkaKafka2' from JSON`,
  );
}

/** @internal */
export const InputKafkaKafka1$inboundSchema: z.ZodType<
  InputKafkaKafka1,
  z.ZodTypeDef,
  unknown
> = z.object({
  sendToRoutes: z.boolean().default(true),
  id: z.string().optional(),
  type: TypeKafkaOption$inboundSchema,
  disabled: z.boolean().default(false),
  pipeline: z.string().optional(),
  environment: z.string().optional(),
  pqEnabled: z.boolean().default(false),
  streamtags: z.array(z.string()).optional(),
  connections: z.array(ConnectionsType$inboundSchema).optional(),
  pq: PqType$inboundSchema.optional(),
  brokers: z.array(z.string()),
  topics: z.array(z.string()),
  groupId: z.string().default("Cribl"),
  fromBeginning: z.boolean().default(true),
  kafkaSchemaRegistry: KafkaSchemaRegistryType$inboundSchema.optional(),
  connectionTimeout: z.number().default(10000),
  requestTimeout: z.number().default(60000),
  maxRetries: z.number().default(5),
  maxBackOff: z.number().default(30000),
  initialBackoff: z.number().default(300),
  backoffRate: z.number().default(2),
  authenticationTimeout: z.number().default(10000),
  reauthenticationThreshold: z.number().default(10000),
  sasl: SaslType$inboundSchema.optional(),
  tls: Tls1Type$inboundSchema.optional(),
  sessionTimeout: z.number().default(30000),
  rebalanceTimeout: z.number().default(60000),
  heartbeatInterval: z.number().default(3000),
  autoCommitInterval: z.number().optional(),
  autoCommitThreshold: z.number().optional(),
  maxBytesPerPartition: z.number().default(1048576),
  maxBytes: z.number().default(10485760),
  maxSocketErrors: z.number().default(0),
  metadata: z.array(Metadata1Type$inboundSchema).optional(),
  description: z.string().optional(),
});
/** @internal */
export type InputKafkaKafka1$Outbound = {
  sendToRoutes: boolean;
  id?: string | undefined;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  connections?: Array<ConnectionsType$Outbound> | undefined;
  pq?: PqType$Outbound | undefined;
  brokers: Array<string>;
  topics: Array<string>;
  groupId: string;
  fromBeginning: boolean;
  kafkaSchemaRegistry?: KafkaSchemaRegistryType$Outbound | undefined;
  connectionTimeout: number;
  requestTimeout: number;
  maxRetries: number;
  maxBackOff: number;
  initialBackoff: number;
  backoffRate: number;
  authenticationTimeout: number;
  reauthenticationThreshold: number;
  sasl?: SaslType$Outbound | undefined;
  tls?: Tls1Type$Outbound | undefined;
  sessionTimeout: number;
  rebalanceTimeout: number;
  heartbeatInterval: number;
  autoCommitInterval?: number | undefined;
  autoCommitThreshold?: number | undefined;
  maxBytesPerPartition: number;
  maxBytes: number;
  maxSocketErrors: number;
  metadata?: Array<Metadata1Type$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputKafkaKafka1$outboundSchema: z.ZodType<
  InputKafkaKafka1$Outbound,
  z.ZodTypeDef,
  InputKafkaKafka1
> = z.object({
  sendToRoutes: z.boolean().default(true),
  id: z.string().optional(),
  type: TypeKafkaOption$outboundSchema,
  disabled: z.boolean().default(false),
  pipeline: z.string().optional(),
  environment: z.string().optional(),
  pqEnabled: z.boolean().default(false),
  streamtags: z.array(z.string()).optional(),
  connections: z.array(ConnectionsType$outboundSchema).optional(),
  pq: PqType$outboundSchema.optional(),
  brokers: z.array(z.string()),
  topics: z.array(z.string()),
  groupId: z.string().default("Cribl"),
  fromBeginning: z.boolean().default(true),
  kafkaSchemaRegistry: KafkaSchemaRegistryType$outboundSchema.optional(),
  connectionTimeout: z.number().default(10000),
  requestTimeout: z.number().default(60000),
  maxRetries: z.number().default(5),
  maxBackOff: z.number().default(30000),
  initialBackoff: z.number().default(300),
  backoffRate: z.number().default(2),
  authenticationTimeout: z.number().default(10000),
  reauthenticationThreshold: z.number().default(10000),
  sasl: SaslType$outboundSchema.optional(),
  tls: Tls1Type$outboundSchema.optional(),
  sessionTimeout: z.number().default(30000),
  rebalanceTimeout: z.number().default(60000),
  heartbeatInterval: z.number().default(3000),
  autoCommitInterval: z.number().optional(),
  autoCommitThreshold: z.number().optional(),
  maxBytesPerPartition: z.number().default(1048576),
  maxBytes: z.number().default(10485760),
  maxSocketErrors: z.number().default(0),
  metadata: z.array(Metadata1Type$outboundSchema).optional(),
  description: z.string().optional(),
});

export function inputKafkaKafka1ToJSON(
  inputKafkaKafka1: InputKafkaKafka1,
): string {
  return JSON.stringify(
    InputKafkaKafka1$outboundSchema.parse(inputKafkaKafka1),
  );
}
export function inputKafkaKafka1FromJSON(
  jsonString: string,
): SafeParseResult<InputKafkaKafka1, SDKValidationError> {
  return safeParse(
    jsonString,
    (x) => InputKafkaKafka1$inboundSchema.parse(JSON.parse(x)),
    `Failed to parse 'InputKafkaKafka1' from JSON`,
  );
}

/** @internal */
export const InputKafka$inboundSchema: z.ZodType<
  InputKafka,
  z.ZodTypeDef,
  unknown
> = z.union([
  z.lazy(() => InputKafkaKafka2$inboundSchema),
  z.lazy(() => InputKafkaKafka4$inboundSchema),
  z.lazy(() => InputKafkaKafka1$inboundSchema),
  z.lazy(() => InputKafkaKafka3$inboundSchema),
]);
/** @internal */
export type InputKafka$Outbound =
  | InputKafkaKafka2$Outbound
  | InputKafkaKafka4$Outbound
  | InputKafkaKafka1$Outbound
  | InputKafkaKafka3$Outbound;

/** @internal */
export const InputKafka$outboundSchema: z.ZodType<
  InputKafka$Outbound,
  z.ZodTypeDef,
  InputKafka
> = z.union([
  z.lazy(() => InputKafkaKafka2$outboundSchema),
  z.lazy(() => InputKafkaKafka4$outboundSchema),
  z.lazy(() => InputKafkaKafka1$outboundSchema),
  z.lazy(() => InputKafkaKafka3$outboundSchema),
]);

export function inputKafkaToJSON(inputKafka: InputKafka): string {
  return JSON.stringify(InputKafka$outboundSchema.parse(inputKafka));
}
export function inputKafkaFromJSON(
  jsonString: string,
): SafeParseResult<InputKafka, SDKValidationError> {
  return safeParse(
    jsonString,
    (x) => InputKafka$inboundSchema.parse(JSON.parse(x)),
    `Failed to parse 'InputKafka' from JSON`,
  );
}
