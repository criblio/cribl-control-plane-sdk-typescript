/*
 * Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT.
 */

import * as z from "zod/v3";
import * as openEnums from "../../types/enums.js";
import { OpenEnum } from "../../types/enums.js";
import * as models from "../index.js";
import {
  CreateOutputAuthenticationMethodAzureLogs,
  CreateOutputAuthenticationMethodAzureLogs$outboundSchema,
  CreateOutputOutputAzureEventhub,
  CreateOutputOutputAzureEventhub$Outbound,
  CreateOutputOutputAzureEventhub$outboundSchema,
  CreateOutputOutputChronicle,
  CreateOutputOutputChronicle$Outbound,
  CreateOutputOutputChronicle$outboundSchema,
  CreateOutputOutputClickHouse,
  CreateOutputOutputClickHouse$Outbound,
  CreateOutputOutputClickHouse$outboundSchema,
  CreateOutputOutputCloudflareR2,
  CreateOutputOutputCloudflareR2$Outbound,
  CreateOutputOutputCloudflareR2$outboundSchema,
  CreateOutputOutputCloudwatch,
  CreateOutputOutputCloudwatch$Outbound,
  CreateOutputOutputCloudwatch$outboundSchema,
  CreateOutputOutputConfluentCloud,
  CreateOutputOutputConfluentCloud$Outbound,
  CreateOutputOutputConfluentCloud$outboundSchema,
  CreateOutputOutputCriblHttp,
  CreateOutputOutputCriblHttp$Outbound,
  CreateOutputOutputCriblHttp$outboundSchema,
  CreateOutputOutputCriblLake,
  CreateOutputOutputCriblLake$Outbound,
  CreateOutputOutputCriblLake$outboundSchema,
  CreateOutputOutputCriblSearchEngine,
  CreateOutputOutputCriblSearchEngine$Outbound,
  CreateOutputOutputCriblSearchEngine$outboundSchema,
  CreateOutputOutputCriblTcp,
  CreateOutputOutputCriblTcp$Outbound,
  CreateOutputOutputCriblTcp$outboundSchema,
  CreateOutputOutputCrowdstrikeNextGenSiem,
  CreateOutputOutputCrowdstrikeNextGenSiem$Outbound,
  CreateOutputOutputCrowdstrikeNextGenSiem$outboundSchema,
  CreateOutputOutputDatabricks,
  CreateOutputOutputDatabricks$Outbound,
  CreateOutputOutputDatabricks$outboundSchema,
  CreateOutputOutputDatadog,
  CreateOutputOutputDatadog$Outbound,
  CreateOutputOutputDatadog$outboundSchema,
  CreateOutputOutputDataset,
  CreateOutputOutputDataset$Outbound,
  CreateOutputOutputDataset$outboundSchema,
  CreateOutputOutputDiskSpool,
  CreateOutputOutputDiskSpool$Outbound,
  CreateOutputOutputDiskSpool$outboundSchema,
  CreateOutputOutputDlS3,
  CreateOutputOutputDlS3$Outbound,
  CreateOutputOutputDlS3$outboundSchema,
  CreateOutputOutputDynatraceHttp,
  CreateOutputOutputDynatraceHttp$Outbound,
  CreateOutputOutputDynatraceHttp$outboundSchema,
  CreateOutputOutputDynatraceOtlp,
  CreateOutputOutputDynatraceOtlp$Outbound,
  CreateOutputOutputDynatraceOtlp$outboundSchema,
  CreateOutputOutputElastic,
  CreateOutputOutputElastic$Outbound,
  CreateOutputOutputElastic$outboundSchema,
  CreateOutputOutputElasticCloud,
  CreateOutputOutputElasticCloud$Outbound,
  CreateOutputOutputElasticCloud$outboundSchema,
  CreateOutputOutputExabeam,
  CreateOutputOutputExabeam$Outbound,
  CreateOutputOutputExabeam$outboundSchema,
  CreateOutputOutputGoogleChronicle,
  CreateOutputOutputGoogleChronicle$Outbound,
  CreateOutputOutputGoogleChronicle$outboundSchema,
  CreateOutputOutputGoogleCloudLogging,
  CreateOutputOutputGoogleCloudLogging$Outbound,
  CreateOutputOutputGoogleCloudLogging$outboundSchema,
  CreateOutputOutputGoogleCloudStorage,
  CreateOutputOutputGoogleCloudStorage$Outbound,
  CreateOutputOutputGoogleCloudStorage$outboundSchema,
  CreateOutputOutputGooglePubsub,
  CreateOutputOutputGooglePubsub$Outbound,
  CreateOutputOutputGooglePubsub$outboundSchema,
  CreateOutputOutputGrafanaCloudUnion,
  CreateOutputOutputGrafanaCloudUnion$Outbound,
  CreateOutputOutputGrafanaCloudUnion$outboundSchema,
  CreateOutputOutputGraphite,
  CreateOutputOutputGraphite$Outbound,
  CreateOutputOutputGraphite$outboundSchema,
  CreateOutputOutputHoneycomb,
  CreateOutputOutputHoneycomb$Outbound,
  CreateOutputOutputHoneycomb$outboundSchema,
  CreateOutputOutputHumioHec,
  CreateOutputOutputHumioHec$Outbound,
  CreateOutputOutputHumioHec$outboundSchema,
  CreateOutputOutputInfluxdb,
  CreateOutputOutputInfluxdb$Outbound,
  CreateOutputOutputInfluxdb$outboundSchema,
  CreateOutputOutputKafka,
  CreateOutputOutputKafka$Outbound,
  CreateOutputOutputKafka$outboundSchema,
  CreateOutputOutputKinesis,
  CreateOutputOutputKinesis$Outbound,
  CreateOutputOutputKinesis$outboundSchema,
  CreateOutputOutputLoki,
  CreateOutputOutputLoki$Outbound,
  CreateOutputOutputLoki$outboundSchema,
  CreateOutputOutputMicrosoftFabric,
  CreateOutputOutputMicrosoftFabric$Outbound,
  CreateOutputOutputMicrosoftFabric$outboundSchema,
  CreateOutputOutputMinio,
  CreateOutputOutputMinio$Outbound,
  CreateOutputOutputMinio$outboundSchema,
  CreateOutputOutputMsk,
  CreateOutputOutputMsk$Outbound,
  CreateOutputOutputMsk$outboundSchema,
  CreateOutputOutputNetflow,
  CreateOutputOutputNetflow$Outbound,
  CreateOutputOutputNetflow$outboundSchema,
  CreateOutputOutputNewrelic,
  CreateOutputOutputNewrelic$Outbound,
  CreateOutputOutputNewrelic$outboundSchema,
  CreateOutputOutputNewrelicEvents,
  CreateOutputOutputNewrelicEvents$Outbound,
  CreateOutputOutputNewrelicEvents$outboundSchema,
  CreateOutputOutputOpenTelemetry,
  CreateOutputOutputOpenTelemetry$Outbound,
  CreateOutputOutputOpenTelemetry$outboundSchema,
  CreateOutputOutputPrometheus,
  CreateOutputOutputPrometheus$Outbound,
  CreateOutputOutputPrometheus$outboundSchema,
  CreateOutputOutputRing,
  CreateOutputOutputRing$Outbound,
  CreateOutputOutputRing$outboundSchema,
  CreateOutputOutputRouter,
  CreateOutputOutputRouter$Outbound,
  CreateOutputOutputRouter$outboundSchema,
  CreateOutputOutputSecurityLake,
  CreateOutputOutputSecurityLake$Outbound,
  CreateOutputOutputSecurityLake$outboundSchema,
  CreateOutputOutputSentinelOneAiSiem,
  CreateOutputOutputSentinelOneAiSiem$Outbound,
  CreateOutputOutputSentinelOneAiSiem$outboundSchema,
  CreateOutputOutputServiceNow,
  CreateOutputOutputServiceNow$Outbound,
  CreateOutputOutputServiceNow$outboundSchema,
  CreateOutputOutputSnmp,
  CreateOutputOutputSnmp$Outbound,
  CreateOutputOutputSnmp$outboundSchema,
  CreateOutputOutputSns,
  CreateOutputOutputSns$Outbound,
  CreateOutputOutputSns$outboundSchema,
  CreateOutputOutputSqs,
  CreateOutputOutputSqs$Outbound,
  CreateOutputOutputSqs$outboundSchema,
  CreateOutputOutputStatsd,
  CreateOutputOutputStatsd$Outbound,
  CreateOutputOutputStatsd$outboundSchema,
  CreateOutputOutputStatsdExt,
  CreateOutputOutputStatsdExt$Outbound,
  CreateOutputOutputStatsdExt$outboundSchema,
  CreateOutputOutputSumoLogic,
  CreateOutputOutputSumoLogic$Outbound,
  CreateOutputOutputSumoLogic$outboundSchema,
  CreateOutputOutputXsiam,
  CreateOutputOutputXsiam$Outbound,
  CreateOutputOutputXsiam$outboundSchema,
  CreateOutputPqControlsAzureLogs,
  CreateOutputPqControlsAzureLogs$Outbound,
  CreateOutputPqControlsAzureLogs$outboundSchema,
} from "./createoutputpqcontrolsazurelogs.js";

export type CreateOutputOutputAzureLogs = {
  /**
   * Unique ID for this output
   */
  id: string;
  type: "azure_logs";
  /**
   * Pipeline to process data before sending out to this output
   */
  pipeline?: string | undefined;
  /**
   * Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
   */
  systemFields?: Array<string> | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * The Log Type of events sent to this LogAnalytics workspace. Defaults to `Cribl`. Use only letters, numbers, and `_` characters, and can't exceed 100 characters. Can be overwritten by event field __logType.
   */
  logType: string;
  /**
   * Optional Resource ID of the Azure resource to associate the data with. Can be overridden by the __resourceId event field. This ID populates the _ResourceId property, allowing the data to be included in resource-centric queries. If the ID is neither specified nor overridden, resource-centric queries will omit the data.
   */
  resourceId?: string | undefined;
  /**
   * Maximum number of ongoing requests before blocking
   */
  concurrency?: number | undefined;
  /**
   * Maximum size, in KB, of the request body
   */
  maxPayloadSizeKB?: number | undefined;
  /**
   * Maximum number of events to include in the request body. Default is 0 (unlimited).
   */
  maxPayloadEvents?: number | undefined;
  compress?: boolean | undefined;
  /**
   * Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
   *
   * @remarks
   *         Enabled by default. When this setting is also present in TLS Settings (Client Side),
   *         that value will take precedence.
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * Amount of time, in seconds, to wait for a request to complete before canceling it
   */
  timeoutSec?: number | undefined;
  /**
   * Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
   */
  flushPeriodSec?: number | undefined;
  /**
   * Headers to add to all events
   */
  extraHttpHeaders?: Array<models.ItemsTypeExtraHttpHeaders> | undefined;
  /**
   * Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
   */
  useRoundRobinDns?: boolean | undefined;
  /**
   * Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
   */
  failedRequestLoggingMode?: models.FailedRequestLoggingModeOptions | undefined;
  /**
   * List of headers that are safe to log in plain text
   */
  safeHeaders?: Array<string> | undefined;
  /**
   * The DNS name of the Log API endpoint that sends log data to a Log Analytics workspace in Azure Monitor. Defaults to .ods.opinsights.azure.com. @{product} will add a prefix and suffix to construct a URI in this format: <https://<Workspace_ID><your_DNS_name>/api/logs?api-version=<API version>.
   */
  apiUrl?: string | undefined;
  /**
   * Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
   */
  responseRetrySettings?:
    | Array<models.ItemsTypeResponseRetrySettings>
    | undefined;
  timeoutRetrySettings?: models.TimeoutRetrySettingsType | undefined;
  /**
   * Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
   */
  responseHonorRetryAfterHeader?: boolean | undefined;
  /**
   * How to handle events when all receivers are exerting backpressure
   */
  onBackpressure?: models.BackpressureBehaviorOptions | undefined;
  /**
   * Enter workspace ID and workspace key directly, or select a stored secret
   */
  authType?: CreateOutputAuthenticationMethodAzureLogs | undefined;
  description?: string | undefined;
  /**
   * Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
   */
  pqStrictOrdering?: boolean | undefined;
  /**
   * Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
   */
  pqRatePerSec?: number | undefined;
  /**
   * In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
   */
  pqMode?: models.ModeOptions | undefined;
  /**
   * The maximum number of events to hold in memory before writing the events to disk
   */
  pqMaxBufferSize?: number | undefined;
  /**
   * How long (in seconds) to wait for backpressure to resolve before engaging the queue
   */
  pqMaxBackpressureSec?: number | undefined;
  /**
   * The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
   */
  pqMaxFileSize?: string | undefined;
  /**
   * The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
   */
  pqMaxSize?: string | undefined;
  /**
   * The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
   */
  pqPath?: string | undefined;
  /**
   * Codec to use to compress the persisted data
   */
  pqCompress?: models.CompressionOptionsPq | undefined;
  /**
   * How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
   */
  pqOnBackpressure?: models.QueueFullBehaviorOptions | undefined;
  pqControls?: CreateOutputPqControlsAzureLogs | undefined;
  /**
   * Azure Log Analytics Workspace ID. See Azure Dashboard Workspace > Advanced settings.
   */
  workspaceId?: string | undefined;
  /**
   * Azure Log Analytics Workspace Primary or Secondary Shared Key. See Azure Dashboard Workspace > Advanced settings.
   */
  workspaceKey?: string | undefined;
  /**
   * Select or create a stored secret that references your access key and secret key
   */
  keypairSecret?: string | undefined;
  /**
   * Binds 'workspaceId' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'workspaceId' at runtime.
   */
  __template_workspaceId?: string | undefined;
  /**
   * Binds 'workspaceKey' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'workspaceKey' at runtime.
   */
  __template_workspaceKey?: string | undefined;
};

export const CreateOutputIngestionMode = {
  /**
   * Batching
   */
  Batching: "batching",
  /**
   * Streaming
   */
  Streaming: "streaming",
} as const;
export type CreateOutputIngestionMode = OpenEnum<
  typeof CreateOutputIngestionMode
>;

/**
 * The type of OAuth 2.0 client credentials grant flow to use
 */
export const CreateOutputOauthTypeAuthenticationMethod = {
  /**
   * Client secret
   */
  ClientSecret: "clientSecret",
  /**
   * Client secret (text secret)
   */
  ClientTextSecret: "clientTextSecret",
  /**
   * Certificate
   */
  Certificate: "certificate",
} as const;
/**
 * The type of OAuth 2.0 client credentials grant flow to use
 */
export type CreateOutputOauthTypeAuthenticationMethod = OpenEnum<
  typeof CreateOutputOauthTypeAuthenticationMethod
>;

export type CreateOutputCertificate = {
  /**
   * The certificate you registered as credentials for your app in the Azure portal
   */
  certificateName?: string | undefined;
};

export const CreateOutputPrefixOptional = {
  /**
   * drop-by
   */
  DropBy: "dropBy",
  /**
   * ingest-by
   */
  IngestBy: "ingestBy",
} as const;
export type CreateOutputPrefixOptional = OpenEnum<
  typeof CreateOutputPrefixOptional
>;

export type CreateOutputExtentTag = {
  prefix?: CreateOutputPrefixOptional | undefined;
  value: string;
};

export type CreateOutputIngestIfNotExist = {
  value: string;
};

/**
 * Level of ingestion status reporting. Defaults to FailuresOnly.
 */
export const CreateOutputReportLevel = {
  /**
   * FailuresOnly
   */
  FailuresOnly: "failuresOnly",
  /**
   * DoNotReport
   */
  DoNotReport: "doNotReport",
  /**
   * FailuresAndSuccesses
   */
  FailuresAndSuccesses: "failuresAndSuccesses",
} as const;
/**
 * Level of ingestion status reporting. Defaults to FailuresOnly.
 */
export type CreateOutputReportLevel = OpenEnum<typeof CreateOutputReportLevel>;

/**
 * Target of the ingestion status reporting. Defaults to Queue.
 */
export const CreateOutputReportMethod = {
  /**
   * Queue
   */
  Queue: "queue",
  /**
   * Table
   */
  Table: "table",
  /**
   * QueueAndTable
   */
  QueueAndTable: "queueAndTable",
} as const;
/**
 * Target of the ingestion status reporting. Defaults to Queue.
 */
export type CreateOutputReportMethod = OpenEnum<
  typeof CreateOutputReportMethod
>;

export type CreateOutputAdditionalProperty = {
  key: string;
  value: string;
};

export type CreateOutputPqControlsAzureDataExplorer = {};

export type CreateOutputOutputAzureDataExplorer = {
  /**
   * Unique ID for this output
   */
  id: string;
  type: "azure_data_explorer";
  /**
   * Pipeline to process data before sending out to this output
   */
  pipeline?: string | undefined;
  /**
   * Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
   */
  systemFields?: Array<string> | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * The base URI for your cluster. Typically, `https://<cluster>.<region>.kusto.windows.net`.
   */
  clusterUrl: string;
  /**
   * Name of the database containing the table where data will be ingested
   */
  database: string;
  /**
   * Name of the table to ingest data into
   */
  table: string;
  /**
   * When saving or starting the Destination, validate the database name and credentials; also validate table name, except when creating a new table. Disable if your Azure app does not have both the Database Viewer and the Table Viewer role.
   */
  validateDatabaseSettings?: boolean | undefined;
  ingestMode?: CreateOutputIngestionMode | undefined;
  /**
   * Endpoint used to acquire authentication tokens from Azure
   */
  oauthEndpoint: models.MicrosoftEntraIdAuthenticationEndpointOptionsSasl;
  /**
   * Directory ID (tenant identifier) in Azure Active Directory
   */
  tenantId: string;
  /**
   * client_id to pass in the OAuth request parameter
   */
  clientId: string;
  /**
   * Scope to pass in the OAuth request parameter
   */
  scope: string;
  /**
   * The type of OAuth 2.0 client credentials grant flow to use
   */
  oauthType: CreateOutputOauthTypeAuthenticationMethod;
  description?: string | undefined;
  /**
   * The client secret that you generated for your app in the Azure portal
   */
  clientSecret?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
  certificate?: CreateOutputCertificate | undefined;
  /**
   * Format of the output data
   */
  format?: models.DataFormatOptions | undefined;
  /**
   * Data compression format to apply to HTTP content before it is delivered
   */
  compress: models.CompressionOptions2;
  /**
   * Compression level to apply before moving files to final destination
   */
  compressionLevel?: models.CompressionLevelOptions | undefined;
  /**
   * Automatically calculate the schema based on the events of each Parquet file generated
   */
  automaticSchema?: boolean | undefined;
  /**
   * To add a new schema, navigate to Processing > Knowledge > Parquet Schemas
   */
  parquetSchema?: string | undefined;
  /**
   * Determines which data types are supported and how they are represented
   */
  parquetVersion?: models.ParquetVersionOptions | undefined;
  /**
   * Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
   */
  parquetDataPageVersion?: models.DataPageVersionOptions | undefined;
  /**
   * The number of rows that every group will contain. The final group can contain a smaller number of rows.
   */
  parquetRowGroupLength?: number | undefined;
  /**
   * Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
   */
  parquetPageSize?: string | undefined;
  /**
   * Log up to 3 rows that @{product} skips due to data mismatch
   */
  shouldLogInvalidRows?: boolean | undefined;
  /**
   * The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
   */
  keyValueMetadata?: Array<models.ItemsTypeKeyValueMetadata> | undefined;
  /**
   * Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
   */
  enableStatistics?: boolean | undefined;
  /**
   * One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
   */
  enableWritePageIndex?: boolean | undefined;
  /**
   * Parquet tools can use the checksum of a Parquet page to verify data integrity
   */
  enablePageChecksum?: boolean | undefined;
  /**
   * Remove empty staging directories after moving files
   */
  removeEmptyDirs?: boolean | undefined;
  /**
   * How frequently, in seconds, to clean up empty directories
   */
  emptyDirCleanupSec?: number | undefined;
  /**
   * Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.
   */
  directoryBatchSize?: number | undefined;
  /**
   * If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
   */
  deadletterEnabled?: boolean | undefined;
  /**
   * Storage location for files that fail to reach their final destination after maximum retries are exceeded
   */
  deadletterPath?: string | undefined;
  /**
   * The maximum number of times a file will attempt to move to its final destination before being dead-lettered
   */
  maxRetryNum?: number | undefined;
  /**
   * Send a JSON mapping object instead of specifying an existing named data mapping
   */
  isMappingObj?: boolean | undefined;
  /**
   * Enter a JSON object that defines your desired data mapping
   */
  mappingObj?: string | undefined;
  /**
   * Enter the name of a data mapping associated with your target table. Or, if incoming event and target table fields match exactly, you can leave the field empty.
   */
  mappingRef?: string | undefined;
  /**
   * The ingestion service URI for your cluster. Typically, `https://ingest-<cluster>.<region>.kusto.windows.net`.
   */
  ingestUrl?: string | undefined;
  /**
   * How to handle events when all receivers are exerting backpressure
   */
  onBackpressure?: models.BackpressureBehaviorOptions | undefined;
  /**
   * Filesystem location in which to buffer files before compressing and moving to final destination. Use performant and stable storage.
   */
  stagePath?: string | undefined;
  /**
   * JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
   */
  fileNameSuffix?: string | undefined;
  /**
   * Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
   */
  maxFileSizeMB?: number | undefined;
  /**
   * Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
   */
  maxFileOpenTimeSec?: number | undefined;
  /**
   * Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
   */
  maxFileIdleTimeSec?: number | undefined;
  /**
   * Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
   */
  maxOpenFiles?: number | undefined;
  /**
   * Maximum number of parts to upload in parallel per file
   */
  maxConcurrentFileParts?: number | undefined;
  /**
   * How to handle events when disk space is below the global 'Min free disk space' limit
   */
  onDiskFullBackpressure?: models.DiskSpaceProtectionOptions | undefined;
  /**
   * Add the Output ID value to staging location
   */
  addIdToStagePath?: boolean | undefined;
  retrySettings?: models.RetrySettingsType | undefined;
  /**
   * Amount of time, in seconds, to wait for a request to complete before canceling it
   */
  timeoutSec?: number | undefined;
  /**
   * Bypass the data management service's aggregation mechanism
   */
  flushImmediately?: boolean | undefined;
  /**
   * Prevent blob deletion after ingestion is complete
   */
  retainBlobOnSuccess?: boolean | undefined;
  /**
   * Strings or tags associated with the extent (ingested data shard)
   */
  extentTags?: Array<CreateOutputExtentTag> | undefined;
  /**
   * Prevents duplicate ingestion by verifying whether an extent with the specified ingest-by tag already exists
   */
  ingestIfNotExists?: Array<CreateOutputIngestIfNotExist> | undefined;
  /**
   * Level of ingestion status reporting. Defaults to FailuresOnly.
   */
  reportLevel?: CreateOutputReportLevel | undefined;
  /**
   * Target of the ingestion status reporting. Defaults to Queue.
   */
  reportMethod?: CreateOutputReportMethod | undefined;
  /**
   * Optionally, enter additional configuration properties to send to the ingestion service
   */
  additionalProperties?: Array<CreateOutputAdditionalProperty> | undefined;
  /**
   * Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
   */
  responseRetrySettings?:
    | Array<models.ItemsTypeResponseRetrySettings>
    | undefined;
  timeoutRetrySettings?: models.TimeoutRetrySettingsType | undefined;
  /**
   * Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
   */
  responseHonorRetryAfterHeader?: boolean | undefined;
  /**
   * Maximum number of ongoing requests before blocking
   */
  concurrency?: number | undefined;
  /**
   * Maximum size, in KB, of the request body
   */
  maxPayloadSizeKB?: number | undefined;
  /**
   * Maximum number of events to include in the request body. Default is 0 (unlimited).
   */
  maxPayloadEvents?: number | undefined;
  /**
   * Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
   */
  flushPeriodSec?: number | undefined;
  /**
   * Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
   *
   * @remarks
   *         Enabled by default. When this setting is also present in TLS Settings (Client Side),
   *         that value will take precedence.
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
   */
  useRoundRobinDns?: boolean | undefined;
  /**
   * Disable to close the connection immediately after sending the outgoing request
   */
  keepAlive?: boolean | undefined;
  /**
   * Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
   */
  pqStrictOrdering?: boolean | undefined;
  /**
   * Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
   */
  pqRatePerSec?: number | undefined;
  /**
   * In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
   */
  pqMode?: models.ModeOptions | undefined;
  /**
   * The maximum number of events to hold in memory before writing the events to disk
   */
  pqMaxBufferSize?: number | undefined;
  /**
   * How long (in seconds) to wait for backpressure to resolve before engaging the queue
   */
  pqMaxBackpressureSec?: number | undefined;
  /**
   * The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
   */
  pqMaxFileSize?: string | undefined;
  /**
   * The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
   */
  pqMaxSize?: string | undefined;
  /**
   * The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
   */
  pqPath?: string | undefined;
  /**
   * Codec to use to compress the persisted data
   */
  pqCompress?: models.CompressionOptionsPq | undefined;
  /**
   * How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
   */
  pqOnBackpressure?: models.QueueFullBehaviorOptions | undefined;
  pqControls?: CreateOutputPqControlsAzureDataExplorer | undefined;
  /**
   * Binds 'clusterUrl' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'clusterUrl' at runtime.
   */
  __template_clusterUrl?: string | undefined;
  /**
   * Binds 'database' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'database' at runtime.
   */
  __template_database?: string | undefined;
  /**
   * Binds 'table' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'table' at runtime.
   */
  __template_table?: string | undefined;
  /**
   * Binds 'tenantId' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'tenantId' at runtime.
   */
  __template_tenantId?: string | undefined;
  /**
   * Binds 'clientId' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'clientId' at runtime.
   */
  __template_clientId?: string | undefined;
  /**
   * Binds 'scope' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'scope' at runtime.
   */
  __template_scope?: string | undefined;
  /**
   * Binds 'clientSecret' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'clientSecret' at runtime.
   */
  __template_clientSecret?: string | undefined;
  /**
   * Binds 'format' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'format' at runtime.
   */
  __template_format?: string | undefined;
  /**
   * Binds 'ingestUrl' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'ingestUrl' at runtime.
   */
  __template_ingestUrl?: string | undefined;
};

export const CreateOutputBlobAccessTier = {
  /**
   * Default account access tier
   */
  Inferred: "Inferred",
  /**
   * Hot tier
   */
  Hot: "Hot",
  /**
   * Cool tier
   */
  Cool: "Cool",
  /**
   * Cold tier
   */
  Cold: "Cold",
  /**
   * Archive tier
   */
  Archive: "Archive",
} as const;
export type CreateOutputBlobAccessTier = OpenEnum<
  typeof CreateOutputBlobAccessTier
>;

export type CreateOutputOutputAzureBlob = {
  /**
   * Unique ID for this output
   */
  id: string;
  type: "azure_blob";
  /**
   * Pipeline to process data before sending out to this output
   */
  pipeline?: string | undefined;
  /**
   * Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
   */
  systemFields?: Array<string> | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * The Azure Blob Storage container name. Name can include only lowercase letters, numbers, and hyphens. For dynamic container names, enter a JavaScript expression within quotes or backticks, to be evaluated at initialization. The expression can evaluate to a constant value and can reference Global Variables, such as `myContainer-${C.env["CRIBL_WORKER_ID"]}`.
   */
  containerName: string;
  /**
   * Create the configured container in Azure Blob Storage if it does not already exist
   */
  createContainer?: boolean | undefined;
  /**
   * Root directory prepended to path before uploading. Value can be a JavaScript expression enclosed in quotes or backticks, to be evaluated at initialization. The expression can evaluate to a constant value and can reference Global Variables, such as `myBlobPrefix-${C.env["CRIBL_WORKER_ID"]}`.
   */
  destPath?: string | undefined;
  /**
   * Filesystem location in which to buffer files before compressing and moving to final destination. Use performant and stable storage.
   */
  stagePath: string;
  /**
   * Add the Output ID value to staging location
   */
  addIdToStagePath?: boolean | undefined;
  /**
   * Maximum number of parts to upload in parallel per file
   */
  maxConcurrentFileParts?: number | undefined;
  /**
   * Remove empty staging directories after moving files
   */
  removeEmptyDirs?: boolean | undefined;
  /**
   * JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.
   */
  partitionExpr?: string | undefined;
  /**
   * Format of the output data
   */
  format?: models.DataFormatOptions | undefined;
  /**
   * JavaScript expression to define the output filename prefix (can be constant)
   */
  baseFileName?: string | undefined;
  /**
   * JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
   */
  fileNameSuffix?: string | undefined;
  /**
   * Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
   */
  maxFileSizeMB?: number | undefined;
  /**
   * Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
   */
  maxFileOpenTimeSec?: number | undefined;
  /**
   * Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
   */
  maxFileIdleTimeSec?: number | undefined;
  /**
   * Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
   */
  maxOpenFiles?: number | undefined;
  /**
   * If set, this line will be written to the beginning of each output file
   */
  headerLine?: string | undefined;
  /**
   * Buffer size used to write to a file
   */
  writeHighWaterMark?: number | undefined;
  /**
   * How to handle events when all receivers are exerting backpressure
   */
  onBackpressure?: models.BackpressureBehaviorOptions1 | undefined;
  /**
   * If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
   */
  deadletterEnabled?: boolean | undefined;
  /**
   * How to handle events when disk space is below the global 'Min free disk space' limit
   */
  onDiskFullBackpressure?: models.DiskSpaceProtectionOptions | undefined;
  /**
   * Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.
   */
  forceCloseOnShutdown?: boolean | undefined;
  retrySettings?: models.RetrySettingsType | undefined;
  authType?: models.AuthenticationMethodOptions | undefined;
  storageClass?: CreateOutputBlobAccessTier | undefined;
  description?: string | undefined;
  /**
   * Data compression format to apply to HTTP content before it is delivered
   */
  compress?: models.CompressionOptions2 | undefined;
  /**
   * Compression level to apply before moving files to final destination
   */
  compressionLevel?: models.CompressionLevelOptions | undefined;
  /**
   * Automatically calculate the schema based on the events of each Parquet file generated
   */
  automaticSchema?: boolean | undefined;
  /**
   * To add a new schema, navigate to Processing > Knowledge > Parquet Schemas
   */
  parquetSchema?: string | undefined;
  /**
   * Determines which data types are supported and how they are represented
   */
  parquetVersion?: models.ParquetVersionOptions | undefined;
  /**
   * Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
   */
  parquetDataPageVersion?: models.DataPageVersionOptions | undefined;
  /**
   * The number of rows that every group will contain. The final group can contain a smaller number of rows.
   */
  parquetRowGroupLength?: number | undefined;
  /**
   * Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
   */
  parquetPageSize?: string | undefined;
  /**
   * Log up to 3 rows that @{product} skips due to data mismatch
   */
  shouldLogInvalidRows?: boolean | undefined;
  /**
   * The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
   */
  keyValueMetadata?: Array<models.ItemsTypeKeyValueMetadata> | undefined;
  /**
   * Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
   */
  enableStatistics?: boolean | undefined;
  /**
   * One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
   */
  enableWritePageIndex?: boolean | undefined;
  /**
   * Parquet tools can use the checksum of a Parquet page to verify data integrity
   */
  enablePageChecksum?: boolean | undefined;
  /**
   * How frequently, in seconds, to clean up empty directories
   */
  emptyDirCleanupSec?: number | undefined;
  /**
   * Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.
   */
  directoryBatchSize?: number | undefined;
  /**
   * Storage location for files that fail to reach their final destination after maximum retries are exceeded
   */
  deadletterPath?: string | undefined;
  /**
   * The maximum number of times a file will attempt to move to its final destination before being dead-lettered
   */
  maxRetryNum?: number | undefined;
  /**
   * Enter your Azure Storage account connection string. If left blank, Stream will fall back to env.AZURE_STORAGE_CONNECTION_STRING.
   */
  connectionString?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
  /**
   * The name of your Azure storage account
   */
  storageAccountName?: string | undefined;
  /**
   * The service principal's tenant ID
   */
  tenantId?: string | undefined;
  /**
   * The service principal's client ID
   */
  clientId?: string | undefined;
  /**
   * The Azure cloud to use. Defaults to Azure Public Cloud.
   */
  azureCloud?: string | undefined;
  /**
   * Endpoint suffix for the service URL. Takes precedence over the Azure Cloud setting. Defaults to core.windows.net.
   */
  endpointSuffix?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  clientTextSecret?: string | undefined;
  certificate?: models.CertificateTypeAzureBlobAuthTypeClientCert | undefined;
  /**
   * Binds 'containerName' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'containerName' at runtime.
   */
  __template_containerName?: string | undefined;
  /**
   * Binds 'format' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'format' at runtime.
   */
  __template_format?: string | undefined;
  /**
   * Binds 'connectionString' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'connectionString' at runtime.
   */
  __template_connectionString?: string | undefined;
  /**
   * Binds 'tenantId' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'tenantId' at runtime.
   */
  __template_tenantId?: string | undefined;
  /**
   * Binds 'clientId' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'clientId' at runtime.
   */
  __template_clientId?: string | undefined;
};

export type CreateOutputOutputS3 = {
  /**
   * Unique ID for this output
   */
  id: string;
  type: "s3";
  /**
   * Pipeline to process data before sending out to this output
   */
  pipeline?: string | undefined;
  /**
   * Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
   */
  systemFields?: Array<string> | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`
   */
  bucket: string;
  /**
   * Region where the S3 bucket is located
   */
  region?: string | undefined;
  /**
   * Secret key. This value can be a constant or a JavaScript expression. Example: `${C.env.SOME_SECRET}`)
   */
  awsSecretKey?: string | undefined;
  /**
   * AWS authentication method. Choose Auto to use IAM roles.
   */
  awsAuthenticationMethod?: string | undefined;
  /**
   * S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.
   */
  endpoint?: string | undefined;
  /**
   * Signature version to use for signing S3 requests
   */
  signatureVersion?: models.SignatureVersionOptionsS3CollectorConf | undefined;
  /**
   * Reuse connections between requests, which can improve performance
   */
  reuseConnections?: boolean | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * Use Assume Role credentials to access S3
   */
  enableAssumeRole?: boolean | undefined;
  /**
   * Amazon Resource Name (ARN) of the role to assume
   */
  assumeRoleArn?: string | undefined;
  /**
   * External ID to use when assuming role
   */
  assumeRoleExternalId?: string | undefined;
  /**
   * Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
   */
  durationSeconds?: number | undefined;
  /**
   * Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.
   */
  stagePath: string;
  /**
   * Add the Output ID value to staging location
   */
  addIdToStagePath?: boolean | undefined;
  /**
   * Prefix to prepend to files before uploading. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `myKeyPrefix-${C.vars.myVar}`
   */
  destPath?: string | undefined;
  /**
   * Object ACL to assign to uploaded objects
   */
  objectACL?: models.ObjectAclOptions | undefined;
  /**
   * Storage class to select for uploaded objects
   */
  storageClass?: models.StorageClassOptions | undefined;
  serverSideEncryption?:
    | models.ServerSideEncryptionForUploadedObjectsOptions
    | undefined;
  /**
   * ID or ARN of the KMS customer-managed key to use for encryption
   */
  kmsKeyId?: string | undefined;
  /**
   * Remove empty staging directories after moving files
   */
  removeEmptyDirs?: boolean | undefined;
  /**
   * JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.
   */
  partitionExpr?: string | undefined;
  /**
   * Format of the output data
   */
  format?: models.DataFormatOptions | undefined;
  /**
   * JavaScript expression to define the output filename prefix (can be constant)
   */
  baseFileName?: string | undefined;
  /**
   * JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
   */
  fileNameSuffix?: string | undefined;
  /**
   * Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
   */
  maxFileSizeMB?: number | undefined;
  /**
   * Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
   */
  maxOpenFiles?: number | undefined;
  /**
   * If set, this line will be written to the beginning of each output file
   */
  headerLine?: string | undefined;
  /**
   * Buffer size used to write to a file
   */
  writeHighWaterMark?: number | undefined;
  /**
   * How to handle events when all receivers are exerting backpressure
   */
  onBackpressure?: models.BackpressureBehaviorOptions1 | undefined;
  /**
   * If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
   */
  deadletterEnabled?: boolean | undefined;
  /**
   * How to handle events when disk space is below the global 'Min free disk space' limit
   */
  onDiskFullBackpressure?: models.DiskSpaceProtectionOptions | undefined;
  /**
   * Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.
   */
  forceCloseOnShutdown?: boolean | undefined;
  retrySettings?: models.RetrySettingsType | undefined;
  /**
   * Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
   */
  maxFileOpenTimeSec?: number | undefined;
  /**
   * Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
   */
  maxFileIdleTimeSec?: number | undefined;
  /**
   * Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.
   */
  maxConcurrentFileParts?: number | undefined;
  /**
   * Disable if you can access files within the bucket but not the bucket itself
   */
  verifyPermissions?: boolean | undefined;
  /**
   * Maximum number of files that can be waiting for upload before backpressure is applied
   */
  maxClosingFilesToBackpressure?: number | undefined;
  description?: string | undefined;
  /**
   * This value can be a constant or a JavaScript expression (`${C.env.SOME_ACCESS_KEY}`)
   */
  awsApiKey?: string | undefined;
  /**
   * Select or create a stored secret that references your access key and secret key
   */
  awsSecret?: string | undefined;
  /**
   * Data compression format to apply to HTTP content before it is delivered
   */
  compress?: models.CompressionOptions2 | undefined;
  /**
   * Compression level to apply before moving files to final destination
   */
  compressionLevel?: models.CompressionLevelOptions | undefined;
  /**
   * Automatically calculate the schema based on the events of each Parquet file generated
   */
  automaticSchema?: boolean | undefined;
  /**
   * To add a new schema, navigate to Processing > Knowledge > Parquet Schemas
   */
  parquetSchema?: string | undefined;
  /**
   * Determines which data types are supported and how they are represented
   */
  parquetVersion?: models.ParquetVersionOptions | undefined;
  /**
   * Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
   */
  parquetDataPageVersion?: models.DataPageVersionOptions | undefined;
  /**
   * The number of rows that every group will contain. The final group can contain a smaller number of rows.
   */
  parquetRowGroupLength?: number | undefined;
  /**
   * Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
   */
  parquetPageSize?: string | undefined;
  /**
   * Log up to 3 rows that @{product} skips due to data mismatch
   */
  shouldLogInvalidRows?: boolean | undefined;
  /**
   * The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
   */
  keyValueMetadata?: Array<models.ItemsTypeKeyValueMetadata> | undefined;
  /**
   * Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
   */
  enableStatistics?: boolean | undefined;
  /**
   * One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
   */
  enableWritePageIndex?: boolean | undefined;
  /**
   * Parquet tools can use the checksum of a Parquet page to verify data integrity
   */
  enablePageChecksum?: boolean | undefined;
  /**
   * How frequently, in seconds, to clean up empty directories
   */
  emptyDirCleanupSec?: number | undefined;
  /**
   * Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.
   */
  directoryBatchSize?: number | undefined;
  /**
   * Storage location for files that fail to reach their final destination after maximum retries are exceeded
   */
  deadletterPath?: string | undefined;
  /**
   * The maximum number of times a file will attempt to move to its final destination before being dead-lettered
   */
  maxRetryNum?: number | undefined;
  /**
   * Binds 'bucket' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'bucket' at runtime.
   */
  __template_bucket?: string | undefined;
  /**
   * Binds 'region' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'region' at runtime.
   */
  __template_region?: string | undefined;
  /**
   * Binds 'awsSecretKey' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'awsSecretKey' at runtime.
   */
  __template_awsSecretKey?: string | undefined;
  /**
   * Binds 'assumeRoleArn' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'assumeRoleArn' at runtime.
   */
  __template_assumeRoleArn?: string | undefined;
  /**
   * Binds 'assumeRoleExternalId' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'assumeRoleExternalId' at runtime.
   */
  __template_assumeRoleExternalId?: string | undefined;
  /**
   * Binds 'format' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'format' at runtime.
   */
  __template_format?: string | undefined;
  /**
   * Binds 'awsApiKey' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'awsApiKey' at runtime.
   */
  __template_awsApiKey?: string | undefined;
};

export type CreateOutputOutputFilesystem = {
  /**
   * Unique ID for this output
   */
  id: string;
  type: "filesystem";
  /**
   * Pipeline to process data before sending out to this output
   */
  pipeline?: string | undefined;
  /**
   * Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
   */
  systemFields?: Array<string> | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Final destination for the output files
   */
  destPath: string;
  /**
   * Filesystem location in which to buffer files before compressing and moving to final destination. Use performant, stable storage.
   */
  stagePath?: string | undefined;
  /**
   * Add the Output ID value to staging location
   */
  addIdToStagePath?: boolean | undefined;
  /**
   * Remove empty staging directories after moving files
   */
  removeEmptyDirs?: boolean | undefined;
  /**
   * JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.
   */
  partitionExpr?: string | undefined;
  /**
   * Format of the output data
   */
  format?: models.DataFormatOptions | undefined;
  /**
   * JavaScript expression to define the output filename prefix (can be constant)
   */
  baseFileName?: string | undefined;
  /**
   * JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
   */
  fileNameSuffix?: string | undefined;
  /**
   * Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
   */
  maxFileSizeMB?: number | undefined;
  /**
   * Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
   */
  maxFileOpenTimeSec?: number | undefined;
  /**
   * Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
   */
  maxFileIdleTimeSec?: number | undefined;
  /**
   * Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
   */
  maxOpenFiles?: number | undefined;
  /**
   * If set, this line will be written to the beginning of each output file
   */
  headerLine?: string | undefined;
  /**
   * Buffer size used to write to a file
   */
  writeHighWaterMark?: number | undefined;
  /**
   * How to handle events when all receivers are exerting backpressure
   */
  onBackpressure?: models.BackpressureBehaviorOptions1 | undefined;
  /**
   * If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
   */
  deadletterEnabled?: boolean | undefined;
  /**
   * How to handle events when disk space is below the global 'Min free disk space' limit
   */
  onDiskFullBackpressure?: models.DiskSpaceProtectionOptions | undefined;
  /**
   * Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.
   */
  forceCloseOnShutdown?: boolean | undefined;
  retrySettings?: models.RetrySettingsType | undefined;
  description?: string | undefined;
  /**
   * Data compression format to apply to HTTP content before it is delivered
   */
  compress?: models.CompressionOptions2 | undefined;
  /**
   * Compression level to apply before moving files to final destination
   */
  compressionLevel?: models.CompressionLevelOptions | undefined;
  /**
   * Automatically calculate the schema based on the events of each Parquet file generated
   */
  automaticSchema?: boolean | undefined;
  /**
   * To add a new schema, navigate to Processing > Knowledge > Parquet Schemas
   */
  parquetSchema?: string | undefined;
  /**
   * Determines which data types are supported and how they are represented
   */
  parquetVersion?: models.ParquetVersionOptions | undefined;
  /**
   * Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
   */
  parquetDataPageVersion?: models.DataPageVersionOptions | undefined;
  /**
   * The number of rows that every group will contain. The final group can contain a smaller number of rows.
   */
  parquetRowGroupLength?: number | undefined;
  /**
   * Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
   */
  parquetPageSize?: string | undefined;
  /**
   * Log up to 3 rows that @{product} skips due to data mismatch
   */
  shouldLogInvalidRows?: boolean | undefined;
  /**
   * The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
   */
  keyValueMetadata?: Array<models.ItemsTypeKeyValueMetadata> | undefined;
  /**
   * Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
   */
  enableStatistics?: boolean | undefined;
  /**
   * One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
   */
  enableWritePageIndex?: boolean | undefined;
  /**
   * Parquet tools can use the checksum of a Parquet page to verify data integrity
   */
  enablePageChecksum?: boolean | undefined;
  /**
   * How frequently, in seconds, to clean up empty directories
   */
  emptyDirCleanupSec?: number | undefined;
  /**
   * Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.
   */
  directoryBatchSize?: number | undefined;
  /**
   * Storage location for files that fail to reach their final destination after maximum retries are exceeded
   */
  deadletterPath?: string | undefined;
  /**
   * The maximum number of times a file will attempt to move to its final destination before being dead-lettered
   */
  maxRetryNum?: number | undefined;
  /**
   * Binds 'format' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'format' at runtime.
   */
  __template_format?: string | undefined;
};

export type CreateOutputPqControlsSignalfx = {};

export type CreateOutputOutputSignalfx = {
  /**
   * Unique ID for this output
   */
  id: string;
  type: "signalfx";
  /**
   * Pipeline to process data before sending out to this output
   */
  pipeline?: string | undefined;
  /**
   * Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
   */
  systemFields?: Array<string> | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
   */
  authType?: models.AuthenticationMethodOptionsAuthTokensItems | undefined;
  /**
   * SignalFx realm name, e.g. "us0". For a complete list of available SignalFx realm names, please check [here](https://docs.splunk.com/observability/en/get-started/service-description.html#sd-regions).
   */
  realm: string;
  /**
   * Maximum number of ongoing requests before blocking
   */
  concurrency?: number | undefined;
  /**
   * Maximum size, in KB, of the request body
   */
  maxPayloadSizeKB?: number | undefined;
  /**
   * Maximum number of events to include in the request body. Default is 0 (unlimited).
   */
  maxPayloadEvents?: number | undefined;
  /**
   * Compress the payload body before sending
   */
  compress?: boolean | undefined;
  /**
   * Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
   *
   * @remarks
   *         Enabled by default. When this setting is also present in TLS Settings (Client Side),
   *         that value will take precedence.
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * Amount of time, in seconds, to wait for a request to complete before canceling it
   */
  timeoutSec?: number | undefined;
  /**
   * Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
   */
  flushPeriodSec?: number | undefined;
  /**
   * Headers to add to all events
   */
  extraHttpHeaders?: Array<models.ItemsTypeExtraHttpHeaders> | undefined;
  /**
   * Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
   */
  useRoundRobinDns?: boolean | undefined;
  /**
   * Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
   */
  failedRequestLoggingMode?: models.FailedRequestLoggingModeOptions | undefined;
  /**
   * List of headers that are safe to log in plain text
   */
  safeHeaders?: Array<string> | undefined;
  /**
   * Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
   */
  responseRetrySettings?:
    | Array<models.ItemsTypeResponseRetrySettings>
    | undefined;
  timeoutRetrySettings?: models.TimeoutRetrySettingsType | undefined;
  /**
   * Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
   */
  responseHonorRetryAfterHeader?: boolean | undefined;
  /**
   * How to handle events when all receivers are exerting backpressure
   */
  onBackpressure?: models.BackpressureBehaviorOptions | undefined;
  description?: string | undefined;
  /**
   * SignalFx API access token (see [here](https://docs.signalfx.com/en/latest/admin-guide/tokens.html#working-with-access-tokens))
   */
  token?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
  /**
   * Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
   */
  pqStrictOrdering?: boolean | undefined;
  /**
   * Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
   */
  pqRatePerSec?: number | undefined;
  /**
   * In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
   */
  pqMode?: models.ModeOptions | undefined;
  /**
   * The maximum number of events to hold in memory before writing the events to disk
   */
  pqMaxBufferSize?: number | undefined;
  /**
   * How long (in seconds) to wait for backpressure to resolve before engaging the queue
   */
  pqMaxBackpressureSec?: number | undefined;
  /**
   * The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
   */
  pqMaxFileSize?: string | undefined;
  /**
   * The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
   */
  pqMaxSize?: string | undefined;
  /**
   * The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
   */
  pqPath?: string | undefined;
  /**
   * Codec to use to compress the persisted data
   */
  pqCompress?: models.CompressionOptionsPq | undefined;
  /**
   * How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
   */
  pqOnBackpressure?: models.QueueFullBehaviorOptions | undefined;
  pqControls?: CreateOutputPqControlsSignalfx | undefined;
};

export type CreateOutputPqControlsWavefront = {};

export type CreateOutputOutputWavefront = {
  /**
   * Unique ID for this output
   */
  id: string;
  type: "wavefront";
  /**
   * Pipeline to process data before sending out to this output
   */
  pipeline?: string | undefined;
  /**
   * Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
   */
  systemFields?: Array<string> | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
   */
  authType?: models.AuthenticationMethodOptionsAuthTokensItems | undefined;
  /**
   * WaveFront domain name, e.g. "longboard"
   */
  domain: string;
  /**
   * Maximum number of ongoing requests before blocking
   */
  concurrency?: number | undefined;
  /**
   * Maximum size, in KB, of the request body
   */
  maxPayloadSizeKB?: number | undefined;
  /**
   * Maximum number of events to include in the request body. Default is 0 (unlimited).
   */
  maxPayloadEvents?: number | undefined;
  /**
   * Compress the payload body before sending
   */
  compress?: boolean | undefined;
  /**
   * Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
   *
   * @remarks
   *         Enabled by default. When this setting is also present in TLS Settings (Client Side),
   *         that value will take precedence.
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * Amount of time, in seconds, to wait for a request to complete before canceling it
   */
  timeoutSec?: number | undefined;
  /**
   * Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
   */
  flushPeriodSec?: number | undefined;
  /**
   * Headers to add to all events
   */
  extraHttpHeaders?: Array<models.ItemsTypeExtraHttpHeaders> | undefined;
  /**
   * Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
   */
  useRoundRobinDns?: boolean | undefined;
  /**
   * Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
   */
  failedRequestLoggingMode?: models.FailedRequestLoggingModeOptions | undefined;
  /**
   * List of headers that are safe to log in plain text
   */
  safeHeaders?: Array<string> | undefined;
  /**
   * Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
   */
  responseRetrySettings?:
    | Array<models.ItemsTypeResponseRetrySettings>
    | undefined;
  timeoutRetrySettings?: models.TimeoutRetrySettingsType | undefined;
  /**
   * Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
   */
  responseHonorRetryAfterHeader?: boolean | undefined;
  /**
   * How to handle events when all receivers are exerting backpressure
   */
  onBackpressure?: models.BackpressureBehaviorOptions | undefined;
  description?: string | undefined;
  /**
   * WaveFront API authentication token (see [here](https://docs.wavefront.com/wavefront_api.html#generating-an-api-token))
   */
  token?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
  /**
   * Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
   */
  pqStrictOrdering?: boolean | undefined;
  /**
   * Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
   */
  pqRatePerSec?: number | undefined;
  /**
   * In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
   */
  pqMode?: models.ModeOptions | undefined;
  /**
   * The maximum number of events to hold in memory before writing the events to disk
   */
  pqMaxBufferSize?: number | undefined;
  /**
   * How long (in seconds) to wait for backpressure to resolve before engaging the queue
   */
  pqMaxBackpressureSec?: number | undefined;
  /**
   * The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
   */
  pqMaxFileSize?: string | undefined;
  /**
   * The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
   */
  pqMaxSize?: string | undefined;
  /**
   * The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
   */
  pqPath?: string | undefined;
  /**
   * Codec to use to compress the persisted data
   */
  pqCompress?: models.CompressionOptionsPq | undefined;
  /**
   * How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
   */
  pqOnBackpressure?: models.QueueFullBehaviorOptions | undefined;
  pqControls?: CreateOutputPqControlsWavefront | undefined;
};

export type CreateOutputPqControlsTcpjson = {};

export type CreateOutputOutputTcpjson = {
  /**
   * Unique ID for this output
   */
  id: string;
  type: "tcpjson";
  /**
   * Pipeline to process data before sending out to this output
   */
  pipeline?: string | undefined;
  /**
   * Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
   */
  systemFields?: Array<string> | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Use load-balanced destinations
   */
  loadBalanced?: boolean | undefined;
  /**
   * Codec to use to compress the data before sending
   */
  compression?: models.CompressionOptions1 | undefined;
  /**
   * Use to troubleshoot issues with sending data
   */
  logFailedRequests?: boolean | undefined;
  /**
   * Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.
   */
  throttleRatePerSec?: string | undefined;
  tls?: models.TlsSettingsClientSideTypeKafkaSchemaRegistry | undefined;
  /**
   * Amount of time (milliseconds) to wait for the connection to establish before retrying
   */
  connectionTimeout?: number | undefined;
  /**
   * Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead
   */
  writeTimeout?: number | undefined;
  /**
   * The number of minutes before the internally generated authentication token expires, valid values between 1 and 60
   */
  tokenTTLMinutes?: number | undefined;
  /**
   * Upon connection, send a header-like record containing the auth token and other metadata.This record will not contain an actual event – only subsequent records will.
   */
  sendHeader?: boolean | undefined;
  /**
   * How to handle events when all receivers are exerting backpressure
   */
  onBackpressure?: models.BackpressureBehaviorOptions | undefined;
  /**
   * Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
   */
  authType?: models.AuthenticationMethodOptionsAuthTokensItems | undefined;
  description?: string | undefined;
  /**
   * The hostname of the receiver
   */
  host?: string | undefined;
  /**
   * The port to connect to on the provided host
   */
  port?: number | undefined;
  /**
   * Exclude all IPs of the current host from the list of any resolved hostnames
   */
  excludeSelf?: boolean | undefined;
  /**
   * Set of hosts to load-balance data to
   */
  hosts?: Array<models.ItemsTypeHosts> | undefined;
  /**
   * The interval in which to re-resolve any hostnames and pick up destinations from A records
   */
  dnsResolvePeriodSec?: number | undefined;
  /**
   * How far back in time to keep traffic stats for load balancing purposes
   */
  loadBalanceStatsPeriodSec?: number | undefined;
  /**
   * Maximum number of concurrent connections (per Worker Process). A random set of IPs will be picked on every DNS resolution period. Use 0 for unlimited.
   */
  maxConcurrentSenders?: number | undefined;
  /**
   * Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
   */
  pqStrictOrdering?: boolean | undefined;
  /**
   * Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
   */
  pqRatePerSec?: number | undefined;
  /**
   * In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
   */
  pqMode?: models.ModeOptions | undefined;
  /**
   * The maximum number of events to hold in memory before writing the events to disk
   */
  pqMaxBufferSize?: number | undefined;
  /**
   * How long (in seconds) to wait for backpressure to resolve before engaging the queue
   */
  pqMaxBackpressureSec?: number | undefined;
  /**
   * The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
   */
  pqMaxFileSize?: string | undefined;
  /**
   * The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
   */
  pqMaxSize?: string | undefined;
  /**
   * The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
   */
  pqPath?: string | undefined;
  /**
   * Codec to use to compress the persisted data
   */
  pqCompress?: models.CompressionOptionsPq | undefined;
  /**
   * How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
   */
  pqOnBackpressure?: models.QueueFullBehaviorOptions | undefined;
  pqControls?: CreateOutputPqControlsTcpjson | undefined;
  /**
   * Optional authentication token to include as part of the connection header
   */
  authToken?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
  /**
   * Binds 'host' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'host' at runtime.
   */
  __template_host?: string | undefined;
  /**
   * Binds 'port' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'port' at runtime.
   */
  __template_port?: string | undefined;
};

export type CreateOutputPqControlsWizHec = {};

export type CreateOutputOutputWizHec = {
  /**
   * Unique ID for this output
   */
  id: string;
  type: "wiz_hec";
  /**
   * Pipeline to process data before sending out to this output
   */
  pipeline?: string | undefined;
  /**
   * Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
   */
  systemFields?: Array<string> | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * In the Splunk app, define which Splunk processing queue to send the events after HEC processing.
   */
  nextQueue?: string | undefined;
  /**
   * In the Splunk app, set the value of _TCP_ROUTING for events that do not have _ctrl._TCP_ROUTING set.
   */
  tcpRouting?: string | undefined;
  tls?: models.TlsSettingsClientSideType1 | undefined;
  /**
   * Maximum number of ongoing requests before blocking
   */
  concurrency?: number | undefined;
  /**
   * Maximum size, in KB, of the request body
   */
  maxPayloadSizeKB?: number | undefined;
  /**
   * Maximum number of events to include in the request body. Default is 0 (unlimited).
   */
  maxPayloadEvents?: number | undefined;
  /**
   * Compress the payload body before sending
   */
  compress?: boolean | undefined;
  /**
   * Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
   *
   * @remarks
   *         Enabled by default. When this setting is also present in TLS Settings (Client Side),
   *         that value will take precedence.
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * Amount of time, in seconds, to wait for a request to complete before canceling it
   */
  timeoutSec?: number | undefined;
  /**
   * Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
   */
  flushPeriodSec?: number | undefined;
  /**
   * Headers to add to all events
   */
  extraHttpHeaders?: Array<models.ItemsTypeExtraHttpHeaders> | undefined;
  /**
   * Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
   */
  failedRequestLoggingMode?: models.FailedRequestLoggingModeOptions | undefined;
  /**
   * List of headers that are safe to log in plain text
   */
  safeHeaders?: Array<string> | undefined;
  /**
   * Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
   */
  authType?: models.AuthenticationMethodOptionsAuthTokensItems | undefined;
  /**
   * Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
   */
  responseRetrySettings?:
    | Array<models.ItemsTypeResponseRetrySettings>
    | undefined;
  timeoutRetrySettings?: models.TimeoutRetrySettingsType | undefined;
  /**
   * Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
   */
  responseHonorRetryAfterHeader?: boolean | undefined;
  /**
   * How to handle events when all receivers are exerting backpressure
   */
  onBackpressure?: models.BackpressureBehaviorOptions | undefined;
  /**
   * The unique identifier for the specific Cribl connector defined in your Wiz Settings. This is used to cross-validate the bearer token and ensure traffic is originating from the authorized integration.
   */
  wiz_connector_id: string;
  /**
   * Your Wiz deployment environment.
   */
  wiz_environment: string;
  /**
   * Your Wiz deployment data center (e.g., us1, us8, eu1). From Tenant Info → Data Center and Regions → Tenant Data Center in your Wiz console.
   */
  data_center: string;
  wiz_sourcetype: string;
  description?: string | undefined;
  /**
   * Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
   */
  pqStrictOrdering?: boolean | undefined;
  /**
   * Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
   */
  pqRatePerSec?: number | undefined;
  /**
   * In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
   */
  pqMode?: models.ModeOptions | undefined;
  /**
   * The maximum number of events to hold in memory before writing the events to disk
   */
  pqMaxBufferSize?: number | undefined;
  /**
   * How long (in seconds) to wait for backpressure to resolve before engaging the queue
   */
  pqMaxBackpressureSec?: number | undefined;
  /**
   * The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
   */
  pqMaxFileSize?: string | undefined;
  /**
   * The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
   */
  pqMaxSize?: string | undefined;
  /**
   * The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
   */
  pqPath?: string | undefined;
  /**
   * Codec to use to compress the persisted data
   */
  pqCompress?: models.CompressionOptionsPq | undefined;
  /**
   * How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
   */
  pqOnBackpressure?: models.QueueFullBehaviorOptions | undefined;
  pqControls?: CreateOutputPqControlsWizHec | undefined;
  /**
   * Wiz Defend Auth token
   */
  token?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
  /**
   * Binds 'wiz_environment' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'wiz_environment' at runtime.
   */
  __template_wiz_environment?: string | undefined;
  /**
   * Binds 'data_center' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'data_center' at runtime.
   */
  __template_data_center?: string | undefined;
  /**
   * Binds 'wiz_sourcetype' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'wiz_sourcetype' at runtime.
   */
  __template_wiz_sourcetype?: string | undefined;
};

export type CreateOutputUrlSplunkHec = {
  /**
   * URL to a Splunk HEC endpoint to send events to, e.g., http://localhost:8088/services/collector/event
   */
  url: string;
  /**
   * Assign a weight (>0) to each endpoint to indicate its traffic-handling capability
   */
  weight?: number | undefined;
  /**
   * Binds 'url' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'url' at runtime.
   */
  __template_url?: string | undefined;
};

export type CreateOutputPqControlsSplunkHec = {};

export type CreateOutputOutputSplunkHec = {
  /**
   * Unique ID for this output
   */
  id: string;
  type: "splunk_hec";
  /**
   * Pipeline to process data before sending out to this output
   */
  pipeline?: string | undefined;
  /**
   * Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
   */
  systemFields?: Array<string> | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Enable for optimal performance. Even if you have one hostname, it can expand to multiple IPs. If disabled, consider enabling round-robin DNS.
   */
  loadBalanced?: boolean | undefined;
  /**
   * In the Splunk app, define which Splunk processing queue to send the events after HEC processing.
   */
  nextQueue?: string | undefined;
  /**
   * In the Splunk app, set the value of _TCP_ROUTING for events that do not have _ctrl._TCP_ROUTING set.
   */
  tcpRouting?: string | undefined;
  tls?: models.TlsSettingsClientSideType1 | undefined;
  /**
   * Maximum number of ongoing requests before blocking
   */
  concurrency?: number | undefined;
  /**
   * Maximum size, in KB, of the request body
   */
  maxPayloadSizeKB?: number | undefined;
  /**
   * Maximum number of events to include in the request body. Default is 0 (unlimited).
   */
  maxPayloadEvents?: number | undefined;
  /**
   * Compress the payload body before sending
   */
  compress?: boolean | undefined;
  /**
   * Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
   *
   * @remarks
   *         Enabled by default. When this setting is also present in TLS Settings (Client Side),
   *         that value will take precedence.
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * Amount of time, in seconds, to wait for a request to complete before canceling it
   */
  timeoutSec?: number | undefined;
  /**
   * Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
   */
  flushPeriodSec?: number | undefined;
  /**
   * Headers to add to all events
   */
  extraHttpHeaders?: Array<models.ItemsTypeExtraHttpHeaders> | undefined;
  /**
   * Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
   */
  failedRequestLoggingMode?: models.FailedRequestLoggingModeOptions | undefined;
  /**
   * List of headers that are safe to log in plain text
   */
  safeHeaders?: Array<string> | undefined;
  /**
   * Output metrics in multiple-metric format, supported in Splunk 8.0 and above to allow multiple metrics in a single event.
   */
  enableMultiMetrics?: boolean | undefined;
  /**
   * Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
   */
  authType?: models.AuthenticationMethodOptionsAuthTokensItems | undefined;
  /**
   * Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
   */
  responseRetrySettings?:
    | Array<models.ItemsTypeResponseRetrySettings>
    | undefined;
  timeoutRetrySettings?: models.TimeoutRetrySettingsType | undefined;
  /**
   * Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
   */
  responseHonorRetryAfterHeader?: boolean | undefined;
  /**
   * How to handle events when all receivers are exerting backpressure
   */
  onBackpressure?: models.BackpressureBehaviorOptions | undefined;
  description?: string | undefined;
  /**
   * URL to a Splunk HEC endpoint to send events to, e.g., http://localhost:8088/services/collector/event
   */
  url?: string | undefined;
  /**
   * Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
   */
  useRoundRobinDns?: boolean | undefined;
  /**
   * Exclude all IPs of the current host from the list of any resolved hostnames
   */
  excludeSelf?: boolean | undefined;
  urls?: Array<CreateOutputUrlSplunkHec> | undefined;
  /**
   * The interval in which to re-resolve any hostnames and pick up destinations from A records
   */
  dnsResolvePeriodSec?: number | undefined;
  /**
   * How far back in time to keep traffic stats for load balancing purposes
   */
  loadBalanceStatsPeriodSec?: number | undefined;
  /**
   * Splunk HEC authentication token
   */
  token?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
  /**
   * Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
   */
  pqStrictOrdering?: boolean | undefined;
  /**
   * Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
   */
  pqRatePerSec?: number | undefined;
  /**
   * In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
   */
  pqMode?: models.ModeOptions | undefined;
  /**
   * The maximum number of events to hold in memory before writing the events to disk
   */
  pqMaxBufferSize?: number | undefined;
  /**
   * How long (in seconds) to wait for backpressure to resolve before engaging the queue
   */
  pqMaxBackpressureSec?: number | undefined;
  /**
   * The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
   */
  pqMaxFileSize?: string | undefined;
  /**
   * The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
   */
  pqMaxSize?: string | undefined;
  /**
   * The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
   */
  pqPath?: string | undefined;
  /**
   * Codec to use to compress the persisted data
   */
  pqCompress?: models.CompressionOptionsPq | undefined;
  /**
   * How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
   */
  pqOnBackpressure?: models.QueueFullBehaviorOptions | undefined;
  pqControls?: CreateOutputPqControlsSplunkHec | undefined;
  /**
   * Binds 'url' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'url' at runtime.
   */
  __template_url?: string | undefined;
};

export type CreateOutputAuthToken = {
  /**
   * Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
   */
  authType?: models.AuthenticationMethodOptionsAuthTokensItems | undefined;
  /**
   * Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted.
   */
  authToken?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
};

/**
 * List of configurations to set up indexer discovery in Splunk Indexer clustering environment.
 */
export type CreateOutputIndexerDiscoveryConfigs = {
  /**
   * Clustering site of the indexers from where indexers need to be discovered. In case of single site cluster, it defaults to 'default' site.
   */
  site: string;
  /**
   * Full URI of Splunk cluster manager (scheme://host:port). Example: https://managerAddress:8089
   */
  masterUri: string;
  /**
   * Time interval, in seconds, between two consecutive indexer list fetches from cluster manager
   */
  refreshIntervalSec: number;
  /**
   * During indexer discovery, reject cluster manager certificates that are not authorized by the system's CA. Disable to allow untrusted (for example, self-signed) certificates.
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * Tokens required to authenticate to cluster manager for indexer discovery
   */
  authTokens?: Array<CreateOutputAuthToken> | undefined;
  /**
   * Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
   */
  authType?: models.AuthenticationMethodOptionsAuthTokensItems | undefined;
  /**
   * Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted.
   */
  authToken?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
};

export type CreateOutputPqControlsSplunkLb = {};

export type CreateOutputOutputSplunkLb = {
  /**
   * Unique ID for this output
   */
  id: string;
  type: "splunk_lb";
  /**
   * Pipeline to process data before sending out to this output
   */
  pipeline?: string | undefined;
  /**
   * Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
   */
  systemFields?: Array<string> | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * The interval in which to re-resolve any hostnames and pick up destinations from A records
   */
  dnsResolvePeriodSec?: number | undefined;
  /**
   * How far back in time to keep traffic stats for load balancing purposes
   */
  loadBalanceStatsPeriodSec?: number | undefined;
  /**
   * Maximum number of concurrent connections (per Worker Process). A random set of IPs will be picked on every DNS resolution period. Use 0 for unlimited.
   */
  maxConcurrentSenders?: number | undefined;
  /**
   * How to serialize nested fields into index-time fields
   */
  nestedFields?: models.NestedFieldSerializationOptions | undefined;
  /**
   * Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.
   */
  throttleRatePerSec?: string | undefined;
  /**
   * Amount of time (milliseconds) to wait for the connection to establish before retrying
   */
  connectionTimeout?: number | undefined;
  /**
   * Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead
   */
  writeTimeout?: number | undefined;
  tls?: models.TlsSettingsClientSideTypeKafkaSchemaRegistry | undefined;
  /**
   * Output metrics in multiple-metric format in a single event. Supported in Splunk 8.0 and above.
   */
  enableMultiMetrics?: boolean | undefined;
  /**
   * Check if indexer is shutting down and stop sending data. This helps minimize data loss during shutdown.
   */
  enableACK?: boolean | undefined;
  /**
   * Use to troubleshoot issues with sending data
   */
  logFailedRequests?: boolean | undefined;
  /**
   * The highest S2S protocol version to advertise during handshake
   */
  maxS2Sversion?: models.MaxS2SVersionOptions | undefined;
  /**
   * How to handle events when all receivers are exerting backpressure
   */
  onBackpressure?: models.BackpressureBehaviorOptions | undefined;
  /**
   * Automatically discover indexers in indexer clustering environment.
   */
  indexerDiscovery?: boolean | undefined;
  /**
   * How long (in milliseconds) each LB endpoint can report blocked before the Destination reports unhealthy, blocking the sender. (Grace period for fluctuations.) Use 0 to disable; max 1 minute.
   */
  senderUnhealthyTimeAllowance?: number | undefined;
  /**
   * Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
   */
  authType?: models.AuthenticationMethodOptionsAuthTokensItems | undefined;
  description?: string | undefined;
  /**
   * Maximum number of times healthcheck can fail before we close connection. If set to 0 (disabled), and the connection to Splunk is forcibly closed, some data loss might occur.
   */
  maxFailedHealthChecks?: number | undefined;
  /**
   * Controls whether the sender should send compressed data to the server. Select 'Disabled' to reject compressed connections or 'Always' to ignore server's configuration and send compressed data.
   */
  compress?: models.CompressionOptions | undefined;
  /**
   * List of configurations to set up indexer discovery in Splunk Indexer clustering environment.
   */
  indexerDiscoveryConfigs?: CreateOutputIndexerDiscoveryConfigs | undefined;
  /**
   * Exclude all IPs of the current host from the list of any resolved hostnames
   */
  excludeSelf?: boolean | undefined;
  /**
   * Set of Splunk indexers to load-balance data to.
   */
  hosts: Array<models.ItemsTypeHosts>;
  /**
   * Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
   */
  pqStrictOrdering?: boolean | undefined;
  /**
   * Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
   */
  pqRatePerSec?: number | undefined;
  /**
   * In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
   */
  pqMode?: models.ModeOptions | undefined;
  /**
   * The maximum number of events to hold in memory before writing the events to disk
   */
  pqMaxBufferSize?: number | undefined;
  /**
   * How long (in seconds) to wait for backpressure to resolve before engaging the queue
   */
  pqMaxBackpressureSec?: number | undefined;
  /**
   * The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
   */
  pqMaxFileSize?: string | undefined;
  /**
   * The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
   */
  pqMaxSize?: string | undefined;
  /**
   * The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
   */
  pqPath?: string | undefined;
  /**
   * Codec to use to compress the persisted data
   */
  pqCompress?: models.CompressionOptionsPq | undefined;
  /**
   * How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
   */
  pqOnBackpressure?: models.QueueFullBehaviorOptions | undefined;
  pqControls?: CreateOutputPqControlsSplunkLb | undefined;
  /**
   * Shared secret token to use when establishing a connection to a Splunk indexer.
   */
  authToken?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
};

export type CreateOutputPqControlsSplunk = {};

export type CreateOutputOutputSplunk = {
  /**
   * Unique ID for this output
   */
  id: string;
  type: "splunk";
  /**
   * Pipeline to process data before sending out to this output
   */
  pipeline?: string | undefined;
  /**
   * Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
   */
  systemFields?: Array<string> | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * The hostname of the receiver
   */
  host: string;
  /**
   * The port to connect to on the provided host
   */
  port: number;
  /**
   * How to serialize nested fields into index-time fields
   */
  nestedFields?: models.NestedFieldSerializationOptions | undefined;
  /**
   * Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.
   */
  throttleRatePerSec?: string | undefined;
  /**
   * Amount of time (milliseconds) to wait for the connection to establish before retrying
   */
  connectionTimeout?: number | undefined;
  /**
   * Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead
   */
  writeTimeout?: number | undefined;
  tls?: models.TlsSettingsClientSideTypeKafkaSchemaRegistry | undefined;
  /**
   * Output metrics in multiple-metric format in a single event. Supported in Splunk 8.0 and above.
   */
  enableMultiMetrics?: boolean | undefined;
  /**
   * Check if indexer is shutting down and stop sending data. This helps minimize data loss during shutdown.
   */
  enableACK?: boolean | undefined;
  /**
   * Use to troubleshoot issues with sending data
   */
  logFailedRequests?: boolean | undefined;
  /**
   * The highest S2S protocol version to advertise during handshake
   */
  maxS2Sversion?: models.MaxS2SVersionOptions | undefined;
  /**
   * How to handle events when all receivers are exerting backpressure
   */
  onBackpressure?: models.BackpressureBehaviorOptions | undefined;
  /**
   * Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
   */
  authType?: models.AuthenticationMethodOptionsAuthTokensItems | undefined;
  description?: string | undefined;
  /**
   * Maximum number of times healthcheck can fail before we close connection. If set to 0 (disabled), and the connection to Splunk is forcibly closed, some data loss might occur.
   */
  maxFailedHealthChecks?: number | undefined;
  /**
   * Controls whether the sender should send compressed data to the server. Select 'Disabled' to reject compressed connections or 'Always' to ignore server's configuration and send compressed data.
   */
  compress?: models.CompressionOptions | undefined;
  /**
   * Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
   */
  pqStrictOrdering?: boolean | undefined;
  /**
   * Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
   */
  pqRatePerSec?: number | undefined;
  /**
   * In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
   */
  pqMode?: models.ModeOptions | undefined;
  /**
   * The maximum number of events to hold in memory before writing the events to disk
   */
  pqMaxBufferSize?: number | undefined;
  /**
   * How long (in seconds) to wait for backpressure to resolve before engaging the queue
   */
  pqMaxBackpressureSec?: number | undefined;
  /**
   * The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
   */
  pqMaxFileSize?: string | undefined;
  /**
   * The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
   */
  pqMaxSize?: string | undefined;
  /**
   * The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
   */
  pqPath?: string | undefined;
  /**
   * Codec to use to compress the persisted data
   */
  pqCompress?: models.CompressionOptionsPq | undefined;
  /**
   * How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
   */
  pqOnBackpressure?: models.QueueFullBehaviorOptions | undefined;
  pqControls?: CreateOutputPqControlsSplunk | undefined;
  /**
   * Shared secret token to use when establishing a connection to a Splunk indexer.
   */
  authToken?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
  /**
   * Binds 'host' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'host' at runtime.
   */
  __template_host?: string | undefined;
  /**
   * Binds 'port' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'port' at runtime.
   */
  __template_port?: string | undefined;
};

/**
 * The network protocol to use for sending out syslog messages
 */
export const CreateOutputProtocolSyslog = {
  /**
   * TCP
   */
  Tcp: "tcp",
  /**
   * UDP
   */
  Udp: "udp",
} as const;
/**
 * The network protocol to use for sending out syslog messages
 */
export type CreateOutputProtocolSyslog = OpenEnum<
  typeof CreateOutputProtocolSyslog
>;

/**
 * Default value for message facility. Will be overwritten by value of __facility if set. Defaults to user.
 */
export const CreateOutputFacility = {
  Zero: 0,
  One: 1,
  Two: 2,
  Three: 3,
  Four: 4,
  Five: 5,
  Six: 6,
  Seven: 7,
  Eight: 8,
  Nine: 9,
  Ten: 10,
  Eleven: 11,
  Twelve: 12,
  Thirteen: 13,
  Fourteen: 14,
  Fifteen: 15,
  Sixteen: 16,
  Seventeen: 17,
  Eighteen: 18,
  Nineteen: 19,
  Twenty: 20,
  TwentyOne: 21,
} as const;
/**
 * Default value for message facility. Will be overwritten by value of __facility if set. Defaults to user.
 */
export type CreateOutputFacility = OpenEnum<typeof CreateOutputFacility>;

/**
 * Default value for message severity. Will be overwritten by value of __severity if set. Defaults to notice.
 */
export const CreateOutputSeveritySyslog = {
  /**
   * emergency
   */
  Emergency: 0,
  /**
   * alert
   */
  Alert: 1,
  /**
   * critical
   */
  Critical: 2,
  /**
   * error
   */
  Error: 3,
  /**
   * warning
   */
  Warning: 4,
  /**
   * notice
   */
  Notice: 5,
  /**
   * info
   */
  Info: 6,
  /**
   * debug
   */
  Debug: 7,
} as const;
/**
 * Default value for message severity. Will be overwritten by value of __severity if set. Defaults to notice.
 */
export type CreateOutputSeveritySyslog = OpenEnum<
  typeof CreateOutputSeveritySyslog
>;

/**
 * The syslog message format depending on the receiver's support
 */
export const CreateOutputMessageFormat = {
  /**
   * RFC3164
   */
  Rfc3164: "rfc3164",
  /**
   * RFC5424
   */
  Rfc5424: "rfc5424",
} as const;
/**
 * The syslog message format depending on the receiver's support
 */
export type CreateOutputMessageFormat = OpenEnum<
  typeof CreateOutputMessageFormat
>;

/**
 * Timestamp format to use when serializing event's time field
 */
export const CreateOutputTimestampFormat = {
  /**
   * Syslog
   */
  Syslog: "syslog",
  /**
   * ISO8601
   */
  Iso8601: "iso8601",
} as const;
/**
 * Timestamp format to use when serializing event's time field
 */
export type CreateOutputTimestampFormat = OpenEnum<
  typeof CreateOutputTimestampFormat
>;

export type CreateOutputPqControlsSyslog = {};

export type CreateOutputOutputSyslog = {
  /**
   * Unique ID for this output
   */
  id: string;
  type: "syslog";
  /**
   * Pipeline to process data before sending out to this output
   */
  pipeline?: string | undefined;
  /**
   * Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
   */
  systemFields?: Array<string> | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * The network protocol to use for sending out syslog messages
   */
  protocol?: CreateOutputProtocolSyslog | undefined;
  /**
   * Default value for message facility. Will be overwritten by value of __facility if set. Defaults to user.
   */
  facility?: CreateOutputFacility | undefined;
  /**
   * Default value for message severity. Will be overwritten by value of __severity if set. Defaults to notice.
   */
  severity?: CreateOutputSeveritySyslog | undefined;
  /**
   * Default name for device or application that originated the message. Defaults to Cribl, but will be overwritten by value of __appname if set.
   */
  appName?: string | undefined;
  /**
   * The syslog message format depending on the receiver's support
   */
  messageFormat?: CreateOutputMessageFormat | undefined;
  /**
   * Timestamp format to use when serializing event's time field
   */
  timestampFormat?: CreateOutputTimestampFormat | undefined;
  /**
   * Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.
   */
  throttleRatePerSec?: string | undefined;
  /**
   * Prefix messages with the byte count of the message. If disabled, no prefix will be set, and the message will be appended with a \n.
   */
  octetCountFraming?: boolean | undefined;
  /**
   * Use to troubleshoot issues with sending data
   */
  logFailedRequests?: boolean | undefined;
  description?: string | undefined;
  /**
   * For optimal performance, enable load balancing even if you have one hostname, as it can expand to multiple IPs.  If this setting is disabled, consider enabling round-robin DNS.
   */
  loadBalanced?: boolean | undefined;
  /**
   * The hostname of the receiver
   */
  host?: string | undefined;
  /**
   * The port to connect to on the provided host
   */
  port?: number | undefined;
  /**
   * Exclude all IPs of the current host from the list of any resolved hostnames
   */
  excludeSelf?: boolean | undefined;
  /**
   * Set of hosts to load-balance data to
   */
  hosts?: Array<models.ItemsTypeHosts> | undefined;
  /**
   * The interval in which to re-resolve any hostnames and pick up destinations from A records
   */
  dnsResolvePeriodSec?: number | undefined;
  /**
   * How far back in time to keep traffic stats for load balancing purposes
   */
  loadBalanceStatsPeriodSec?: number | undefined;
  /**
   * Maximum number of concurrent connections (per Worker Process). A random set of IPs will be picked on every DNS resolution period. Use 0 for unlimited.
   */
  maxConcurrentSenders?: number | undefined;
  /**
   * Amount of time (milliseconds) to wait for the connection to establish before retrying
   */
  connectionTimeout?: number | undefined;
  /**
   * Amount of time (milliseconds) to wait for a write to complete before assuming connection is dead
   */
  writeTimeout?: number | undefined;
  tls?: models.TlsSettingsClientSideTypeKafkaSchemaRegistry | undefined;
  /**
   * How to handle events when all receivers are exerting backpressure
   */
  onBackpressure?: models.BackpressureBehaviorOptions | undefined;
  /**
   * Maximum size of syslog messages. Make sure this value is less than or equal to the MTU to avoid UDP packet fragmentation.
   */
  maxRecordSize?: number | undefined;
  /**
   * How often to resolve the destination hostname to an IP address. Ignored if the destination is an IP address. A value of 0 means every message sent will incur a DNS lookup.
   */
  udpDnsResolvePeriodSec?: number | undefined;
  /**
   * Send Syslog traffic using the original event's Source IP and port. To enable this, you must install the external `udp-sender` helper binary at `/usr/bin/udp-sender` on all Worker Nodes and grant it the `CAP_NET_RAW` capability.
   */
  enableIpSpoofing?: boolean | undefined;
  /**
   * Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
   */
  pqStrictOrdering?: boolean | undefined;
  /**
   * Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
   */
  pqRatePerSec?: number | undefined;
  /**
   * In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
   */
  pqMode?: models.ModeOptions | undefined;
  /**
   * The maximum number of events to hold in memory before writing the events to disk
   */
  pqMaxBufferSize?: number | undefined;
  /**
   * How long (in seconds) to wait for backpressure to resolve before engaging the queue
   */
  pqMaxBackpressureSec?: number | undefined;
  /**
   * The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
   */
  pqMaxFileSize?: string | undefined;
  /**
   * The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
   */
  pqMaxSize?: string | undefined;
  /**
   * The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
   */
  pqPath?: string | undefined;
  /**
   * Codec to use to compress the persisted data
   */
  pqCompress?: models.CompressionOptionsPq | undefined;
  /**
   * How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
   */
  pqOnBackpressure?: models.QueueFullBehaviorOptions | undefined;
  pqControls?: CreateOutputPqControlsSyslog | undefined;
  /**
   * Binds 'host' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'host' at runtime.
   */
  __template_host?: string | undefined;
  /**
   * Binds 'port' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'port' at runtime.
   */
  __template_port?: string | undefined;
};

export type CreateOutputOutputDevnull = {
  /**
   * Unique ID for this output
   */
  id: string;
  type: "devnull";
  /**
   * Pipeline to process data before sending out to this output
   */
  pipeline?: string | undefined;
  /**
   * Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
   */
  systemFields?: Array<string> | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
};

export const CreateOutputAuthType = {
  Oauth: "oauth",
} as const;
export type CreateOutputAuthType = OpenEnum<typeof CreateOutputAuthType>;

/**
 * Enter the data collection endpoint URL or the individual ID
 */
export const CreateOutputEndpointConfiguration = {
  /**
   * URL
   */
  Url: "url",
  /**
   * ID
   */
  Id: "ID",
} as const;
/**
 * Enter the data collection endpoint URL or the individual ID
 */
export type CreateOutputEndpointConfiguration = OpenEnum<
  typeof CreateOutputEndpointConfiguration
>;

export const CreateOutputFormatSentinel = {
  Ndjson: "ndjson",
  JsonArray: "json_array",
  Custom: "custom",
  Advanced: "advanced",
} as const;
export type CreateOutputFormatSentinel = OpenEnum<
  typeof CreateOutputFormatSentinel
>;

export type CreateOutputPqControlsSentinel = {};

export type CreateOutputOutputSentinel = {
  /**
   * Unique ID for this output
   */
  id: string;
  type: "sentinel";
  /**
   * Pipeline to process data before sending out to this output
   */
  pipeline?: string | undefined;
  /**
   * Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
   */
  systemFields?: Array<string> | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Disable to close the connection immediately after sending the outgoing request
   */
  keepAlive?: boolean | undefined;
  /**
   * Maximum number of ongoing requests before blocking
   */
  concurrency?: number | undefined;
  /**
   * Maximum size (KB) of the request body (defaults to the API's maximum limit of 1000 KB)
   */
  maxPayloadSizeKB?: number | undefined;
  /**
   * Maximum number of events to include in the request body. Default is 0 (unlimited).
   */
  maxPayloadEvents?: number | undefined;
  /**
   * Compress the payload body before sending
   */
  compress?: boolean | undefined;
  /**
   * Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
   *
   * @remarks
   *         Enabled by default. When this setting is also present in TLS Settings (Client Side),
   *         that value will take precedence.
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * Amount of time, in seconds, to wait for a request to complete before canceling it
   */
  timeoutSec?: number | undefined;
  /**
   * Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
   */
  flushPeriodSec?: number | undefined;
  /**
   * Headers to add to all events. You can also add headers dynamically on a per-event basis in the __headers field, as explained in [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook/#internal-fields).
   */
  extraHttpHeaders?: Array<models.ItemsTypeExtraHttpHeaders> | undefined;
  /**
   * Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
   */
  useRoundRobinDns?: boolean | undefined;
  /**
   * Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
   */
  failedRequestLoggingMode?: models.FailedRequestLoggingModeOptions | undefined;
  /**
   * List of headers that are safe to log in plain text
   */
  safeHeaders?: Array<string> | undefined;
  /**
   * Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
   */
  responseRetrySettings?:
    | Array<models.ItemsTypeResponseRetrySettings>
    | undefined;
  timeoutRetrySettings?: models.TimeoutRetrySettingsType | undefined;
  /**
   * Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
   */
  responseHonorRetryAfterHeader?: boolean | undefined;
  /**
   * How to handle events when all receivers are exerting backpressure
   */
  onBackpressure?: models.BackpressureBehaviorOptions | undefined;
  authType?: CreateOutputAuthType | undefined;
  /**
   * URL for OAuth
   */
  loginUrl: string;
  /**
   * Secret parameter value to pass in request body
   */
  secret: string;
  /**
   * JavaScript expression to compute the Client ID for the Azure application. Can be a constant.
   */
  client_id: string;
  /**
   * Scope to pass in the OAuth request
   */
  scope?: string | undefined;
  /**
   * Enter the data collection endpoint URL or the individual ID
   */
  endpointURLConfiguration: CreateOutputEndpointConfiguration;
  /**
   * Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
   */
  totalMemoryLimitKB?: number | undefined;
  description?: string | undefined;
  format?: CreateOutputFormatSentinel | undefined;
  /**
   * Expression to evaluate on events to generate output. Example: `raw=${_raw}`. See [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook#custom-format) for other examples. If empty, the full event is sent as stringified JSON.
   */
  customSourceExpression?: string | undefined;
  /**
   * Whether to drop events when the source expression evaluates to null
   */
  customDropWhenNull?: boolean | undefined;
  /**
   * Delimiter string to insert between individual events. Defaults to newline character.
   */
  customEventDelimiter?: string | undefined;
  /**
   * Content type to use for request. Defaults to application/x-ndjson. Any content types set in Advanced Settings > Extra HTTP headers will override this entry.
   */
  customContentType?: string | undefined;
  /**
   * Expression specifying how to format the payload for each batch. To reference the events to send, use the `${events}` variable. Example expression: `{ "items" : [${events}] }` would send the batch inside a JSON object.
   */
  customPayloadExpression?: string | undefined;
  /**
   * HTTP content-type header value
   */
  advancedContentType?: string | undefined;
  /**
   * Custom JavaScript code to format incoming event data accessible through the __e variable. The formatted content is added to (__e['__eventOut']) if available. Otherwise, the original event is serialized as JSON. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.
   */
  formatEventCode?: string | undefined;
  /**
   * Optional JavaScript code to format the payload sent to the Destination. The payload, containing a batch of formatted events, is accessible through the __e['payload'] variable. The formatted payload is returned in the __e['__payloadOut'] variable. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.
   */
  formatPayloadCode?: string | undefined;
  /**
   * Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
   */
  pqStrictOrdering?: boolean | undefined;
  /**
   * Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
   */
  pqRatePerSec?: number | undefined;
  /**
   * In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
   */
  pqMode?: models.ModeOptions | undefined;
  /**
   * The maximum number of events to hold in memory before writing the events to disk
   */
  pqMaxBufferSize?: number | undefined;
  /**
   * How long (in seconds) to wait for backpressure to resolve before engaging the queue
   */
  pqMaxBackpressureSec?: number | undefined;
  /**
   * The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
   */
  pqMaxFileSize?: string | undefined;
  /**
   * The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
   */
  pqMaxSize?: string | undefined;
  /**
   * The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
   */
  pqPath?: string | undefined;
  /**
   * Codec to use to compress the persisted data
   */
  pqCompress?: models.CompressionOptionsPq | undefined;
  /**
   * How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
   */
  pqOnBackpressure?: models.QueueFullBehaviorOptions | undefined;
  pqControls?: CreateOutputPqControlsSentinel | undefined;
  /**
   * URL to send events to. Can be overwritten by an event's __url field.
   */
  url?: string | undefined;
  /**
   * Immutable ID for the Data Collection Rule (DCR)
   */
  dcrID?: string | undefined;
  /**
   * Data collection endpoint (DCE) URL. In the format: `https://<Endpoint-Name>-<Identifier>.<Region>.ingest.monitor.azure.com`
   */
  dceEndpoint?: string | undefined;
  /**
   * The name of the stream (Sentinel table) in which to store the events
   */
  streamName?: string | undefined;
  /**
   * Binds 'loginUrl' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'loginUrl' at runtime.
   */
  __template_loginUrl?: string | undefined;
  /**
   * Binds 'secret' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'secret' at runtime.
   */
  __template_secret?: string | undefined;
  /**
   * Binds 'client_id' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'client_id' at runtime.
   */
  __template_client_id?: string | undefined;
  /**
   * Binds 'scope' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'scope' at runtime.
   */
  __template_scope?: string | undefined;
  /**
   * Binds 'url' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'url' at runtime.
   */
  __template_url?: string | undefined;
  /**
   * Binds 'dcrID' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'dcrID' at runtime.
   */
  __template_dcrID?: string | undefined;
  /**
   * Binds 'dceEndpoint' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'dceEndpoint' at runtime.
   */
  __template_dceEndpoint?: string | undefined;
  /**
   * Binds 'streamName' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'streamName' at runtime.
   */
  __template_streamName?: string | undefined;
};

/**
 * How to format events before sending out
 */
export const CreateOutputFormatWebhook = {
  /**
   * NDJSON (Newline Delimited JSON)
   */
  Ndjson: "ndjson",
  /**
   * JSON Array
   */
  JsonArray: "json_array",
  /**
   * Custom
   */
  Custom: "custom",
  /**
   * Advanced
   */
  Advanced: "advanced",
} as const;
/**
 * How to format events before sending out
 */
export type CreateOutputFormatWebhook = OpenEnum<
  typeof CreateOutputFormatWebhook
>;

/**
 * Authentication method to use for the HTTP request
 */
export const CreateOutputAuthenticationTypeWebhook = {
  /**
   * None
   */
  None: "none",
  /**
   * Basic
   */
  Basic: "basic",
  /**
   * Basic (credentials secret)
   */
  CredentialsSecret: "credentialsSecret",
  /**
   * Token
   */
  Token: "token",
  /**
   * Token (text secret)
   */
  TextSecret: "textSecret",
  /**
   * OAuth
   */
  Oauth: "oauth",
} as const;
/**
 * Authentication method to use for the HTTP request
 */
export type CreateOutputAuthenticationTypeWebhook = OpenEnum<
  typeof CreateOutputAuthenticationTypeWebhook
>;

export type CreateOutputPqControlsWebhook = {};

export type CreateOutputOauthParam = {
  /**
   * OAuth parameter name
   */
  name: string;
  /**
   * OAuth parameter value
   */
  value: string;
};

export type CreateOutputOauthHeader = {
  /**
   * OAuth header name
   */
  name: string;
  /**
   * OAuth header value
   */
  value: string;
};

export type CreateOutputUrlWebhook = {
  /**
   * URL of a webhook endpoint to send events to, such as http://localhost:10200
   */
  url: string;
  /**
   * Assign a weight (>0) to each endpoint to indicate its traffic-handling capability
   */
  weight?: number | undefined;
  /**
   * Binds 'url' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'url' at runtime.
   */
  __template_url?: string | undefined;
};

export type CreateOutputOutputWebhook = {
  /**
   * Unique ID for this output
   */
  id: string;
  type: "webhook";
  /**
   * Pipeline to process data before sending out to this output
   */
  pipeline?: string | undefined;
  /**
   * Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
   */
  systemFields?: Array<string> | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * The method to use when sending events
   */
  method?: models.MethodOptions | undefined;
  /**
   * How to format events before sending out
   */
  format?: CreateOutputFormatWebhook | undefined;
  /**
   * Disable to close the connection immediately after sending the outgoing request
   */
  keepAlive?: boolean | undefined;
  /**
   * Maximum number of ongoing requests before blocking
   */
  concurrency?: number | undefined;
  /**
   * Maximum size, in KB, of the request body
   */
  maxPayloadSizeKB?: number | undefined;
  /**
   * Maximum number of events to include in the request body. Default is 0 (unlimited).
   */
  maxPayloadEvents?: number | undefined;
  /**
   * Compress the payload body before sending
   */
  compress?: boolean | undefined;
  /**
   * Reject certificates not authorized by a CA in the CA certificate path or by another trusted CA (such as the system's).
   *
   * @remarks
   *         Enabled by default. When this setting is also present in TLS Settings (Client Side),
   *         that value will take precedence.
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * Amount of time, in seconds, to wait for a request to complete before canceling it
   */
  timeoutSec?: number | undefined;
  /**
   * Maximum time between requests. Small values could cause the payload size to be smaller than the configured Body size limit.
   */
  flushPeriodSec?: number | undefined;
  /**
   * Headers to add to all events. You can also add headers dynamically on a per-event basis in the __headers field, as explained in [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook/#internal-fields).
   */
  extraHttpHeaders?: Array<models.ItemsTypeExtraHttpHeaders> | undefined;
  /**
   * Enable round-robin DNS lookup. When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned. For optimal performance, consider enabling this setting for non-load balanced destinations.
   */
  useRoundRobinDns?: boolean | undefined;
  /**
   * Data to log when a request fails. All headers are redacted by default, unless listed as safe headers below.
   */
  failedRequestLoggingMode?: models.FailedRequestLoggingModeOptions | undefined;
  /**
   * List of headers that are safe to log in plain text
   */
  safeHeaders?: Array<string> | undefined;
  /**
   * Automatically retry after unsuccessful response status codes, such as 429 (Too Many Requests) or 503 (Service Unavailable)
   */
  responseRetrySettings?:
    | Array<models.ItemsTypeResponseRetrySettings>
    | undefined;
  timeoutRetrySettings?: models.TimeoutRetrySettingsType | undefined;
  /**
   * Honor any Retry-After header that specifies a delay (in seconds) no longer than 180 seconds after the retry request. @{product} limits the delay to 180 seconds, even if the Retry-After header specifies a longer delay. When enabled, takes precedence over user-configured retry options. When disabled, all Retry-After headers are ignored.
   */
  responseHonorRetryAfterHeader?: boolean | undefined;
  /**
   * How to handle events when all receivers are exerting backpressure
   */
  onBackpressure?: models.BackpressureBehaviorOptions | undefined;
  /**
   * Authentication method to use for the HTTP request
   */
  authType?: CreateOutputAuthenticationTypeWebhook | undefined;
  tls?: models.TlsSettingsClientSideType1 | undefined;
  /**
   * Maximum total size of the batches waiting to be sent. If left blank, defaults to 5 times the max body size (if set). If 0, no limit is enforced.
   */
  totalMemoryLimitKB?: number | undefined;
  /**
   * Enable for optimal performance. Even if you have one hostname, it can expand to multiple IPs. If disabled, consider enabling round-robin DNS.
   */
  loadBalanced?: boolean | undefined;
  description?: string | undefined;
  /**
   * Expression to evaluate on events to generate output. Example: `raw=${_raw}`. See [Cribl Docs](https://docs.cribl.io/stream/destinations-webhook#custom-format) for other examples. If empty, the full event is sent as stringified JSON.
   */
  customSourceExpression?: string | undefined;
  /**
   * Whether to drop events when the source expression evaluates to null
   */
  customDropWhenNull?: boolean | undefined;
  /**
   * Delimiter string to insert between individual events. Defaults to newline character.
   */
  customEventDelimiter?: string | undefined;
  /**
   * Content type to use for request. Defaults to application/x-ndjson. Any content types set in Advanced Settings > Extra HTTP headers will override this entry.
   */
  customContentType?: string | undefined;
  /**
   * Expression specifying how to format the payload for each batch. To reference the events to send, use the `${events}` variable. Example expression: `{ "items" : [${events}] }` would send the batch inside a JSON object.
   */
  customPayloadExpression?: string | undefined;
  /**
   * HTTP content-type header value
   */
  advancedContentType?: string | undefined;
  /**
   * Custom JavaScript code to format incoming event data accessible through the __e variable. The formatted content is added to (__e['__eventOut']) if available. Otherwise, the original event is serialized as JSON. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.
   */
  formatEventCode?: string | undefined;
  /**
   * Optional JavaScript code to format the payload sent to the Destination. The payload, containing a batch of formatted events, is accessible through the __e['payload'] variable. The formatted payload is returned in the __e['__payloadOut'] variable. Caution: This function is evaluated in an unprotected context, allowing you to execute almost any JavaScript code.
   */
  formatPayloadCode?: string | undefined;
  /**
   * Use FIFO (first in, first out) processing. Disable to forward new events to receivers before queue is flushed.
   */
  pqStrictOrdering?: boolean | undefined;
  /**
   * Throttling rate (in events per second) to impose while writing to Destinations from PQ. Defaults to 0, which disables throttling.
   */
  pqRatePerSec?: number | undefined;
  /**
   * In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem.
   */
  pqMode?: models.ModeOptions | undefined;
  /**
   * The maximum number of events to hold in memory before writing the events to disk
   */
  pqMaxBufferSize?: number | undefined;
  /**
   * How long (in seconds) to wait for backpressure to resolve before engaging the queue
   */
  pqMaxBackpressureSec?: number | undefined;
  /**
   * The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)
   */
  pqMaxFileSize?: string | undefined;
  /**
   * The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc.
   */
  pqMaxSize?: string | undefined;
  /**
   * The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>.
   */
  pqPath?: string | undefined;
  /**
   * Codec to use to compress the persisted data
   */
  pqCompress?: models.CompressionOptionsPq | undefined;
  /**
   * How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged.
   */
  pqOnBackpressure?: models.QueueFullBehaviorOptions | undefined;
  pqControls?: CreateOutputPqControlsWebhook | undefined;
  username?: string | undefined;
  password?: string | undefined;
  /**
   * Bearer token to include in the authorization header
   */
  token?: string | undefined;
  /**
   * Select or create a secret that references your credentials
   */
  credentialsSecret?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
  /**
   * URL for OAuth
   */
  loginUrl?: string | undefined;
  /**
   * Secret parameter name to pass in request body
   */
  secretParamName?: string | undefined;
  /**
   * Secret parameter value to pass in request body
   */
  secret?: string | undefined;
  /**
   * Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
   */
  tokenAttributeName?: string | undefined;
  /**
   * JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.
   */
  authHeaderExpr?: string | undefined;
  /**
   * How often the OAuth token should be refreshed.
   */
  tokenTimeoutSecs?: number | undefined;
  /**
   * Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
   */
  oauthParams?: Array<CreateOutputOauthParam> | undefined;
  /**
   * Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
   */
  oauthHeaders?: Array<CreateOutputOauthHeader> | undefined;
  /**
   * URL of a webhook endpoint to send events to, such as http://localhost:10200
   */
  url?: string | undefined;
  /**
   * Exclude all IPs of the current host from the list of any resolved hostnames
   */
  excludeSelf?: boolean | undefined;
  urls?: Array<CreateOutputUrlWebhook> | undefined;
  /**
   * The interval in which to re-resolve any hostnames and pick up destinations from A records
   */
  dnsResolvePeriodSec?: number | undefined;
  /**
   * How far back in time to keep traffic stats for load balancing purposes
   */
  loadBalanceStatsPeriodSec?: number | undefined;
  /**
   * Binds 'loginUrl' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'loginUrl' at runtime.
   */
  __template_loginUrl?: string | undefined;
  /**
   * Binds 'secret' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'secret' at runtime.
   */
  __template_secret?: string | undefined;
  /**
   * Binds 'url' to a variable for dynamic value resolution. Set to variable ID (pack-scoped) or 'cribl.'/'edge.' prefixed ID (group-scoped). Variable value overrides 'url' at runtime.
   */
  __template_url?: string | undefined;
};

export type CreateOutputOutputDefault = {
  /**
   * Unique ID for this output
   */
  id: string;
  type: "default";
  /**
   * Pipeline to process data before sending out to this output
   */
  pipeline?: string | undefined;
  /**
   * Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
   */
  systemFields?: Array<string> | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * ID of the default output. This will be used whenever a nonexistent/deleted output is referenced.
   */
  defaultId: string | null;
};

/**
 * Output object
 */
export type CreateOutputRequest =
  | CreateOutputOutputDefault
  | CreateOutputOutputWebhook
  | CreateOutputOutputSentinel
  | CreateOutputOutputDevnull
  | CreateOutputOutputSyslog
  | CreateOutputOutputSplunk
  | CreateOutputOutputSplunkLb
  | CreateOutputOutputSplunkHec
  | CreateOutputOutputWizHec
  | CreateOutputOutputTcpjson
  | CreateOutputOutputWavefront
  | CreateOutputOutputSignalfx
  | CreateOutputOutputFilesystem
  | CreateOutputOutputS3
  | CreateOutputOutputAzureBlob
  | CreateOutputOutputAzureDataExplorer
  | CreateOutputOutputAzureLogs
  | CreateOutputOutputKinesis
  | CreateOutputOutputHoneycomb
  | CreateOutputOutputAzureEventhub
  | CreateOutputOutputGoogleChronicle
  | CreateOutputOutputGoogleCloudStorage
  | CreateOutputOutputGoogleCloudLogging
  | CreateOutputOutputGooglePubsub
  | CreateOutputOutputExabeam
  | CreateOutputOutputKafka
  | CreateOutputOutputConfluentCloud
  | CreateOutputOutputMsk
  | CreateOutputOutputElastic
  | CreateOutputOutputElasticCloud
  | CreateOutputOutputNewrelic
  | CreateOutputOutputNewrelicEvents
  | CreateOutputOutputInfluxdb
  | CreateOutputOutputCloudwatch
  | CreateOutputOutputMinio
  | CreateOutputOutputStatsd
  | CreateOutputOutputStatsdExt
  | CreateOutputOutputGraphite
  | CreateOutputOutputRouter
  | CreateOutputOutputSns
  | CreateOutputOutputSqs
  | CreateOutputOutputSnmp
  | CreateOutputOutputSumoLogic
  | CreateOutputOutputDatadog
  | (CreateOutputOutputGrafanaCloudUnion & { type: "grafana_cloud" })
  | CreateOutputOutputLoki
  | CreateOutputOutputPrometheus
  | CreateOutputOutputRing
  | CreateOutputOutputOpenTelemetry
  | CreateOutputOutputServiceNow
  | CreateOutputOutputDataset
  | CreateOutputOutputCriblTcp
  | CreateOutputOutputCriblHttp
  | CreateOutputOutputCriblSearchEngine
  | CreateOutputOutputHumioHec
  | CreateOutputOutputCrowdstrikeNextGenSiem
  | CreateOutputOutputDlS3
  | CreateOutputOutputSecurityLake
  | CreateOutputOutputCriblLake
  | CreateOutputOutputDiskSpool
  | CreateOutputOutputClickHouse
  | CreateOutputOutputXsiam
  | CreateOutputOutputNetflow
  | CreateOutputOutputDynatraceHttp
  | CreateOutputOutputDynatraceOtlp
  | CreateOutputOutputSentinelOneAiSiem
  | CreateOutputOutputChronicle
  | CreateOutputOutputDatabricks
  | CreateOutputOutputMicrosoftFabric
  | CreateOutputOutputCloudflareR2;

/** @internal */
export type CreateOutputOutputAzureLogs$Outbound = {
  id: string;
  type: "azure_logs";
  pipeline?: string | undefined;
  systemFields?: Array<string> | undefined;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  logType: string;
  resourceId?: string | undefined;
  concurrency?: number | undefined;
  maxPayloadSizeKB?: number | undefined;
  maxPayloadEvents?: number | undefined;
  compress?: boolean | undefined;
  rejectUnauthorized?: boolean | undefined;
  timeoutSec?: number | undefined;
  flushPeriodSec?: number | undefined;
  extraHttpHeaders?:
    | Array<models.ItemsTypeExtraHttpHeaders$Outbound>
    | undefined;
  useRoundRobinDns?: boolean | undefined;
  failedRequestLoggingMode?: string | undefined;
  safeHeaders?: Array<string> | undefined;
  apiUrl?: string | undefined;
  responseRetrySettings?:
    | Array<models.ItemsTypeResponseRetrySettings$Outbound>
    | undefined;
  timeoutRetrySettings?: models.TimeoutRetrySettingsType$Outbound | undefined;
  responseHonorRetryAfterHeader?: boolean | undefined;
  onBackpressure?: string | undefined;
  authType?: string | undefined;
  description?: string | undefined;
  pqStrictOrdering?: boolean | undefined;
  pqRatePerSec?: number | undefined;
  pqMode?: string | undefined;
  pqMaxBufferSize?: number | undefined;
  pqMaxBackpressureSec?: number | undefined;
  pqMaxFileSize?: string | undefined;
  pqMaxSize?: string | undefined;
  pqPath?: string | undefined;
  pqCompress?: string | undefined;
  pqOnBackpressure?: string | undefined;
  pqControls?: CreateOutputPqControlsAzureLogs$Outbound | undefined;
  workspaceId?: string | undefined;
  workspaceKey?: string | undefined;
  keypairSecret?: string | undefined;
  __template_workspaceId?: string | undefined;
  __template_workspaceKey?: string | undefined;
};

/** @internal */
export const CreateOutputOutputAzureLogs$outboundSchema: z.ZodType<
  CreateOutputOutputAzureLogs$Outbound,
  z.ZodTypeDef,
  CreateOutputOutputAzureLogs
> = z.object({
  id: z.string(),
  type: z.literal("azure_logs"),
  pipeline: z.string().optional(),
  systemFields: z.array(z.string()).optional(),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  logType: z.string(),
  resourceId: z.string().optional(),
  concurrency: z.number().optional(),
  maxPayloadSizeKB: z.number().optional(),
  maxPayloadEvents: z.number().optional(),
  compress: z.boolean().optional(),
  rejectUnauthorized: z.boolean().optional(),
  timeoutSec: z.number().optional(),
  flushPeriodSec: z.number().optional(),
  extraHttpHeaders: z.array(models.ItemsTypeExtraHttpHeaders$outboundSchema)
    .optional(),
  useRoundRobinDns: z.boolean().optional(),
  failedRequestLoggingMode: models
    .FailedRequestLoggingModeOptions$outboundSchema.optional(),
  safeHeaders: z.array(z.string()).optional(),
  apiUrl: z.string().optional(),
  responseRetrySettings: z.array(
    models.ItemsTypeResponseRetrySettings$outboundSchema,
  ).optional(),
  timeoutRetrySettings: models.TimeoutRetrySettingsType$outboundSchema
    .optional(),
  responseHonorRetryAfterHeader: z.boolean().optional(),
  onBackpressure: models.BackpressureBehaviorOptions$outboundSchema.optional(),
  authType: CreateOutputAuthenticationMethodAzureLogs$outboundSchema.optional(),
  description: z.string().optional(),
  pqStrictOrdering: z.boolean().optional(),
  pqRatePerSec: z.number().optional(),
  pqMode: models.ModeOptions$outboundSchema.optional(),
  pqMaxBufferSize: z.number().optional(),
  pqMaxBackpressureSec: z.number().optional(),
  pqMaxFileSize: z.string().optional(),
  pqMaxSize: z.string().optional(),
  pqPath: z.string().optional(),
  pqCompress: models.CompressionOptionsPq$outboundSchema.optional(),
  pqOnBackpressure: models.QueueFullBehaviorOptions$outboundSchema.optional(),
  pqControls: CreateOutputPqControlsAzureLogs$outboundSchema.optional(),
  workspaceId: z.string().optional(),
  workspaceKey: z.string().optional(),
  keypairSecret: z.string().optional(),
  __template_workspaceId: z.string().optional(),
  __template_workspaceKey: z.string().optional(),
});

export function createOutputOutputAzureLogsToJSON(
  createOutputOutputAzureLogs: CreateOutputOutputAzureLogs,
): string {
  return JSON.stringify(
    CreateOutputOutputAzureLogs$outboundSchema.parse(
      createOutputOutputAzureLogs,
    ),
  );
}

/** @internal */
export const CreateOutputIngestionMode$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  CreateOutputIngestionMode
> = openEnums.outboundSchema(CreateOutputIngestionMode);

/** @internal */
export const CreateOutputOauthTypeAuthenticationMethod$outboundSchema:
  z.ZodType<string, z.ZodTypeDef, CreateOutputOauthTypeAuthenticationMethod> =
    openEnums.outboundSchema(CreateOutputOauthTypeAuthenticationMethod);

/** @internal */
export type CreateOutputCertificate$Outbound = {
  certificateName?: string | undefined;
};

/** @internal */
export const CreateOutputCertificate$outboundSchema: z.ZodType<
  CreateOutputCertificate$Outbound,
  z.ZodTypeDef,
  CreateOutputCertificate
> = z.object({
  certificateName: z.string().optional(),
});

export function createOutputCertificateToJSON(
  createOutputCertificate: CreateOutputCertificate,
): string {
  return JSON.stringify(
    CreateOutputCertificate$outboundSchema.parse(createOutputCertificate),
  );
}

/** @internal */
export const CreateOutputPrefixOptional$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  CreateOutputPrefixOptional
> = openEnums.outboundSchema(CreateOutputPrefixOptional);

/** @internal */
export type CreateOutputExtentTag$Outbound = {
  prefix?: string | undefined;
  value: string;
};

/** @internal */
export const CreateOutputExtentTag$outboundSchema: z.ZodType<
  CreateOutputExtentTag$Outbound,
  z.ZodTypeDef,
  CreateOutputExtentTag
> = z.object({
  prefix: CreateOutputPrefixOptional$outboundSchema.optional(),
  value: z.string(),
});

export function createOutputExtentTagToJSON(
  createOutputExtentTag: CreateOutputExtentTag,
): string {
  return JSON.stringify(
    CreateOutputExtentTag$outboundSchema.parse(createOutputExtentTag),
  );
}

/** @internal */
export type CreateOutputIngestIfNotExist$Outbound = {
  value: string;
};

/** @internal */
export const CreateOutputIngestIfNotExist$outboundSchema: z.ZodType<
  CreateOutputIngestIfNotExist$Outbound,
  z.ZodTypeDef,
  CreateOutputIngestIfNotExist
> = z.object({
  value: z.string(),
});

export function createOutputIngestIfNotExistToJSON(
  createOutputIngestIfNotExist: CreateOutputIngestIfNotExist,
): string {
  return JSON.stringify(
    CreateOutputIngestIfNotExist$outboundSchema.parse(
      createOutputIngestIfNotExist,
    ),
  );
}

/** @internal */
export const CreateOutputReportLevel$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  CreateOutputReportLevel
> = openEnums.outboundSchema(CreateOutputReportLevel);

/** @internal */
export const CreateOutputReportMethod$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  CreateOutputReportMethod
> = openEnums.outboundSchema(CreateOutputReportMethod);

/** @internal */
export type CreateOutputAdditionalProperty$Outbound = {
  key: string;
  value: string;
};

/** @internal */
export const CreateOutputAdditionalProperty$outboundSchema: z.ZodType<
  CreateOutputAdditionalProperty$Outbound,
  z.ZodTypeDef,
  CreateOutputAdditionalProperty
> = z.object({
  key: z.string(),
  value: z.string(),
});

export function createOutputAdditionalPropertyToJSON(
  createOutputAdditionalProperty: CreateOutputAdditionalProperty,
): string {
  return JSON.stringify(
    CreateOutputAdditionalProperty$outboundSchema.parse(
      createOutputAdditionalProperty,
    ),
  );
}

/** @internal */
export type CreateOutputPqControlsAzureDataExplorer$Outbound = {};

/** @internal */
export const CreateOutputPqControlsAzureDataExplorer$outboundSchema: z.ZodType<
  CreateOutputPqControlsAzureDataExplorer$Outbound,
  z.ZodTypeDef,
  CreateOutputPqControlsAzureDataExplorer
> = z.object({});

export function createOutputPqControlsAzureDataExplorerToJSON(
  createOutputPqControlsAzureDataExplorer:
    CreateOutputPqControlsAzureDataExplorer,
): string {
  return JSON.stringify(
    CreateOutputPqControlsAzureDataExplorer$outboundSchema.parse(
      createOutputPqControlsAzureDataExplorer,
    ),
  );
}

/** @internal */
export type CreateOutputOutputAzureDataExplorer$Outbound = {
  id: string;
  type: "azure_data_explorer";
  pipeline?: string | undefined;
  systemFields?: Array<string> | undefined;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  clusterUrl: string;
  database: string;
  table: string;
  validateDatabaseSettings?: boolean | undefined;
  ingestMode?: string | undefined;
  oauthEndpoint: string;
  tenantId: string;
  clientId: string;
  scope: string;
  oauthType: string;
  description?: string | undefined;
  clientSecret?: string | undefined;
  textSecret?: string | undefined;
  certificate?: CreateOutputCertificate$Outbound | undefined;
  format?: string | undefined;
  compress: string;
  compressionLevel?: string | undefined;
  automaticSchema?: boolean | undefined;
  parquetSchema?: string | undefined;
  parquetVersion?: string | undefined;
  parquetDataPageVersion?: string | undefined;
  parquetRowGroupLength?: number | undefined;
  parquetPageSize?: string | undefined;
  shouldLogInvalidRows?: boolean | undefined;
  keyValueMetadata?:
    | Array<models.ItemsTypeKeyValueMetadata$Outbound>
    | undefined;
  enableStatistics?: boolean | undefined;
  enableWritePageIndex?: boolean | undefined;
  enablePageChecksum?: boolean | undefined;
  removeEmptyDirs?: boolean | undefined;
  emptyDirCleanupSec?: number | undefined;
  directoryBatchSize?: number | undefined;
  deadletterEnabled?: boolean | undefined;
  deadletterPath?: string | undefined;
  maxRetryNum?: number | undefined;
  isMappingObj?: boolean | undefined;
  mappingObj?: string | undefined;
  mappingRef?: string | undefined;
  ingestUrl?: string | undefined;
  onBackpressure?: string | undefined;
  stagePath?: string | undefined;
  fileNameSuffix?: string | undefined;
  maxFileSizeMB?: number | undefined;
  maxFileOpenTimeSec?: number | undefined;
  maxFileIdleTimeSec?: number | undefined;
  maxOpenFiles?: number | undefined;
  maxConcurrentFileParts?: number | undefined;
  onDiskFullBackpressure?: string | undefined;
  addIdToStagePath?: boolean | undefined;
  retrySettings?: models.RetrySettingsType$Outbound | undefined;
  timeoutSec?: number | undefined;
  flushImmediately?: boolean | undefined;
  retainBlobOnSuccess?: boolean | undefined;
  extentTags?: Array<CreateOutputExtentTag$Outbound> | undefined;
  ingestIfNotExists?: Array<CreateOutputIngestIfNotExist$Outbound> | undefined;
  reportLevel?: string | undefined;
  reportMethod?: string | undefined;
  additionalProperties?:
    | Array<CreateOutputAdditionalProperty$Outbound>
    | undefined;
  responseRetrySettings?:
    | Array<models.ItemsTypeResponseRetrySettings$Outbound>
    | undefined;
  timeoutRetrySettings?: models.TimeoutRetrySettingsType$Outbound | undefined;
  responseHonorRetryAfterHeader?: boolean | undefined;
  concurrency?: number | undefined;
  maxPayloadSizeKB?: number | undefined;
  maxPayloadEvents?: number | undefined;
  flushPeriodSec?: number | undefined;
  rejectUnauthorized?: boolean | undefined;
  useRoundRobinDns?: boolean | undefined;
  keepAlive?: boolean | undefined;
  pqStrictOrdering?: boolean | undefined;
  pqRatePerSec?: number | undefined;
  pqMode?: string | undefined;
  pqMaxBufferSize?: number | undefined;
  pqMaxBackpressureSec?: number | undefined;
  pqMaxFileSize?: string | undefined;
  pqMaxSize?: string | undefined;
  pqPath?: string | undefined;
  pqCompress?: string | undefined;
  pqOnBackpressure?: string | undefined;
  pqControls?: CreateOutputPqControlsAzureDataExplorer$Outbound | undefined;
  __template_clusterUrl?: string | undefined;
  __template_database?: string | undefined;
  __template_table?: string | undefined;
  __template_tenantId?: string | undefined;
  __template_clientId?: string | undefined;
  __template_scope?: string | undefined;
  __template_clientSecret?: string | undefined;
  __template_format?: string | undefined;
  __template_ingestUrl?: string | undefined;
};

/** @internal */
export const CreateOutputOutputAzureDataExplorer$outboundSchema: z.ZodType<
  CreateOutputOutputAzureDataExplorer$Outbound,
  z.ZodTypeDef,
  CreateOutputOutputAzureDataExplorer
> = z.object({
  id: z.string(),
  type: z.literal("azure_data_explorer"),
  pipeline: z.string().optional(),
  systemFields: z.array(z.string()).optional(),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  clusterUrl: z.string(),
  database: z.string(),
  table: z.string(),
  validateDatabaseSettings: z.boolean().optional(),
  ingestMode: CreateOutputIngestionMode$outboundSchema.optional(),
  oauthEndpoint:
    models.MicrosoftEntraIdAuthenticationEndpointOptionsSasl$outboundSchema,
  tenantId: z.string(),
  clientId: z.string(),
  scope: z.string(),
  oauthType: CreateOutputOauthTypeAuthenticationMethod$outboundSchema,
  description: z.string().optional(),
  clientSecret: z.string().optional(),
  textSecret: z.string().optional(),
  certificate: z.lazy(() => CreateOutputCertificate$outboundSchema).optional(),
  format: models.DataFormatOptions$outboundSchema.optional(),
  compress: models.CompressionOptions2$outboundSchema,
  compressionLevel: models.CompressionLevelOptions$outboundSchema.optional(),
  automaticSchema: z.boolean().optional(),
  parquetSchema: z.string().optional(),
  parquetVersion: models.ParquetVersionOptions$outboundSchema.optional(),
  parquetDataPageVersion: models.DataPageVersionOptions$outboundSchema
    .optional(),
  parquetRowGroupLength: z.number().optional(),
  parquetPageSize: z.string().optional(),
  shouldLogInvalidRows: z.boolean().optional(),
  keyValueMetadata: z.array(models.ItemsTypeKeyValueMetadata$outboundSchema)
    .optional(),
  enableStatistics: z.boolean().optional(),
  enableWritePageIndex: z.boolean().optional(),
  enablePageChecksum: z.boolean().optional(),
  removeEmptyDirs: z.boolean().optional(),
  emptyDirCleanupSec: z.number().optional(),
  directoryBatchSize: z.number().optional(),
  deadletterEnabled: z.boolean().optional(),
  deadletterPath: z.string().optional(),
  maxRetryNum: z.number().optional(),
  isMappingObj: z.boolean().optional(),
  mappingObj: z.string().optional(),
  mappingRef: z.string().optional(),
  ingestUrl: z.string().optional(),
  onBackpressure: models.BackpressureBehaviorOptions$outboundSchema.optional(),
  stagePath: z.string().optional(),
  fileNameSuffix: z.string().optional(),
  maxFileSizeMB: z.number().optional(),
  maxFileOpenTimeSec: z.number().optional(),
  maxFileIdleTimeSec: z.number().optional(),
  maxOpenFiles: z.number().optional(),
  maxConcurrentFileParts: z.number().optional(),
  onDiskFullBackpressure: models.DiskSpaceProtectionOptions$outboundSchema
    .optional(),
  addIdToStagePath: z.boolean().optional(),
  retrySettings: models.RetrySettingsType$outboundSchema.optional(),
  timeoutSec: z.number().optional(),
  flushImmediately: z.boolean().optional(),
  retainBlobOnSuccess: z.boolean().optional(),
  extentTags: z.array(z.lazy(() => CreateOutputExtentTag$outboundSchema))
    .optional(),
  ingestIfNotExists: z.array(
    z.lazy(() => CreateOutputIngestIfNotExist$outboundSchema),
  ).optional(),
  reportLevel: CreateOutputReportLevel$outboundSchema.optional(),
  reportMethod: CreateOutputReportMethod$outboundSchema.optional(),
  additionalProperties: z.array(
    z.lazy(() => CreateOutputAdditionalProperty$outboundSchema),
  ).optional(),
  responseRetrySettings: z.array(
    models.ItemsTypeResponseRetrySettings$outboundSchema,
  ).optional(),
  timeoutRetrySettings: models.TimeoutRetrySettingsType$outboundSchema
    .optional(),
  responseHonorRetryAfterHeader: z.boolean().optional(),
  concurrency: z.number().optional(),
  maxPayloadSizeKB: z.number().optional(),
  maxPayloadEvents: z.number().optional(),
  flushPeriodSec: z.number().optional(),
  rejectUnauthorized: z.boolean().optional(),
  useRoundRobinDns: z.boolean().optional(),
  keepAlive: z.boolean().optional(),
  pqStrictOrdering: z.boolean().optional(),
  pqRatePerSec: z.number().optional(),
  pqMode: models.ModeOptions$outboundSchema.optional(),
  pqMaxBufferSize: z.number().optional(),
  pqMaxBackpressureSec: z.number().optional(),
  pqMaxFileSize: z.string().optional(),
  pqMaxSize: z.string().optional(),
  pqPath: z.string().optional(),
  pqCompress: models.CompressionOptionsPq$outboundSchema.optional(),
  pqOnBackpressure: models.QueueFullBehaviorOptions$outboundSchema.optional(),
  pqControls: z.lazy(() =>
    CreateOutputPqControlsAzureDataExplorer$outboundSchema
  ).optional(),
  __template_clusterUrl: z.string().optional(),
  __template_database: z.string().optional(),
  __template_table: z.string().optional(),
  __template_tenantId: z.string().optional(),
  __template_clientId: z.string().optional(),
  __template_scope: z.string().optional(),
  __template_clientSecret: z.string().optional(),
  __template_format: z.string().optional(),
  __template_ingestUrl: z.string().optional(),
});

export function createOutputOutputAzureDataExplorerToJSON(
  createOutputOutputAzureDataExplorer: CreateOutputOutputAzureDataExplorer,
): string {
  return JSON.stringify(
    CreateOutputOutputAzureDataExplorer$outboundSchema.parse(
      createOutputOutputAzureDataExplorer,
    ),
  );
}

/** @internal */
export const CreateOutputBlobAccessTier$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  CreateOutputBlobAccessTier
> = openEnums.outboundSchema(CreateOutputBlobAccessTier);

/** @internal */
export type CreateOutputOutputAzureBlob$Outbound = {
  id: string;
  type: "azure_blob";
  pipeline?: string | undefined;
  systemFields?: Array<string> | undefined;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  containerName: string;
  createContainer?: boolean | undefined;
  destPath?: string | undefined;
  stagePath: string;
  addIdToStagePath?: boolean | undefined;
  maxConcurrentFileParts?: number | undefined;
  removeEmptyDirs?: boolean | undefined;
  partitionExpr?: string | undefined;
  format?: string | undefined;
  baseFileName?: string | undefined;
  fileNameSuffix?: string | undefined;
  maxFileSizeMB?: number | undefined;
  maxFileOpenTimeSec?: number | undefined;
  maxFileIdleTimeSec?: number | undefined;
  maxOpenFiles?: number | undefined;
  headerLine?: string | undefined;
  writeHighWaterMark?: number | undefined;
  onBackpressure?: string | undefined;
  deadletterEnabled?: boolean | undefined;
  onDiskFullBackpressure?: string | undefined;
  forceCloseOnShutdown?: boolean | undefined;
  retrySettings?: models.RetrySettingsType$Outbound | undefined;
  authType?: string | undefined;
  storageClass?: string | undefined;
  description?: string | undefined;
  compress?: string | undefined;
  compressionLevel?: string | undefined;
  automaticSchema?: boolean | undefined;
  parquetSchema?: string | undefined;
  parquetVersion?: string | undefined;
  parquetDataPageVersion?: string | undefined;
  parquetRowGroupLength?: number | undefined;
  parquetPageSize?: string | undefined;
  shouldLogInvalidRows?: boolean | undefined;
  keyValueMetadata?:
    | Array<models.ItemsTypeKeyValueMetadata$Outbound>
    | undefined;
  enableStatistics?: boolean | undefined;
  enableWritePageIndex?: boolean | undefined;
  enablePageChecksum?: boolean | undefined;
  emptyDirCleanupSec?: number | undefined;
  directoryBatchSize?: number | undefined;
  deadletterPath?: string | undefined;
  maxRetryNum?: number | undefined;
  connectionString?: string | undefined;
  textSecret?: string | undefined;
  storageAccountName?: string | undefined;
  tenantId?: string | undefined;
  clientId?: string | undefined;
  azureCloud?: string | undefined;
  endpointSuffix?: string | undefined;
  clientTextSecret?: string | undefined;
  certificate?:
    | models.CertificateTypeAzureBlobAuthTypeClientCert$Outbound
    | undefined;
  __template_containerName?: string | undefined;
  __template_format?: string | undefined;
  __template_connectionString?: string | undefined;
  __template_tenantId?: string | undefined;
  __template_clientId?: string | undefined;
};

/** @internal */
export const CreateOutputOutputAzureBlob$outboundSchema: z.ZodType<
  CreateOutputOutputAzureBlob$Outbound,
  z.ZodTypeDef,
  CreateOutputOutputAzureBlob
> = z.object({
  id: z.string(),
  type: z.literal("azure_blob"),
  pipeline: z.string().optional(),
  systemFields: z.array(z.string()).optional(),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  containerName: z.string(),
  createContainer: z.boolean().optional(),
  destPath: z.string().optional(),
  stagePath: z.string(),
  addIdToStagePath: z.boolean().optional(),
  maxConcurrentFileParts: z.number().optional(),
  removeEmptyDirs: z.boolean().optional(),
  partitionExpr: z.string().optional(),
  format: models.DataFormatOptions$outboundSchema.optional(),
  baseFileName: z.string().optional(),
  fileNameSuffix: z.string().optional(),
  maxFileSizeMB: z.number().optional(),
  maxFileOpenTimeSec: z.number().optional(),
  maxFileIdleTimeSec: z.number().optional(),
  maxOpenFiles: z.number().optional(),
  headerLine: z.string().optional(),
  writeHighWaterMark: z.number().optional(),
  onBackpressure: models.BackpressureBehaviorOptions1$outboundSchema.optional(),
  deadletterEnabled: z.boolean().optional(),
  onDiskFullBackpressure: models.DiskSpaceProtectionOptions$outboundSchema
    .optional(),
  forceCloseOnShutdown: z.boolean().optional(),
  retrySettings: models.RetrySettingsType$outboundSchema.optional(),
  authType: models.AuthenticationMethodOptions$outboundSchema.optional(),
  storageClass: CreateOutputBlobAccessTier$outboundSchema.optional(),
  description: z.string().optional(),
  compress: models.CompressionOptions2$outboundSchema.optional(),
  compressionLevel: models.CompressionLevelOptions$outboundSchema.optional(),
  automaticSchema: z.boolean().optional(),
  parquetSchema: z.string().optional(),
  parquetVersion: models.ParquetVersionOptions$outboundSchema.optional(),
  parquetDataPageVersion: models.DataPageVersionOptions$outboundSchema
    .optional(),
  parquetRowGroupLength: z.number().optional(),
  parquetPageSize: z.string().optional(),
  shouldLogInvalidRows: z.boolean().optional(),
  keyValueMetadata: z.array(models.ItemsTypeKeyValueMetadata$outboundSchema)
    .optional(),
  enableStatistics: z.boolean().optional(),
  enableWritePageIndex: z.boolean().optional(),
  enablePageChecksum: z.boolean().optional(),
  emptyDirCleanupSec: z.number().optional(),
  directoryBatchSize: z.number().optional(),
  deadletterPath: z.string().optional(),
  maxRetryNum: z.number().optional(),
  connectionString: z.string().optional(),
  textSecret: z.string().optional(),
  storageAccountName: z.string().optional(),
  tenantId: z.string().optional(),
  clientId: z.string().optional(),
  azureCloud: z.string().optional(),
  endpointSuffix: z.string().optional(),
  clientTextSecret: z.string().optional(),
  certificate: models.CertificateTypeAzureBlobAuthTypeClientCert$outboundSchema
    .optional(),
  __template_containerName: z.string().optional(),
  __template_format: z.string().optional(),
  __template_connectionString: z.string().optional(),
  __template_tenantId: z.string().optional(),
  __template_clientId: z.string().optional(),
});

export function createOutputOutputAzureBlobToJSON(
  createOutputOutputAzureBlob: CreateOutputOutputAzureBlob,
): string {
  return JSON.stringify(
    CreateOutputOutputAzureBlob$outboundSchema.parse(
      createOutputOutputAzureBlob,
    ),
  );
}

/** @internal */
export type CreateOutputOutputS3$Outbound = {
  id: string;
  type: "s3";
  pipeline?: string | undefined;
  systemFields?: Array<string> | undefined;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  bucket: string;
  region?: string | undefined;
  awsSecretKey?: string | undefined;
  awsAuthenticationMethod?: string | undefined;
  endpoint?: string | undefined;
  signatureVersion?: string | undefined;
  reuseConnections?: boolean | undefined;
  rejectUnauthorized?: boolean | undefined;
  enableAssumeRole?: boolean | undefined;
  assumeRoleArn?: string | undefined;
  assumeRoleExternalId?: string | undefined;
  durationSeconds?: number | undefined;
  stagePath: string;
  addIdToStagePath?: boolean | undefined;
  destPath?: string | undefined;
  objectACL?: string | undefined;
  storageClass?: string | undefined;
  serverSideEncryption?: string | undefined;
  kmsKeyId?: string | undefined;
  removeEmptyDirs?: boolean | undefined;
  partitionExpr?: string | undefined;
  format?: string | undefined;
  baseFileName?: string | undefined;
  fileNameSuffix?: string | undefined;
  maxFileSizeMB?: number | undefined;
  maxOpenFiles?: number | undefined;
  headerLine?: string | undefined;
  writeHighWaterMark?: number | undefined;
  onBackpressure?: string | undefined;
  deadletterEnabled?: boolean | undefined;
  onDiskFullBackpressure?: string | undefined;
  forceCloseOnShutdown?: boolean | undefined;
  retrySettings?: models.RetrySettingsType$Outbound | undefined;
  maxFileOpenTimeSec?: number | undefined;
  maxFileIdleTimeSec?: number | undefined;
  maxConcurrentFileParts?: number | undefined;
  verifyPermissions?: boolean | undefined;
  maxClosingFilesToBackpressure?: number | undefined;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  awsSecret?: string | undefined;
  compress?: string | undefined;
  compressionLevel?: string | undefined;
  automaticSchema?: boolean | undefined;
  parquetSchema?: string | undefined;
  parquetVersion?: string | undefined;
  parquetDataPageVersion?: string | undefined;
  parquetRowGroupLength?: number | undefined;
  parquetPageSize?: string | undefined;
  shouldLogInvalidRows?: boolean | undefined;
  keyValueMetadata?:
    | Array<models.ItemsTypeKeyValueMetadata$Outbound>
    | undefined;
  enableStatistics?: boolean | undefined;
  enableWritePageIndex?: boolean | undefined;
  enablePageChecksum?: boolean | undefined;
  emptyDirCleanupSec?: number | undefined;
  directoryBatchSize?: number | undefined;
  deadletterPath?: string | undefined;
  maxRetryNum?: number | undefined;
  __template_bucket?: string | undefined;
  __template_region?: string | undefined;
  __template_awsSecretKey?: string | undefined;
  __template_assumeRoleArn?: string | undefined;
  __template_assumeRoleExternalId?: string | undefined;
  __template_format?: string | undefined;
  __template_awsApiKey?: string | undefined;
};

/** @internal */
export const CreateOutputOutputS3$outboundSchema: z.ZodType<
  CreateOutputOutputS3$Outbound,
  z.ZodTypeDef,
  CreateOutputOutputS3
> = z.object({
  id: z.string(),
  type: z.literal("s3"),
  pipeline: z.string().optional(),
  systemFields: z.array(z.string()).optional(),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  bucket: z.string(),
  region: z.string().optional(),
  awsSecretKey: z.string().optional(),
  awsAuthenticationMethod: z.string().optional(),
  endpoint: z.string().optional(),
  signatureVersion: models.SignatureVersionOptionsS3CollectorConf$outboundSchema
    .optional(),
  reuseConnections: z.boolean().optional(),
  rejectUnauthorized: z.boolean().optional(),
  enableAssumeRole: z.boolean().optional(),
  assumeRoleArn: z.string().optional(),
  assumeRoleExternalId: z.string().optional(),
  durationSeconds: z.number().optional(),
  stagePath: z.string(),
  addIdToStagePath: z.boolean().optional(),
  destPath: z.string().optional(),
  objectACL: models.ObjectAclOptions$outboundSchema.optional(),
  storageClass: models.StorageClassOptions$outboundSchema.optional(),
  serverSideEncryption: models
    .ServerSideEncryptionForUploadedObjectsOptions$outboundSchema.optional(),
  kmsKeyId: z.string().optional(),
  removeEmptyDirs: z.boolean().optional(),
  partitionExpr: z.string().optional(),
  format: models.DataFormatOptions$outboundSchema.optional(),
  baseFileName: z.string().optional(),
  fileNameSuffix: z.string().optional(),
  maxFileSizeMB: z.number().optional(),
  maxOpenFiles: z.number().optional(),
  headerLine: z.string().optional(),
  writeHighWaterMark: z.number().optional(),
  onBackpressure: models.BackpressureBehaviorOptions1$outboundSchema.optional(),
  deadletterEnabled: z.boolean().optional(),
  onDiskFullBackpressure: models.DiskSpaceProtectionOptions$outboundSchema
    .optional(),
  forceCloseOnShutdown: z.boolean().optional(),
  retrySettings: models.RetrySettingsType$outboundSchema.optional(),
  maxFileOpenTimeSec: z.number().optional(),
  maxFileIdleTimeSec: z.number().optional(),
  maxConcurrentFileParts: z.number().optional(),
  verifyPermissions: z.boolean().optional(),
  maxClosingFilesToBackpressure: z.number().optional(),
  description: z.string().optional(),
  awsApiKey: z.string().optional(),
  awsSecret: z.string().optional(),
  compress: models.CompressionOptions2$outboundSchema.optional(),
  compressionLevel: models.CompressionLevelOptions$outboundSchema.optional(),
  automaticSchema: z.boolean().optional(),
  parquetSchema: z.string().optional(),
  parquetVersion: models.ParquetVersionOptions$outboundSchema.optional(),
  parquetDataPageVersion: models.DataPageVersionOptions$outboundSchema
    .optional(),
  parquetRowGroupLength: z.number().optional(),
  parquetPageSize: z.string().optional(),
  shouldLogInvalidRows: z.boolean().optional(),
  keyValueMetadata: z.array(models.ItemsTypeKeyValueMetadata$outboundSchema)
    .optional(),
  enableStatistics: z.boolean().optional(),
  enableWritePageIndex: z.boolean().optional(),
  enablePageChecksum: z.boolean().optional(),
  emptyDirCleanupSec: z.number().optional(),
  directoryBatchSize: z.number().optional(),
  deadletterPath: z.string().optional(),
  maxRetryNum: z.number().optional(),
  __template_bucket: z.string().optional(),
  __template_region: z.string().optional(),
  __template_awsSecretKey: z.string().optional(),
  __template_assumeRoleArn: z.string().optional(),
  __template_assumeRoleExternalId: z.string().optional(),
  __template_format: z.string().optional(),
  __template_awsApiKey: z.string().optional(),
});

export function createOutputOutputS3ToJSON(
  createOutputOutputS3: CreateOutputOutputS3,
): string {
  return JSON.stringify(
    CreateOutputOutputS3$outboundSchema.parse(createOutputOutputS3),
  );
}

/** @internal */
export type CreateOutputOutputFilesystem$Outbound = {
  id: string;
  type: "filesystem";
  pipeline?: string | undefined;
  systemFields?: Array<string> | undefined;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  destPath: string;
  stagePath?: string | undefined;
  addIdToStagePath?: boolean | undefined;
  removeEmptyDirs?: boolean | undefined;
  partitionExpr?: string | undefined;
  format?: string | undefined;
  baseFileName?: string | undefined;
  fileNameSuffix?: string | undefined;
  maxFileSizeMB?: number | undefined;
  maxFileOpenTimeSec?: number | undefined;
  maxFileIdleTimeSec?: number | undefined;
  maxOpenFiles?: number | undefined;
  headerLine?: string | undefined;
  writeHighWaterMark?: number | undefined;
  onBackpressure?: string | undefined;
  deadletterEnabled?: boolean | undefined;
  onDiskFullBackpressure?: string | undefined;
  forceCloseOnShutdown?: boolean | undefined;
  retrySettings?: models.RetrySettingsType$Outbound | undefined;
  description?: string | undefined;
  compress?: string | undefined;
  compressionLevel?: string | undefined;
  automaticSchema?: boolean | undefined;
  parquetSchema?: string | undefined;
  parquetVersion?: string | undefined;
  parquetDataPageVersion?: string | undefined;
  parquetRowGroupLength?: number | undefined;
  parquetPageSize?: string | undefined;
  shouldLogInvalidRows?: boolean | undefined;
  keyValueMetadata?:
    | Array<models.ItemsTypeKeyValueMetadata$Outbound>
    | undefined;
  enableStatistics?: boolean | undefined;
  enableWritePageIndex?: boolean | undefined;
  enablePageChecksum?: boolean | undefined;
  emptyDirCleanupSec?: number | undefined;
  directoryBatchSize?: number | undefined;
  deadletterPath?: string | undefined;
  maxRetryNum?: number | undefined;
  __template_format?: string | undefined;
};

/** @internal */
export const CreateOutputOutputFilesystem$outboundSchema: z.ZodType<
  CreateOutputOutputFilesystem$Outbound,
  z.ZodTypeDef,
  CreateOutputOutputFilesystem
> = z.object({
  id: z.string(),
  type: z.literal("filesystem"),
  pipeline: z.string().optional(),
  systemFields: z.array(z.string()).optional(),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  destPath: z.string(),
  stagePath: z.string().optional(),
  addIdToStagePath: z.boolean().optional(),
  removeEmptyDirs: z.boolean().optional(),
  partitionExpr: z.string().optional(),
  format: models.DataFormatOptions$outboundSchema.optional(),
  baseFileName: z.string().optional(),
  fileNameSuffix: z.string().optional(),
  maxFileSizeMB: z.number().optional(),
  maxFileOpenTimeSec: z.number().optional(),
  maxFileIdleTimeSec: z.number().optional(),
  maxOpenFiles: z.number().optional(),
  headerLine: z.string().optional(),
  writeHighWaterMark: z.number().optional(),
  onBackpressure: models.BackpressureBehaviorOptions1$outboundSchema.optional(),
  deadletterEnabled: z.boolean().optional(),
  onDiskFullBackpressure: models.DiskSpaceProtectionOptions$outboundSchema
    .optional(),
  forceCloseOnShutdown: z.boolean().optional(),
  retrySettings: models.RetrySettingsType$outboundSchema.optional(),
  description: z.string().optional(),
  compress: models.CompressionOptions2$outboundSchema.optional(),
  compressionLevel: models.CompressionLevelOptions$outboundSchema.optional(),
  automaticSchema: z.boolean().optional(),
  parquetSchema: z.string().optional(),
  parquetVersion: models.ParquetVersionOptions$outboundSchema.optional(),
  parquetDataPageVersion: models.DataPageVersionOptions$outboundSchema
    .optional(),
  parquetRowGroupLength: z.number().optional(),
  parquetPageSize: z.string().optional(),
  shouldLogInvalidRows: z.boolean().optional(),
  keyValueMetadata: z.array(models.ItemsTypeKeyValueMetadata$outboundSchema)
    .optional(),
  enableStatistics: z.boolean().optional(),
  enableWritePageIndex: z.boolean().optional(),
  enablePageChecksum: z.boolean().optional(),
  emptyDirCleanupSec: z.number().optional(),
  directoryBatchSize: z.number().optional(),
  deadletterPath: z.string().optional(),
  maxRetryNum: z.number().optional(),
  __template_format: z.string().optional(),
});

export function createOutputOutputFilesystemToJSON(
  createOutputOutputFilesystem: CreateOutputOutputFilesystem,
): string {
  return JSON.stringify(
    CreateOutputOutputFilesystem$outboundSchema.parse(
      createOutputOutputFilesystem,
    ),
  );
}

/** @internal */
export type CreateOutputPqControlsSignalfx$Outbound = {};

/** @internal */
export const CreateOutputPqControlsSignalfx$outboundSchema: z.ZodType<
  CreateOutputPqControlsSignalfx$Outbound,
  z.ZodTypeDef,
  CreateOutputPqControlsSignalfx
> = z.object({});

export function createOutputPqControlsSignalfxToJSON(
  createOutputPqControlsSignalfx: CreateOutputPqControlsSignalfx,
): string {
  return JSON.stringify(
    CreateOutputPqControlsSignalfx$outboundSchema.parse(
      createOutputPqControlsSignalfx,
    ),
  );
}

/** @internal */
export type CreateOutputOutputSignalfx$Outbound = {
  id: string;
  type: "signalfx";
  pipeline?: string | undefined;
  systemFields?: Array<string> | undefined;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  authType?: string | undefined;
  realm: string;
  concurrency?: number | undefined;
  maxPayloadSizeKB?: number | undefined;
  maxPayloadEvents?: number | undefined;
  compress?: boolean | undefined;
  rejectUnauthorized?: boolean | undefined;
  timeoutSec?: number | undefined;
  flushPeriodSec?: number | undefined;
  extraHttpHeaders?:
    | Array<models.ItemsTypeExtraHttpHeaders$Outbound>
    | undefined;
  useRoundRobinDns?: boolean | undefined;
  failedRequestLoggingMode?: string | undefined;
  safeHeaders?: Array<string> | undefined;
  responseRetrySettings?:
    | Array<models.ItemsTypeResponseRetrySettings$Outbound>
    | undefined;
  timeoutRetrySettings?: models.TimeoutRetrySettingsType$Outbound | undefined;
  responseHonorRetryAfterHeader?: boolean | undefined;
  onBackpressure?: string | undefined;
  description?: string | undefined;
  token?: string | undefined;
  textSecret?: string | undefined;
  pqStrictOrdering?: boolean | undefined;
  pqRatePerSec?: number | undefined;
  pqMode?: string | undefined;
  pqMaxBufferSize?: number | undefined;
  pqMaxBackpressureSec?: number | undefined;
  pqMaxFileSize?: string | undefined;
  pqMaxSize?: string | undefined;
  pqPath?: string | undefined;
  pqCompress?: string | undefined;
  pqOnBackpressure?: string | undefined;
  pqControls?: CreateOutputPqControlsSignalfx$Outbound | undefined;
};

/** @internal */
export const CreateOutputOutputSignalfx$outboundSchema: z.ZodType<
  CreateOutputOutputSignalfx$Outbound,
  z.ZodTypeDef,
  CreateOutputOutputSignalfx
> = z.object({
  id: z.string(),
  type: z.literal("signalfx"),
  pipeline: z.string().optional(),
  systemFields: z.array(z.string()).optional(),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  authType: models.AuthenticationMethodOptionsAuthTokensItems$outboundSchema
    .optional(),
  realm: z.string(),
  concurrency: z.number().optional(),
  maxPayloadSizeKB: z.number().optional(),
  maxPayloadEvents: z.number().optional(),
  compress: z.boolean().optional(),
  rejectUnauthorized: z.boolean().optional(),
  timeoutSec: z.number().optional(),
  flushPeriodSec: z.number().optional(),
  extraHttpHeaders: z.array(models.ItemsTypeExtraHttpHeaders$outboundSchema)
    .optional(),
  useRoundRobinDns: z.boolean().optional(),
  failedRequestLoggingMode: models
    .FailedRequestLoggingModeOptions$outboundSchema.optional(),
  safeHeaders: z.array(z.string()).optional(),
  responseRetrySettings: z.array(
    models.ItemsTypeResponseRetrySettings$outboundSchema,
  ).optional(),
  timeoutRetrySettings: models.TimeoutRetrySettingsType$outboundSchema
    .optional(),
  responseHonorRetryAfterHeader: z.boolean().optional(),
  onBackpressure: models.BackpressureBehaviorOptions$outboundSchema.optional(),
  description: z.string().optional(),
  token: z.string().optional(),
  textSecret: z.string().optional(),
  pqStrictOrdering: z.boolean().optional(),
  pqRatePerSec: z.number().optional(),
  pqMode: models.ModeOptions$outboundSchema.optional(),
  pqMaxBufferSize: z.number().optional(),
  pqMaxBackpressureSec: z.number().optional(),
  pqMaxFileSize: z.string().optional(),
  pqMaxSize: z.string().optional(),
  pqPath: z.string().optional(),
  pqCompress: models.CompressionOptionsPq$outboundSchema.optional(),
  pqOnBackpressure: models.QueueFullBehaviorOptions$outboundSchema.optional(),
  pqControls: z.lazy(() => CreateOutputPqControlsSignalfx$outboundSchema)
    .optional(),
});

export function createOutputOutputSignalfxToJSON(
  createOutputOutputSignalfx: CreateOutputOutputSignalfx,
): string {
  return JSON.stringify(
    CreateOutputOutputSignalfx$outboundSchema.parse(createOutputOutputSignalfx),
  );
}

/** @internal */
export type CreateOutputPqControlsWavefront$Outbound = {};

/** @internal */
export const CreateOutputPqControlsWavefront$outboundSchema: z.ZodType<
  CreateOutputPqControlsWavefront$Outbound,
  z.ZodTypeDef,
  CreateOutputPqControlsWavefront
> = z.object({});

export function createOutputPqControlsWavefrontToJSON(
  createOutputPqControlsWavefront: CreateOutputPqControlsWavefront,
): string {
  return JSON.stringify(
    CreateOutputPqControlsWavefront$outboundSchema.parse(
      createOutputPqControlsWavefront,
    ),
  );
}

/** @internal */
export type CreateOutputOutputWavefront$Outbound = {
  id: string;
  type: "wavefront";
  pipeline?: string | undefined;
  systemFields?: Array<string> | undefined;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  authType?: string | undefined;
  domain: string;
  concurrency?: number | undefined;
  maxPayloadSizeKB?: number | undefined;
  maxPayloadEvents?: number | undefined;
  compress?: boolean | undefined;
  rejectUnauthorized?: boolean | undefined;
  timeoutSec?: number | undefined;
  flushPeriodSec?: number | undefined;
  extraHttpHeaders?:
    | Array<models.ItemsTypeExtraHttpHeaders$Outbound>
    | undefined;
  useRoundRobinDns?: boolean | undefined;
  failedRequestLoggingMode?: string | undefined;
  safeHeaders?: Array<string> | undefined;
  responseRetrySettings?:
    | Array<models.ItemsTypeResponseRetrySettings$Outbound>
    | undefined;
  timeoutRetrySettings?: models.TimeoutRetrySettingsType$Outbound | undefined;
  responseHonorRetryAfterHeader?: boolean | undefined;
  onBackpressure?: string | undefined;
  description?: string | undefined;
  token?: string | undefined;
  textSecret?: string | undefined;
  pqStrictOrdering?: boolean | undefined;
  pqRatePerSec?: number | undefined;
  pqMode?: string | undefined;
  pqMaxBufferSize?: number | undefined;
  pqMaxBackpressureSec?: number | undefined;
  pqMaxFileSize?: string | undefined;
  pqMaxSize?: string | undefined;
  pqPath?: string | undefined;
  pqCompress?: string | undefined;
  pqOnBackpressure?: string | undefined;
  pqControls?: CreateOutputPqControlsWavefront$Outbound | undefined;
};

/** @internal */
export const CreateOutputOutputWavefront$outboundSchema: z.ZodType<
  CreateOutputOutputWavefront$Outbound,
  z.ZodTypeDef,
  CreateOutputOutputWavefront
> = z.object({
  id: z.string(),
  type: z.literal("wavefront"),
  pipeline: z.string().optional(),
  systemFields: z.array(z.string()).optional(),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  authType: models.AuthenticationMethodOptionsAuthTokensItems$outboundSchema
    .optional(),
  domain: z.string(),
  concurrency: z.number().optional(),
  maxPayloadSizeKB: z.number().optional(),
  maxPayloadEvents: z.number().optional(),
  compress: z.boolean().optional(),
  rejectUnauthorized: z.boolean().optional(),
  timeoutSec: z.number().optional(),
  flushPeriodSec: z.number().optional(),
  extraHttpHeaders: z.array(models.ItemsTypeExtraHttpHeaders$outboundSchema)
    .optional(),
  useRoundRobinDns: z.boolean().optional(),
  failedRequestLoggingMode: models
    .FailedRequestLoggingModeOptions$outboundSchema.optional(),
  safeHeaders: z.array(z.string()).optional(),
  responseRetrySettings: z.array(
    models.ItemsTypeResponseRetrySettings$outboundSchema,
  ).optional(),
  timeoutRetrySettings: models.TimeoutRetrySettingsType$outboundSchema
    .optional(),
  responseHonorRetryAfterHeader: z.boolean().optional(),
  onBackpressure: models.BackpressureBehaviorOptions$outboundSchema.optional(),
  description: z.string().optional(),
  token: z.string().optional(),
  textSecret: z.string().optional(),
  pqStrictOrdering: z.boolean().optional(),
  pqRatePerSec: z.number().optional(),
  pqMode: models.ModeOptions$outboundSchema.optional(),
  pqMaxBufferSize: z.number().optional(),
  pqMaxBackpressureSec: z.number().optional(),
  pqMaxFileSize: z.string().optional(),
  pqMaxSize: z.string().optional(),
  pqPath: z.string().optional(),
  pqCompress: models.CompressionOptionsPq$outboundSchema.optional(),
  pqOnBackpressure: models.QueueFullBehaviorOptions$outboundSchema.optional(),
  pqControls: z.lazy(() => CreateOutputPqControlsWavefront$outboundSchema)
    .optional(),
});

export function createOutputOutputWavefrontToJSON(
  createOutputOutputWavefront: CreateOutputOutputWavefront,
): string {
  return JSON.stringify(
    CreateOutputOutputWavefront$outboundSchema.parse(
      createOutputOutputWavefront,
    ),
  );
}

/** @internal */
export type CreateOutputPqControlsTcpjson$Outbound = {};

/** @internal */
export const CreateOutputPqControlsTcpjson$outboundSchema: z.ZodType<
  CreateOutputPqControlsTcpjson$Outbound,
  z.ZodTypeDef,
  CreateOutputPqControlsTcpjson
> = z.object({});

export function createOutputPqControlsTcpjsonToJSON(
  createOutputPqControlsTcpjson: CreateOutputPqControlsTcpjson,
): string {
  return JSON.stringify(
    CreateOutputPqControlsTcpjson$outboundSchema.parse(
      createOutputPqControlsTcpjson,
    ),
  );
}

/** @internal */
export type CreateOutputOutputTcpjson$Outbound = {
  id: string;
  type: "tcpjson";
  pipeline?: string | undefined;
  systemFields?: Array<string> | undefined;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  loadBalanced?: boolean | undefined;
  compression?: string | undefined;
  logFailedRequests?: boolean | undefined;
  throttleRatePerSec?: string | undefined;
  tls?:
    | models.TlsSettingsClientSideTypeKafkaSchemaRegistry$Outbound
    | undefined;
  connectionTimeout?: number | undefined;
  writeTimeout?: number | undefined;
  tokenTTLMinutes?: number | undefined;
  sendHeader?: boolean | undefined;
  onBackpressure?: string | undefined;
  authType?: string | undefined;
  description?: string | undefined;
  host?: string | undefined;
  port?: number | undefined;
  excludeSelf?: boolean | undefined;
  hosts?: Array<models.ItemsTypeHosts$Outbound> | undefined;
  dnsResolvePeriodSec?: number | undefined;
  loadBalanceStatsPeriodSec?: number | undefined;
  maxConcurrentSenders?: number | undefined;
  pqStrictOrdering?: boolean | undefined;
  pqRatePerSec?: number | undefined;
  pqMode?: string | undefined;
  pqMaxBufferSize?: number | undefined;
  pqMaxBackpressureSec?: number | undefined;
  pqMaxFileSize?: string | undefined;
  pqMaxSize?: string | undefined;
  pqPath?: string | undefined;
  pqCompress?: string | undefined;
  pqOnBackpressure?: string | undefined;
  pqControls?: CreateOutputPqControlsTcpjson$Outbound | undefined;
  authToken?: string | undefined;
  textSecret?: string | undefined;
  __template_host?: string | undefined;
  __template_port?: string | undefined;
};

/** @internal */
export const CreateOutputOutputTcpjson$outboundSchema: z.ZodType<
  CreateOutputOutputTcpjson$Outbound,
  z.ZodTypeDef,
  CreateOutputOutputTcpjson
> = z.object({
  id: z.string(),
  type: z.literal("tcpjson"),
  pipeline: z.string().optional(),
  systemFields: z.array(z.string()).optional(),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  loadBalanced: z.boolean().optional(),
  compression: models.CompressionOptions1$outboundSchema.optional(),
  logFailedRequests: z.boolean().optional(),
  throttleRatePerSec: z.string().optional(),
  tls: models.TlsSettingsClientSideTypeKafkaSchemaRegistry$outboundSchema
    .optional(),
  connectionTimeout: z.number().optional(),
  writeTimeout: z.number().optional(),
  tokenTTLMinutes: z.number().optional(),
  sendHeader: z.boolean().optional(),
  onBackpressure: models.BackpressureBehaviorOptions$outboundSchema.optional(),
  authType: models.AuthenticationMethodOptionsAuthTokensItems$outboundSchema
    .optional(),
  description: z.string().optional(),
  host: z.string().optional(),
  port: z.number().optional(),
  excludeSelf: z.boolean().optional(),
  hosts: z.array(models.ItemsTypeHosts$outboundSchema).optional(),
  dnsResolvePeriodSec: z.number().optional(),
  loadBalanceStatsPeriodSec: z.number().optional(),
  maxConcurrentSenders: z.number().optional(),
  pqStrictOrdering: z.boolean().optional(),
  pqRatePerSec: z.number().optional(),
  pqMode: models.ModeOptions$outboundSchema.optional(),
  pqMaxBufferSize: z.number().optional(),
  pqMaxBackpressureSec: z.number().optional(),
  pqMaxFileSize: z.string().optional(),
  pqMaxSize: z.string().optional(),
  pqPath: z.string().optional(),
  pqCompress: models.CompressionOptionsPq$outboundSchema.optional(),
  pqOnBackpressure: models.QueueFullBehaviorOptions$outboundSchema.optional(),
  pqControls: z.lazy(() => CreateOutputPqControlsTcpjson$outboundSchema)
    .optional(),
  authToken: z.string().optional(),
  textSecret: z.string().optional(),
  __template_host: z.string().optional(),
  __template_port: z.string().optional(),
});

export function createOutputOutputTcpjsonToJSON(
  createOutputOutputTcpjson: CreateOutputOutputTcpjson,
): string {
  return JSON.stringify(
    CreateOutputOutputTcpjson$outboundSchema.parse(createOutputOutputTcpjson),
  );
}

/** @internal */
export type CreateOutputPqControlsWizHec$Outbound = {};

/** @internal */
export const CreateOutputPqControlsWizHec$outboundSchema: z.ZodType<
  CreateOutputPqControlsWizHec$Outbound,
  z.ZodTypeDef,
  CreateOutputPqControlsWizHec
> = z.object({});

export function createOutputPqControlsWizHecToJSON(
  createOutputPqControlsWizHec: CreateOutputPqControlsWizHec,
): string {
  return JSON.stringify(
    CreateOutputPqControlsWizHec$outboundSchema.parse(
      createOutputPqControlsWizHec,
    ),
  );
}

/** @internal */
export type CreateOutputOutputWizHec$Outbound = {
  id: string;
  type: "wiz_hec";
  pipeline?: string | undefined;
  systemFields?: Array<string> | undefined;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  nextQueue?: string | undefined;
  tcpRouting?: string | undefined;
  tls?: models.TlsSettingsClientSideType1$Outbound | undefined;
  concurrency?: number | undefined;
  maxPayloadSizeKB?: number | undefined;
  maxPayloadEvents?: number | undefined;
  compress?: boolean | undefined;
  rejectUnauthorized?: boolean | undefined;
  timeoutSec?: number | undefined;
  flushPeriodSec?: number | undefined;
  extraHttpHeaders?:
    | Array<models.ItemsTypeExtraHttpHeaders$Outbound>
    | undefined;
  failedRequestLoggingMode?: string | undefined;
  safeHeaders?: Array<string> | undefined;
  authType?: string | undefined;
  responseRetrySettings?:
    | Array<models.ItemsTypeResponseRetrySettings$Outbound>
    | undefined;
  timeoutRetrySettings?: models.TimeoutRetrySettingsType$Outbound | undefined;
  responseHonorRetryAfterHeader?: boolean | undefined;
  onBackpressure?: string | undefined;
  wiz_connector_id: string;
  wiz_environment: string;
  data_center: string;
  wiz_sourcetype: string;
  description?: string | undefined;
  pqStrictOrdering?: boolean | undefined;
  pqRatePerSec?: number | undefined;
  pqMode?: string | undefined;
  pqMaxBufferSize?: number | undefined;
  pqMaxBackpressureSec?: number | undefined;
  pqMaxFileSize?: string | undefined;
  pqMaxSize?: string | undefined;
  pqPath?: string | undefined;
  pqCompress?: string | undefined;
  pqOnBackpressure?: string | undefined;
  pqControls?: CreateOutputPqControlsWizHec$Outbound | undefined;
  token?: string | undefined;
  textSecret?: string | undefined;
  __template_wiz_environment?: string | undefined;
  __template_data_center?: string | undefined;
  __template_wiz_sourcetype?: string | undefined;
};

/** @internal */
export const CreateOutputOutputWizHec$outboundSchema: z.ZodType<
  CreateOutputOutputWizHec$Outbound,
  z.ZodTypeDef,
  CreateOutputOutputWizHec
> = z.object({
  id: z.string(),
  type: z.literal("wiz_hec"),
  pipeline: z.string().optional(),
  systemFields: z.array(z.string()).optional(),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  nextQueue: z.string().optional(),
  tcpRouting: z.string().optional(),
  tls: models.TlsSettingsClientSideType1$outboundSchema.optional(),
  concurrency: z.number().optional(),
  maxPayloadSizeKB: z.number().optional(),
  maxPayloadEvents: z.number().optional(),
  compress: z.boolean().optional(),
  rejectUnauthorized: z.boolean().optional(),
  timeoutSec: z.number().optional(),
  flushPeriodSec: z.number().optional(),
  extraHttpHeaders: z.array(models.ItemsTypeExtraHttpHeaders$outboundSchema)
    .optional(),
  failedRequestLoggingMode: models
    .FailedRequestLoggingModeOptions$outboundSchema.optional(),
  safeHeaders: z.array(z.string()).optional(),
  authType: models.AuthenticationMethodOptionsAuthTokensItems$outboundSchema
    .optional(),
  responseRetrySettings: z.array(
    models.ItemsTypeResponseRetrySettings$outboundSchema,
  ).optional(),
  timeoutRetrySettings: models.TimeoutRetrySettingsType$outboundSchema
    .optional(),
  responseHonorRetryAfterHeader: z.boolean().optional(),
  onBackpressure: models.BackpressureBehaviorOptions$outboundSchema.optional(),
  wiz_connector_id: z.string(),
  wiz_environment: z.string(),
  data_center: z.string(),
  wiz_sourcetype: z.string(),
  description: z.string().optional(),
  pqStrictOrdering: z.boolean().optional(),
  pqRatePerSec: z.number().optional(),
  pqMode: models.ModeOptions$outboundSchema.optional(),
  pqMaxBufferSize: z.number().optional(),
  pqMaxBackpressureSec: z.number().optional(),
  pqMaxFileSize: z.string().optional(),
  pqMaxSize: z.string().optional(),
  pqPath: z.string().optional(),
  pqCompress: models.CompressionOptionsPq$outboundSchema.optional(),
  pqOnBackpressure: models.QueueFullBehaviorOptions$outboundSchema.optional(),
  pqControls: z.lazy(() => CreateOutputPqControlsWizHec$outboundSchema)
    .optional(),
  token: z.string().optional(),
  textSecret: z.string().optional(),
  __template_wiz_environment: z.string().optional(),
  __template_data_center: z.string().optional(),
  __template_wiz_sourcetype: z.string().optional(),
});

export function createOutputOutputWizHecToJSON(
  createOutputOutputWizHec: CreateOutputOutputWizHec,
): string {
  return JSON.stringify(
    CreateOutputOutputWizHec$outboundSchema.parse(createOutputOutputWizHec),
  );
}

/** @internal */
export type CreateOutputUrlSplunkHec$Outbound = {
  url: string;
  weight?: number | undefined;
  __template_url?: string | undefined;
};

/** @internal */
export const CreateOutputUrlSplunkHec$outboundSchema: z.ZodType<
  CreateOutputUrlSplunkHec$Outbound,
  z.ZodTypeDef,
  CreateOutputUrlSplunkHec
> = z.object({
  url: z.string(),
  weight: z.number().optional(),
  __template_url: z.string().optional(),
});

export function createOutputUrlSplunkHecToJSON(
  createOutputUrlSplunkHec: CreateOutputUrlSplunkHec,
): string {
  return JSON.stringify(
    CreateOutputUrlSplunkHec$outboundSchema.parse(createOutputUrlSplunkHec),
  );
}

/** @internal */
export type CreateOutputPqControlsSplunkHec$Outbound = {};

/** @internal */
export const CreateOutputPqControlsSplunkHec$outboundSchema: z.ZodType<
  CreateOutputPqControlsSplunkHec$Outbound,
  z.ZodTypeDef,
  CreateOutputPqControlsSplunkHec
> = z.object({});

export function createOutputPqControlsSplunkHecToJSON(
  createOutputPqControlsSplunkHec: CreateOutputPqControlsSplunkHec,
): string {
  return JSON.stringify(
    CreateOutputPqControlsSplunkHec$outboundSchema.parse(
      createOutputPqControlsSplunkHec,
    ),
  );
}

/** @internal */
export type CreateOutputOutputSplunkHec$Outbound = {
  id: string;
  type: "splunk_hec";
  pipeline?: string | undefined;
  systemFields?: Array<string> | undefined;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  loadBalanced?: boolean | undefined;
  nextQueue?: string | undefined;
  tcpRouting?: string | undefined;
  tls?: models.TlsSettingsClientSideType1$Outbound | undefined;
  concurrency?: number | undefined;
  maxPayloadSizeKB?: number | undefined;
  maxPayloadEvents?: number | undefined;
  compress?: boolean | undefined;
  rejectUnauthorized?: boolean | undefined;
  timeoutSec?: number | undefined;
  flushPeriodSec?: number | undefined;
  extraHttpHeaders?:
    | Array<models.ItemsTypeExtraHttpHeaders$Outbound>
    | undefined;
  failedRequestLoggingMode?: string | undefined;
  safeHeaders?: Array<string> | undefined;
  enableMultiMetrics?: boolean | undefined;
  authType?: string | undefined;
  responseRetrySettings?:
    | Array<models.ItemsTypeResponseRetrySettings$Outbound>
    | undefined;
  timeoutRetrySettings?: models.TimeoutRetrySettingsType$Outbound | undefined;
  responseHonorRetryAfterHeader?: boolean | undefined;
  onBackpressure?: string | undefined;
  description?: string | undefined;
  url?: string | undefined;
  useRoundRobinDns?: boolean | undefined;
  excludeSelf?: boolean | undefined;
  urls?: Array<CreateOutputUrlSplunkHec$Outbound> | undefined;
  dnsResolvePeriodSec?: number | undefined;
  loadBalanceStatsPeriodSec?: number | undefined;
  token?: string | undefined;
  textSecret?: string | undefined;
  pqStrictOrdering?: boolean | undefined;
  pqRatePerSec?: number | undefined;
  pqMode?: string | undefined;
  pqMaxBufferSize?: number | undefined;
  pqMaxBackpressureSec?: number | undefined;
  pqMaxFileSize?: string | undefined;
  pqMaxSize?: string | undefined;
  pqPath?: string | undefined;
  pqCompress?: string | undefined;
  pqOnBackpressure?: string | undefined;
  pqControls?: CreateOutputPqControlsSplunkHec$Outbound | undefined;
  __template_url?: string | undefined;
};

/** @internal */
export const CreateOutputOutputSplunkHec$outboundSchema: z.ZodType<
  CreateOutputOutputSplunkHec$Outbound,
  z.ZodTypeDef,
  CreateOutputOutputSplunkHec
> = z.object({
  id: z.string(),
  type: z.literal("splunk_hec"),
  pipeline: z.string().optional(),
  systemFields: z.array(z.string()).optional(),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  loadBalanced: z.boolean().optional(),
  nextQueue: z.string().optional(),
  tcpRouting: z.string().optional(),
  tls: models.TlsSettingsClientSideType1$outboundSchema.optional(),
  concurrency: z.number().optional(),
  maxPayloadSizeKB: z.number().optional(),
  maxPayloadEvents: z.number().optional(),
  compress: z.boolean().optional(),
  rejectUnauthorized: z.boolean().optional(),
  timeoutSec: z.number().optional(),
  flushPeriodSec: z.number().optional(),
  extraHttpHeaders: z.array(models.ItemsTypeExtraHttpHeaders$outboundSchema)
    .optional(),
  failedRequestLoggingMode: models
    .FailedRequestLoggingModeOptions$outboundSchema.optional(),
  safeHeaders: z.array(z.string()).optional(),
  enableMultiMetrics: z.boolean().optional(),
  authType: models.AuthenticationMethodOptionsAuthTokensItems$outboundSchema
    .optional(),
  responseRetrySettings: z.array(
    models.ItemsTypeResponseRetrySettings$outboundSchema,
  ).optional(),
  timeoutRetrySettings: models.TimeoutRetrySettingsType$outboundSchema
    .optional(),
  responseHonorRetryAfterHeader: z.boolean().optional(),
  onBackpressure: models.BackpressureBehaviorOptions$outboundSchema.optional(),
  description: z.string().optional(),
  url: z.string().optional(),
  useRoundRobinDns: z.boolean().optional(),
  excludeSelf: z.boolean().optional(),
  urls: z.array(z.lazy(() => CreateOutputUrlSplunkHec$outboundSchema))
    .optional(),
  dnsResolvePeriodSec: z.number().optional(),
  loadBalanceStatsPeriodSec: z.number().optional(),
  token: z.string().optional(),
  textSecret: z.string().optional(),
  pqStrictOrdering: z.boolean().optional(),
  pqRatePerSec: z.number().optional(),
  pqMode: models.ModeOptions$outboundSchema.optional(),
  pqMaxBufferSize: z.number().optional(),
  pqMaxBackpressureSec: z.number().optional(),
  pqMaxFileSize: z.string().optional(),
  pqMaxSize: z.string().optional(),
  pqPath: z.string().optional(),
  pqCompress: models.CompressionOptionsPq$outboundSchema.optional(),
  pqOnBackpressure: models.QueueFullBehaviorOptions$outboundSchema.optional(),
  pqControls: z.lazy(() => CreateOutputPqControlsSplunkHec$outboundSchema)
    .optional(),
  __template_url: z.string().optional(),
});

export function createOutputOutputSplunkHecToJSON(
  createOutputOutputSplunkHec: CreateOutputOutputSplunkHec,
): string {
  return JSON.stringify(
    CreateOutputOutputSplunkHec$outboundSchema.parse(
      createOutputOutputSplunkHec,
    ),
  );
}

/** @internal */
export type CreateOutputAuthToken$Outbound = {
  authType?: string | undefined;
  authToken?: string | undefined;
  textSecret?: string | undefined;
};

/** @internal */
export const CreateOutputAuthToken$outboundSchema: z.ZodType<
  CreateOutputAuthToken$Outbound,
  z.ZodTypeDef,
  CreateOutputAuthToken
> = z.object({
  authType: models.AuthenticationMethodOptionsAuthTokensItems$outboundSchema
    .optional(),
  authToken: z.string().optional(),
  textSecret: z.string().optional(),
});

export function createOutputAuthTokenToJSON(
  createOutputAuthToken: CreateOutputAuthToken,
): string {
  return JSON.stringify(
    CreateOutputAuthToken$outboundSchema.parse(createOutputAuthToken),
  );
}

/** @internal */
export type CreateOutputIndexerDiscoveryConfigs$Outbound = {
  site: string;
  masterUri: string;
  refreshIntervalSec: number;
  rejectUnauthorized?: boolean | undefined;
  authTokens?: Array<CreateOutputAuthToken$Outbound> | undefined;
  authType?: string | undefined;
  authToken?: string | undefined;
  textSecret?: string | undefined;
};

/** @internal */
export const CreateOutputIndexerDiscoveryConfigs$outboundSchema: z.ZodType<
  CreateOutputIndexerDiscoveryConfigs$Outbound,
  z.ZodTypeDef,
  CreateOutputIndexerDiscoveryConfigs
> = z.object({
  site: z.string(),
  masterUri: z.string(),
  refreshIntervalSec: z.number(),
  rejectUnauthorized: z.boolean().optional(),
  authTokens: z.array(z.lazy(() => CreateOutputAuthToken$outboundSchema))
    .optional(),
  authType: models.AuthenticationMethodOptionsAuthTokensItems$outboundSchema
    .optional(),
  authToken: z.string().optional(),
  textSecret: z.string().optional(),
});

export function createOutputIndexerDiscoveryConfigsToJSON(
  createOutputIndexerDiscoveryConfigs: CreateOutputIndexerDiscoveryConfigs,
): string {
  return JSON.stringify(
    CreateOutputIndexerDiscoveryConfigs$outboundSchema.parse(
      createOutputIndexerDiscoveryConfigs,
    ),
  );
}

/** @internal */
export type CreateOutputPqControlsSplunkLb$Outbound = {};

/** @internal */
export const CreateOutputPqControlsSplunkLb$outboundSchema: z.ZodType<
  CreateOutputPqControlsSplunkLb$Outbound,
  z.ZodTypeDef,
  CreateOutputPqControlsSplunkLb
> = z.object({});

export function createOutputPqControlsSplunkLbToJSON(
  createOutputPqControlsSplunkLb: CreateOutputPqControlsSplunkLb,
): string {
  return JSON.stringify(
    CreateOutputPqControlsSplunkLb$outboundSchema.parse(
      createOutputPqControlsSplunkLb,
    ),
  );
}

/** @internal */
export type CreateOutputOutputSplunkLb$Outbound = {
  id: string;
  type: "splunk_lb";
  pipeline?: string | undefined;
  systemFields?: Array<string> | undefined;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  dnsResolvePeriodSec?: number | undefined;
  loadBalanceStatsPeriodSec?: number | undefined;
  maxConcurrentSenders?: number | undefined;
  nestedFields?: string | undefined;
  throttleRatePerSec?: string | undefined;
  connectionTimeout?: number | undefined;
  writeTimeout?: number | undefined;
  tls?:
    | models.TlsSettingsClientSideTypeKafkaSchemaRegistry$Outbound
    | undefined;
  enableMultiMetrics?: boolean | undefined;
  enableACK?: boolean | undefined;
  logFailedRequests?: boolean | undefined;
  maxS2Sversion?: string | undefined;
  onBackpressure?: string | undefined;
  indexerDiscovery?: boolean | undefined;
  senderUnhealthyTimeAllowance?: number | undefined;
  authType?: string | undefined;
  description?: string | undefined;
  maxFailedHealthChecks?: number | undefined;
  compress?: string | undefined;
  indexerDiscoveryConfigs?:
    | CreateOutputIndexerDiscoveryConfigs$Outbound
    | undefined;
  excludeSelf?: boolean | undefined;
  hosts: Array<models.ItemsTypeHosts$Outbound>;
  pqStrictOrdering?: boolean | undefined;
  pqRatePerSec?: number | undefined;
  pqMode?: string | undefined;
  pqMaxBufferSize?: number | undefined;
  pqMaxBackpressureSec?: number | undefined;
  pqMaxFileSize?: string | undefined;
  pqMaxSize?: string | undefined;
  pqPath?: string | undefined;
  pqCompress?: string | undefined;
  pqOnBackpressure?: string | undefined;
  pqControls?: CreateOutputPqControlsSplunkLb$Outbound | undefined;
  authToken?: string | undefined;
  textSecret?: string | undefined;
};

/** @internal */
export const CreateOutputOutputSplunkLb$outboundSchema: z.ZodType<
  CreateOutputOutputSplunkLb$Outbound,
  z.ZodTypeDef,
  CreateOutputOutputSplunkLb
> = z.object({
  id: z.string(),
  type: z.literal("splunk_lb"),
  pipeline: z.string().optional(),
  systemFields: z.array(z.string()).optional(),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  dnsResolvePeriodSec: z.number().optional(),
  loadBalanceStatsPeriodSec: z.number().optional(),
  maxConcurrentSenders: z.number().optional(),
  nestedFields: models.NestedFieldSerializationOptions$outboundSchema
    .optional(),
  throttleRatePerSec: z.string().optional(),
  connectionTimeout: z.number().optional(),
  writeTimeout: z.number().optional(),
  tls: models.TlsSettingsClientSideTypeKafkaSchemaRegistry$outboundSchema
    .optional(),
  enableMultiMetrics: z.boolean().optional(),
  enableACK: z.boolean().optional(),
  logFailedRequests: z.boolean().optional(),
  maxS2Sversion: models.MaxS2SVersionOptions$outboundSchema.optional(),
  onBackpressure: models.BackpressureBehaviorOptions$outboundSchema.optional(),
  indexerDiscovery: z.boolean().optional(),
  senderUnhealthyTimeAllowance: z.number().optional(),
  authType: models.AuthenticationMethodOptionsAuthTokensItems$outboundSchema
    .optional(),
  description: z.string().optional(),
  maxFailedHealthChecks: z.number().optional(),
  compress: models.CompressionOptions$outboundSchema.optional(),
  indexerDiscoveryConfigs: z.lazy(() =>
    CreateOutputIndexerDiscoveryConfigs$outboundSchema
  ).optional(),
  excludeSelf: z.boolean().optional(),
  hosts: z.array(models.ItemsTypeHosts$outboundSchema),
  pqStrictOrdering: z.boolean().optional(),
  pqRatePerSec: z.number().optional(),
  pqMode: models.ModeOptions$outboundSchema.optional(),
  pqMaxBufferSize: z.number().optional(),
  pqMaxBackpressureSec: z.number().optional(),
  pqMaxFileSize: z.string().optional(),
  pqMaxSize: z.string().optional(),
  pqPath: z.string().optional(),
  pqCompress: models.CompressionOptionsPq$outboundSchema.optional(),
  pqOnBackpressure: models.QueueFullBehaviorOptions$outboundSchema.optional(),
  pqControls: z.lazy(() => CreateOutputPqControlsSplunkLb$outboundSchema)
    .optional(),
  authToken: z.string().optional(),
  textSecret: z.string().optional(),
});

export function createOutputOutputSplunkLbToJSON(
  createOutputOutputSplunkLb: CreateOutputOutputSplunkLb,
): string {
  return JSON.stringify(
    CreateOutputOutputSplunkLb$outboundSchema.parse(createOutputOutputSplunkLb),
  );
}

/** @internal */
export type CreateOutputPqControlsSplunk$Outbound = {};

/** @internal */
export const CreateOutputPqControlsSplunk$outboundSchema: z.ZodType<
  CreateOutputPqControlsSplunk$Outbound,
  z.ZodTypeDef,
  CreateOutputPqControlsSplunk
> = z.object({});

export function createOutputPqControlsSplunkToJSON(
  createOutputPqControlsSplunk: CreateOutputPqControlsSplunk,
): string {
  return JSON.stringify(
    CreateOutputPqControlsSplunk$outboundSchema.parse(
      createOutputPqControlsSplunk,
    ),
  );
}

/** @internal */
export type CreateOutputOutputSplunk$Outbound = {
  id: string;
  type: "splunk";
  pipeline?: string | undefined;
  systemFields?: Array<string> | undefined;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  host: string;
  port: number;
  nestedFields?: string | undefined;
  throttleRatePerSec?: string | undefined;
  connectionTimeout?: number | undefined;
  writeTimeout?: number | undefined;
  tls?:
    | models.TlsSettingsClientSideTypeKafkaSchemaRegistry$Outbound
    | undefined;
  enableMultiMetrics?: boolean | undefined;
  enableACK?: boolean | undefined;
  logFailedRequests?: boolean | undefined;
  maxS2Sversion?: string | undefined;
  onBackpressure?: string | undefined;
  authType?: string | undefined;
  description?: string | undefined;
  maxFailedHealthChecks?: number | undefined;
  compress?: string | undefined;
  pqStrictOrdering?: boolean | undefined;
  pqRatePerSec?: number | undefined;
  pqMode?: string | undefined;
  pqMaxBufferSize?: number | undefined;
  pqMaxBackpressureSec?: number | undefined;
  pqMaxFileSize?: string | undefined;
  pqMaxSize?: string | undefined;
  pqPath?: string | undefined;
  pqCompress?: string | undefined;
  pqOnBackpressure?: string | undefined;
  pqControls?: CreateOutputPqControlsSplunk$Outbound | undefined;
  authToken?: string | undefined;
  textSecret?: string | undefined;
  __template_host?: string | undefined;
  __template_port?: string | undefined;
};

/** @internal */
export const CreateOutputOutputSplunk$outboundSchema: z.ZodType<
  CreateOutputOutputSplunk$Outbound,
  z.ZodTypeDef,
  CreateOutputOutputSplunk
> = z.object({
  id: z.string(),
  type: z.literal("splunk"),
  pipeline: z.string().optional(),
  systemFields: z.array(z.string()).optional(),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  host: z.string(),
  port: z.number(),
  nestedFields: models.NestedFieldSerializationOptions$outboundSchema
    .optional(),
  throttleRatePerSec: z.string().optional(),
  connectionTimeout: z.number().optional(),
  writeTimeout: z.number().optional(),
  tls: models.TlsSettingsClientSideTypeKafkaSchemaRegistry$outboundSchema
    .optional(),
  enableMultiMetrics: z.boolean().optional(),
  enableACK: z.boolean().optional(),
  logFailedRequests: z.boolean().optional(),
  maxS2Sversion: models.MaxS2SVersionOptions$outboundSchema.optional(),
  onBackpressure: models.BackpressureBehaviorOptions$outboundSchema.optional(),
  authType: models.AuthenticationMethodOptionsAuthTokensItems$outboundSchema
    .optional(),
  description: z.string().optional(),
  maxFailedHealthChecks: z.number().optional(),
  compress: models.CompressionOptions$outboundSchema.optional(),
  pqStrictOrdering: z.boolean().optional(),
  pqRatePerSec: z.number().optional(),
  pqMode: models.ModeOptions$outboundSchema.optional(),
  pqMaxBufferSize: z.number().optional(),
  pqMaxBackpressureSec: z.number().optional(),
  pqMaxFileSize: z.string().optional(),
  pqMaxSize: z.string().optional(),
  pqPath: z.string().optional(),
  pqCompress: models.CompressionOptionsPq$outboundSchema.optional(),
  pqOnBackpressure: models.QueueFullBehaviorOptions$outboundSchema.optional(),
  pqControls: z.lazy(() => CreateOutputPqControlsSplunk$outboundSchema)
    .optional(),
  authToken: z.string().optional(),
  textSecret: z.string().optional(),
  __template_host: z.string().optional(),
  __template_port: z.string().optional(),
});

export function createOutputOutputSplunkToJSON(
  createOutputOutputSplunk: CreateOutputOutputSplunk,
): string {
  return JSON.stringify(
    CreateOutputOutputSplunk$outboundSchema.parse(createOutputOutputSplunk),
  );
}

/** @internal */
export const CreateOutputProtocolSyslog$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  CreateOutputProtocolSyslog
> = openEnums.outboundSchema(CreateOutputProtocolSyslog);

/** @internal */
export const CreateOutputFacility$outboundSchema: z.ZodType<
  number,
  z.ZodTypeDef,
  CreateOutputFacility
> = openEnums.outboundSchemaInt(CreateOutputFacility);

/** @internal */
export const CreateOutputSeveritySyslog$outboundSchema: z.ZodType<
  number,
  z.ZodTypeDef,
  CreateOutputSeveritySyslog
> = openEnums.outboundSchemaInt(CreateOutputSeveritySyslog);

/** @internal */
export const CreateOutputMessageFormat$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  CreateOutputMessageFormat
> = openEnums.outboundSchema(CreateOutputMessageFormat);

/** @internal */
export const CreateOutputTimestampFormat$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  CreateOutputTimestampFormat
> = openEnums.outboundSchema(CreateOutputTimestampFormat);

/** @internal */
export type CreateOutputPqControlsSyslog$Outbound = {};

/** @internal */
export const CreateOutputPqControlsSyslog$outboundSchema: z.ZodType<
  CreateOutputPqControlsSyslog$Outbound,
  z.ZodTypeDef,
  CreateOutputPqControlsSyslog
> = z.object({});

export function createOutputPqControlsSyslogToJSON(
  createOutputPqControlsSyslog: CreateOutputPqControlsSyslog,
): string {
  return JSON.stringify(
    CreateOutputPqControlsSyslog$outboundSchema.parse(
      createOutputPqControlsSyslog,
    ),
  );
}

/** @internal */
export type CreateOutputOutputSyslog$Outbound = {
  id: string;
  type: "syslog";
  pipeline?: string | undefined;
  systemFields?: Array<string> | undefined;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  protocol?: string | undefined;
  facility?: number | undefined;
  severity?: number | undefined;
  appName?: string | undefined;
  messageFormat?: string | undefined;
  timestampFormat?: string | undefined;
  throttleRatePerSec?: string | undefined;
  octetCountFraming?: boolean | undefined;
  logFailedRequests?: boolean | undefined;
  description?: string | undefined;
  loadBalanced?: boolean | undefined;
  host?: string | undefined;
  port?: number | undefined;
  excludeSelf?: boolean | undefined;
  hosts?: Array<models.ItemsTypeHosts$Outbound> | undefined;
  dnsResolvePeriodSec?: number | undefined;
  loadBalanceStatsPeriodSec?: number | undefined;
  maxConcurrentSenders?: number | undefined;
  connectionTimeout?: number | undefined;
  writeTimeout?: number | undefined;
  tls?:
    | models.TlsSettingsClientSideTypeKafkaSchemaRegistry$Outbound
    | undefined;
  onBackpressure?: string | undefined;
  maxRecordSize?: number | undefined;
  udpDnsResolvePeriodSec?: number | undefined;
  enableIpSpoofing?: boolean | undefined;
  pqStrictOrdering?: boolean | undefined;
  pqRatePerSec?: number | undefined;
  pqMode?: string | undefined;
  pqMaxBufferSize?: number | undefined;
  pqMaxBackpressureSec?: number | undefined;
  pqMaxFileSize?: string | undefined;
  pqMaxSize?: string | undefined;
  pqPath?: string | undefined;
  pqCompress?: string | undefined;
  pqOnBackpressure?: string | undefined;
  pqControls?: CreateOutputPqControlsSyslog$Outbound | undefined;
  __template_host?: string | undefined;
  __template_port?: string | undefined;
};

/** @internal */
export const CreateOutputOutputSyslog$outboundSchema: z.ZodType<
  CreateOutputOutputSyslog$Outbound,
  z.ZodTypeDef,
  CreateOutputOutputSyslog
> = z.object({
  id: z.string(),
  type: z.literal("syslog"),
  pipeline: z.string().optional(),
  systemFields: z.array(z.string()).optional(),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  protocol: CreateOutputProtocolSyslog$outboundSchema.optional(),
  facility: CreateOutputFacility$outboundSchema.optional(),
  severity: CreateOutputSeveritySyslog$outboundSchema.optional(),
  appName: z.string().optional(),
  messageFormat: CreateOutputMessageFormat$outboundSchema.optional(),
  timestampFormat: CreateOutputTimestampFormat$outboundSchema.optional(),
  throttleRatePerSec: z.string().optional(),
  octetCountFraming: z.boolean().optional(),
  logFailedRequests: z.boolean().optional(),
  description: z.string().optional(),
  loadBalanced: z.boolean().optional(),
  host: z.string().optional(),
  port: z.number().optional(),
  excludeSelf: z.boolean().optional(),
  hosts: z.array(models.ItemsTypeHosts$outboundSchema).optional(),
  dnsResolvePeriodSec: z.number().optional(),
  loadBalanceStatsPeriodSec: z.number().optional(),
  maxConcurrentSenders: z.number().optional(),
  connectionTimeout: z.number().optional(),
  writeTimeout: z.number().optional(),
  tls: models.TlsSettingsClientSideTypeKafkaSchemaRegistry$outboundSchema
    .optional(),
  onBackpressure: models.BackpressureBehaviorOptions$outboundSchema.optional(),
  maxRecordSize: z.number().optional(),
  udpDnsResolvePeriodSec: z.number().optional(),
  enableIpSpoofing: z.boolean().optional(),
  pqStrictOrdering: z.boolean().optional(),
  pqRatePerSec: z.number().optional(),
  pqMode: models.ModeOptions$outboundSchema.optional(),
  pqMaxBufferSize: z.number().optional(),
  pqMaxBackpressureSec: z.number().optional(),
  pqMaxFileSize: z.string().optional(),
  pqMaxSize: z.string().optional(),
  pqPath: z.string().optional(),
  pqCompress: models.CompressionOptionsPq$outboundSchema.optional(),
  pqOnBackpressure: models.QueueFullBehaviorOptions$outboundSchema.optional(),
  pqControls: z.lazy(() => CreateOutputPqControlsSyslog$outboundSchema)
    .optional(),
  __template_host: z.string().optional(),
  __template_port: z.string().optional(),
});

export function createOutputOutputSyslogToJSON(
  createOutputOutputSyslog: CreateOutputOutputSyslog,
): string {
  return JSON.stringify(
    CreateOutputOutputSyslog$outboundSchema.parse(createOutputOutputSyslog),
  );
}

/** @internal */
export type CreateOutputOutputDevnull$Outbound = {
  id: string;
  type: "devnull";
  pipeline?: string | undefined;
  systemFields?: Array<string> | undefined;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
};

/** @internal */
export const CreateOutputOutputDevnull$outboundSchema: z.ZodType<
  CreateOutputOutputDevnull$Outbound,
  z.ZodTypeDef,
  CreateOutputOutputDevnull
> = z.object({
  id: z.string(),
  type: z.literal("devnull"),
  pipeline: z.string().optional(),
  systemFields: z.array(z.string()).optional(),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
});

export function createOutputOutputDevnullToJSON(
  createOutputOutputDevnull: CreateOutputOutputDevnull,
): string {
  return JSON.stringify(
    CreateOutputOutputDevnull$outboundSchema.parse(createOutputOutputDevnull),
  );
}

/** @internal */
export const CreateOutputAuthType$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  CreateOutputAuthType
> = openEnums.outboundSchema(CreateOutputAuthType);

/** @internal */
export const CreateOutputEndpointConfiguration$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  CreateOutputEndpointConfiguration
> = openEnums.outboundSchema(CreateOutputEndpointConfiguration);

/** @internal */
export const CreateOutputFormatSentinel$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  CreateOutputFormatSentinel
> = openEnums.outboundSchema(CreateOutputFormatSentinel);

/** @internal */
export type CreateOutputPqControlsSentinel$Outbound = {};

/** @internal */
export const CreateOutputPqControlsSentinel$outboundSchema: z.ZodType<
  CreateOutputPqControlsSentinel$Outbound,
  z.ZodTypeDef,
  CreateOutputPqControlsSentinel
> = z.object({});

export function createOutputPqControlsSentinelToJSON(
  createOutputPqControlsSentinel: CreateOutputPqControlsSentinel,
): string {
  return JSON.stringify(
    CreateOutputPqControlsSentinel$outboundSchema.parse(
      createOutputPqControlsSentinel,
    ),
  );
}

/** @internal */
export type CreateOutputOutputSentinel$Outbound = {
  id: string;
  type: "sentinel";
  pipeline?: string | undefined;
  systemFields?: Array<string> | undefined;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  keepAlive?: boolean | undefined;
  concurrency?: number | undefined;
  maxPayloadSizeKB?: number | undefined;
  maxPayloadEvents?: number | undefined;
  compress?: boolean | undefined;
  rejectUnauthorized?: boolean | undefined;
  timeoutSec?: number | undefined;
  flushPeriodSec?: number | undefined;
  extraHttpHeaders?:
    | Array<models.ItemsTypeExtraHttpHeaders$Outbound>
    | undefined;
  useRoundRobinDns?: boolean | undefined;
  failedRequestLoggingMode?: string | undefined;
  safeHeaders?: Array<string> | undefined;
  responseRetrySettings?:
    | Array<models.ItemsTypeResponseRetrySettings$Outbound>
    | undefined;
  timeoutRetrySettings?: models.TimeoutRetrySettingsType$Outbound | undefined;
  responseHonorRetryAfterHeader?: boolean | undefined;
  onBackpressure?: string | undefined;
  authType?: string | undefined;
  loginUrl: string;
  secret: string;
  client_id: string;
  scope?: string | undefined;
  endpointURLConfiguration: string;
  totalMemoryLimitKB?: number | undefined;
  description?: string | undefined;
  format?: string | undefined;
  customSourceExpression?: string | undefined;
  customDropWhenNull?: boolean | undefined;
  customEventDelimiter?: string | undefined;
  customContentType?: string | undefined;
  customPayloadExpression?: string | undefined;
  advancedContentType?: string | undefined;
  formatEventCode?: string | undefined;
  formatPayloadCode?: string | undefined;
  pqStrictOrdering?: boolean | undefined;
  pqRatePerSec?: number | undefined;
  pqMode?: string | undefined;
  pqMaxBufferSize?: number | undefined;
  pqMaxBackpressureSec?: number | undefined;
  pqMaxFileSize?: string | undefined;
  pqMaxSize?: string | undefined;
  pqPath?: string | undefined;
  pqCompress?: string | undefined;
  pqOnBackpressure?: string | undefined;
  pqControls?: CreateOutputPqControlsSentinel$Outbound | undefined;
  url?: string | undefined;
  dcrID?: string | undefined;
  dceEndpoint?: string | undefined;
  streamName?: string | undefined;
  __template_loginUrl?: string | undefined;
  __template_secret?: string | undefined;
  __template_client_id?: string | undefined;
  __template_scope?: string | undefined;
  __template_url?: string | undefined;
  __template_dcrID?: string | undefined;
  __template_dceEndpoint?: string | undefined;
  __template_streamName?: string | undefined;
};

/** @internal */
export const CreateOutputOutputSentinel$outboundSchema: z.ZodType<
  CreateOutputOutputSentinel$Outbound,
  z.ZodTypeDef,
  CreateOutputOutputSentinel
> = z.object({
  id: z.string(),
  type: z.literal("sentinel"),
  pipeline: z.string().optional(),
  systemFields: z.array(z.string()).optional(),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  keepAlive: z.boolean().optional(),
  concurrency: z.number().optional(),
  maxPayloadSizeKB: z.number().optional(),
  maxPayloadEvents: z.number().optional(),
  compress: z.boolean().optional(),
  rejectUnauthorized: z.boolean().optional(),
  timeoutSec: z.number().optional(),
  flushPeriodSec: z.number().optional(),
  extraHttpHeaders: z.array(models.ItemsTypeExtraHttpHeaders$outboundSchema)
    .optional(),
  useRoundRobinDns: z.boolean().optional(),
  failedRequestLoggingMode: models
    .FailedRequestLoggingModeOptions$outboundSchema.optional(),
  safeHeaders: z.array(z.string()).optional(),
  responseRetrySettings: z.array(
    models.ItemsTypeResponseRetrySettings$outboundSchema,
  ).optional(),
  timeoutRetrySettings: models.TimeoutRetrySettingsType$outboundSchema
    .optional(),
  responseHonorRetryAfterHeader: z.boolean().optional(),
  onBackpressure: models.BackpressureBehaviorOptions$outboundSchema.optional(),
  authType: CreateOutputAuthType$outboundSchema.optional(),
  loginUrl: z.string(),
  secret: z.string(),
  client_id: z.string(),
  scope: z.string().optional(),
  endpointURLConfiguration: CreateOutputEndpointConfiguration$outboundSchema,
  totalMemoryLimitKB: z.number().optional(),
  description: z.string().optional(),
  format: CreateOutputFormatSentinel$outboundSchema.optional(),
  customSourceExpression: z.string().optional(),
  customDropWhenNull: z.boolean().optional(),
  customEventDelimiter: z.string().optional(),
  customContentType: z.string().optional(),
  customPayloadExpression: z.string().optional(),
  advancedContentType: z.string().optional(),
  formatEventCode: z.string().optional(),
  formatPayloadCode: z.string().optional(),
  pqStrictOrdering: z.boolean().optional(),
  pqRatePerSec: z.number().optional(),
  pqMode: models.ModeOptions$outboundSchema.optional(),
  pqMaxBufferSize: z.number().optional(),
  pqMaxBackpressureSec: z.number().optional(),
  pqMaxFileSize: z.string().optional(),
  pqMaxSize: z.string().optional(),
  pqPath: z.string().optional(),
  pqCompress: models.CompressionOptionsPq$outboundSchema.optional(),
  pqOnBackpressure: models.QueueFullBehaviorOptions$outboundSchema.optional(),
  pqControls: z.lazy(() => CreateOutputPqControlsSentinel$outboundSchema)
    .optional(),
  url: z.string().optional(),
  dcrID: z.string().optional(),
  dceEndpoint: z.string().optional(),
  streamName: z.string().optional(),
  __template_loginUrl: z.string().optional(),
  __template_secret: z.string().optional(),
  __template_client_id: z.string().optional(),
  __template_scope: z.string().optional(),
  __template_url: z.string().optional(),
  __template_dcrID: z.string().optional(),
  __template_dceEndpoint: z.string().optional(),
  __template_streamName: z.string().optional(),
});

export function createOutputOutputSentinelToJSON(
  createOutputOutputSentinel: CreateOutputOutputSentinel,
): string {
  return JSON.stringify(
    CreateOutputOutputSentinel$outboundSchema.parse(createOutputOutputSentinel),
  );
}

/** @internal */
export const CreateOutputFormatWebhook$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  CreateOutputFormatWebhook
> = openEnums.outboundSchema(CreateOutputFormatWebhook);

/** @internal */
export const CreateOutputAuthenticationTypeWebhook$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  CreateOutputAuthenticationTypeWebhook
> = openEnums.outboundSchema(CreateOutputAuthenticationTypeWebhook);

/** @internal */
export type CreateOutputPqControlsWebhook$Outbound = {};

/** @internal */
export const CreateOutputPqControlsWebhook$outboundSchema: z.ZodType<
  CreateOutputPqControlsWebhook$Outbound,
  z.ZodTypeDef,
  CreateOutputPqControlsWebhook
> = z.object({});

export function createOutputPqControlsWebhookToJSON(
  createOutputPqControlsWebhook: CreateOutputPqControlsWebhook,
): string {
  return JSON.stringify(
    CreateOutputPqControlsWebhook$outboundSchema.parse(
      createOutputPqControlsWebhook,
    ),
  );
}

/** @internal */
export type CreateOutputOauthParam$Outbound = {
  name: string;
  value: string;
};

/** @internal */
export const CreateOutputOauthParam$outboundSchema: z.ZodType<
  CreateOutputOauthParam$Outbound,
  z.ZodTypeDef,
  CreateOutputOauthParam
> = z.object({
  name: z.string(),
  value: z.string(),
});

export function createOutputOauthParamToJSON(
  createOutputOauthParam: CreateOutputOauthParam,
): string {
  return JSON.stringify(
    CreateOutputOauthParam$outboundSchema.parse(createOutputOauthParam),
  );
}

/** @internal */
export type CreateOutputOauthHeader$Outbound = {
  name: string;
  value: string;
};

/** @internal */
export const CreateOutputOauthHeader$outboundSchema: z.ZodType<
  CreateOutputOauthHeader$Outbound,
  z.ZodTypeDef,
  CreateOutputOauthHeader
> = z.object({
  name: z.string(),
  value: z.string(),
});

export function createOutputOauthHeaderToJSON(
  createOutputOauthHeader: CreateOutputOauthHeader,
): string {
  return JSON.stringify(
    CreateOutputOauthHeader$outboundSchema.parse(createOutputOauthHeader),
  );
}

/** @internal */
export type CreateOutputUrlWebhook$Outbound = {
  url: string;
  weight?: number | undefined;
  __template_url?: string | undefined;
};

/** @internal */
export const CreateOutputUrlWebhook$outboundSchema: z.ZodType<
  CreateOutputUrlWebhook$Outbound,
  z.ZodTypeDef,
  CreateOutputUrlWebhook
> = z.object({
  url: z.string(),
  weight: z.number().optional(),
  __template_url: z.string().optional(),
});

export function createOutputUrlWebhookToJSON(
  createOutputUrlWebhook: CreateOutputUrlWebhook,
): string {
  return JSON.stringify(
    CreateOutputUrlWebhook$outboundSchema.parse(createOutputUrlWebhook),
  );
}

/** @internal */
export type CreateOutputOutputWebhook$Outbound = {
  id: string;
  type: "webhook";
  pipeline?: string | undefined;
  systemFields?: Array<string> | undefined;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  method?: string | undefined;
  format?: string | undefined;
  keepAlive?: boolean | undefined;
  concurrency?: number | undefined;
  maxPayloadSizeKB?: number | undefined;
  maxPayloadEvents?: number | undefined;
  compress?: boolean | undefined;
  rejectUnauthorized?: boolean | undefined;
  timeoutSec?: number | undefined;
  flushPeriodSec?: number | undefined;
  extraHttpHeaders?:
    | Array<models.ItemsTypeExtraHttpHeaders$Outbound>
    | undefined;
  useRoundRobinDns?: boolean | undefined;
  failedRequestLoggingMode?: string | undefined;
  safeHeaders?: Array<string> | undefined;
  responseRetrySettings?:
    | Array<models.ItemsTypeResponseRetrySettings$Outbound>
    | undefined;
  timeoutRetrySettings?: models.TimeoutRetrySettingsType$Outbound | undefined;
  responseHonorRetryAfterHeader?: boolean | undefined;
  onBackpressure?: string | undefined;
  authType?: string | undefined;
  tls?: models.TlsSettingsClientSideType1$Outbound | undefined;
  totalMemoryLimitKB?: number | undefined;
  loadBalanced?: boolean | undefined;
  description?: string | undefined;
  customSourceExpression?: string | undefined;
  customDropWhenNull?: boolean | undefined;
  customEventDelimiter?: string | undefined;
  customContentType?: string | undefined;
  customPayloadExpression?: string | undefined;
  advancedContentType?: string | undefined;
  formatEventCode?: string | undefined;
  formatPayloadCode?: string | undefined;
  pqStrictOrdering?: boolean | undefined;
  pqRatePerSec?: number | undefined;
  pqMode?: string | undefined;
  pqMaxBufferSize?: number | undefined;
  pqMaxBackpressureSec?: number | undefined;
  pqMaxFileSize?: string | undefined;
  pqMaxSize?: string | undefined;
  pqPath?: string | undefined;
  pqCompress?: string | undefined;
  pqOnBackpressure?: string | undefined;
  pqControls?: CreateOutputPqControlsWebhook$Outbound | undefined;
  username?: string | undefined;
  password?: string | undefined;
  token?: string | undefined;
  credentialsSecret?: string | undefined;
  textSecret?: string | undefined;
  loginUrl?: string | undefined;
  secretParamName?: string | undefined;
  secret?: string | undefined;
  tokenAttributeName?: string | undefined;
  authHeaderExpr?: string | undefined;
  tokenTimeoutSecs?: number | undefined;
  oauthParams?: Array<CreateOutputOauthParam$Outbound> | undefined;
  oauthHeaders?: Array<CreateOutputOauthHeader$Outbound> | undefined;
  url?: string | undefined;
  excludeSelf?: boolean | undefined;
  urls?: Array<CreateOutputUrlWebhook$Outbound> | undefined;
  dnsResolvePeriodSec?: number | undefined;
  loadBalanceStatsPeriodSec?: number | undefined;
  __template_loginUrl?: string | undefined;
  __template_secret?: string | undefined;
  __template_url?: string | undefined;
};

/** @internal */
export const CreateOutputOutputWebhook$outboundSchema: z.ZodType<
  CreateOutputOutputWebhook$Outbound,
  z.ZodTypeDef,
  CreateOutputOutputWebhook
> = z.object({
  id: z.string(),
  type: z.literal("webhook"),
  pipeline: z.string().optional(),
  systemFields: z.array(z.string()).optional(),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  method: models.MethodOptions$outboundSchema.optional(),
  format: CreateOutputFormatWebhook$outboundSchema.optional(),
  keepAlive: z.boolean().optional(),
  concurrency: z.number().optional(),
  maxPayloadSizeKB: z.number().optional(),
  maxPayloadEvents: z.number().optional(),
  compress: z.boolean().optional(),
  rejectUnauthorized: z.boolean().optional(),
  timeoutSec: z.number().optional(),
  flushPeriodSec: z.number().optional(),
  extraHttpHeaders: z.array(models.ItemsTypeExtraHttpHeaders$outboundSchema)
    .optional(),
  useRoundRobinDns: z.boolean().optional(),
  failedRequestLoggingMode: models
    .FailedRequestLoggingModeOptions$outboundSchema.optional(),
  safeHeaders: z.array(z.string()).optional(),
  responseRetrySettings: z.array(
    models.ItemsTypeResponseRetrySettings$outboundSchema,
  ).optional(),
  timeoutRetrySettings: models.TimeoutRetrySettingsType$outboundSchema
    .optional(),
  responseHonorRetryAfterHeader: z.boolean().optional(),
  onBackpressure: models.BackpressureBehaviorOptions$outboundSchema.optional(),
  authType: CreateOutputAuthenticationTypeWebhook$outboundSchema.optional(),
  tls: models.TlsSettingsClientSideType1$outboundSchema.optional(),
  totalMemoryLimitKB: z.number().optional(),
  loadBalanced: z.boolean().optional(),
  description: z.string().optional(),
  customSourceExpression: z.string().optional(),
  customDropWhenNull: z.boolean().optional(),
  customEventDelimiter: z.string().optional(),
  customContentType: z.string().optional(),
  customPayloadExpression: z.string().optional(),
  advancedContentType: z.string().optional(),
  formatEventCode: z.string().optional(),
  formatPayloadCode: z.string().optional(),
  pqStrictOrdering: z.boolean().optional(),
  pqRatePerSec: z.number().optional(),
  pqMode: models.ModeOptions$outboundSchema.optional(),
  pqMaxBufferSize: z.number().optional(),
  pqMaxBackpressureSec: z.number().optional(),
  pqMaxFileSize: z.string().optional(),
  pqMaxSize: z.string().optional(),
  pqPath: z.string().optional(),
  pqCompress: models.CompressionOptionsPq$outboundSchema.optional(),
  pqOnBackpressure: models.QueueFullBehaviorOptions$outboundSchema.optional(),
  pqControls: z.lazy(() => CreateOutputPqControlsWebhook$outboundSchema)
    .optional(),
  username: z.string().optional(),
  password: z.string().optional(),
  token: z.string().optional(),
  credentialsSecret: z.string().optional(),
  textSecret: z.string().optional(),
  loginUrl: z.string().optional(),
  secretParamName: z.string().optional(),
  secret: z.string().optional(),
  tokenAttributeName: z.string().optional(),
  authHeaderExpr: z.string().optional(),
  tokenTimeoutSecs: z.number().optional(),
  oauthParams: z.array(z.lazy(() => CreateOutputOauthParam$outboundSchema))
    .optional(),
  oauthHeaders: z.array(z.lazy(() => CreateOutputOauthHeader$outboundSchema))
    .optional(),
  url: z.string().optional(),
  excludeSelf: z.boolean().optional(),
  urls: z.array(z.lazy(() => CreateOutputUrlWebhook$outboundSchema)).optional(),
  dnsResolvePeriodSec: z.number().optional(),
  loadBalanceStatsPeriodSec: z.number().optional(),
  __template_loginUrl: z.string().optional(),
  __template_secret: z.string().optional(),
  __template_url: z.string().optional(),
});

export function createOutputOutputWebhookToJSON(
  createOutputOutputWebhook: CreateOutputOutputWebhook,
): string {
  return JSON.stringify(
    CreateOutputOutputWebhook$outboundSchema.parse(createOutputOutputWebhook),
  );
}

/** @internal */
export type CreateOutputOutputDefault$Outbound = {
  id: string;
  type: "default";
  pipeline?: string | undefined;
  systemFields?: Array<string> | undefined;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  defaultId: string | null;
};

/** @internal */
export const CreateOutputOutputDefault$outboundSchema: z.ZodType<
  CreateOutputOutputDefault$Outbound,
  z.ZodTypeDef,
  CreateOutputOutputDefault
> = z.object({
  id: z.string(),
  type: z.literal("default"),
  pipeline: z.string().optional(),
  systemFields: z.array(z.string()).optional(),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  defaultId: z.nullable(z.string()),
});

export function createOutputOutputDefaultToJSON(
  createOutputOutputDefault: CreateOutputOutputDefault,
): string {
  return JSON.stringify(
    CreateOutputOutputDefault$outboundSchema.parse(createOutputOutputDefault),
  );
}

/** @internal */
export type CreateOutputRequest$Outbound =
  | CreateOutputOutputDefault$Outbound
  | CreateOutputOutputWebhook$Outbound
  | CreateOutputOutputSentinel$Outbound
  | CreateOutputOutputDevnull$Outbound
  | CreateOutputOutputSyslog$Outbound
  | CreateOutputOutputSplunk$Outbound
  | CreateOutputOutputSplunkLb$Outbound
  | CreateOutputOutputSplunkHec$Outbound
  | CreateOutputOutputWizHec$Outbound
  | CreateOutputOutputTcpjson$Outbound
  | CreateOutputOutputWavefront$Outbound
  | CreateOutputOutputSignalfx$Outbound
  | CreateOutputOutputFilesystem$Outbound
  | CreateOutputOutputS3$Outbound
  | CreateOutputOutputAzureBlob$Outbound
  | CreateOutputOutputAzureDataExplorer$Outbound
  | CreateOutputOutputAzureLogs$Outbound
  | CreateOutputOutputKinesis$Outbound
  | CreateOutputOutputHoneycomb$Outbound
  | CreateOutputOutputAzureEventhub$Outbound
  | CreateOutputOutputGoogleChronicle$Outbound
  | CreateOutputOutputGoogleCloudStorage$Outbound
  | CreateOutputOutputGoogleCloudLogging$Outbound
  | CreateOutputOutputGooglePubsub$Outbound
  | CreateOutputOutputExabeam$Outbound
  | CreateOutputOutputKafka$Outbound
  | CreateOutputOutputConfluentCloud$Outbound
  | CreateOutputOutputMsk$Outbound
  | CreateOutputOutputElastic$Outbound
  | CreateOutputOutputElasticCloud$Outbound
  | CreateOutputOutputNewrelic$Outbound
  | CreateOutputOutputNewrelicEvents$Outbound
  | CreateOutputOutputInfluxdb$Outbound
  | CreateOutputOutputCloudwatch$Outbound
  | CreateOutputOutputMinio$Outbound
  | CreateOutputOutputStatsd$Outbound
  | CreateOutputOutputStatsdExt$Outbound
  | CreateOutputOutputGraphite$Outbound
  | CreateOutputOutputRouter$Outbound
  | CreateOutputOutputSns$Outbound
  | CreateOutputOutputSqs$Outbound
  | CreateOutputOutputSnmp$Outbound
  | CreateOutputOutputSumoLogic$Outbound
  | CreateOutputOutputDatadog$Outbound
  | (CreateOutputOutputGrafanaCloudUnion$Outbound & { type: "grafana_cloud" })
  | CreateOutputOutputLoki$Outbound
  | CreateOutputOutputPrometheus$Outbound
  | CreateOutputOutputRing$Outbound
  | CreateOutputOutputOpenTelemetry$Outbound
  | CreateOutputOutputServiceNow$Outbound
  | CreateOutputOutputDataset$Outbound
  | CreateOutputOutputCriblTcp$Outbound
  | CreateOutputOutputCriblHttp$Outbound
  | CreateOutputOutputCriblSearchEngine$Outbound
  | CreateOutputOutputHumioHec$Outbound
  | CreateOutputOutputCrowdstrikeNextGenSiem$Outbound
  | CreateOutputOutputDlS3$Outbound
  | CreateOutputOutputSecurityLake$Outbound
  | CreateOutputOutputCriblLake$Outbound
  | CreateOutputOutputDiskSpool$Outbound
  | CreateOutputOutputClickHouse$Outbound
  | CreateOutputOutputXsiam$Outbound
  | CreateOutputOutputNetflow$Outbound
  | CreateOutputOutputDynatraceHttp$Outbound
  | CreateOutputOutputDynatraceOtlp$Outbound
  | CreateOutputOutputSentinelOneAiSiem$Outbound
  | CreateOutputOutputChronicle$Outbound
  | CreateOutputOutputDatabricks$Outbound
  | CreateOutputOutputMicrosoftFabric$Outbound
  | CreateOutputOutputCloudflareR2$Outbound;

/** @internal */
export const CreateOutputRequest$outboundSchema: z.ZodType<
  CreateOutputRequest$Outbound,
  z.ZodTypeDef,
  CreateOutputRequest
> = z.union([
  z.lazy(() => CreateOutputOutputDefault$outboundSchema),
  z.lazy(() => CreateOutputOutputWebhook$outboundSchema),
  z.lazy(() => CreateOutputOutputSentinel$outboundSchema),
  z.lazy(() => CreateOutputOutputDevnull$outboundSchema),
  z.lazy(() => CreateOutputOutputSyslog$outboundSchema),
  z.lazy(() => CreateOutputOutputSplunk$outboundSchema),
  z.lazy(() => CreateOutputOutputSplunkLb$outboundSchema),
  z.lazy(() => CreateOutputOutputSplunkHec$outboundSchema),
  z.lazy(() => CreateOutputOutputWizHec$outboundSchema),
  z.lazy(() => CreateOutputOutputTcpjson$outboundSchema),
  z.lazy(() => CreateOutputOutputWavefront$outboundSchema),
  z.lazy(() => CreateOutputOutputSignalfx$outboundSchema),
  z.lazy(() => CreateOutputOutputFilesystem$outboundSchema),
  z.lazy(() => CreateOutputOutputS3$outboundSchema),
  z.lazy(() => CreateOutputOutputAzureBlob$outboundSchema),
  z.lazy(() => CreateOutputOutputAzureDataExplorer$outboundSchema),
  z.lazy(() => CreateOutputOutputAzureLogs$outboundSchema),
  CreateOutputOutputKinesis$outboundSchema,
  CreateOutputOutputHoneycomb$outboundSchema,
  CreateOutputOutputAzureEventhub$outboundSchema,
  CreateOutputOutputGoogleChronicle$outboundSchema,
  CreateOutputOutputGoogleCloudStorage$outboundSchema,
  CreateOutputOutputGoogleCloudLogging$outboundSchema,
  CreateOutputOutputGooglePubsub$outboundSchema,
  CreateOutputOutputExabeam$outboundSchema,
  CreateOutputOutputKafka$outboundSchema,
  CreateOutputOutputConfluentCloud$outboundSchema,
  CreateOutputOutputMsk$outboundSchema,
  CreateOutputOutputElastic$outboundSchema,
  CreateOutputOutputElasticCloud$outboundSchema,
  CreateOutputOutputNewrelic$outboundSchema,
  CreateOutputOutputNewrelicEvents$outboundSchema,
  CreateOutputOutputInfluxdb$outboundSchema,
  CreateOutputOutputCloudwatch$outboundSchema,
  CreateOutputOutputMinio$outboundSchema,
  CreateOutputOutputStatsd$outboundSchema,
  CreateOutputOutputStatsdExt$outboundSchema,
  CreateOutputOutputGraphite$outboundSchema,
  CreateOutputOutputRouter$outboundSchema,
  CreateOutputOutputSns$outboundSchema,
  CreateOutputOutputSqs$outboundSchema,
  CreateOutputOutputSnmp$outboundSchema,
  CreateOutputOutputSumoLogic$outboundSchema,
  CreateOutputOutputDatadog$outboundSchema,
  CreateOutputOutputGrafanaCloudUnion$outboundSchema.and(
    z.object({ type: z.literal("grafana_cloud") }),
  ),
  CreateOutputOutputLoki$outboundSchema,
  CreateOutputOutputPrometheus$outboundSchema,
  CreateOutputOutputRing$outboundSchema,
  CreateOutputOutputOpenTelemetry$outboundSchema,
  CreateOutputOutputServiceNow$outboundSchema,
  CreateOutputOutputDataset$outboundSchema,
  CreateOutputOutputCriblTcp$outboundSchema,
  CreateOutputOutputCriblHttp$outboundSchema,
  CreateOutputOutputCriblSearchEngine$outboundSchema,
  CreateOutputOutputHumioHec$outboundSchema,
  CreateOutputOutputCrowdstrikeNextGenSiem$outboundSchema,
  CreateOutputOutputDlS3$outboundSchema,
  CreateOutputOutputSecurityLake$outboundSchema,
  CreateOutputOutputCriblLake$outboundSchema,
  CreateOutputOutputDiskSpool$outboundSchema,
  CreateOutputOutputClickHouse$outboundSchema,
  CreateOutputOutputXsiam$outboundSchema,
  CreateOutputOutputNetflow$outboundSchema,
  CreateOutputOutputDynatraceHttp$outboundSchema,
  CreateOutputOutputDynatraceOtlp$outboundSchema,
  CreateOutputOutputSentinelOneAiSiem$outboundSchema,
  CreateOutputOutputChronicle$outboundSchema,
  CreateOutputOutputDatabricks$outboundSchema,
  CreateOutputOutputMicrosoftFabric$outboundSchema,
  CreateOutputOutputCloudflareR2$outboundSchema,
]);

export function createOutputRequestToJSON(
  createOutputRequest: CreateOutputRequest,
): string {
  return JSON.stringify(
    CreateOutputRequest$outboundSchema.parse(createOutputRequest),
  );
}
