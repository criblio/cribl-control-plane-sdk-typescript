/*
 * Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT.
 */

import * as z from "zod/v3";
import * as openEnums from "../../types/enums.js";
import { ClosedEnum, OpenEnum } from "../../types/enums.js";
import * as models from "../index.js";

export const InputCloudflareHecType = {
  CloudflareHec: "cloudflare_hec",
} as const;
export type InputCloudflareHecType = ClosedEnum<typeof InputCloudflareHecType>;

/**
 * Select Secret to use a text secret to authenticate
 */
export const InputCloudflareHecAuthenticationMethod = {
  Secret: "secret",
  Manual: "manual",
} as const;
/**
 * Select Secret to use a text secret to authenticate
 */
export type InputCloudflareHecAuthenticationMethod = OpenEnum<
  typeof InputCloudflareHecAuthenticationMethod
>;

export type InputCloudflareHecAuthToken = {
  /**
   * Select Secret to use a text secret to authenticate
   */
  authType?: InputCloudflareHecAuthenticationMethod | undefined;
  /**
   * Select or create a stored text secret
   */
  tokenSecret?: string | undefined;
  /**
   * Shared secret to be provided by any client (Authorization: <token>)
   */
  token?: string | undefined;
  enabled?: boolean | undefined;
  description?: string | undefined;
  /**
   * Enter the values you want to allow in the HEC event index field at the token level. Supports wildcards. To skip validation, leave blank.
   */
  allowedIndexesAtToken?: Array<string> | undefined;
  /**
   * Fields to add to events referencing this token
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
};

export type InputCloudflareHecPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputCloudflareHecType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokens?: Array<InputCloudflareHecAuthToken> | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  enableHealthCheck?: any | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Absolute path on which to listen for the Cloudflare HTTP Event Collector API requests. This input supports the /event endpoint.
   */
  hecAPI: string;
  /**
   * Fields to add to every event. May be overridden by fields added at the token or request level.
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * List values allowed in HEC event index field. Leave blank to skip validation. Supports wildcards. The values here can expand index validation at the token level.
   */
  allowedIndexes?: Array<string> | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * HTTP origins to which @{product} should send CORS (cross-origin resource sharing) Access-Control-Allow-* headers. Supports wildcards.
   */
  accessControlAllowOrigin?: Array<string> | undefined;
  /**
   * HTTP headers that @{product} will send to allowed origins as "Access-Control-Allow-Headers" in a CORS preflight response. Use "*" to allow all headers.
   */
  accessControlAllowHeaders?: Array<string> | undefined;
  /**
   * Emit per-token (<prefix>.http.perToken) and summary (<prefix>.http.summary) request metrics
   */
  emitTokenMetrics?: boolean | undefined;
  description?: string | undefined;
};

export type InputCloudflareHecPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputCloudflareHecType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokens?: Array<InputCloudflareHecAuthToken> | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  enableHealthCheck?: any | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Absolute path on which to listen for the Cloudflare HTTP Event Collector API requests. This input supports the /event endpoint.
   */
  hecAPI: string;
  /**
   * Fields to add to every event. May be overridden by fields added at the token or request level.
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * List values allowed in HEC event index field. Leave blank to skip validation. Supports wildcards. The values here can expand index validation at the token level.
   */
  allowedIndexes?: Array<string> | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * HTTP origins to which @{product} should send CORS (cross-origin resource sharing) Access-Control-Allow-* headers. Supports wildcards.
   */
  accessControlAllowOrigin?: Array<string> | undefined;
  /**
   * HTTP headers that @{product} will send to allowed origins as "Access-Control-Allow-Headers" in a CORS preflight response. Use "*" to allow all headers.
   */
  accessControlAllowHeaders?: Array<string> | undefined;
  /**
   * Emit per-token (<prefix>.http.perToken) and summary (<prefix>.http.summary) request metrics
   */
  emitTokenMetrics?: boolean | undefined;
  description?: string | undefined;
};

export type InputCloudflareHecSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputCloudflareHecType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokens?: Array<InputCloudflareHecAuthToken> | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  enableHealthCheck?: any | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Absolute path on which to listen for the Cloudflare HTTP Event Collector API requests. This input supports the /event endpoint.
   */
  hecAPI: string;
  /**
   * Fields to add to every event. May be overridden by fields added at the token or request level.
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * List values allowed in HEC event index field. Leave blank to skip validation. Supports wildcards. The values here can expand index validation at the token level.
   */
  allowedIndexes?: Array<string> | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * HTTP origins to which @{product} should send CORS (cross-origin resource sharing) Access-Control-Allow-* headers. Supports wildcards.
   */
  accessControlAllowOrigin?: Array<string> | undefined;
  /**
   * HTTP headers that @{product} will send to allowed origins as "Access-Control-Allow-Headers" in a CORS preflight response. Use "*" to allow all headers.
   */
  accessControlAllowHeaders?: Array<string> | undefined;
  /**
   * Emit per-token (<prefix>.http.perToken) and summary (<prefix>.http.summary) request metrics
   */
  emitTokenMetrics?: boolean | undefined;
  description?: string | undefined;
};

export type InputCloudflareHecSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputCloudflareHecType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokens?: Array<InputCloudflareHecAuthToken> | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  enableHealthCheck?: any | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Absolute path on which to listen for the Cloudflare HTTP Event Collector API requests. This input supports the /event endpoint.
   */
  hecAPI: string;
  /**
   * Fields to add to every event. May be overridden by fields added at the token or request level.
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * List values allowed in HEC event index field. Leave blank to skip validation. Supports wildcards. The values here can expand index validation at the token level.
   */
  allowedIndexes?: Array<string> | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * HTTP origins to which @{product} should send CORS (cross-origin resource sharing) Access-Control-Allow-* headers. Supports wildcards.
   */
  accessControlAllowOrigin?: Array<string> | undefined;
  /**
   * HTTP headers that @{product} will send to allowed origins as "Access-Control-Allow-Headers" in a CORS preflight response. Use "*" to allow all headers.
   */
  accessControlAllowHeaders?: Array<string> | undefined;
  /**
   * Emit per-token (<prefix>.http.perToken) and summary (<prefix>.http.summary) request metrics
   */
  emitTokenMetrics?: boolean | undefined;
  description?: string | undefined;
};

export type InputCloudflareHec =
  | InputCloudflareHecSendToRoutesTrueWithConnectionsConstraint
  | InputCloudflareHecSendToRoutesFalseWithConnectionsConstraint
  | InputCloudflareHecPqEnabledFalseWithPqConstraint
  | InputCloudflareHecPqEnabledTrueWithPqConstraint;

export const InputZscalerHecType = {
  ZscalerHec: "zscaler_hec",
} as const;
export type InputZscalerHecType = ClosedEnum<typeof InputZscalerHecType>;

export type InputZscalerHecAuthToken = {
  /**
   * Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
   */
  authType?: models.AuthenticationMethodOptionsAuthTokensItems | undefined;
  /**
   * Select or create a stored text secret
   */
  tokenSecret?: string | undefined;
  /**
   * Shared secret to be provided by any client (Authorization: <token>)
   */
  token: string;
  enabled?: boolean | undefined;
  description?: string | undefined;
  /**
   * Enter the values you want to allow in the HEC event index field at the token level. Supports wildcards. To skip validation, leave blank.
   */
  allowedIndexesAtToken?: Array<string> | undefined;
  /**
   * Fields to add to events referencing this token
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
};

export type InputZscalerHecPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputZscalerHecType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokens?: Array<InputZscalerHecAuthToken> | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  enableHealthCheck?: any | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Absolute path on which to listen for the Zscaler HTTP Event Collector API requests. This input supports the /event endpoint.
   */
  hecAPI?: string | undefined;
  /**
   * Fields to add to every event. May be overridden by fields added at the token or request level.
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * List values allowed in HEC event index field. Leave blank to skip validation. Supports wildcards. The values here can expand index validation at the token level.
   */
  allowedIndexes?: Array<string> | undefined;
  /**
   * Whether to enable Zscaler HEC acknowledgements
   */
  hecAcks?: boolean | undefined;
  /**
   * Optionally, list HTTP origins to which @{product} should send CORS (cross-origin resource sharing) Access-Control-Allow-* headers. Supports wildcards.
   */
  accessControlAllowOrigin?: Array<string> | undefined;
  /**
   * Optionally, list HTTP headers that @{product} will send to allowed origins as "Access-Control-Allow-Headers" in a CORS preflight response. Use "*" to allow all headers.
   */
  accessControlAllowHeaders?: Array<string> | undefined;
  /**
   * Enable to emit per-token (<prefix>.http.perToken) and summary (<prefix>.http.summary) request metrics
   */
  emitTokenMetrics?: boolean | undefined;
  description?: string | undefined;
};

export type InputZscalerHecPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputZscalerHecType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokens?: Array<InputZscalerHecAuthToken> | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  enableHealthCheck?: any | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Absolute path on which to listen for the Zscaler HTTP Event Collector API requests. This input supports the /event endpoint.
   */
  hecAPI?: string | undefined;
  /**
   * Fields to add to every event. May be overridden by fields added at the token or request level.
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * List values allowed in HEC event index field. Leave blank to skip validation. Supports wildcards. The values here can expand index validation at the token level.
   */
  allowedIndexes?: Array<string> | undefined;
  /**
   * Whether to enable Zscaler HEC acknowledgements
   */
  hecAcks?: boolean | undefined;
  /**
   * Optionally, list HTTP origins to which @{product} should send CORS (cross-origin resource sharing) Access-Control-Allow-* headers. Supports wildcards.
   */
  accessControlAllowOrigin?: Array<string> | undefined;
  /**
   * Optionally, list HTTP headers that @{product} will send to allowed origins as "Access-Control-Allow-Headers" in a CORS preflight response. Use "*" to allow all headers.
   */
  accessControlAllowHeaders?: Array<string> | undefined;
  /**
   * Enable to emit per-token (<prefix>.http.perToken) and summary (<prefix>.http.summary) request metrics
   */
  emitTokenMetrics?: boolean | undefined;
  description?: string | undefined;
};

export type InputZscalerHecSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputZscalerHecType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokens?: Array<InputZscalerHecAuthToken> | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  enableHealthCheck?: any | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Absolute path on which to listen for the Zscaler HTTP Event Collector API requests. This input supports the /event endpoint.
   */
  hecAPI?: string | undefined;
  /**
   * Fields to add to every event. May be overridden by fields added at the token or request level.
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * List values allowed in HEC event index field. Leave blank to skip validation. Supports wildcards. The values here can expand index validation at the token level.
   */
  allowedIndexes?: Array<string> | undefined;
  /**
   * Whether to enable Zscaler HEC acknowledgements
   */
  hecAcks?: boolean | undefined;
  /**
   * Optionally, list HTTP origins to which @{product} should send CORS (cross-origin resource sharing) Access-Control-Allow-* headers. Supports wildcards.
   */
  accessControlAllowOrigin?: Array<string> | undefined;
  /**
   * Optionally, list HTTP headers that @{product} will send to allowed origins as "Access-Control-Allow-Headers" in a CORS preflight response. Use "*" to allow all headers.
   */
  accessControlAllowHeaders?: Array<string> | undefined;
  /**
   * Enable to emit per-token (<prefix>.http.perToken) and summary (<prefix>.http.summary) request metrics
   */
  emitTokenMetrics?: boolean | undefined;
  description?: string | undefined;
};

export type InputZscalerHecSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputZscalerHecType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokens?: Array<InputZscalerHecAuthToken> | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  enableHealthCheck?: any | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Absolute path on which to listen for the Zscaler HTTP Event Collector API requests. This input supports the /event endpoint.
   */
  hecAPI?: string | undefined;
  /**
   * Fields to add to every event. May be overridden by fields added at the token or request level.
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * List values allowed in HEC event index field. Leave blank to skip validation. Supports wildcards. The values here can expand index validation at the token level.
   */
  allowedIndexes?: Array<string> | undefined;
  /**
   * Whether to enable Zscaler HEC acknowledgements
   */
  hecAcks?: boolean | undefined;
  /**
   * Optionally, list HTTP origins to which @{product} should send CORS (cross-origin resource sharing) Access-Control-Allow-* headers. Supports wildcards.
   */
  accessControlAllowOrigin?: Array<string> | undefined;
  /**
   * Optionally, list HTTP headers that @{product} will send to allowed origins as "Access-Control-Allow-Headers" in a CORS preflight response. Use "*" to allow all headers.
   */
  accessControlAllowHeaders?: Array<string> | undefined;
  /**
   * Enable to emit per-token (<prefix>.http.perToken) and summary (<prefix>.http.summary) request metrics
   */
  emitTokenMetrics?: boolean | undefined;
  description?: string | undefined;
};

export type InputZscalerHec =
  | InputZscalerHecSendToRoutesTrueWithConnectionsConstraint
  | InputZscalerHecSendToRoutesFalseWithConnectionsConstraint
  | InputZscalerHecPqEnabledFalseWithPqConstraint
  | InputZscalerHecPqEnabledTrueWithPqConstraint;

export const InputSecurityLakeType = {
  SecurityLake: "security_lake",
} as const;
export type InputSecurityLakeType = ClosedEnum<typeof InputSecurityLakeType>;

export type InputSecurityLakePqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputSecurityLakeType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.
   */
  queueName: string;
  /**
   * Regex matching file names to download and process. Defaults to: .*
   */
  fileFilter?: string | undefined;
  /**
   * SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.
   */
  awsAccountId?: string | undefined;
  /**
   * AWS authentication method. Choose Auto to use IAM roles.
   */
  awsAuthenticationMethod?: string | undefined;
  awsSecretKey?: string | undefined;
  /**
   * AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region.
   */
  region?: string | undefined;
  /**
   * S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.
   */
  endpoint?: string | undefined;
  /**
   * Signature version to use for signing S3 requests
   */
  signatureVersion?: models.SignatureVersionOptionsS3CollectorConf | undefined;
  /**
   * Reuse connections between requests, which can improve performance
   */
  reuseConnections?: boolean | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.
   */
  maxMessages?: number | undefined;
  /**
   * After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).
   */
  visibilityTimeout?: number | undefined;
  /**
   * How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.
   */
  numReceivers?: number | undefined;
  /**
   * Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure.
   */
  socketTimeout?: number | undefined;
  /**
   * Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.
   */
  skipOnError?: boolean | undefined;
  /**
   * Attach SQS notification metadata to a __sqsMetadata field on each event
   */
  includeSqsMetadata?: boolean | undefined;
  /**
   * Use Assume Role credentials to access Amazon S3
   */
  enableAssumeRole?: boolean | undefined;
  /**
   * Amazon Resource Name (ARN) of the role to assume
   */
  assumeRoleArn?: string | undefined;
  /**
   * External ID to use when assuming role
   */
  assumeRoleExternalId?: string | undefined;
  /**
   * Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
   */
  durationSeconds?: number | undefined;
  /**
   * Use Assume Role credentials when accessing Amazon SQS
   */
  enableSQSAssumeRole?: boolean | undefined;
  preprocess?: models.PreprocessTypeSavedJobCollectionInput | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Maximum file size for each Parquet chunk
   */
  parquetChunkSizeMB?: number | undefined;
  /**
   * The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified.
   */
  parquetChunkDownloadTimeout?: number | undefined;
  checkpointing?: models.CheckpointingType | undefined;
  /**
   * How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.
   */
  pollTimeout?: number | undefined;
  /**
   * Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters.
   */
  encoding?: string | undefined;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  /**
   * Select or create a stored secret that references your access key and secret key
   */
  awsSecret?: string | undefined;
  tagAfterProcessing?: models.TagAfterProcessingOptions | undefined;
  /**
   * The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.
   */
  processedTagKey?: string | undefined;
  /**
   * The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.
   */
  processedTagValue?: string | undefined;
};

export type InputSecurityLakePqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputSecurityLakeType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.
   */
  queueName: string;
  /**
   * Regex matching file names to download and process. Defaults to: .*
   */
  fileFilter?: string | undefined;
  /**
   * SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.
   */
  awsAccountId?: string | undefined;
  /**
   * AWS authentication method. Choose Auto to use IAM roles.
   */
  awsAuthenticationMethod?: string | undefined;
  awsSecretKey?: string | undefined;
  /**
   * AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region.
   */
  region?: string | undefined;
  /**
   * S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.
   */
  endpoint?: string | undefined;
  /**
   * Signature version to use for signing S3 requests
   */
  signatureVersion?: models.SignatureVersionOptionsS3CollectorConf | undefined;
  /**
   * Reuse connections between requests, which can improve performance
   */
  reuseConnections?: boolean | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.
   */
  maxMessages?: number | undefined;
  /**
   * After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).
   */
  visibilityTimeout?: number | undefined;
  /**
   * How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.
   */
  numReceivers?: number | undefined;
  /**
   * Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure.
   */
  socketTimeout?: number | undefined;
  /**
   * Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.
   */
  skipOnError?: boolean | undefined;
  /**
   * Attach SQS notification metadata to a __sqsMetadata field on each event
   */
  includeSqsMetadata?: boolean | undefined;
  /**
   * Use Assume Role credentials to access Amazon S3
   */
  enableAssumeRole?: boolean | undefined;
  /**
   * Amazon Resource Name (ARN) of the role to assume
   */
  assumeRoleArn?: string | undefined;
  /**
   * External ID to use when assuming role
   */
  assumeRoleExternalId?: string | undefined;
  /**
   * Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
   */
  durationSeconds?: number | undefined;
  /**
   * Use Assume Role credentials when accessing Amazon SQS
   */
  enableSQSAssumeRole?: boolean | undefined;
  preprocess?: models.PreprocessTypeSavedJobCollectionInput | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Maximum file size for each Parquet chunk
   */
  parquetChunkSizeMB?: number | undefined;
  /**
   * The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified.
   */
  parquetChunkDownloadTimeout?: number | undefined;
  checkpointing?: models.CheckpointingType | undefined;
  /**
   * How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.
   */
  pollTimeout?: number | undefined;
  /**
   * Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters.
   */
  encoding?: string | undefined;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  /**
   * Select or create a stored secret that references your access key and secret key
   */
  awsSecret?: string | undefined;
  tagAfterProcessing?: models.TagAfterProcessingOptions | undefined;
  /**
   * The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.
   */
  processedTagKey?: string | undefined;
  /**
   * The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.
   */
  processedTagValue?: string | undefined;
};

export type InputSecurityLakeSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputSecurityLakeType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.
   */
  queueName: string;
  /**
   * Regex matching file names to download and process. Defaults to: .*
   */
  fileFilter?: string | undefined;
  /**
   * SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.
   */
  awsAccountId?: string | undefined;
  /**
   * AWS authentication method. Choose Auto to use IAM roles.
   */
  awsAuthenticationMethod?: string | undefined;
  awsSecretKey?: string | undefined;
  /**
   * AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region.
   */
  region?: string | undefined;
  /**
   * S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.
   */
  endpoint?: string | undefined;
  /**
   * Signature version to use for signing S3 requests
   */
  signatureVersion?: models.SignatureVersionOptionsS3CollectorConf | undefined;
  /**
   * Reuse connections between requests, which can improve performance
   */
  reuseConnections?: boolean | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.
   */
  maxMessages?: number | undefined;
  /**
   * After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).
   */
  visibilityTimeout?: number | undefined;
  /**
   * How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.
   */
  numReceivers?: number | undefined;
  /**
   * Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure.
   */
  socketTimeout?: number | undefined;
  /**
   * Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.
   */
  skipOnError?: boolean | undefined;
  /**
   * Attach SQS notification metadata to a __sqsMetadata field on each event
   */
  includeSqsMetadata?: boolean | undefined;
  /**
   * Use Assume Role credentials to access Amazon S3
   */
  enableAssumeRole?: boolean | undefined;
  /**
   * Amazon Resource Name (ARN) of the role to assume
   */
  assumeRoleArn?: string | undefined;
  /**
   * External ID to use when assuming role
   */
  assumeRoleExternalId?: string | undefined;
  /**
   * Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
   */
  durationSeconds?: number | undefined;
  /**
   * Use Assume Role credentials when accessing Amazon SQS
   */
  enableSQSAssumeRole?: boolean | undefined;
  preprocess?: models.PreprocessTypeSavedJobCollectionInput | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Maximum file size for each Parquet chunk
   */
  parquetChunkSizeMB?: number | undefined;
  /**
   * The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified.
   */
  parquetChunkDownloadTimeout?: number | undefined;
  checkpointing?: models.CheckpointingType | undefined;
  /**
   * How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.
   */
  pollTimeout?: number | undefined;
  /**
   * Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters.
   */
  encoding?: string | undefined;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  /**
   * Select or create a stored secret that references your access key and secret key
   */
  awsSecret?: string | undefined;
  tagAfterProcessing?: models.TagAfterProcessingOptions | undefined;
  /**
   * The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.
   */
  processedTagKey?: string | undefined;
  /**
   * The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.
   */
  processedTagValue?: string | undefined;
};

export type InputSecurityLakeSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputSecurityLakeType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.
   */
  queueName: string;
  /**
   * Regex matching file names to download and process. Defaults to: .*
   */
  fileFilter?: string | undefined;
  /**
   * SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.
   */
  awsAccountId?: string | undefined;
  /**
   * AWS authentication method. Choose Auto to use IAM roles.
   */
  awsAuthenticationMethod?: string | undefined;
  awsSecretKey?: string | undefined;
  /**
   * AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region.
   */
  region?: string | undefined;
  /**
   * S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.
   */
  endpoint?: string | undefined;
  /**
   * Signature version to use for signing S3 requests
   */
  signatureVersion?: models.SignatureVersionOptionsS3CollectorConf | undefined;
  /**
   * Reuse connections between requests, which can improve performance
   */
  reuseConnections?: boolean | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.
   */
  maxMessages?: number | undefined;
  /**
   * After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).
   */
  visibilityTimeout?: number | undefined;
  /**
   * How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.
   */
  numReceivers?: number | undefined;
  /**
   * Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure.
   */
  socketTimeout?: number | undefined;
  /**
   * Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.
   */
  skipOnError?: boolean | undefined;
  /**
   * Attach SQS notification metadata to a __sqsMetadata field on each event
   */
  includeSqsMetadata?: boolean | undefined;
  /**
   * Use Assume Role credentials to access Amazon S3
   */
  enableAssumeRole?: boolean | undefined;
  /**
   * Amazon Resource Name (ARN) of the role to assume
   */
  assumeRoleArn?: string | undefined;
  /**
   * External ID to use when assuming role
   */
  assumeRoleExternalId?: string | undefined;
  /**
   * Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
   */
  durationSeconds?: number | undefined;
  /**
   * Use Assume Role credentials when accessing Amazon SQS
   */
  enableSQSAssumeRole?: boolean | undefined;
  preprocess?: models.PreprocessTypeSavedJobCollectionInput | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Maximum file size for each Parquet chunk
   */
  parquetChunkSizeMB?: number | undefined;
  /**
   * The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified.
   */
  parquetChunkDownloadTimeout?: number | undefined;
  checkpointing?: models.CheckpointingType | undefined;
  /**
   * How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.
   */
  pollTimeout?: number | undefined;
  /**
   * Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters.
   */
  encoding?: string | undefined;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  /**
   * Select or create a stored secret that references your access key and secret key
   */
  awsSecret?: string | undefined;
  tagAfterProcessing?: models.TagAfterProcessingOptions | undefined;
  /**
   * The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.
   */
  processedTagKey?: string | undefined;
  /**
   * The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.
   */
  processedTagValue?: string | undefined;
};

export type InputSecurityLake =
  | InputSecurityLakeSendToRoutesTrueWithConnectionsConstraint
  | InputSecurityLakeSendToRoutesFalseWithConnectionsConstraint
  | InputSecurityLakePqEnabledFalseWithPqConstraint
  | InputSecurityLakePqEnabledTrueWithPqConstraint;

export const InputNetflowType = {
  Netflow: "netflow",
} as const;
export type InputNetflowType = ClosedEnum<typeof InputNetflowType>;

export type InputNetflowPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputNetflowType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port?: number | undefined;
  /**
   * Allow forwarding of events to a NetFlow destination. Enabling this feature will generate an extra event containing __netflowRaw which can be routed to a NetFlow destination. Note that these events will not count against ingest quota.
   */
  enablePassThrough?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist.
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.
   */
  udpSocketRxBufSize?: number | undefined;
  /**
   * Specifies how many minutes NetFlow v9 templates are cached before being discarded if not refreshed. Adjust based on your network's template update frequency to optimize performance and memory usage.
   */
  templateCacheMinutes?: number | undefined;
  /**
   * Accept messages in Netflow V5 format.
   */
  v5Enabled?: boolean | undefined;
  /**
   * Accept messages in Netflow V9 format.
   */
  v9Enabled?: boolean | undefined;
  /**
   * Accept messages in IPFIX format.
   */
  ipfixEnabled?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputNetflowPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputNetflowType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port?: number | undefined;
  /**
   * Allow forwarding of events to a NetFlow destination. Enabling this feature will generate an extra event containing __netflowRaw which can be routed to a NetFlow destination. Note that these events will not count against ingest quota.
   */
  enablePassThrough?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist.
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.
   */
  udpSocketRxBufSize?: number | undefined;
  /**
   * Specifies how many minutes NetFlow v9 templates are cached before being discarded if not refreshed. Adjust based on your network's template update frequency to optimize performance and memory usage.
   */
  templateCacheMinutes?: number | undefined;
  /**
   * Accept messages in Netflow V5 format.
   */
  v5Enabled?: boolean | undefined;
  /**
   * Accept messages in Netflow V9 format.
   */
  v9Enabled?: boolean | undefined;
  /**
   * Accept messages in IPFIX format.
   */
  ipfixEnabled?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputNetflowSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputNetflowType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port?: number | undefined;
  /**
   * Allow forwarding of events to a NetFlow destination. Enabling this feature will generate an extra event containing __netflowRaw which can be routed to a NetFlow destination. Note that these events will not count against ingest quota.
   */
  enablePassThrough?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist.
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.
   */
  udpSocketRxBufSize?: number | undefined;
  /**
   * Specifies how many minutes NetFlow v9 templates are cached before being discarded if not refreshed. Adjust based on your network's template update frequency to optimize performance and memory usage.
   */
  templateCacheMinutes?: number | undefined;
  /**
   * Accept messages in Netflow V5 format.
   */
  v5Enabled?: boolean | undefined;
  /**
   * Accept messages in Netflow V9 format.
   */
  v9Enabled?: boolean | undefined;
  /**
   * Accept messages in IPFIX format.
   */
  ipfixEnabled?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputNetflowSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputNetflowType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port?: number | undefined;
  /**
   * Allow forwarding of events to a NetFlow destination. Enabling this feature will generate an extra event containing __netflowRaw which can be routed to a NetFlow destination. Note that these events will not count against ingest quota.
   */
  enablePassThrough?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist.
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.
   */
  udpSocketRxBufSize?: number | undefined;
  /**
   * Specifies how many minutes NetFlow v9 templates are cached before being discarded if not refreshed. Adjust based on your network's template update frequency to optimize performance and memory usage.
   */
  templateCacheMinutes?: number | undefined;
  /**
   * Accept messages in Netflow V5 format.
   */
  v5Enabled?: boolean | undefined;
  /**
   * Accept messages in Netflow V9 format.
   */
  v9Enabled?: boolean | undefined;
  /**
   * Accept messages in IPFIX format.
   */
  ipfixEnabled?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputNetflow =
  | InputNetflowSendToRoutesTrueWithConnectionsConstraint
  | InputNetflowSendToRoutesFalseWithConnectionsConstraint
  | InputNetflowPqEnabledFalseWithPqConstraint
  | InputNetflowPqEnabledTrueWithPqConstraint;

export const InputWizWebhookType = {
  WizWebhook: "wiz_webhook",
} as const;
export type InputWizWebhookType = ClosedEnum<typeof InputWizWebhookType>;

export type InputWizWebhookPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputWizWebhookType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokens?: Array<string> | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * List of URI paths accepted by this input. Wildcards are supported (such as /api/v* /hook). Defaults to allow all.
   */
  allowedPaths?: Array<string> | undefined;
  /**
   * List of HTTP methods accepted by this input. Wildcards are supported (such as P*, GET). Defaults to allow all.
   */
  allowedMethods?: Array<string> | undefined;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokensExt?: Array<models.ItemsTypeAuthTokensExt> | undefined;
  description?: string | undefined;
};

export type InputWizWebhookPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputWizWebhookType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokens?: Array<string> | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * List of URI paths accepted by this input. Wildcards are supported (such as /api/v* /hook). Defaults to allow all.
   */
  allowedPaths?: Array<string> | undefined;
  /**
   * List of HTTP methods accepted by this input. Wildcards are supported (such as P*, GET). Defaults to allow all.
   */
  allowedMethods?: Array<string> | undefined;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokensExt?: Array<models.ItemsTypeAuthTokensExt> | undefined;
  description?: string | undefined;
};

export type InputWizWebhookSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputWizWebhookType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokens?: Array<string> | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * List of URI paths accepted by this input. Wildcards are supported (such as /api/v* /hook). Defaults to allow all.
   */
  allowedPaths?: Array<string> | undefined;
  /**
   * List of HTTP methods accepted by this input. Wildcards are supported (such as P*, GET). Defaults to allow all.
   */
  allowedMethods?: Array<string> | undefined;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokensExt?: Array<models.ItemsTypeAuthTokensExt> | undefined;
  description?: string | undefined;
};

export type InputWizWebhookSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputWizWebhookType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokens?: Array<string> | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * List of URI paths accepted by this input. Wildcards are supported (such as /api/v* /hook). Defaults to allow all.
   */
  allowedPaths?: Array<string> | undefined;
  /**
   * List of HTTP methods accepted by this input. Wildcards are supported (such as P*, GET). Defaults to allow all.
   */
  allowedMethods?: Array<string> | undefined;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokensExt?: Array<models.ItemsTypeAuthTokensExt> | undefined;
  description?: string | undefined;
};

export type InputWizWebhook =
  | InputWizWebhookSendToRoutesTrueWithConnectionsConstraint
  | InputWizWebhookSendToRoutesFalseWithConnectionsConstraint
  | InputWizWebhookPqEnabledFalseWithPqConstraint
  | InputWizWebhookPqEnabledTrueWithPqConstraint;

export const InputWizType = {
  Wiz: "wiz",
} as const;
export type InputWizType = ClosedEnum<typeof InputWizType>;

export type ManageState = {};

/**
 * Collector runtime log level
 */
export const InputWizLogLevel = {
  Error: "error",
  Warn: "warn",
  Info: "info",
  Debug: "debug",
  Silly: "silly",
} as const;
/**
 * Collector runtime log level
 */
export type InputWizLogLevel = OpenEnum<typeof InputWizLogLevel>;

export type InputWizContentConfig = {
  /**
   * The name of the Wiz query
   */
  contentType: string;
  contentDescription?: string | undefined;
  enabled?: boolean | undefined;
  /**
   * Track collection progress between consecutive scheduled executions
   */
  stateTracking?: boolean | undefined;
  /**
   * JavaScript expression that defines how to update the state from an event. Use the event's data and the current state to compute the new state. See [Understanding State Expression Fields](https://docs.cribl.io/stream/collectors-rest#state-tracking-expression-fields) for more information.
   */
  stateUpdateExpression?: string | undefined;
  /**
   * JavaScript expression that defines which state to keep when merging a task's newly reported state with previously saved state. Evaluates `prevState` and `newState` variables, resolving to the state to keep.
   */
  stateMergeExpression?: string | undefined;
  manageState?: ManageState | undefined;
  /**
   * Template for POST body to send with the Collect request. Reference global variables, or functions using template params: `${C.vars.myVar}`, or `${Date.now()}`, `${param}`.
   */
  contentQuery: string;
  /**
   * A cron schedule on which to run this job
   */
  cronSchedule?: string | undefined;
  /**
   * Earliest time, relative to now. Format supported: [+|-]<time_integer><time_unit>@<snap-to_time_unit> (ex: -1hr, -42m, -42m@h)
   */
  earliest?: string | undefined;
  /**
   * Latest time, relative to now. Format supported: [+|-]<time_integer><time_unit>@<snap-to_time_unit> (ex: -1hr, -42m, -42m@h)
   */
  latest?: string | undefined;
  /**
   * Maximum time the job is allowed to run (examples: 30, 45s, 15m). Units default to seconds if not specified. Enter 0 for unlimited time.
   */
  jobTimeout?: string | undefined;
  /**
   * Collector runtime log level
   */
  logLevel?: InputWizLogLevel | undefined;
  /**
   * Maximum number of pages to retrieve per collection task. Defaults to 0. Set to 0 to retrieve all pages.
   */
  maxPages?: number | undefined;
};

export type InputWizPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputWizType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * The Wiz GraphQL API endpoint. Example: https://api.us1.app.wiz.io/graphql
   */
  endpoint?: string | undefined;
  /**
   * The authentication URL to generate an OAuth token
   */
  authUrl: string;
  /**
   * The audience to use when requesting an OAuth token for a custom auth URL. When not specified, `wiz-api` will be used.
   */
  authAudienceOverride?: string | undefined;
  /**
   * The client ID of the Wiz application
   */
  clientId: string;
  contentConfig: Array<InputWizContentConfig>;
  /**
   * HTTP request inactivity timeout. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How often workers should check in with the scheduler to keep job subscription alive
   */
  keepAliveTime?: number | undefined;
  /**
   * The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.
   */
  maxMissedKeepAlives?: number | undefined;
  /**
   * Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.
   */
  ttl?: string | undefined;
  /**
   * When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.
   */
  ignoreGroupJobsLimit?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  retryRules?: models.RetryRulesType | undefined;
  /**
   * Enter client secret directly, or select a stored secret
   */
  authType?: models.AuthenticationMethodOptions1 | undefined;
  description?: string | undefined;
  /**
   * The client secret of the Wiz application
   */
  clientSecret?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
};

export type InputWizPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputWizType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * The Wiz GraphQL API endpoint. Example: https://api.us1.app.wiz.io/graphql
   */
  endpoint?: string | undefined;
  /**
   * The authentication URL to generate an OAuth token
   */
  authUrl: string;
  /**
   * The audience to use when requesting an OAuth token for a custom auth URL. When not specified, `wiz-api` will be used.
   */
  authAudienceOverride?: string | undefined;
  /**
   * The client ID of the Wiz application
   */
  clientId: string;
  contentConfig: Array<InputWizContentConfig>;
  /**
   * HTTP request inactivity timeout. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How often workers should check in with the scheduler to keep job subscription alive
   */
  keepAliveTime?: number | undefined;
  /**
   * The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.
   */
  maxMissedKeepAlives?: number | undefined;
  /**
   * Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.
   */
  ttl?: string | undefined;
  /**
   * When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.
   */
  ignoreGroupJobsLimit?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  retryRules?: models.RetryRulesType | undefined;
  /**
   * Enter client secret directly, or select a stored secret
   */
  authType?: models.AuthenticationMethodOptions1 | undefined;
  description?: string | undefined;
  /**
   * The client secret of the Wiz application
   */
  clientSecret?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
};

export type InputWizSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputWizType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * The Wiz GraphQL API endpoint. Example: https://api.us1.app.wiz.io/graphql
   */
  endpoint?: string | undefined;
  /**
   * The authentication URL to generate an OAuth token
   */
  authUrl: string;
  /**
   * The audience to use when requesting an OAuth token for a custom auth URL. When not specified, `wiz-api` will be used.
   */
  authAudienceOverride?: string | undefined;
  /**
   * The client ID of the Wiz application
   */
  clientId: string;
  contentConfig: Array<InputWizContentConfig>;
  /**
   * HTTP request inactivity timeout. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How often workers should check in with the scheduler to keep job subscription alive
   */
  keepAliveTime?: number | undefined;
  /**
   * The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.
   */
  maxMissedKeepAlives?: number | undefined;
  /**
   * Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.
   */
  ttl?: string | undefined;
  /**
   * When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.
   */
  ignoreGroupJobsLimit?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  retryRules?: models.RetryRulesType | undefined;
  /**
   * Enter client secret directly, or select a stored secret
   */
  authType?: models.AuthenticationMethodOptions1 | undefined;
  description?: string | undefined;
  /**
   * The client secret of the Wiz application
   */
  clientSecret?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
};

export type InputWizSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputWizType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * The Wiz GraphQL API endpoint. Example: https://api.us1.app.wiz.io/graphql
   */
  endpoint?: string | undefined;
  /**
   * The authentication URL to generate an OAuth token
   */
  authUrl: string;
  /**
   * The audience to use when requesting an OAuth token for a custom auth URL. When not specified, `wiz-api` will be used.
   */
  authAudienceOverride?: string | undefined;
  /**
   * The client ID of the Wiz application
   */
  clientId: string;
  contentConfig: Array<InputWizContentConfig>;
  /**
   * HTTP request inactivity timeout. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How often workers should check in with the scheduler to keep job subscription alive
   */
  keepAliveTime?: number | undefined;
  /**
   * The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.
   */
  maxMissedKeepAlives?: number | undefined;
  /**
   * Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.
   */
  ttl?: string | undefined;
  /**
   * When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.
   */
  ignoreGroupJobsLimit?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  retryRules?: models.RetryRulesType | undefined;
  /**
   * Enter client secret directly, or select a stored secret
   */
  authType?: models.AuthenticationMethodOptions1 | undefined;
  description?: string | undefined;
  /**
   * The client secret of the Wiz application
   */
  clientSecret?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
};

export type InputWiz =
  | InputWizSendToRoutesTrueWithConnectionsConstraint
  | InputWizSendToRoutesFalseWithConnectionsConstraint
  | InputWizPqEnabledFalseWithPqConstraint
  | InputWizPqEnabledTrueWithPqConstraint;

export const PqEnabledTrueWithPqConstraintInputJournalFilesType = {
  JournalFiles: "journal_files",
} as const;
export type PqEnabledTrueWithPqConstraintInputJournalFilesType = ClosedEnum<
  typeof PqEnabledTrueWithPqConstraintInputJournalFilesType
>;

export type PqEnabledTrueWithPqConstraintRule = {
  /**
   * JavaScript expression applied to Journal objects. Return 'true' to include it.
   */
  filter: string;
  /**
   * Optional description of this rule's purpose
   */
  description?: string | undefined;
};

export type InputJournalFilesPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: PqEnabledTrueWithPqConstraintInputJournalFilesType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Directory path to search for journals. Environment variables will be resolved, e.g. $CRIBL_EDGE_FS_ROOT/var/log/journal/$MACHINE_ID.
   */
  path: string;
  /**
   * Time, in seconds, between scanning for journals.
   */
  interval?: number | undefined;
  /**
   * The full path of discovered journals are matched against this wildcard list.
   */
  journals: Array<string>;
  /**
   * Add rules to decide which journal objects to allow. Events are generated if no rules are given or if all the rules' expressions evaluate to true.
   */
  rules?: Array<PqEnabledTrueWithPqConstraintRule> | undefined;
  /**
   * Skip log messages that are not part of the current boot session.
   */
  currentBoot?: boolean | undefined;
  /**
   * The maximum log message age, in duration form (e.g,: 60s, 4h, 3d, 1w).  Default of no value will apply no max age filters.
   */
  maxAgeDur?: string | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export const PqEnabledFalseWithPqConstraintInputJournalFilesType = {
  JournalFiles: "journal_files",
} as const;
export type PqEnabledFalseWithPqConstraintInputJournalFilesType = ClosedEnum<
  typeof PqEnabledFalseWithPqConstraintInputJournalFilesType
>;

export type PqEnabledFalseWithPqConstraintRule = {
  /**
   * JavaScript expression applied to Journal objects. Return 'true' to include it.
   */
  filter: string;
  /**
   * Optional description of this rule's purpose
   */
  description?: string | undefined;
};

export type InputJournalFilesPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: PqEnabledFalseWithPqConstraintInputJournalFilesType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Directory path to search for journals. Environment variables will be resolved, e.g. $CRIBL_EDGE_FS_ROOT/var/log/journal/$MACHINE_ID.
   */
  path: string;
  /**
   * Time, in seconds, between scanning for journals.
   */
  interval?: number | undefined;
  /**
   * The full path of discovered journals are matched against this wildcard list.
   */
  journals: Array<string>;
  /**
   * Add rules to decide which journal objects to allow. Events are generated if no rules are given or if all the rules' expressions evaluate to true.
   */
  rules?: Array<PqEnabledFalseWithPqConstraintRule> | undefined;
  /**
   * Skip log messages that are not part of the current boot session.
   */
  currentBoot?: boolean | undefined;
  /**
   * The maximum log message age, in duration form (e.g,: 60s, 4h, 3d, 1w).  Default of no value will apply no max age filters.
   */
  maxAgeDur?: string | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export const SendToRoutesFalseWithConnectionsConstraintInputJournalFilesType = {
  JournalFiles: "journal_files",
} as const;
export type SendToRoutesFalseWithConnectionsConstraintInputJournalFilesType =
  ClosedEnum<
    typeof SendToRoutesFalseWithConnectionsConstraintInputJournalFilesType
  >;

export type SendToRoutesFalseWithConnectionsConstraintRule = {
  /**
   * JavaScript expression applied to Journal objects. Return 'true' to include it.
   */
  filter: string;
  /**
   * Optional description of this rule's purpose
   */
  description?: string | undefined;
};

export type InputJournalFilesSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: SendToRoutesFalseWithConnectionsConstraintInputJournalFilesType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Directory path to search for journals. Environment variables will be resolved, e.g. $CRIBL_EDGE_FS_ROOT/var/log/journal/$MACHINE_ID.
   */
  path: string;
  /**
   * Time, in seconds, between scanning for journals.
   */
  interval?: number | undefined;
  /**
   * The full path of discovered journals are matched against this wildcard list.
   */
  journals: Array<string>;
  /**
   * Add rules to decide which journal objects to allow. Events are generated if no rules are given or if all the rules' expressions evaluate to true.
   */
  rules?: Array<SendToRoutesFalseWithConnectionsConstraintRule> | undefined;
  /**
   * Skip log messages that are not part of the current boot session.
   */
  currentBoot?: boolean | undefined;
  /**
   * The maximum log message age, in duration form (e.g,: 60s, 4h, 3d, 1w).  Default of no value will apply no max age filters.
   */
  maxAgeDur?: string | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export const SendToRoutesTrueWithConnectionsConstraintInputJournalFilesType = {
  JournalFiles: "journal_files",
} as const;
export type SendToRoutesTrueWithConnectionsConstraintInputJournalFilesType =
  ClosedEnum<
    typeof SendToRoutesTrueWithConnectionsConstraintInputJournalFilesType
  >;

export type SendToRoutesTrueWithConnectionsConstraintRule = {
  /**
   * JavaScript expression applied to Journal objects. Return 'true' to include it.
   */
  filter: string;
  /**
   * Optional description of this rule's purpose
   */
  description?: string | undefined;
};

export type InputJournalFilesSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: SendToRoutesTrueWithConnectionsConstraintInputJournalFilesType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Directory path to search for journals. Environment variables will be resolved, e.g. $CRIBL_EDGE_FS_ROOT/var/log/journal/$MACHINE_ID.
   */
  path: string;
  /**
   * Time, in seconds, between scanning for journals.
   */
  interval?: number | undefined;
  /**
   * The full path of discovered journals are matched against this wildcard list.
   */
  journals: Array<string>;
  /**
   * Add rules to decide which journal objects to allow. Events are generated if no rules are given or if all the rules' expressions evaluate to true.
   */
  rules?: Array<SendToRoutesTrueWithConnectionsConstraintRule> | undefined;
  /**
   * Skip log messages that are not part of the current boot session.
   */
  currentBoot?: boolean | undefined;
  /**
   * The maximum log message age, in duration form (e.g,: 60s, 4h, 3d, 1w).  Default of no value will apply no max age filters.
   */
  maxAgeDur?: string | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputJournalFiles =
  | InputJournalFilesSendToRoutesTrueWithConnectionsConstraint
  | InputJournalFilesSendToRoutesFalseWithConnectionsConstraint
  | InputJournalFilesPqEnabledFalseWithPqConstraint
  | InputJournalFilesPqEnabledTrueWithPqConstraint;

export const InputRawUdpType = {
  RawUdp: "raw_udp",
} as const;
export type InputRawUdpType = ClosedEnum<typeof InputRawUdpType>;

export type InputRawUdpPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputRawUdpType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  /**
   * Maximum number of events to buffer when downstream is blocking.
   */
  maxBufferSize?: number | undefined;
  /**
   * Regex matching IP addresses that are allowed to send data
   */
  ipWhitelistRegex?: string | undefined;
  /**
   * If true, each UDP packet is assumed to contain a single message. If false, each UDP packet is assumed to contain multiple messages, separated by newlines.
   */
  singleMsgUdpPackets?: boolean | undefined;
  /**
   * If true, a __rawBytes field will be added to each event containing the raw bytes of the datagram.
   */
  ingestRawBytes?: boolean | undefined;
  /**
   * Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.
   */
  udpSocketRxBufSize?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputRawUdpPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputRawUdpType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  /**
   * Maximum number of events to buffer when downstream is blocking.
   */
  maxBufferSize?: number | undefined;
  /**
   * Regex matching IP addresses that are allowed to send data
   */
  ipWhitelistRegex?: string | undefined;
  /**
   * If true, each UDP packet is assumed to contain a single message. If false, each UDP packet is assumed to contain multiple messages, separated by newlines.
   */
  singleMsgUdpPackets?: boolean | undefined;
  /**
   * If true, a __rawBytes field will be added to each event containing the raw bytes of the datagram.
   */
  ingestRawBytes?: boolean | undefined;
  /**
   * Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.
   */
  udpSocketRxBufSize?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputRawUdpSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputRawUdpType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  /**
   * Maximum number of events to buffer when downstream is blocking.
   */
  maxBufferSize?: number | undefined;
  /**
   * Regex matching IP addresses that are allowed to send data
   */
  ipWhitelistRegex?: string | undefined;
  /**
   * If true, each UDP packet is assumed to contain a single message. If false, each UDP packet is assumed to contain multiple messages, separated by newlines.
   */
  singleMsgUdpPackets?: boolean | undefined;
  /**
   * If true, a __rawBytes field will be added to each event containing the raw bytes of the datagram.
   */
  ingestRawBytes?: boolean | undefined;
  /**
   * Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.
   */
  udpSocketRxBufSize?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputRawUdpSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputRawUdpType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  /**
   * Maximum number of events to buffer when downstream is blocking.
   */
  maxBufferSize?: number | undefined;
  /**
   * Regex matching IP addresses that are allowed to send data
   */
  ipWhitelistRegex?: string | undefined;
  /**
   * If true, each UDP packet is assumed to contain a single message. If false, each UDP packet is assumed to contain multiple messages, separated by newlines.
   */
  singleMsgUdpPackets?: boolean | undefined;
  /**
   * If true, a __rawBytes field will be added to each event containing the raw bytes of the datagram.
   */
  ingestRawBytes?: boolean | undefined;
  /**
   * Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.
   */
  udpSocketRxBufSize?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputRawUdp =
  | InputRawUdpSendToRoutesTrueWithConnectionsConstraint
  | InputRawUdpSendToRoutesFalseWithConnectionsConstraint
  | InputRawUdpPqEnabledFalseWithPqConstraint
  | InputRawUdpPqEnabledTrueWithPqConstraint;

export const InputWinEventLogsType = {
  WinEventLogs: "win_event_logs",
} as const;
export type InputWinEventLogsType = ClosedEnum<typeof InputWinEventLogsType>;

/**
 * Read all stored and future event logs, or only future events
 */
export const ReadMode = {
  /**
   * Entire log
   */
  Oldest: "oldest",
  /**
   * From last entry
   */
  Newest: "newest",
} as const;
/**
 * Read all stored and future event logs, or only future events
 */
export type ReadMode = OpenEnum<typeof ReadMode>;

/**
 * Format of individual events
 */
export const EventFormat = {
  /**
   * JSON
   */
  Json: "json",
  /**
   * XML
   */
  Xml: "xml",
} as const;
/**
 * Format of individual events
 */
export type EventFormat = OpenEnum<typeof EventFormat>;

export type InputWinEventLogsPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputWinEventLogsType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Enter the event logs to collect. Run "Get-WinEvent -ListLog *" in PowerShell to see the available logs.
   */
  logNames: Array<string>;
  /**
   * Read all stored and future event logs, or only future events
   */
  readMode?: ReadMode | undefined;
  /**
   * Format of individual events
   */
  eventFormat?: EventFormat | undefined;
  /**
   * Enable to use built-in tools (PowerShell for JSON, wevtutil for XML) to collect event logs instead of native API (default) [Learn more](https://docs.cribl.io/edge/sources-windows-event-logs/#advanced-settings)
   */
  disableNativeModule?: boolean | undefined;
  /**
   * Time, in seconds, between checking for new entries (Applicable for pre-4.8.0 nodes that use Windows Tools)
   */
  interval?: number | undefined;
  /**
   * The maximum number of events to read in one polling interval. A batch size higher than 500 can cause delays when pulling from multiple event logs. (Applicable for pre-4.8.0 nodes that use Windows Tools)
   */
  batchSize?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * The maximum number of bytes in an event before it is flushed to the pipelines
   */
  maxEventBytes?: number | undefined;
  description?: string | undefined;
  /**
   * Enable/disable the rendering of localized event message strings (Applicable for 4.8.0 nodes and newer that use the Native API)
   */
  disableJsonRendering?: boolean | undefined;
  /**
   * Enable/disable the rendering of localized event message strings (Applicable for 4.8.0 nodes and newer that use the Native API)
   */
  disableXmlRendering?: boolean | undefined;
};

export type InputWinEventLogsPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputWinEventLogsType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Enter the event logs to collect. Run "Get-WinEvent -ListLog *" in PowerShell to see the available logs.
   */
  logNames: Array<string>;
  /**
   * Read all stored and future event logs, or only future events
   */
  readMode?: ReadMode | undefined;
  /**
   * Format of individual events
   */
  eventFormat?: EventFormat | undefined;
  /**
   * Enable to use built-in tools (PowerShell for JSON, wevtutil for XML) to collect event logs instead of native API (default) [Learn more](https://docs.cribl.io/edge/sources-windows-event-logs/#advanced-settings)
   */
  disableNativeModule?: boolean | undefined;
  /**
   * Time, in seconds, between checking for new entries (Applicable for pre-4.8.0 nodes that use Windows Tools)
   */
  interval?: number | undefined;
  /**
   * The maximum number of events to read in one polling interval. A batch size higher than 500 can cause delays when pulling from multiple event logs. (Applicable for pre-4.8.0 nodes that use Windows Tools)
   */
  batchSize?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * The maximum number of bytes in an event before it is flushed to the pipelines
   */
  maxEventBytes?: number | undefined;
  description?: string | undefined;
  /**
   * Enable/disable the rendering of localized event message strings (Applicable for 4.8.0 nodes and newer that use the Native API)
   */
  disableJsonRendering?: boolean | undefined;
  /**
   * Enable/disable the rendering of localized event message strings (Applicable for 4.8.0 nodes and newer that use the Native API)
   */
  disableXmlRendering?: boolean | undefined;
};

export type InputWinEventLogsSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputWinEventLogsType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Enter the event logs to collect. Run "Get-WinEvent -ListLog *" in PowerShell to see the available logs.
   */
  logNames: Array<string>;
  /**
   * Read all stored and future event logs, or only future events
   */
  readMode?: ReadMode | undefined;
  /**
   * Format of individual events
   */
  eventFormat?: EventFormat | undefined;
  /**
   * Enable to use built-in tools (PowerShell for JSON, wevtutil for XML) to collect event logs instead of native API (default) [Learn more](https://docs.cribl.io/edge/sources-windows-event-logs/#advanced-settings)
   */
  disableNativeModule?: boolean | undefined;
  /**
   * Time, in seconds, between checking for new entries (Applicable for pre-4.8.0 nodes that use Windows Tools)
   */
  interval?: number | undefined;
  /**
   * The maximum number of events to read in one polling interval. A batch size higher than 500 can cause delays when pulling from multiple event logs. (Applicable for pre-4.8.0 nodes that use Windows Tools)
   */
  batchSize?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * The maximum number of bytes in an event before it is flushed to the pipelines
   */
  maxEventBytes?: number | undefined;
  description?: string | undefined;
  /**
   * Enable/disable the rendering of localized event message strings (Applicable for 4.8.0 nodes and newer that use the Native API)
   */
  disableJsonRendering?: boolean | undefined;
  /**
   * Enable/disable the rendering of localized event message strings (Applicable for 4.8.0 nodes and newer that use the Native API)
   */
  disableXmlRendering?: boolean | undefined;
};

export type InputWinEventLogsSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputWinEventLogsType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Enter the event logs to collect. Run "Get-WinEvent -ListLog *" in PowerShell to see the available logs.
   */
  logNames: Array<string>;
  /**
   * Read all stored and future event logs, or only future events
   */
  readMode?: ReadMode | undefined;
  /**
   * Format of individual events
   */
  eventFormat?: EventFormat | undefined;
  /**
   * Enable to use built-in tools (PowerShell for JSON, wevtutil for XML) to collect event logs instead of native API (default) [Learn more](https://docs.cribl.io/edge/sources-windows-event-logs/#advanced-settings)
   */
  disableNativeModule?: boolean | undefined;
  /**
   * Time, in seconds, between checking for new entries (Applicable for pre-4.8.0 nodes that use Windows Tools)
   */
  interval?: number | undefined;
  /**
   * The maximum number of events to read in one polling interval. A batch size higher than 500 can cause delays when pulling from multiple event logs. (Applicable for pre-4.8.0 nodes that use Windows Tools)
   */
  batchSize?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * The maximum number of bytes in an event before it is flushed to the pipelines
   */
  maxEventBytes?: number | undefined;
  description?: string | undefined;
  /**
   * Enable/disable the rendering of localized event message strings (Applicable for 4.8.0 nodes and newer that use the Native API)
   */
  disableJsonRendering?: boolean | undefined;
  /**
   * Enable/disable the rendering of localized event message strings (Applicable for 4.8.0 nodes and newer that use the Native API)
   */
  disableXmlRendering?: boolean | undefined;
};

export type InputWinEventLogs =
  | InputWinEventLogsSendToRoutesTrueWithConnectionsConstraint
  | InputWinEventLogsSendToRoutesFalseWithConnectionsConstraint
  | InputWinEventLogsPqEnabledFalseWithPqConstraint
  | InputWinEventLogsPqEnabledTrueWithPqConstraint;

export const InputWefType = {
  Wef: "wef",
} as const;
export type InputWefType = ClosedEnum<typeof InputWefType>;

/**
 * How to authenticate incoming client connections
 */
export const InputWefAuthenticationMethod = {
  /**
   * Client certificate
   */
  ClientCert: "clientCert",
  /**
   * Kerberos
   */
  Kerberos: "kerberos",
} as const;
/**
 * How to authenticate incoming client connections
 */
export type InputWefAuthenticationMethod = OpenEnum<
  typeof InputWefAuthenticationMethod
>;

export type MTLSSettings = {
  /**
   * Enable TLS
   */
  disabled?: boolean | undefined;
  /**
   * Required for WEF certificate authentication
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * Required for WEF certificate authentication
   */
  requestCert?: boolean | undefined;
  /**
   * Name of the predefined certificate
   */
  certificateName?: string | undefined;
  /**
   * Path on server containing the private key to use. PEM format. Can reference $ENV_VARS.
   */
  privKeyPath: string;
  /**
   * Passphrase to use to decrypt private key
   */
  passphrase?: string | undefined;
  /**
   * Path on server containing certificates to use. PEM format. Can reference $ENV_VARS.
   */
  certPath: string;
  /**
   * Server path containing CA certificates (in PEM format) to use. Can reference $ENV_VARS. If multiple certificates are present in a .pem, each must directly certify the one preceding it.
   */
  caPath: string;
  /**
   * Regex matching allowable common names in peer certificates' subject attribute
   */
  commonNameRegex?: string | undefined;
  minVersion?:
    | models.MinimumTlsVersionOptionsKafkaSchemaRegistryTls
    | undefined;
  maxVersion?:
    | models.MaximumTlsVersionOptionsKafkaSchemaRegistryTls
    | undefined;
  /**
   * Enable OCSP check of certificate
   */
  ocspCheck?: boolean | undefined;
  keytab?: any | undefined;
  principal?: any | undefined;
  /**
   * If enabled, checks will fail on any OCSP error. Otherwise, checks will fail only when a certificate is revoked, ignoring other errors.
   */
  ocspCheckFailClose?: boolean | undefined;
};

/**
 * Content format in which the endpoint should deliver events
 */
export const CreateInputFormat = {
  Raw: "Raw",
  RenderedText: "RenderedText",
} as const;
/**
 * Content format in which the endpoint should deliver events
 */
export type CreateInputFormat = OpenEnum<typeof CreateInputFormat>;

export const QueryBuilderMode = {
  Simple: "simple",
  Xml: "xml",
} as const;
export type QueryBuilderMode = OpenEnum<typeof QueryBuilderMode>;

export type Query = {
  /**
   * The Path attribute from the relevant XML Select element
   */
  path: string;
  /**
   * The XPath query inside the relevant XML Select element
   */
  queryExpression: string;
};

export type Subscription = {
  subscriptionName: string;
  /**
   * Version UUID for this subscription. If any subscription parameters are modified, this value will change.
   */
  version?: string | undefined;
  /**
   * Content format in which the endpoint should deliver events
   */
  contentFormat?: CreateInputFormat | undefined;
  /**
   * Maximum time (in seconds) between endpoint checkins before considering it unavailable
   */
  heartbeatInterval?: number | undefined;
  /**
   * Interval (in seconds) over which the endpoint should collect events before sending them to Stream
   */
  batchTimeout?: number | undefined;
  /**
   * Newly subscribed endpoints will send previously existing events. Disable to receive new events only.
   */
  readExistingEvents?: boolean | undefined;
  /**
   * Keep track of which events have been received, resuming from that point after a re-subscription. This setting takes precedence over 'Read existing events'. See [Cribl Docs](https://docs.cribl.io/stream/sources-wef/#subscriptions) for more details.
   */
  sendBookmarks?: boolean | undefined;
  /**
   * Receive compressed events from the source
   */
  compress?: boolean | undefined;
  /**
   * The DNS names of the endpoints that should forward these events. You may use wildcards, such as *.mydomain.com
   */
  targets: Array<string>;
  /**
   * The RFC-3066 locale the Windows clients should use when sending events. Defaults to "en-US".
   */
  locale?: string | undefined;
  querySelector?: QueryBuilderMode | undefined;
  /**
   * Fields to add to events ingested under this subscription
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  queries?: Array<Query> | undefined;
  /**
   * The XPath query to use for selecting events
   */
  xmlQuery?: string | undefined;
};

export type InputWefPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputWefType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port?: number | undefined;
  /**
   * How to authenticate incoming client connections
   */
  authMethod?: InputWefAuthenticationMethod | undefined;
  tls?: MTLSSettings | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Preserve the clients original IP address in the __srcIpPort field when connecting through an HTTP proxy that supports the X-Forwarded-For header. This does not apply to TCP-layer Proxy Protocol v1/v2.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * SHA1 fingerprint expected by the client, if it does not match the first certificate in the configured CA chain
   */
  caFingerprint?: string | undefined;
  /**
   * Path to the keytab file containing the service principal credentials. @{product} will use `/etc/krb5.keytab` if not provided.
   */
  keytab?: string | undefined;
  /**
   * Kerberos principal used for authentication, typically in the form HTTP/<hostname>@<REALM>
   */
  principal?: string | undefined;
  /**
   * Allow events to be ingested even if their MachineID does not match the client certificate CN
   */
  allowMachineIdMismatch?: boolean | undefined;
  /**
   * Subscriptions to events on forwarding endpoints
   */
  subscriptions: Array<Subscription>;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
  /**
   * Log a warning if the client certificate authority (CA) fingerprint does not match the expected value. A mismatch prevents Cribl from receiving events from the Windows Event Forwarder.
   */
  logFingerprintMismatch?: boolean | undefined;
};

export type InputWefPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputWefType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port?: number | undefined;
  /**
   * How to authenticate incoming client connections
   */
  authMethod?: InputWefAuthenticationMethod | undefined;
  tls?: MTLSSettings | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Preserve the clients original IP address in the __srcIpPort field when connecting through an HTTP proxy that supports the X-Forwarded-For header. This does not apply to TCP-layer Proxy Protocol v1/v2.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * SHA1 fingerprint expected by the client, if it does not match the first certificate in the configured CA chain
   */
  caFingerprint?: string | undefined;
  /**
   * Path to the keytab file containing the service principal credentials. @{product} will use `/etc/krb5.keytab` if not provided.
   */
  keytab?: string | undefined;
  /**
   * Kerberos principal used for authentication, typically in the form HTTP/<hostname>@<REALM>
   */
  principal?: string | undefined;
  /**
   * Allow events to be ingested even if their MachineID does not match the client certificate CN
   */
  allowMachineIdMismatch?: boolean | undefined;
  /**
   * Subscriptions to events on forwarding endpoints
   */
  subscriptions: Array<Subscription>;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
  /**
   * Log a warning if the client certificate authority (CA) fingerprint does not match the expected value. A mismatch prevents Cribl from receiving events from the Windows Event Forwarder.
   */
  logFingerprintMismatch?: boolean | undefined;
};

export type InputWefSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputWefType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port?: number | undefined;
  /**
   * How to authenticate incoming client connections
   */
  authMethod?: InputWefAuthenticationMethod | undefined;
  tls?: MTLSSettings | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Preserve the clients original IP address in the __srcIpPort field when connecting through an HTTP proxy that supports the X-Forwarded-For header. This does not apply to TCP-layer Proxy Protocol v1/v2.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * SHA1 fingerprint expected by the client, if it does not match the first certificate in the configured CA chain
   */
  caFingerprint?: string | undefined;
  /**
   * Path to the keytab file containing the service principal credentials. @{product} will use `/etc/krb5.keytab` if not provided.
   */
  keytab?: string | undefined;
  /**
   * Kerberos principal used for authentication, typically in the form HTTP/<hostname>@<REALM>
   */
  principal?: string | undefined;
  /**
   * Allow events to be ingested even if their MachineID does not match the client certificate CN
   */
  allowMachineIdMismatch?: boolean | undefined;
  /**
   * Subscriptions to events on forwarding endpoints
   */
  subscriptions: Array<Subscription>;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
  /**
   * Log a warning if the client certificate authority (CA) fingerprint does not match the expected value. A mismatch prevents Cribl from receiving events from the Windows Event Forwarder.
   */
  logFingerprintMismatch?: boolean | undefined;
};

export type InputWefSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputWefType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port?: number | undefined;
  /**
   * How to authenticate incoming client connections
   */
  authMethod?: InputWefAuthenticationMethod | undefined;
  tls?: MTLSSettings | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Preserve the clients original IP address in the __srcIpPort field when connecting through an HTTP proxy that supports the X-Forwarded-For header. This does not apply to TCP-layer Proxy Protocol v1/v2.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * SHA1 fingerprint expected by the client, if it does not match the first certificate in the configured CA chain
   */
  caFingerprint?: string | undefined;
  /**
   * Path to the keytab file containing the service principal credentials. @{product} will use `/etc/krb5.keytab` if not provided.
   */
  keytab?: string | undefined;
  /**
   * Kerberos principal used for authentication, typically in the form HTTP/<hostname>@<REALM>
   */
  principal?: string | undefined;
  /**
   * Allow events to be ingested even if their MachineID does not match the client certificate CN
   */
  allowMachineIdMismatch?: boolean | undefined;
  /**
   * Subscriptions to events on forwarding endpoints
   */
  subscriptions: Array<Subscription>;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
  /**
   * Log a warning if the client certificate authority (CA) fingerprint does not match the expected value. A mismatch prevents Cribl from receiving events from the Windows Event Forwarder.
   */
  logFingerprintMismatch?: boolean | undefined;
};

export type InputWef =
  | InputWefSendToRoutesTrueWithConnectionsConstraint
  | InputWefSendToRoutesFalseWithConnectionsConstraint
  | InputWefPqEnabledFalseWithPqConstraint
  | InputWefPqEnabledTrueWithPqConstraint;

export const InputAppscopeType = {
  Appscope: "appscope",
} as const;
export type InputAppscopeType = ClosedEnum<typeof InputAppscopeType>;

export type Allow = {
  /**
   * Specify the name of a process or family of processes.
   */
  procname: string;
  /**
   * Specify a string to substring-match against process command-line.
   */
  arg?: string | undefined;
  /**
   * Choose a config to apply to processes that match the process name and/or argument.
   */
  config: string;
};

export type InputAppscopeFilter = {
  /**
   * Specify processes that AppScope should be loaded into, and the config to use.
   */
  allow?: Array<Allow> | undefined;
  /**
   * To override the UNIX domain socket or address/port specified in General Settings (while leaving Authentication settings as is), enter a URL.
   */
  transportURL?: string | undefined;
};

export type InputAppscopePersistence = {
  /**
   * Spool events and metrics on disk for Cribl Edge and Search
   */
  enable?: boolean | undefined;
  /**
   * Time span for each file bucket
   */
  timeWindow?: string | undefined;
  /**
   * Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.
   */
  maxDataSize?: string | undefined;
  /**
   * Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.
   */
  maxDataTime?: string | undefined;
  compress?: models.DataCompressionFormatOptionsPersistence | undefined;
  /**
   * Path to use to write metrics. Defaults to $CRIBL_HOME/state/appscope
   */
  destPath?: string | undefined;
};

export type InputAppscopePqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputAppscopeType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Regex matching IP addresses that are allowed to establish a connection
   */
  ipWhitelistRegex?: string | undefined;
  /**
   * Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.
   */
  maxActiveCxn?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.
   */
  socketIdleTimeout?: number | undefined;
  /**
   * How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.
   */
  socketEndingMaxWait?: number | undefined;
  /**
   * The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.
   */
  socketMaxLifespan?: number | undefined;
  /**
   * Enable if the connection is proxied by a device that supports proxy protocol v1 or v2
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * Toggle to Yes to specify a file-backed UNIX domain socket connection, instead of a network host and port.
   */
  enableUnixPath?: boolean | undefined;
  filter?: InputAppscopeFilter | undefined;
  persistence?: InputAppscopePersistence | undefined;
  /**
   * Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
   */
  authType?: models.AuthenticationMethodOptionsAuthTokensItems | undefined;
  description?: string | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port?: number | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Path to the UNIX domain socket to listen on.
   */
  unixSocketPath?: string | undefined;
  /**
   * Permissions to set for socket e.g., 777. If empty, falls back to the runtime user's default permissions.
   */
  unixSocketPerms?: string | undefined;
  /**
   * Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted.
   */
  authToken?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
};

export type InputAppscopePqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputAppscopeType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Regex matching IP addresses that are allowed to establish a connection
   */
  ipWhitelistRegex?: string | undefined;
  /**
   * Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.
   */
  maxActiveCxn?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.
   */
  socketIdleTimeout?: number | undefined;
  /**
   * How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.
   */
  socketEndingMaxWait?: number | undefined;
  /**
   * The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.
   */
  socketMaxLifespan?: number | undefined;
  /**
   * Enable if the connection is proxied by a device that supports proxy protocol v1 or v2
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * Toggle to Yes to specify a file-backed UNIX domain socket connection, instead of a network host and port.
   */
  enableUnixPath?: boolean | undefined;
  filter?: InputAppscopeFilter | undefined;
  persistence?: InputAppscopePersistence | undefined;
  /**
   * Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
   */
  authType?: models.AuthenticationMethodOptionsAuthTokensItems | undefined;
  description?: string | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port?: number | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Path to the UNIX domain socket to listen on.
   */
  unixSocketPath?: string | undefined;
  /**
   * Permissions to set for socket e.g., 777. If empty, falls back to the runtime user's default permissions.
   */
  unixSocketPerms?: string | undefined;
  /**
   * Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted.
   */
  authToken?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
};

export type InputAppscopeSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputAppscopeType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Regex matching IP addresses that are allowed to establish a connection
   */
  ipWhitelistRegex?: string | undefined;
  /**
   * Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.
   */
  maxActiveCxn?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.
   */
  socketIdleTimeout?: number | undefined;
  /**
   * How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.
   */
  socketEndingMaxWait?: number | undefined;
  /**
   * The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.
   */
  socketMaxLifespan?: number | undefined;
  /**
   * Enable if the connection is proxied by a device that supports proxy protocol v1 or v2
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * Toggle to Yes to specify a file-backed UNIX domain socket connection, instead of a network host and port.
   */
  enableUnixPath?: boolean | undefined;
  filter?: InputAppscopeFilter | undefined;
  persistence?: InputAppscopePersistence | undefined;
  /**
   * Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
   */
  authType?: models.AuthenticationMethodOptionsAuthTokensItems | undefined;
  description?: string | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port?: number | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Path to the UNIX domain socket to listen on.
   */
  unixSocketPath?: string | undefined;
  /**
   * Permissions to set for socket e.g., 777. If empty, falls back to the runtime user's default permissions.
   */
  unixSocketPerms?: string | undefined;
  /**
   * Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted.
   */
  authToken?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
};

export type InputAppscopeSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputAppscopeType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Regex matching IP addresses that are allowed to establish a connection
   */
  ipWhitelistRegex?: string | undefined;
  /**
   * Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.
   */
  maxActiveCxn?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.
   */
  socketIdleTimeout?: number | undefined;
  /**
   * How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.
   */
  socketEndingMaxWait?: number | undefined;
  /**
   * The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.
   */
  socketMaxLifespan?: number | undefined;
  /**
   * Enable if the connection is proxied by a device that supports proxy protocol v1 or v2
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * Toggle to Yes to specify a file-backed UNIX domain socket connection, instead of a network host and port.
   */
  enableUnixPath?: boolean | undefined;
  filter?: InputAppscopeFilter | undefined;
  persistence?: InputAppscopePersistence | undefined;
  /**
   * Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
   */
  authType?: models.AuthenticationMethodOptionsAuthTokensItems | undefined;
  description?: string | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port?: number | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Path to the UNIX domain socket to listen on.
   */
  unixSocketPath?: string | undefined;
  /**
   * Permissions to set for socket e.g., 777. If empty, falls back to the runtime user's default permissions.
   */
  unixSocketPerms?: string | undefined;
  /**
   * Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted.
   */
  authToken?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
};

export type InputAppscope =
  | InputAppscopeSendToRoutesTrueWithConnectionsConstraint
  | InputAppscopeSendToRoutesFalseWithConnectionsConstraint
  | InputAppscopePqEnabledFalseWithPqConstraint
  | InputAppscopePqEnabledTrueWithPqConstraint;

export const InputTcpType = {
  Tcp: "tcp",
} as const;
export type InputTcpType = ClosedEnum<typeof InputTcpType>;

export type InputTcpPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputTcpType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Regex matching IP addresses that are allowed to establish a connection
   */
  ipWhitelistRegex?: string | undefined;
  /**
   * Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.
   */
  maxActiveCxn?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.
   */
  socketIdleTimeout?: number | undefined;
  /**
   * How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.
   */
  socketEndingMaxWait?: number | undefined;
  /**
   * The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.
   */
  socketMaxLifespan?: number | undefined;
  /**
   * Enable if the connection is proxied by a device that supports proxy protocol v1 or v2
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * Client will pass the header record with every new connection. The header can contain an authToken, and an object with a list of fields and values to add to every event. These fields can be used to simplify Event Breaker selection, routing, etc. Header has this format, and must be followed by a newline: { "authToken" : "myToken", "fields": { "field1": "value1", "field2": "value2" } }
   */
  enableHeader?: boolean | undefined;
  preprocess?: models.PreprocessTypeSavedJobCollectionInput | undefined;
  description?: string | undefined;
  /**
   * Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted.
   */
  authToken?: string | undefined;
  /**
   * Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
   */
  authType?: models.AuthenticationMethodOptionsAuthTokensItems | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
};

export type InputTcpPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputTcpType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Regex matching IP addresses that are allowed to establish a connection
   */
  ipWhitelistRegex?: string | undefined;
  /**
   * Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.
   */
  maxActiveCxn?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.
   */
  socketIdleTimeout?: number | undefined;
  /**
   * How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.
   */
  socketEndingMaxWait?: number | undefined;
  /**
   * The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.
   */
  socketMaxLifespan?: number | undefined;
  /**
   * Enable if the connection is proxied by a device that supports proxy protocol v1 or v2
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * Client will pass the header record with every new connection. The header can contain an authToken, and an object with a list of fields and values to add to every event. These fields can be used to simplify Event Breaker selection, routing, etc. Header has this format, and must be followed by a newline: { "authToken" : "myToken", "fields": { "field1": "value1", "field2": "value2" } }
   */
  enableHeader?: boolean | undefined;
  preprocess?: models.PreprocessTypeSavedJobCollectionInput | undefined;
  description?: string | undefined;
  /**
   * Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted.
   */
  authToken?: string | undefined;
  /**
   * Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
   */
  authType?: models.AuthenticationMethodOptionsAuthTokensItems | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
};

export type InputTcpSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputTcpType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Regex matching IP addresses that are allowed to establish a connection
   */
  ipWhitelistRegex?: string | undefined;
  /**
   * Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.
   */
  maxActiveCxn?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.
   */
  socketIdleTimeout?: number | undefined;
  /**
   * How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.
   */
  socketEndingMaxWait?: number | undefined;
  /**
   * The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.
   */
  socketMaxLifespan?: number | undefined;
  /**
   * Enable if the connection is proxied by a device that supports proxy protocol v1 or v2
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * Client will pass the header record with every new connection. The header can contain an authToken, and an object with a list of fields and values to add to every event. These fields can be used to simplify Event Breaker selection, routing, etc. Header has this format, and must be followed by a newline: { "authToken" : "myToken", "fields": { "field1": "value1", "field2": "value2" } }
   */
  enableHeader?: boolean | undefined;
  preprocess?: models.PreprocessTypeSavedJobCollectionInput | undefined;
  description?: string | undefined;
  /**
   * Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted.
   */
  authToken?: string | undefined;
  /**
   * Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
   */
  authType?: models.AuthenticationMethodOptionsAuthTokensItems | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
};

export type InputTcpSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputTcpType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Regex matching IP addresses that are allowed to establish a connection
   */
  ipWhitelistRegex?: string | undefined;
  /**
   * Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.
   */
  maxActiveCxn?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.
   */
  socketIdleTimeout?: number | undefined;
  /**
   * How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.
   */
  socketEndingMaxWait?: number | undefined;
  /**
   * The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.
   */
  socketMaxLifespan?: number | undefined;
  /**
   * Enable if the connection is proxied by a device that supports proxy protocol v1 or v2
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * Client will pass the header record with every new connection. The header can contain an authToken, and an object with a list of fields and values to add to every event. These fields can be used to simplify Event Breaker selection, routing, etc. Header has this format, and must be followed by a newline: { "authToken" : "myToken", "fields": { "field1": "value1", "field2": "value2" } }
   */
  enableHeader?: boolean | undefined;
  preprocess?: models.PreprocessTypeSavedJobCollectionInput | undefined;
  description?: string | undefined;
  /**
   * Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted.
   */
  authToken?: string | undefined;
  /**
   * Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
   */
  authType?: models.AuthenticationMethodOptionsAuthTokensItems | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
};

export type InputTcp =
  | InputTcpSendToRoutesTrueWithConnectionsConstraint
  | InputTcpSendToRoutesFalseWithConnectionsConstraint
  | InputTcpPqEnabledFalseWithPqConstraint
  | InputTcpPqEnabledTrueWithPqConstraint;

export const PqEnabledTrueWithPqConstraintInputFileType = {
  File: "file",
} as const;
export type PqEnabledTrueWithPqConstraintInputFileType = ClosedEnum<
  typeof PqEnabledTrueWithPqConstraintInputFileType
>;

/**
 * Choose how to discover files to monitor
 */
export const PqEnabledTrueWithPqConstraintMode = {
  /**
   * Manual
   */
  Manual: "manual",
  /**
   * Auto
   */
  Auto: "auto",
} as const;
/**
 * Choose how to discover files to monitor
 */
export type PqEnabledTrueWithPqConstraintMode = OpenEnum<
  typeof PqEnabledTrueWithPqConstraintMode
>;

export type InputFilePqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: PqEnabledTrueWithPqConstraintInputFileType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Choose how to discover files to monitor
   */
  mode?: PqEnabledTrueWithPqConstraintMode | undefined;
  /**
   * Time, in seconds, between scanning for files
   */
  interval?: number | undefined;
  /**
   * The full path of discovered files are matched against this wildcard list
   */
  filenames?: Array<string> | undefined;
  /**
   * Apply filename allowlist to file entries in archive file types, like tar or zip.
   */
  filterArchivedFiles?: boolean | undefined;
  /**
   * Read only new entries at the end of all files discovered at next startup. @{product} will then read newly discovered files from the head. Disable this to resume reading all files from head.
   */
  tailOnly?: boolean | undefined;
  /**
   * Time, in seconds, before an idle file is closed
   */
  idleTimeout?: number | undefined;
  /**
   * The minimum age of files to monitor. Format examples: 30s, 15m, 1h. Age is relative to file modification time. Leave empty to apply no age filters.
   */
  minAgeDur?: string | undefined;
  /**
   * The maximum age of event timestamps to collect. Format examples: 60s, 4h, 3d, 1w. Can be used in conjuction with "Check file modification times". Leave empty to apply no age filters.
   */
  maxAgeDur?: string | undefined;
  /**
   * Skip files with modification times earlier than the maximum age duration
   */
  checkFileModTime?: boolean | undefined;
  /**
   * Forces files containing binary data to be streamed as text
   */
  forceText?: boolean | undefined;
  /**
   * Length of file header bytes to use in hash for unique file identification
   */
  hashLen?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  description?: string | undefined;
  /**
   * Directory path to search for files. Environment variables will be resolved, e.g. $CRIBL_HOME/log/.
   */
  path?: string | undefined;
  /**
   * Set how many subdirectories deep to search. Use 0 to search only files in the given path, 1 to also look in its immediate subdirectories, etc. Leave it empty for unlimited depth.
   */
  depth?: number | undefined;
  suppressMissingPathErrors?: boolean | undefined;
  /**
   * Delete files after they have been collected
   */
  deleteFiles?: boolean | undefined;
  /**
   * Stream binary files as Base64-encoded chunks.
   */
  includeUnidentifiableBinary?: boolean | undefined;
};

export const PqEnabledFalseWithPqConstraintInputFileType = {
  File: "file",
} as const;
export type PqEnabledFalseWithPqConstraintInputFileType = ClosedEnum<
  typeof PqEnabledFalseWithPqConstraintInputFileType
>;

/**
 * Choose how to discover files to monitor
 */
export const PqEnabledFalseWithPqConstraintMode = {
  /**
   * Manual
   */
  Manual: "manual",
  /**
   * Auto
   */
  Auto: "auto",
} as const;
/**
 * Choose how to discover files to monitor
 */
export type PqEnabledFalseWithPqConstraintMode = OpenEnum<
  typeof PqEnabledFalseWithPqConstraintMode
>;

export type InputFilePqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: PqEnabledFalseWithPqConstraintInputFileType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Choose how to discover files to monitor
   */
  mode?: PqEnabledFalseWithPqConstraintMode | undefined;
  /**
   * Time, in seconds, between scanning for files
   */
  interval?: number | undefined;
  /**
   * The full path of discovered files are matched against this wildcard list
   */
  filenames?: Array<string> | undefined;
  /**
   * Apply filename allowlist to file entries in archive file types, like tar or zip.
   */
  filterArchivedFiles?: boolean | undefined;
  /**
   * Read only new entries at the end of all files discovered at next startup. @{product} will then read newly discovered files from the head. Disable this to resume reading all files from head.
   */
  tailOnly?: boolean | undefined;
  /**
   * Time, in seconds, before an idle file is closed
   */
  idleTimeout?: number | undefined;
  /**
   * The minimum age of files to monitor. Format examples: 30s, 15m, 1h. Age is relative to file modification time. Leave empty to apply no age filters.
   */
  minAgeDur?: string | undefined;
  /**
   * The maximum age of event timestamps to collect. Format examples: 60s, 4h, 3d, 1w. Can be used in conjuction with "Check file modification times". Leave empty to apply no age filters.
   */
  maxAgeDur?: string | undefined;
  /**
   * Skip files with modification times earlier than the maximum age duration
   */
  checkFileModTime?: boolean | undefined;
  /**
   * Forces files containing binary data to be streamed as text
   */
  forceText?: boolean | undefined;
  /**
   * Length of file header bytes to use in hash for unique file identification
   */
  hashLen?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  description?: string | undefined;
  /**
   * Directory path to search for files. Environment variables will be resolved, e.g. $CRIBL_HOME/log/.
   */
  path?: string | undefined;
  /**
   * Set how many subdirectories deep to search. Use 0 to search only files in the given path, 1 to also look in its immediate subdirectories, etc. Leave it empty for unlimited depth.
   */
  depth?: number | undefined;
  suppressMissingPathErrors?: boolean | undefined;
  /**
   * Delete files after they have been collected
   */
  deleteFiles?: boolean | undefined;
  /**
   * Stream binary files as Base64-encoded chunks.
   */
  includeUnidentifiableBinary?: boolean | undefined;
};

export const SendToRoutesFalseWithConnectionsConstraintInputFileType = {
  File: "file",
} as const;
export type SendToRoutesFalseWithConnectionsConstraintInputFileType =
  ClosedEnum<typeof SendToRoutesFalseWithConnectionsConstraintInputFileType>;

/**
 * Choose how to discover files to monitor
 */
export const SendToRoutesFalseWithConnectionsConstraintMode = {
  /**
   * Manual
   */
  Manual: "manual",
  /**
   * Auto
   */
  Auto: "auto",
} as const;
/**
 * Choose how to discover files to monitor
 */
export type SendToRoutesFalseWithConnectionsConstraintMode = OpenEnum<
  typeof SendToRoutesFalseWithConnectionsConstraintMode
>;

export type InputFileSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: SendToRoutesFalseWithConnectionsConstraintInputFileType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Choose how to discover files to monitor
   */
  mode?: SendToRoutesFalseWithConnectionsConstraintMode | undefined;
  /**
   * Time, in seconds, between scanning for files
   */
  interval?: number | undefined;
  /**
   * The full path of discovered files are matched against this wildcard list
   */
  filenames?: Array<string> | undefined;
  /**
   * Apply filename allowlist to file entries in archive file types, like tar or zip.
   */
  filterArchivedFiles?: boolean | undefined;
  /**
   * Read only new entries at the end of all files discovered at next startup. @{product} will then read newly discovered files from the head. Disable this to resume reading all files from head.
   */
  tailOnly?: boolean | undefined;
  /**
   * Time, in seconds, before an idle file is closed
   */
  idleTimeout?: number | undefined;
  /**
   * The minimum age of files to monitor. Format examples: 30s, 15m, 1h. Age is relative to file modification time. Leave empty to apply no age filters.
   */
  minAgeDur?: string | undefined;
  /**
   * The maximum age of event timestamps to collect. Format examples: 60s, 4h, 3d, 1w. Can be used in conjuction with "Check file modification times". Leave empty to apply no age filters.
   */
  maxAgeDur?: string | undefined;
  /**
   * Skip files with modification times earlier than the maximum age duration
   */
  checkFileModTime?: boolean | undefined;
  /**
   * Forces files containing binary data to be streamed as text
   */
  forceText?: boolean | undefined;
  /**
   * Length of file header bytes to use in hash for unique file identification
   */
  hashLen?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  description?: string | undefined;
  /**
   * Directory path to search for files. Environment variables will be resolved, e.g. $CRIBL_HOME/log/.
   */
  path?: string | undefined;
  /**
   * Set how many subdirectories deep to search. Use 0 to search only files in the given path, 1 to also look in its immediate subdirectories, etc. Leave it empty for unlimited depth.
   */
  depth?: number | undefined;
  suppressMissingPathErrors?: boolean | undefined;
  /**
   * Delete files after they have been collected
   */
  deleteFiles?: boolean | undefined;
  /**
   * Stream binary files as Base64-encoded chunks.
   */
  includeUnidentifiableBinary?: boolean | undefined;
};

export const SendToRoutesTrueWithConnectionsConstraintInputFileType = {
  File: "file",
} as const;
export type SendToRoutesTrueWithConnectionsConstraintInputFileType = ClosedEnum<
  typeof SendToRoutesTrueWithConnectionsConstraintInputFileType
>;

/**
 * Choose how to discover files to monitor
 */
export const SendToRoutesTrueWithConnectionsConstraintMode = {
  /**
   * Manual
   */
  Manual: "manual",
  /**
   * Auto
   */
  Auto: "auto",
} as const;
/**
 * Choose how to discover files to monitor
 */
export type SendToRoutesTrueWithConnectionsConstraintMode = OpenEnum<
  typeof SendToRoutesTrueWithConnectionsConstraintMode
>;

export type InputFileSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: SendToRoutesTrueWithConnectionsConstraintInputFileType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Choose how to discover files to monitor
   */
  mode?: SendToRoutesTrueWithConnectionsConstraintMode | undefined;
  /**
   * Time, in seconds, between scanning for files
   */
  interval?: number | undefined;
  /**
   * The full path of discovered files are matched against this wildcard list
   */
  filenames?: Array<string> | undefined;
  /**
   * Apply filename allowlist to file entries in archive file types, like tar or zip.
   */
  filterArchivedFiles?: boolean | undefined;
  /**
   * Read only new entries at the end of all files discovered at next startup. @{product} will then read newly discovered files from the head. Disable this to resume reading all files from head.
   */
  tailOnly?: boolean | undefined;
  /**
   * Time, in seconds, before an idle file is closed
   */
  idleTimeout?: number | undefined;
  /**
   * The minimum age of files to monitor. Format examples: 30s, 15m, 1h. Age is relative to file modification time. Leave empty to apply no age filters.
   */
  minAgeDur?: string | undefined;
  /**
   * The maximum age of event timestamps to collect. Format examples: 60s, 4h, 3d, 1w. Can be used in conjuction with "Check file modification times". Leave empty to apply no age filters.
   */
  maxAgeDur?: string | undefined;
  /**
   * Skip files with modification times earlier than the maximum age duration
   */
  checkFileModTime?: boolean | undefined;
  /**
   * Forces files containing binary data to be streamed as text
   */
  forceText?: boolean | undefined;
  /**
   * Length of file header bytes to use in hash for unique file identification
   */
  hashLen?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  description?: string | undefined;
  /**
   * Directory path to search for files. Environment variables will be resolved, e.g. $CRIBL_HOME/log/.
   */
  path?: string | undefined;
  /**
   * Set how many subdirectories deep to search. Use 0 to search only files in the given path, 1 to also look in its immediate subdirectories, etc. Leave it empty for unlimited depth.
   */
  depth?: number | undefined;
  suppressMissingPathErrors?: boolean | undefined;
  /**
   * Delete files after they have been collected
   */
  deleteFiles?: boolean | undefined;
  /**
   * Stream binary files as Base64-encoded chunks.
   */
  includeUnidentifiableBinary?: boolean | undefined;
};

export type InputFile =
  | InputFileSendToRoutesTrueWithConnectionsConstraint
  | InputFileSendToRoutesFalseWithConnectionsConstraint
  | InputFilePqEnabledFalseWithPqConstraint
  | InputFilePqEnabledTrueWithPqConstraint;

export const InputSyslogType2 = {
  Syslog: "syslog",
} as const;
export type InputSyslogType2 = ClosedEnum<typeof InputSyslogType2>;

export type InputSyslogSyslog2 = {
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputSyslogType2;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.
   */
  host?: string | undefined;
  /**
   * Enter UDP port number to listen on. Not required if listening on TCP.
   */
  udpPort?: number | undefined;
  /**
   * Enter TCP port number to listen on. Not required if listening on UDP.
   */
  tcpPort: number;
  /**
   * Maximum number of events to buffer when downstream is blocking. Only applies to UDP.
   */
  maxBufferSize?: number | undefined;
  /**
   * Regex matching IP addresses that are allowed to send data
   */
  ipWhitelistRegex?: string | undefined;
  /**
   * Timezone to assign to timestamps without timezone info
   */
  timestampTimezone?: string | undefined;
  /**
   * Treat UDP packet data received as full syslog message
   */
  singleMsgUdpPackets?: boolean | undefined;
  /**
   * Enable if the connection is proxied by a device that supports Proxy Protocol V1 or V2
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Wildcard list of fields to keep from source data; * = ALL (default)
   */
  keepFieldsList?: Array<string> | undefined;
  /**
   * Enable if incoming messages use octet counting per RFC 6587.
   */
  octetCounting?: boolean | undefined;
  /**
   * Enable if we should infer the syslog framing of the incoming messages.
   */
  inferFraming?: boolean | undefined;
  /**
   * Enable if we should infer octet counting only if the messages comply with RFC 5424.
   */
  strictlyInferOctetCounting?: boolean | undefined;
  /**
   * Enable if RFC 3164-formatted messages have hyphens in the app name portion of the TAG section. If disabled, only alphanumeric characters and underscores are allowed. Ignored for RFC 5424-formatted messages.
   */
  allowNonStandardAppName?: boolean | undefined;
  /**
   * Maximum number of active connections allowed per Worker Process for TCP connections. Use 0 for unlimited.
   */
  maxActiveCxn?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.
   */
  socketIdleTimeout?: number | undefined;
  /**
   * How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.
   */
  socketEndingMaxWait?: number | undefined;
  /**
   * The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.
   */
  socketMaxLifespan?: number | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.
   */
  udpSocketRxBufSize?: number | undefined;
  /**
   * Load balance traffic across all Worker Processes
   */
  enableLoadBalancing?: boolean | undefined;
  description?: string | undefined;
  /**
   * When enabled, parses PROXY protocol headers during the TLS handshake. Disable if compatibility issues arise.
   */
  enableEnhancedProxyHeaderParsing?: boolean | undefined;
};

export const InputSyslogType1 = {
  Syslog: "syslog",
} as const;
export type InputSyslogType1 = ClosedEnum<typeof InputSyslogType1>;

export type InputSyslogSyslog1 = {
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputSyslogType1;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.
   */
  host?: string | undefined;
  /**
   * Enter UDP port number to listen on. Not required if listening on TCP.
   */
  udpPort: number;
  /**
   * Enter TCP port number to listen on. Not required if listening on UDP.
   */
  tcpPort?: number | undefined;
  /**
   * Maximum number of events to buffer when downstream is blocking. Only applies to UDP.
   */
  maxBufferSize?: number | undefined;
  /**
   * Regex matching IP addresses that are allowed to send data
   */
  ipWhitelistRegex?: string | undefined;
  /**
   * Timezone to assign to timestamps without timezone info
   */
  timestampTimezone?: string | undefined;
  /**
   * Treat UDP packet data received as full syslog message
   */
  singleMsgUdpPackets?: boolean | undefined;
  /**
   * Enable if the connection is proxied by a device that supports Proxy Protocol V1 or V2
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Wildcard list of fields to keep from source data; * = ALL (default)
   */
  keepFieldsList?: Array<string> | undefined;
  /**
   * Enable if incoming messages use octet counting per RFC 6587.
   */
  octetCounting?: boolean | undefined;
  /**
   * Enable if we should infer the syslog framing of the incoming messages.
   */
  inferFraming?: boolean | undefined;
  /**
   * Enable if we should infer octet counting only if the messages comply with RFC 5424.
   */
  strictlyInferOctetCounting?: boolean | undefined;
  /**
   * Enable if RFC 3164-formatted messages have hyphens in the app name portion of the TAG section. If disabled, only alphanumeric characters and underscores are allowed. Ignored for RFC 5424-formatted messages.
   */
  allowNonStandardAppName?: boolean | undefined;
  /**
   * Maximum number of active connections allowed per Worker Process for TCP connections. Use 0 for unlimited.
   */
  maxActiveCxn?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.
   */
  socketIdleTimeout?: number | undefined;
  /**
   * How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.
   */
  socketEndingMaxWait?: number | undefined;
  /**
   * The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.
   */
  socketMaxLifespan?: number | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.
   */
  udpSocketRxBufSize?: number | undefined;
  /**
   * Load balance traffic across all Worker Processes
   */
  enableLoadBalancing?: boolean | undefined;
  description?: string | undefined;
  /**
   * When enabled, parses PROXY protocol headers during the TLS handshake. Disable if compatibility issues arise.
   */
  enableEnhancedProxyHeaderParsing?: boolean | undefined;
};

export type InputSyslog = InputSyslogSyslog1 | InputSyslogSyslog2;

export const InputSqsType = {
  Sqs: "sqs",
} as const;
export type InputSqsType = ClosedEnum<typeof InputSqsType>;

/**
 * The queue type used (or created)
 */
export const CreateInputQueueType = {
  /**
   * Standard
   */
  Standard: "standard",
  /**
   * FIFO
   */
  Fifo: "fifo",
} as const;
/**
 * The queue type used (or created)
 */
export type CreateInputQueueType = OpenEnum<typeof CreateInputQueueType>;

export type InputSqsPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputSqsType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * The name, URL, or ARN of the SQS queue to read events from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can only be evaluated at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.
   */
  queueName: string;
  /**
   * The queue type used (or created)
   */
  queueType: CreateInputQueueType;
  /**
   * SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.
   */
  awsAccountId?: string | undefined;
  /**
   * Create queue if it does not exist
   */
  createQueue?: boolean | undefined;
  /**
   * AWS authentication method. Choose Auto to use IAM roles.
   */
  awsAuthenticationMethod?: string | undefined;
  awsSecretKey?: string | undefined;
  /**
   * AWS Region where the SQS queue is located. Required, unless the Queue entry is a URL or ARN that includes a Region.
   */
  region?: string | undefined;
  /**
   * SQS service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to SQS-compatible endpoint.
   */
  endpoint?: string | undefined;
  /**
   * Signature version to use for signing SQS requests
   */
  signatureVersion?: models.SignatureVersionOptions3 | undefined;
  /**
   * Reuse connections between requests, which can improve performance
   */
  reuseConnections?: boolean | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * Use Assume Role credentials to access SQS
   */
  enableAssumeRole?: boolean | undefined;
  /**
   * Amazon Resource Name (ARN) of the role to assume
   */
  assumeRoleArn?: string | undefined;
  /**
   * External ID to use when assuming role
   */
  assumeRoleExternalId?: string | undefined;
  /**
   * Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
   */
  durationSeconds?: number | undefined;
  /**
   * The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.
   */
  maxMessages?: number | undefined;
  /**
   * After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).
   */
  visibilityTimeout?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.
   */
  pollTimeout?: number | undefined;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  /**
   * Select or create a stored secret that references your access key and secret key
   */
  awsSecret?: string | undefined;
  /**
   * How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.
   */
  numReceivers?: number | undefined;
};

export type InputSqsPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputSqsType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * The name, URL, or ARN of the SQS queue to read events from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can only be evaluated at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.
   */
  queueName: string;
  /**
   * The queue type used (or created)
   */
  queueType: CreateInputQueueType;
  /**
   * SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.
   */
  awsAccountId?: string | undefined;
  /**
   * Create queue if it does not exist
   */
  createQueue?: boolean | undefined;
  /**
   * AWS authentication method. Choose Auto to use IAM roles.
   */
  awsAuthenticationMethod?: string | undefined;
  awsSecretKey?: string | undefined;
  /**
   * AWS Region where the SQS queue is located. Required, unless the Queue entry is a URL or ARN that includes a Region.
   */
  region?: string | undefined;
  /**
   * SQS service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to SQS-compatible endpoint.
   */
  endpoint?: string | undefined;
  /**
   * Signature version to use for signing SQS requests
   */
  signatureVersion?: models.SignatureVersionOptions3 | undefined;
  /**
   * Reuse connections between requests, which can improve performance
   */
  reuseConnections?: boolean | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * Use Assume Role credentials to access SQS
   */
  enableAssumeRole?: boolean | undefined;
  /**
   * Amazon Resource Name (ARN) of the role to assume
   */
  assumeRoleArn?: string | undefined;
  /**
   * External ID to use when assuming role
   */
  assumeRoleExternalId?: string | undefined;
  /**
   * Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
   */
  durationSeconds?: number | undefined;
  /**
   * The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.
   */
  maxMessages?: number | undefined;
  /**
   * After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).
   */
  visibilityTimeout?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.
   */
  pollTimeout?: number | undefined;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  /**
   * Select or create a stored secret that references your access key and secret key
   */
  awsSecret?: string | undefined;
  /**
   * How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.
   */
  numReceivers?: number | undefined;
};

export type InputSqsSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputSqsType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * The name, URL, or ARN of the SQS queue to read events from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can only be evaluated at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.
   */
  queueName: string;
  /**
   * The queue type used (or created)
   */
  queueType: CreateInputQueueType;
  /**
   * SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.
   */
  awsAccountId?: string | undefined;
  /**
   * Create queue if it does not exist
   */
  createQueue?: boolean | undefined;
  /**
   * AWS authentication method. Choose Auto to use IAM roles.
   */
  awsAuthenticationMethod?: string | undefined;
  awsSecretKey?: string | undefined;
  /**
   * AWS Region where the SQS queue is located. Required, unless the Queue entry is a URL or ARN that includes a Region.
   */
  region?: string | undefined;
  /**
   * SQS service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to SQS-compatible endpoint.
   */
  endpoint?: string | undefined;
  /**
   * Signature version to use for signing SQS requests
   */
  signatureVersion?: models.SignatureVersionOptions3 | undefined;
  /**
   * Reuse connections between requests, which can improve performance
   */
  reuseConnections?: boolean | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * Use Assume Role credentials to access SQS
   */
  enableAssumeRole?: boolean | undefined;
  /**
   * Amazon Resource Name (ARN) of the role to assume
   */
  assumeRoleArn?: string | undefined;
  /**
   * External ID to use when assuming role
   */
  assumeRoleExternalId?: string | undefined;
  /**
   * Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
   */
  durationSeconds?: number | undefined;
  /**
   * The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.
   */
  maxMessages?: number | undefined;
  /**
   * After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).
   */
  visibilityTimeout?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.
   */
  pollTimeout?: number | undefined;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  /**
   * Select or create a stored secret that references your access key and secret key
   */
  awsSecret?: string | undefined;
  /**
   * How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.
   */
  numReceivers?: number | undefined;
};

export type InputSqsSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputSqsType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * The name, URL, or ARN of the SQS queue to read events from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can only be evaluated at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.
   */
  queueName: string;
  /**
   * The queue type used (or created)
   */
  queueType: CreateInputQueueType;
  /**
   * SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.
   */
  awsAccountId?: string | undefined;
  /**
   * Create queue if it does not exist
   */
  createQueue?: boolean | undefined;
  /**
   * AWS authentication method. Choose Auto to use IAM roles.
   */
  awsAuthenticationMethod?: string | undefined;
  awsSecretKey?: string | undefined;
  /**
   * AWS Region where the SQS queue is located. Required, unless the Queue entry is a URL or ARN that includes a Region.
   */
  region?: string | undefined;
  /**
   * SQS service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to SQS-compatible endpoint.
   */
  endpoint?: string | undefined;
  /**
   * Signature version to use for signing SQS requests
   */
  signatureVersion?: models.SignatureVersionOptions3 | undefined;
  /**
   * Reuse connections between requests, which can improve performance
   */
  reuseConnections?: boolean | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * Use Assume Role credentials to access SQS
   */
  enableAssumeRole?: boolean | undefined;
  /**
   * Amazon Resource Name (ARN) of the role to assume
   */
  assumeRoleArn?: string | undefined;
  /**
   * External ID to use when assuming role
   */
  assumeRoleExternalId?: string | undefined;
  /**
   * Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
   */
  durationSeconds?: number | undefined;
  /**
   * The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.
   */
  maxMessages?: number | undefined;
  /**
   * After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).
   */
  visibilityTimeout?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.
   */
  pollTimeout?: number | undefined;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  /**
   * Select or create a stored secret that references your access key and secret key
   */
  awsSecret?: string | undefined;
  /**
   * How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.
   */
  numReceivers?: number | undefined;
};

export type InputSqs =
  | InputSqsSendToRoutesTrueWithConnectionsConstraint
  | InputSqsSendToRoutesFalseWithConnectionsConstraint
  | InputSqsPqEnabledFalseWithPqConstraint
  | InputSqsPqEnabledTrueWithPqConstraint;

export const InputModelDrivenTelemetryType = {
  ModelDrivenTelemetry: "model_driven_telemetry",
} as const;
export type InputModelDrivenTelemetryType = ClosedEnum<
  typeof InputModelDrivenTelemetryType
>;

export type InputModelDrivenTelemetryPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputModelDrivenTelemetryType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port?: number | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.
   */
  maxActiveCxn?: number | undefined;
  /**
   * Time in milliseconds to allow the server to shutdown gracefully before forcing shutdown. Defaults to 5000.
   */
  shutdownTimeoutMs?: number | undefined;
  description?: string | undefined;
};

export type InputModelDrivenTelemetryPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputModelDrivenTelemetryType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port?: number | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.
   */
  maxActiveCxn?: number | undefined;
  /**
   * Time in milliseconds to allow the server to shutdown gracefully before forcing shutdown. Defaults to 5000.
   */
  shutdownTimeoutMs?: number | undefined;
  description?: string | undefined;
};

export type InputModelDrivenTelemetrySendToRoutesFalseWithConnectionsConstraint =
  {
    /**
     * Select whether to send data to Routes, or directly to Destinations.
     */
    sendToRoutes?: boolean | undefined;
    /**
     * Direct connections to Destinations, and optionally via a Pipeline or a Pack
     */
    connections?: Array<models.ItemsTypeConnections> | undefined;
    /**
     * Unique ID for this input
     */
    id: string;
    type: InputModelDrivenTelemetryType;
    disabled?: boolean | undefined;
    /**
     * Pipeline to process data from this Source before sending it through the Routes
     */
    pipeline?: string | undefined;
    /**
     * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
     */
    environment?: string | undefined;
    /**
     * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
     */
    pqEnabled?: boolean | undefined;
    /**
     * Tags for filtering and grouping in @{product}
     */
    streamtags?: Array<string> | undefined;
    pq?: models.PqType | undefined;
    /**
     * Address to bind on. Defaults to 0.0.0.0 (all addresses).
     */
    host?: string | undefined;
    /**
     * Port to listen on
     */
    port?: number | undefined;
    tls?: models.TlsSettingsServerSideType | undefined;
    /**
     * Fields to add to events from this input
     */
    metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
    /**
     * Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.
     */
    maxActiveCxn?: number | undefined;
    /**
     * Time in milliseconds to allow the server to shutdown gracefully before forcing shutdown. Defaults to 5000.
     */
    shutdownTimeoutMs?: number | undefined;
    description?: string | undefined;
  };

export type InputModelDrivenTelemetrySendToRoutesTrueWithConnectionsConstraint =
  {
    /**
     * Select whether to send data to Routes, or directly to Destinations.
     */
    sendToRoutes?: boolean | undefined;
    /**
     * Direct connections to Destinations, and optionally via a Pipeline or a Pack
     */
    connections?: Array<models.ItemsTypeConnections> | undefined;
    /**
     * Unique ID for this input
     */
    id: string;
    type: InputModelDrivenTelemetryType;
    disabled?: boolean | undefined;
    /**
     * Pipeline to process data from this Source before sending it through the Routes
     */
    pipeline?: string | undefined;
    /**
     * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
     */
    environment?: string | undefined;
    /**
     * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
     */
    pqEnabled?: boolean | undefined;
    /**
     * Tags for filtering and grouping in @{product}
     */
    streamtags?: Array<string> | undefined;
    pq?: models.PqType | undefined;
    /**
     * Address to bind on. Defaults to 0.0.0.0 (all addresses).
     */
    host?: string | undefined;
    /**
     * Port to listen on
     */
    port?: number | undefined;
    tls?: models.TlsSettingsServerSideType | undefined;
    /**
     * Fields to add to events from this input
     */
    metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
    /**
     * Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.
     */
    maxActiveCxn?: number | undefined;
    /**
     * Time in milliseconds to allow the server to shutdown gracefully before forcing shutdown. Defaults to 5000.
     */
    shutdownTimeoutMs?: number | undefined;
    description?: string | undefined;
  };

export type InputModelDrivenTelemetry =
  | InputModelDrivenTelemetrySendToRoutesTrueWithConnectionsConstraint
  | InputModelDrivenTelemetrySendToRoutesFalseWithConnectionsConstraint
  | InputModelDrivenTelemetryPqEnabledFalseWithPqConstraint
  | InputModelDrivenTelemetryPqEnabledTrueWithPqConstraint;

export const InputOpenTelemetryType = {
  OpenTelemetry: "open_telemetry",
} as const;
export type InputOpenTelemetryType = ClosedEnum<typeof InputOpenTelemetryType>;

/**
 * Select whether to leverage gRPC or HTTP for OpenTelemetry
 */
export const CreateInputProtocol = {
  /**
   * gRPC
   */
  Grpc: "grpc",
  /**
   * HTTP
   */
  Http: "http",
} as const;
/**
 * Select whether to leverage gRPC or HTTP for OpenTelemetry
 */
export type CreateInputProtocol = OpenEnum<typeof CreateInputProtocol>;

/**
 * The version of OTLP Protobuf definitions to use when interpreting received data
 */
export const CreateInputOTLPVersion = {
  /**
   * 0.10.0
   */
  ZeroDot10Dot0: "0.10.0",
  /**
   * 1.3.1
   */
  OneDot3Dot1: "1.3.1",
} as const;
/**
 * The version of OTLP Protobuf definitions to use when interpreting received data
 */
export type CreateInputOTLPVersion = OpenEnum<typeof CreateInputOTLPVersion>;

export type InputOpenTelemetryPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputOpenTelemetryType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port?: number | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  enableProxyHeader?: any | undefined;
  captureHeaders?: any | undefined;
  activityLogSampleRate?: any | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 sec.; maximum 600 sec. (10 min.).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Enable to expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist.
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Select whether to leverage gRPC or HTTP for OpenTelemetry
   */
  protocol?: CreateInputProtocol | undefined;
  /**
   * Enable to extract each incoming span to a separate event
   */
  extractSpans?: boolean | undefined;
  /**
   * Enable to extract each incoming Gauge or IntGauge metric to multiple events, one per data point
   */
  extractMetrics?: boolean | undefined;
  /**
   * The version of OTLP Protobuf definitions to use when interpreting received data
   */
  otlpVersion?: CreateInputOTLPVersion | undefined;
  /**
   * OpenTelemetry authentication type
   */
  authType?: models.AuthenticationTypeOptions | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.
   */
  maxActiveCxn?: number | undefined;
  description?: string | undefined;
  username?: string | undefined;
  password?: string | undefined;
  /**
   * Bearer token to include in the authorization header
   */
  token?: string | undefined;
  /**
   * Select or create a secret that references your credentials
   */
  credentialsSecret?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
  /**
   * URL for OAuth
   */
  loginUrl?: string | undefined;
  /**
   * Secret parameter name to pass in request body
   */
  secretParamName?: string | undefined;
  /**
   * Secret parameter value to pass in request body
   */
  secret?: string | undefined;
  /**
   * Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
   */
  tokenAttributeName?: string | undefined;
  /**
   * JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.
   */
  authHeaderExpr?: string | undefined;
  /**
   * How often the OAuth token should be refreshed.
   */
  tokenTimeoutSecs?: number | undefined;
  /**
   * Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
   */
  oauthParams?: Array<models.ItemsTypeOauthParams> | undefined;
  /**
   * Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
   */
  oauthHeaders?: Array<models.ItemsTypeOauthHeaders> | undefined;
  /**
   * Enable to extract each incoming log record to a separate event
   */
  extractLogs?: boolean | undefined;
};

export type InputOpenTelemetryPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputOpenTelemetryType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port?: number | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  enableProxyHeader?: any | undefined;
  captureHeaders?: any | undefined;
  activityLogSampleRate?: any | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 sec.; maximum 600 sec. (10 min.).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Enable to expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist.
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Select whether to leverage gRPC or HTTP for OpenTelemetry
   */
  protocol?: CreateInputProtocol | undefined;
  /**
   * Enable to extract each incoming span to a separate event
   */
  extractSpans?: boolean | undefined;
  /**
   * Enable to extract each incoming Gauge or IntGauge metric to multiple events, one per data point
   */
  extractMetrics?: boolean | undefined;
  /**
   * The version of OTLP Protobuf definitions to use when interpreting received data
   */
  otlpVersion?: CreateInputOTLPVersion | undefined;
  /**
   * OpenTelemetry authentication type
   */
  authType?: models.AuthenticationTypeOptions | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.
   */
  maxActiveCxn?: number | undefined;
  description?: string | undefined;
  username?: string | undefined;
  password?: string | undefined;
  /**
   * Bearer token to include in the authorization header
   */
  token?: string | undefined;
  /**
   * Select or create a secret that references your credentials
   */
  credentialsSecret?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
  /**
   * URL for OAuth
   */
  loginUrl?: string | undefined;
  /**
   * Secret parameter name to pass in request body
   */
  secretParamName?: string | undefined;
  /**
   * Secret parameter value to pass in request body
   */
  secret?: string | undefined;
  /**
   * Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
   */
  tokenAttributeName?: string | undefined;
  /**
   * JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.
   */
  authHeaderExpr?: string | undefined;
  /**
   * How often the OAuth token should be refreshed.
   */
  tokenTimeoutSecs?: number | undefined;
  /**
   * Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
   */
  oauthParams?: Array<models.ItemsTypeOauthParams> | undefined;
  /**
   * Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
   */
  oauthHeaders?: Array<models.ItemsTypeOauthHeaders> | undefined;
  /**
   * Enable to extract each incoming log record to a separate event
   */
  extractLogs?: boolean | undefined;
};

export type InputOpenTelemetrySendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputOpenTelemetryType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port?: number | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  enableProxyHeader?: any | undefined;
  captureHeaders?: any | undefined;
  activityLogSampleRate?: any | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 sec.; maximum 600 sec. (10 min.).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Enable to expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist.
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Select whether to leverage gRPC or HTTP for OpenTelemetry
   */
  protocol?: CreateInputProtocol | undefined;
  /**
   * Enable to extract each incoming span to a separate event
   */
  extractSpans?: boolean | undefined;
  /**
   * Enable to extract each incoming Gauge or IntGauge metric to multiple events, one per data point
   */
  extractMetrics?: boolean | undefined;
  /**
   * The version of OTLP Protobuf definitions to use when interpreting received data
   */
  otlpVersion?: CreateInputOTLPVersion | undefined;
  /**
   * OpenTelemetry authentication type
   */
  authType?: models.AuthenticationTypeOptions | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.
   */
  maxActiveCxn?: number | undefined;
  description?: string | undefined;
  username?: string | undefined;
  password?: string | undefined;
  /**
   * Bearer token to include in the authorization header
   */
  token?: string | undefined;
  /**
   * Select or create a secret that references your credentials
   */
  credentialsSecret?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
  /**
   * URL for OAuth
   */
  loginUrl?: string | undefined;
  /**
   * Secret parameter name to pass in request body
   */
  secretParamName?: string | undefined;
  /**
   * Secret parameter value to pass in request body
   */
  secret?: string | undefined;
  /**
   * Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
   */
  tokenAttributeName?: string | undefined;
  /**
   * JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.
   */
  authHeaderExpr?: string | undefined;
  /**
   * How often the OAuth token should be refreshed.
   */
  tokenTimeoutSecs?: number | undefined;
  /**
   * Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
   */
  oauthParams?: Array<models.ItemsTypeOauthParams> | undefined;
  /**
   * Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
   */
  oauthHeaders?: Array<models.ItemsTypeOauthHeaders> | undefined;
  /**
   * Enable to extract each incoming log record to a separate event
   */
  extractLogs?: boolean | undefined;
};

export type InputOpenTelemetrySendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputOpenTelemetryType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port?: number | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  enableProxyHeader?: any | undefined;
  captureHeaders?: any | undefined;
  activityLogSampleRate?: any | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 sec.; maximum 600 sec. (10 min.).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Enable to expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist.
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Select whether to leverage gRPC or HTTP for OpenTelemetry
   */
  protocol?: CreateInputProtocol | undefined;
  /**
   * Enable to extract each incoming span to a separate event
   */
  extractSpans?: boolean | undefined;
  /**
   * Enable to extract each incoming Gauge or IntGauge metric to multiple events, one per data point
   */
  extractMetrics?: boolean | undefined;
  /**
   * The version of OTLP Protobuf definitions to use when interpreting received data
   */
  otlpVersion?: CreateInputOTLPVersion | undefined;
  /**
   * OpenTelemetry authentication type
   */
  authType?: models.AuthenticationTypeOptions | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.
   */
  maxActiveCxn?: number | undefined;
  description?: string | undefined;
  username?: string | undefined;
  password?: string | undefined;
  /**
   * Bearer token to include in the authorization header
   */
  token?: string | undefined;
  /**
   * Select or create a secret that references your credentials
   */
  credentialsSecret?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
  /**
   * URL for OAuth
   */
  loginUrl?: string | undefined;
  /**
   * Secret parameter name to pass in request body
   */
  secretParamName?: string | undefined;
  /**
   * Secret parameter value to pass in request body
   */
  secret?: string | undefined;
  /**
   * Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
   */
  tokenAttributeName?: string | undefined;
  /**
   * JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.
   */
  authHeaderExpr?: string | undefined;
  /**
   * How often the OAuth token should be refreshed.
   */
  tokenTimeoutSecs?: number | undefined;
  /**
   * Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
   */
  oauthParams?: Array<models.ItemsTypeOauthParams> | undefined;
  /**
   * Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
   */
  oauthHeaders?: Array<models.ItemsTypeOauthHeaders> | undefined;
  /**
   * Enable to extract each incoming log record to a separate event
   */
  extractLogs?: boolean | undefined;
};

export type InputOpenTelemetry =
  | InputOpenTelemetrySendToRoutesTrueWithConnectionsConstraint
  | InputOpenTelemetrySendToRoutesFalseWithConnectionsConstraint
  | InputOpenTelemetryPqEnabledFalseWithPqConstraint
  | InputOpenTelemetryPqEnabledTrueWithPqConstraint;

export const InputSnmpType = {
  Snmp: "snmp",
} as const;
export type InputSnmpType = ClosedEnum<typeof InputSnmpType>;

export const PrivacyProtocol = {
  /**
   * None
   */
  None: "none",
  /**
   * DES
   */
  Des: "des",
  /**
   * AES128
   */
  Aes: "aes",
  /**
   * AES256b (Blumenthal)
   */
  Aes256b: "aes256b",
  /**
   * AES256r (Reeder)
   */
  Aes256r: "aes256r",
} as const;
export type PrivacyProtocol = OpenEnum<typeof PrivacyProtocol>;

export type V3User = {
  name: string;
  authProtocol?: models.AuthenticationProtocolOptionsV3User | undefined;
  authKey?: string | undefined;
  privProtocol?: PrivacyProtocol | undefined;
  privKey?: string | undefined;
};

/**
 * Authentication parameters for SNMPv3 trap. Set the log level to debug if you are experiencing authentication or decryption issues.
 */
export type SNMPv3Authentication = {
  v3AuthEnabled?: boolean | undefined;
  /**
   * Pass through traps that don't match any of the configured users. @{product} will not attempt to decrypt these traps.
   */
  allowUnmatchedTrap?: boolean | undefined;
  /**
   * User credentials for receiving v3 traps
   */
  v3Users?: Array<V3User> | undefined;
};

export type InputSnmpPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputSnmpType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.
   */
  host?: string | undefined;
  /**
   * UDP port to receive SNMP traps on. Defaults to 162.
   */
  port?: number | undefined;
  /**
   * Authentication parameters for SNMPv3 trap. Set the log level to debug if you are experiencing authentication or decryption issues.
   */
  snmpV3Auth?: SNMPv3Authentication | undefined;
  /**
   * Maximum number of events to buffer when downstream is blocking.
   */
  maxBufferSize?: number | undefined;
  /**
   * Regex matching IP addresses that are allowed to send data
   */
  ipWhitelistRegex?: string | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.
   */
  udpSocketRxBufSize?: number | undefined;
  /**
   * If enabled, parses varbinds as an array of objects that include OID, value, and type
   */
  varbindsWithTypes?: boolean | undefined;
  /**
   * If enabled, the parser will attempt to parse varbind octet strings as UTF-8, first, otherwise will fallback to other methods
   */
  bestEffortParsing?: boolean | undefined;
  description?: string | undefined;
};

export type InputSnmpPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputSnmpType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.
   */
  host?: string | undefined;
  /**
   * UDP port to receive SNMP traps on. Defaults to 162.
   */
  port?: number | undefined;
  /**
   * Authentication parameters for SNMPv3 trap. Set the log level to debug if you are experiencing authentication or decryption issues.
   */
  snmpV3Auth?: SNMPv3Authentication | undefined;
  /**
   * Maximum number of events to buffer when downstream is blocking.
   */
  maxBufferSize?: number | undefined;
  /**
   * Regex matching IP addresses that are allowed to send data
   */
  ipWhitelistRegex?: string | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.
   */
  udpSocketRxBufSize?: number | undefined;
  /**
   * If enabled, parses varbinds as an array of objects that include OID, value, and type
   */
  varbindsWithTypes?: boolean | undefined;
  /**
   * If enabled, the parser will attempt to parse varbind octet strings as UTF-8, first, otherwise will fallback to other methods
   */
  bestEffortParsing?: boolean | undefined;
  description?: string | undefined;
};

export type InputSnmpSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputSnmpType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.
   */
  host?: string | undefined;
  /**
   * UDP port to receive SNMP traps on. Defaults to 162.
   */
  port?: number | undefined;
  /**
   * Authentication parameters for SNMPv3 trap. Set the log level to debug if you are experiencing authentication or decryption issues.
   */
  snmpV3Auth?: SNMPv3Authentication | undefined;
  /**
   * Maximum number of events to buffer when downstream is blocking.
   */
  maxBufferSize?: number | undefined;
  /**
   * Regex matching IP addresses that are allowed to send data
   */
  ipWhitelistRegex?: string | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.
   */
  udpSocketRxBufSize?: number | undefined;
  /**
   * If enabled, parses varbinds as an array of objects that include OID, value, and type
   */
  varbindsWithTypes?: boolean | undefined;
  /**
   * If enabled, the parser will attempt to parse varbind octet strings as UTF-8, first, otherwise will fallback to other methods
   */
  bestEffortParsing?: boolean | undefined;
  description?: string | undefined;
};

export type InputSnmpSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputSnmpType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.
   */
  host?: string | undefined;
  /**
   * UDP port to receive SNMP traps on. Defaults to 162.
   */
  port?: number | undefined;
  /**
   * Authentication parameters for SNMPv3 trap. Set the log level to debug if you are experiencing authentication or decryption issues.
   */
  snmpV3Auth?: SNMPv3Authentication | undefined;
  /**
   * Maximum number of events to buffer when downstream is blocking.
   */
  maxBufferSize?: number | undefined;
  /**
   * Regex matching IP addresses that are allowed to send data
   */
  ipWhitelistRegex?: string | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.
   */
  udpSocketRxBufSize?: number | undefined;
  /**
   * If enabled, parses varbinds as an array of objects that include OID, value, and type
   */
  varbindsWithTypes?: boolean | undefined;
  /**
   * If enabled, the parser will attempt to parse varbind octet strings as UTF-8, first, otherwise will fallback to other methods
   */
  bestEffortParsing?: boolean | undefined;
  description?: string | undefined;
};

export type InputSnmp =
  | InputSnmpSendToRoutesTrueWithConnectionsConstraint
  | InputSnmpSendToRoutesFalseWithConnectionsConstraint
  | InputSnmpPqEnabledFalseWithPqConstraint
  | InputSnmpPqEnabledTrueWithPqConstraint;

export const InputS3InventoryType = {
  S3Inventory: "s3_inventory",
} as const;
export type InputS3InventoryType = ClosedEnum<typeof InputS3InventoryType>;

export type InputS3InventoryPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputS3InventoryType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.
   */
  queueName: string;
  /**
   * Regex matching file names to download and process. Defaults to: .*
   */
  fileFilter?: string | undefined;
  /**
   * SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.
   */
  awsAccountId?: string | undefined;
  /**
   * AWS authentication method. Choose Auto to use IAM roles.
   */
  awsAuthenticationMethod?: string | undefined;
  awsSecretKey?: string | undefined;
  /**
   * AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region.
   */
  region?: string | undefined;
  /**
   * S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.
   */
  endpoint?: string | undefined;
  /**
   * Signature version to use for signing S3 requests
   */
  signatureVersion?: models.SignatureVersionOptionsS3CollectorConf | undefined;
  /**
   * Reuse connections between requests, which can improve performance
   */
  reuseConnections?: boolean | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.
   */
  maxMessages?: number | undefined;
  /**
   * After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).
   */
  visibilityTimeout?: number | undefined;
  /**
   * How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.
   */
  numReceivers?: number | undefined;
  /**
   * Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure.
   */
  socketTimeout?: number | undefined;
  /**
   * Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.
   */
  skipOnError?: boolean | undefined;
  /**
   * Attach SQS notification metadata to a __sqsMetadata field on each event
   */
  includeSqsMetadata?: boolean | undefined;
  /**
   * Use Assume Role credentials to access Amazon S3
   */
  enableAssumeRole?: boolean | undefined;
  /**
   * Amazon Resource Name (ARN) of the role to assume
   */
  assumeRoleArn?: string | undefined;
  /**
   * External ID to use when assuming role
   */
  assumeRoleExternalId?: string | undefined;
  /**
   * Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
   */
  durationSeconds?: number | undefined;
  /**
   * Use Assume Role credentials when accessing Amazon SQS
   */
  enableSQSAssumeRole?: boolean | undefined;
  preprocess?: models.PreprocessTypeSavedJobCollectionInput | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Maximum file size for each Parquet chunk
   */
  parquetChunkSizeMB?: number | undefined;
  /**
   * The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified.
   */
  parquetChunkDownloadTimeout?: number | undefined;
  checkpointing?: models.CheckpointingType | undefined;
  /**
   * How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.
   */
  pollTimeout?: number | undefined;
  /**
   * Filename suffix of the manifest checksum file. If a filename matching this suffix is received        in the queue, the matching manifest file will be downloaded and validated against its value. Defaults to "checksum"
   */
  checksumSuffix?: string | undefined;
  /**
   * Maximum download size (KB) of each manifest or checksum file. Manifest files larger than this size will not be read.        Defaults to 4096.
   */
  maxManifestSizeKB?: number | undefined;
  /**
   * If set to Yes, each inventory file in the manifest will be validated against its checksum. Defaults to false
   */
  validateInventoryFiles?: boolean | undefined;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  /**
   * Select or create a stored secret that references your access key and secret key
   */
  awsSecret?: string | undefined;
  tagAfterProcessing?: models.TagAfterProcessingOptions | undefined;
  /**
   * The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.
   */
  processedTagKey?: string | undefined;
  /**
   * The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.
   */
  processedTagValue?: string | undefined;
};

export type InputS3InventoryPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputS3InventoryType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.
   */
  queueName: string;
  /**
   * Regex matching file names to download and process. Defaults to: .*
   */
  fileFilter?: string | undefined;
  /**
   * SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.
   */
  awsAccountId?: string | undefined;
  /**
   * AWS authentication method. Choose Auto to use IAM roles.
   */
  awsAuthenticationMethod?: string | undefined;
  awsSecretKey?: string | undefined;
  /**
   * AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region.
   */
  region?: string | undefined;
  /**
   * S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.
   */
  endpoint?: string | undefined;
  /**
   * Signature version to use for signing S3 requests
   */
  signatureVersion?: models.SignatureVersionOptionsS3CollectorConf | undefined;
  /**
   * Reuse connections between requests, which can improve performance
   */
  reuseConnections?: boolean | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.
   */
  maxMessages?: number | undefined;
  /**
   * After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).
   */
  visibilityTimeout?: number | undefined;
  /**
   * How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.
   */
  numReceivers?: number | undefined;
  /**
   * Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure.
   */
  socketTimeout?: number | undefined;
  /**
   * Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.
   */
  skipOnError?: boolean | undefined;
  /**
   * Attach SQS notification metadata to a __sqsMetadata field on each event
   */
  includeSqsMetadata?: boolean | undefined;
  /**
   * Use Assume Role credentials to access Amazon S3
   */
  enableAssumeRole?: boolean | undefined;
  /**
   * Amazon Resource Name (ARN) of the role to assume
   */
  assumeRoleArn?: string | undefined;
  /**
   * External ID to use when assuming role
   */
  assumeRoleExternalId?: string | undefined;
  /**
   * Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
   */
  durationSeconds?: number | undefined;
  /**
   * Use Assume Role credentials when accessing Amazon SQS
   */
  enableSQSAssumeRole?: boolean | undefined;
  preprocess?: models.PreprocessTypeSavedJobCollectionInput | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Maximum file size for each Parquet chunk
   */
  parquetChunkSizeMB?: number | undefined;
  /**
   * The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified.
   */
  parquetChunkDownloadTimeout?: number | undefined;
  checkpointing?: models.CheckpointingType | undefined;
  /**
   * How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.
   */
  pollTimeout?: number | undefined;
  /**
   * Filename suffix of the manifest checksum file. If a filename matching this suffix is received        in the queue, the matching manifest file will be downloaded and validated against its value. Defaults to "checksum"
   */
  checksumSuffix?: string | undefined;
  /**
   * Maximum download size (KB) of each manifest or checksum file. Manifest files larger than this size will not be read.        Defaults to 4096.
   */
  maxManifestSizeKB?: number | undefined;
  /**
   * If set to Yes, each inventory file in the manifest will be validated against its checksum. Defaults to false
   */
  validateInventoryFiles?: boolean | undefined;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  /**
   * Select or create a stored secret that references your access key and secret key
   */
  awsSecret?: string | undefined;
  tagAfterProcessing?: models.TagAfterProcessingOptions | undefined;
  /**
   * The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.
   */
  processedTagKey?: string | undefined;
  /**
   * The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.
   */
  processedTagValue?: string | undefined;
};

export type InputS3InventorySendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputS3InventoryType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.
   */
  queueName: string;
  /**
   * Regex matching file names to download and process. Defaults to: .*
   */
  fileFilter?: string | undefined;
  /**
   * SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.
   */
  awsAccountId?: string | undefined;
  /**
   * AWS authentication method. Choose Auto to use IAM roles.
   */
  awsAuthenticationMethod?: string | undefined;
  awsSecretKey?: string | undefined;
  /**
   * AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region.
   */
  region?: string | undefined;
  /**
   * S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.
   */
  endpoint?: string | undefined;
  /**
   * Signature version to use for signing S3 requests
   */
  signatureVersion?: models.SignatureVersionOptionsS3CollectorConf | undefined;
  /**
   * Reuse connections between requests, which can improve performance
   */
  reuseConnections?: boolean | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.
   */
  maxMessages?: number | undefined;
  /**
   * After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).
   */
  visibilityTimeout?: number | undefined;
  /**
   * How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.
   */
  numReceivers?: number | undefined;
  /**
   * Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure.
   */
  socketTimeout?: number | undefined;
  /**
   * Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.
   */
  skipOnError?: boolean | undefined;
  /**
   * Attach SQS notification metadata to a __sqsMetadata field on each event
   */
  includeSqsMetadata?: boolean | undefined;
  /**
   * Use Assume Role credentials to access Amazon S3
   */
  enableAssumeRole?: boolean | undefined;
  /**
   * Amazon Resource Name (ARN) of the role to assume
   */
  assumeRoleArn?: string | undefined;
  /**
   * External ID to use when assuming role
   */
  assumeRoleExternalId?: string | undefined;
  /**
   * Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
   */
  durationSeconds?: number | undefined;
  /**
   * Use Assume Role credentials when accessing Amazon SQS
   */
  enableSQSAssumeRole?: boolean | undefined;
  preprocess?: models.PreprocessTypeSavedJobCollectionInput | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Maximum file size for each Parquet chunk
   */
  parquetChunkSizeMB?: number | undefined;
  /**
   * The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified.
   */
  parquetChunkDownloadTimeout?: number | undefined;
  checkpointing?: models.CheckpointingType | undefined;
  /**
   * How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.
   */
  pollTimeout?: number | undefined;
  /**
   * Filename suffix of the manifest checksum file. If a filename matching this suffix is received        in the queue, the matching manifest file will be downloaded and validated against its value. Defaults to "checksum"
   */
  checksumSuffix?: string | undefined;
  /**
   * Maximum download size (KB) of each manifest or checksum file. Manifest files larger than this size will not be read.        Defaults to 4096.
   */
  maxManifestSizeKB?: number | undefined;
  /**
   * If set to Yes, each inventory file in the manifest will be validated against its checksum. Defaults to false
   */
  validateInventoryFiles?: boolean | undefined;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  /**
   * Select or create a stored secret that references your access key and secret key
   */
  awsSecret?: string | undefined;
  tagAfterProcessing?: models.TagAfterProcessingOptions | undefined;
  /**
   * The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.
   */
  processedTagKey?: string | undefined;
  /**
   * The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.
   */
  processedTagValue?: string | undefined;
};

export type InputS3InventorySendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputS3InventoryType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.
   */
  queueName: string;
  /**
   * Regex matching file names to download and process. Defaults to: .*
   */
  fileFilter?: string | undefined;
  /**
   * SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.
   */
  awsAccountId?: string | undefined;
  /**
   * AWS authentication method. Choose Auto to use IAM roles.
   */
  awsAuthenticationMethod?: string | undefined;
  awsSecretKey?: string | undefined;
  /**
   * AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region.
   */
  region?: string | undefined;
  /**
   * S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.
   */
  endpoint?: string | undefined;
  /**
   * Signature version to use for signing S3 requests
   */
  signatureVersion?: models.SignatureVersionOptionsS3CollectorConf | undefined;
  /**
   * Reuse connections between requests, which can improve performance
   */
  reuseConnections?: boolean | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.
   */
  maxMessages?: number | undefined;
  /**
   * After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).
   */
  visibilityTimeout?: number | undefined;
  /**
   * How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.
   */
  numReceivers?: number | undefined;
  /**
   * Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure.
   */
  socketTimeout?: number | undefined;
  /**
   * Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.
   */
  skipOnError?: boolean | undefined;
  /**
   * Attach SQS notification metadata to a __sqsMetadata field on each event
   */
  includeSqsMetadata?: boolean | undefined;
  /**
   * Use Assume Role credentials to access Amazon S3
   */
  enableAssumeRole?: boolean | undefined;
  /**
   * Amazon Resource Name (ARN) of the role to assume
   */
  assumeRoleArn?: string | undefined;
  /**
   * External ID to use when assuming role
   */
  assumeRoleExternalId?: string | undefined;
  /**
   * Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
   */
  durationSeconds?: number | undefined;
  /**
   * Use Assume Role credentials when accessing Amazon SQS
   */
  enableSQSAssumeRole?: boolean | undefined;
  preprocess?: models.PreprocessTypeSavedJobCollectionInput | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Maximum file size for each Parquet chunk
   */
  parquetChunkSizeMB?: number | undefined;
  /**
   * The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified.
   */
  parquetChunkDownloadTimeout?: number | undefined;
  checkpointing?: models.CheckpointingType | undefined;
  /**
   * How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.
   */
  pollTimeout?: number | undefined;
  /**
   * Filename suffix of the manifest checksum file. If a filename matching this suffix is received        in the queue, the matching manifest file will be downloaded and validated against its value. Defaults to "checksum"
   */
  checksumSuffix?: string | undefined;
  /**
   * Maximum download size (KB) of each manifest or checksum file. Manifest files larger than this size will not be read.        Defaults to 4096.
   */
  maxManifestSizeKB?: number | undefined;
  /**
   * If set to Yes, each inventory file in the manifest will be validated against its checksum. Defaults to false
   */
  validateInventoryFiles?: boolean | undefined;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  /**
   * Select or create a stored secret that references your access key and secret key
   */
  awsSecret?: string | undefined;
  tagAfterProcessing?: models.TagAfterProcessingOptions | undefined;
  /**
   * The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.
   */
  processedTagKey?: string | undefined;
  /**
   * The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.
   */
  processedTagValue?: string | undefined;
};

export type InputS3Inventory =
  | InputS3InventorySendToRoutesTrueWithConnectionsConstraint
  | InputS3InventorySendToRoutesFalseWithConnectionsConstraint
  | InputS3InventoryPqEnabledFalseWithPqConstraint
  | InputS3InventoryPqEnabledTrueWithPqConstraint;

export const InputS3Type = {
  S3: "s3",
} as const;
export type InputS3Type = ClosedEnum<typeof InputS3Type>;

export type InputS3PqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputS3Type;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.
   */
  queueName: string;
  /**
   * Regex matching file names to download and process. Defaults to: .*
   */
  fileFilter?: string | undefined;
  /**
   * SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.
   */
  awsAccountId?: string | undefined;
  /**
   * AWS authentication method. Choose Auto to use IAM roles.
   */
  awsAuthenticationMethod?: string | undefined;
  awsSecretKey?: string | undefined;
  /**
   * AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region.
   */
  region?: string | undefined;
  /**
   * S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.
   */
  endpoint?: string | undefined;
  /**
   * Signature version to use for signing S3 requests
   */
  signatureVersion?: models.SignatureVersionOptionsS3CollectorConf | undefined;
  /**
   * Reuse connections between requests, which can improve performance
   */
  reuseConnections?: boolean | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.
   */
  maxMessages?: number | undefined;
  /**
   * After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).
   */
  visibilityTimeout?: number | undefined;
  /**
   * How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.
   */
  numReceivers?: number | undefined;
  /**
   * Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure.
   */
  socketTimeout?: number | undefined;
  /**
   * Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.
   */
  skipOnError?: boolean | undefined;
  /**
   * Attach SQS notification metadata to a __sqsMetadata field on each event
   */
  includeSqsMetadata?: boolean | undefined;
  /**
   * Use Assume Role credentials to access Amazon S3
   */
  enableAssumeRole?: boolean | undefined;
  /**
   * Amazon Resource Name (ARN) of the role to assume
   */
  assumeRoleArn?: string | undefined;
  /**
   * External ID to use when assuming role
   */
  assumeRoleExternalId?: string | undefined;
  /**
   * Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
   */
  durationSeconds?: number | undefined;
  /**
   * Use Assume Role credentials when accessing Amazon SQS
   */
  enableSQSAssumeRole?: boolean | undefined;
  preprocess?: models.PreprocessTypeSavedJobCollectionInput | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Maximum file size for each Parquet chunk
   */
  parquetChunkSizeMB?: number | undefined;
  /**
   * The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified.
   */
  parquetChunkDownloadTimeout?: number | undefined;
  checkpointing?: models.CheckpointingType | undefined;
  /**
   * How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.
   */
  pollTimeout?: number | undefined;
  /**
   * Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters.
   */
  encoding?: string | undefined;
  /**
   * Add a tag to processed S3 objects. Requires s3:GetObjectTagging and s3:PutObjectTagging AWS permissions.
   */
  tagAfterProcessing?: boolean | undefined;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  /**
   * Select or create a stored secret that references your access key and secret key
   */
  awsSecret?: string | undefined;
  /**
   * The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.
   */
  processedTagKey?: string | undefined;
  /**
   * The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.
   */
  processedTagValue?: string | undefined;
};

export type InputS3PqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputS3Type;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.
   */
  queueName: string;
  /**
   * Regex matching file names to download and process. Defaults to: .*
   */
  fileFilter?: string | undefined;
  /**
   * SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.
   */
  awsAccountId?: string | undefined;
  /**
   * AWS authentication method. Choose Auto to use IAM roles.
   */
  awsAuthenticationMethod?: string | undefined;
  awsSecretKey?: string | undefined;
  /**
   * AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region.
   */
  region?: string | undefined;
  /**
   * S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.
   */
  endpoint?: string | undefined;
  /**
   * Signature version to use for signing S3 requests
   */
  signatureVersion?: models.SignatureVersionOptionsS3CollectorConf | undefined;
  /**
   * Reuse connections between requests, which can improve performance
   */
  reuseConnections?: boolean | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.
   */
  maxMessages?: number | undefined;
  /**
   * After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).
   */
  visibilityTimeout?: number | undefined;
  /**
   * How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.
   */
  numReceivers?: number | undefined;
  /**
   * Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure.
   */
  socketTimeout?: number | undefined;
  /**
   * Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.
   */
  skipOnError?: boolean | undefined;
  /**
   * Attach SQS notification metadata to a __sqsMetadata field on each event
   */
  includeSqsMetadata?: boolean | undefined;
  /**
   * Use Assume Role credentials to access Amazon S3
   */
  enableAssumeRole?: boolean | undefined;
  /**
   * Amazon Resource Name (ARN) of the role to assume
   */
  assumeRoleArn?: string | undefined;
  /**
   * External ID to use when assuming role
   */
  assumeRoleExternalId?: string | undefined;
  /**
   * Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
   */
  durationSeconds?: number | undefined;
  /**
   * Use Assume Role credentials when accessing Amazon SQS
   */
  enableSQSAssumeRole?: boolean | undefined;
  preprocess?: models.PreprocessTypeSavedJobCollectionInput | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Maximum file size for each Parquet chunk
   */
  parquetChunkSizeMB?: number | undefined;
  /**
   * The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified.
   */
  parquetChunkDownloadTimeout?: number | undefined;
  checkpointing?: models.CheckpointingType | undefined;
  /**
   * How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.
   */
  pollTimeout?: number | undefined;
  /**
   * Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters.
   */
  encoding?: string | undefined;
  /**
   * Add a tag to processed S3 objects. Requires s3:GetObjectTagging and s3:PutObjectTagging AWS permissions.
   */
  tagAfterProcessing?: boolean | undefined;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  /**
   * Select or create a stored secret that references your access key and secret key
   */
  awsSecret?: string | undefined;
  /**
   * The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.
   */
  processedTagKey?: string | undefined;
  /**
   * The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.
   */
  processedTagValue?: string | undefined;
};

export type InputS3SendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputS3Type;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.
   */
  queueName: string;
  /**
   * Regex matching file names to download and process. Defaults to: .*
   */
  fileFilter?: string | undefined;
  /**
   * SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.
   */
  awsAccountId?: string | undefined;
  /**
   * AWS authentication method. Choose Auto to use IAM roles.
   */
  awsAuthenticationMethod?: string | undefined;
  awsSecretKey?: string | undefined;
  /**
   * AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region.
   */
  region?: string | undefined;
  /**
   * S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.
   */
  endpoint?: string | undefined;
  /**
   * Signature version to use for signing S3 requests
   */
  signatureVersion?: models.SignatureVersionOptionsS3CollectorConf | undefined;
  /**
   * Reuse connections between requests, which can improve performance
   */
  reuseConnections?: boolean | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.
   */
  maxMessages?: number | undefined;
  /**
   * After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).
   */
  visibilityTimeout?: number | undefined;
  /**
   * How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.
   */
  numReceivers?: number | undefined;
  /**
   * Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure.
   */
  socketTimeout?: number | undefined;
  /**
   * Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.
   */
  skipOnError?: boolean | undefined;
  /**
   * Attach SQS notification metadata to a __sqsMetadata field on each event
   */
  includeSqsMetadata?: boolean | undefined;
  /**
   * Use Assume Role credentials to access Amazon S3
   */
  enableAssumeRole?: boolean | undefined;
  /**
   * Amazon Resource Name (ARN) of the role to assume
   */
  assumeRoleArn?: string | undefined;
  /**
   * External ID to use when assuming role
   */
  assumeRoleExternalId?: string | undefined;
  /**
   * Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
   */
  durationSeconds?: number | undefined;
  /**
   * Use Assume Role credentials when accessing Amazon SQS
   */
  enableSQSAssumeRole?: boolean | undefined;
  preprocess?: models.PreprocessTypeSavedJobCollectionInput | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Maximum file size for each Parquet chunk
   */
  parquetChunkSizeMB?: number | undefined;
  /**
   * The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified.
   */
  parquetChunkDownloadTimeout?: number | undefined;
  checkpointing?: models.CheckpointingType | undefined;
  /**
   * How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.
   */
  pollTimeout?: number | undefined;
  /**
   * Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters.
   */
  encoding?: string | undefined;
  /**
   * Add a tag to processed S3 objects. Requires s3:GetObjectTagging and s3:PutObjectTagging AWS permissions.
   */
  tagAfterProcessing?: boolean | undefined;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  /**
   * Select or create a stored secret that references your access key and secret key
   */
  awsSecret?: string | undefined;
  /**
   * The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.
   */
  processedTagKey?: string | undefined;
  /**
   * The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.
   */
  processedTagValue?: string | undefined;
};

export type InputS3SendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputS3Type;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.
   */
  queueName: string;
  /**
   * Regex matching file names to download and process. Defaults to: .*
   */
  fileFilter?: string | undefined;
  /**
   * SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.
   */
  awsAccountId?: string | undefined;
  /**
   * AWS authentication method. Choose Auto to use IAM roles.
   */
  awsAuthenticationMethod?: string | undefined;
  awsSecretKey?: string | undefined;
  /**
   * AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region.
   */
  region?: string | undefined;
  /**
   * S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.
   */
  endpoint?: string | undefined;
  /**
   * Signature version to use for signing S3 requests
   */
  signatureVersion?: models.SignatureVersionOptionsS3CollectorConf | undefined;
  /**
   * Reuse connections between requests, which can improve performance
   */
  reuseConnections?: boolean | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.
   */
  maxMessages?: number | undefined;
  /**
   * After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).
   */
  visibilityTimeout?: number | undefined;
  /**
   * How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.
   */
  numReceivers?: number | undefined;
  /**
   * Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure.
   */
  socketTimeout?: number | undefined;
  /**
   * Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.
   */
  skipOnError?: boolean | undefined;
  /**
   * Attach SQS notification metadata to a __sqsMetadata field on each event
   */
  includeSqsMetadata?: boolean | undefined;
  /**
   * Use Assume Role credentials to access Amazon S3
   */
  enableAssumeRole?: boolean | undefined;
  /**
   * Amazon Resource Name (ARN) of the role to assume
   */
  assumeRoleArn?: string | undefined;
  /**
   * External ID to use when assuming role
   */
  assumeRoleExternalId?: string | undefined;
  /**
   * Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
   */
  durationSeconds?: number | undefined;
  /**
   * Use Assume Role credentials when accessing Amazon SQS
   */
  enableSQSAssumeRole?: boolean | undefined;
  preprocess?: models.PreprocessTypeSavedJobCollectionInput | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Maximum file size for each Parquet chunk
   */
  parquetChunkSizeMB?: number | undefined;
  /**
   * The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified.
   */
  parquetChunkDownloadTimeout?: number | undefined;
  checkpointing?: models.CheckpointingType | undefined;
  /**
   * How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.
   */
  pollTimeout?: number | undefined;
  /**
   * Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters.
   */
  encoding?: string | undefined;
  /**
   * Add a tag to processed S3 objects. Requires s3:GetObjectTagging and s3:PutObjectTagging AWS permissions.
   */
  tagAfterProcessing?: boolean | undefined;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  /**
   * Select or create a stored secret that references your access key and secret key
   */
  awsSecret?: string | undefined;
  /**
   * The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.
   */
  processedTagKey?: string | undefined;
  /**
   * The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.
   */
  processedTagValue?: string | undefined;
};

export type InputS3 =
  | InputS3SendToRoutesTrueWithConnectionsConstraint
  | InputS3SendToRoutesFalseWithConnectionsConstraint
  | InputS3PqEnabledFalseWithPqConstraint
  | InputS3PqEnabledTrueWithPqConstraint;

export const InputMetricsType = {
  Metrics: "metrics",
} as const;
export type InputMetricsType = ClosedEnum<typeof InputMetricsType>;

export type InputMetricsPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputMetricsType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.
   */
  host?: string | undefined;
  /**
   * Enter UDP port number to listen on. Not required if listening on TCP.
   */
  udpPort?: number | undefined;
  /**
   * Enter TCP port number to listen on. Not required if listening on UDP.
   */
  tcpPort?: number | undefined;
  /**
   * Maximum number of events to buffer when downstream is blocking. Only applies to UDP.
   */
  maxBufferSize?: number | undefined;
  /**
   * Regex matching IP addresses that are allowed to send data
   */
  ipWhitelistRegex?: string | undefined;
  /**
   * Enable if the connection is proxied by a device that supports Proxy Protocol V1 or V2
   */
  enableProxyHeader?: boolean | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.
   */
  udpSocketRxBufSize?: number | undefined;
  description?: string | undefined;
};

export type InputMetricsPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputMetricsType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.
   */
  host?: string | undefined;
  /**
   * Enter UDP port number to listen on. Not required if listening on TCP.
   */
  udpPort?: number | undefined;
  /**
   * Enter TCP port number to listen on. Not required if listening on UDP.
   */
  tcpPort?: number | undefined;
  /**
   * Maximum number of events to buffer when downstream is blocking. Only applies to UDP.
   */
  maxBufferSize?: number | undefined;
  /**
   * Regex matching IP addresses that are allowed to send data
   */
  ipWhitelistRegex?: string | undefined;
  /**
   * Enable if the connection is proxied by a device that supports Proxy Protocol V1 or V2
   */
  enableProxyHeader?: boolean | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.
   */
  udpSocketRxBufSize?: number | undefined;
  description?: string | undefined;
};

export type InputMetricsSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputMetricsType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.
   */
  host?: string | undefined;
  /**
   * Enter UDP port number to listen on. Not required if listening on TCP.
   */
  udpPort?: number | undefined;
  /**
   * Enter TCP port number to listen on. Not required if listening on UDP.
   */
  tcpPort?: number | undefined;
  /**
   * Maximum number of events to buffer when downstream is blocking. Only applies to UDP.
   */
  maxBufferSize?: number | undefined;
  /**
   * Regex matching IP addresses that are allowed to send data
   */
  ipWhitelistRegex?: string | undefined;
  /**
   * Enable if the connection is proxied by a device that supports Proxy Protocol V1 or V2
   */
  enableProxyHeader?: boolean | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.
   */
  udpSocketRxBufSize?: number | undefined;
  description?: string | undefined;
};

export type InputMetricsSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputMetricsType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. For IPv4 (all addresses), use the default '0.0.0.0'. For IPv6, enter '::' (all addresses) or specify an IP address.
   */
  host?: string | undefined;
  /**
   * Enter UDP port number to listen on. Not required if listening on TCP.
   */
  udpPort?: number | undefined;
  /**
   * Enter TCP port number to listen on. Not required if listening on UDP.
   */
  tcpPort?: number | undefined;
  /**
   * Maximum number of events to buffer when downstream is blocking. Only applies to UDP.
   */
  maxBufferSize?: number | undefined;
  /**
   * Regex matching IP addresses that are allowed to send data
   */
  ipWhitelistRegex?: string | undefined;
  /**
   * Enable if the connection is proxied by a device that supports Proxy Protocol V1 or V2
   */
  enableProxyHeader?: boolean | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Optionally, set the SO_RCVBUF socket option for the UDP socket. This value tells the operating system how many bytes can be buffered in the kernel before events are dropped. Leave blank to use the OS default. Caution: Increasing this value will affect OS memory utilization.
   */
  udpSocketRxBufSize?: number | undefined;
  description?: string | undefined;
};

export type InputMetrics =
  | InputMetricsSendToRoutesTrueWithConnectionsConstraint
  | InputMetricsSendToRoutesFalseWithConnectionsConstraint
  | InputMetricsPqEnabledFalseWithPqConstraint
  | InputMetricsPqEnabledTrueWithPqConstraint;

export const InputCriblmetricsType = {
  Criblmetrics: "criblmetrics",
} as const;
export type InputCriblmetricsType = ClosedEnum<typeof InputCriblmetricsType>;

export type InputCriblmetricsPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputCriblmetricsType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * A prefix that is applied to the metrics provided by Cribl Stream
   */
  prefix?: string | undefined;
  /**
   * Include granular metrics. Disabling this will drop the following metrics events: `cribl.logstream.host.(in_bytes,in_events,out_bytes,out_events)`, `cribl.logstream.index.(in_bytes,in_events,out_bytes,out_events)`, `cribl.logstream.source.(in_bytes,in_events,out_bytes,out_events)`, `cribl.logstream.sourcetype.(in_bytes,in_events,out_bytes,out_events)`.
   */
  fullFidelity?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputCriblmetricsPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputCriblmetricsType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * A prefix that is applied to the metrics provided by Cribl Stream
   */
  prefix?: string | undefined;
  /**
   * Include granular metrics. Disabling this will drop the following metrics events: `cribl.logstream.host.(in_bytes,in_events,out_bytes,out_events)`, `cribl.logstream.index.(in_bytes,in_events,out_bytes,out_events)`, `cribl.logstream.source.(in_bytes,in_events,out_bytes,out_events)`, `cribl.logstream.sourcetype.(in_bytes,in_events,out_bytes,out_events)`.
   */
  fullFidelity?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputCriblmetricsSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputCriblmetricsType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * A prefix that is applied to the metrics provided by Cribl Stream
   */
  prefix?: string | undefined;
  /**
   * Include granular metrics. Disabling this will drop the following metrics events: `cribl.logstream.host.(in_bytes,in_events,out_bytes,out_events)`, `cribl.logstream.index.(in_bytes,in_events,out_bytes,out_events)`, `cribl.logstream.source.(in_bytes,in_events,out_bytes,out_events)`, `cribl.logstream.sourcetype.(in_bytes,in_events,out_bytes,out_events)`.
   */
  fullFidelity?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputCriblmetricsSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputCriblmetricsType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * A prefix that is applied to the metrics provided by Cribl Stream
   */
  prefix?: string | undefined;
  /**
   * Include granular metrics. Disabling this will drop the following metrics events: `cribl.logstream.host.(in_bytes,in_events,out_bytes,out_events)`, `cribl.logstream.index.(in_bytes,in_events,out_bytes,out_events)`, `cribl.logstream.source.(in_bytes,in_events,out_bytes,out_events)`, `cribl.logstream.sourcetype.(in_bytes,in_events,out_bytes,out_events)`.
   */
  fullFidelity?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputCriblmetrics =
  | InputCriblmetricsSendToRoutesTrueWithConnectionsConstraint
  | InputCriblmetricsSendToRoutesFalseWithConnectionsConstraint
  | InputCriblmetricsPqEnabledFalseWithPqConstraint
  | InputCriblmetricsPqEnabledTrueWithPqConstraint;

export const InputKinesisType = {
  Kinesis: "kinesis",
} as const;
export type InputKinesisType = ClosedEnum<typeof InputKinesisType>;

/**
 * Location at which to start reading a shard for the first time
 */
export const ShardIteratorStart = {
  /**
   * Earliest record
   */
  TrimHorizon: "TRIM_HORIZON",
  /**
   * Latest record
   */
  Latest: "LATEST",
} as const;
/**
 * Location at which to start reading a shard for the first time
 */
export type ShardIteratorStart = OpenEnum<typeof ShardIteratorStart>;

/**
 * Format of data inside the Kinesis Stream records. Gzip compression is automatically detected.
 */
export const RecordDataFormat = {
  /**
   * Cribl
   */
  Cribl: "cribl",
  /**
   * Newline JSON
   */
  Ndjson: "ndjson",
  /**
   * Cloudwatch Logs
   */
  Cloudwatch: "cloudwatch",
  /**
   * Event per line
   */
  Line: "line",
} as const;
/**
 * Format of data inside the Kinesis Stream records. Gzip compression is automatically detected.
 */
export type RecordDataFormat = OpenEnum<typeof RecordDataFormat>;

/**
 * The load-balancing algorithm to use for spreading out shards across Workers and Worker Processes
 */
export const ShardLoadBalancing = {
  /**
   * Consistent Hashing
   */
  ConsistentHashing: "ConsistentHashing",
  /**
   * Round Robin
   */
  RoundRobin: "RoundRobin",
} as const;
/**
 * The load-balancing algorithm to use for spreading out shards across Workers and Worker Processes
 */
export type ShardLoadBalancing = OpenEnum<typeof ShardLoadBalancing>;

export type InputKinesisPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputKinesisType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Kinesis Data Stream to read data from
   */
  streamName: string;
  /**
   * Time interval in minutes between consecutive service calls
   */
  serviceInterval?: number | undefined;
  /**
   * A JavaScript expression to be called with each shardId for the stream. If the expression evaluates to a truthy value, the shard will be processed.
   */
  shardExpr?: string | undefined;
  /**
   * Location at which to start reading a shard for the first time
   */
  shardIteratorType?: ShardIteratorStart | undefined;
  /**
   * Format of data inside the Kinesis Stream records. Gzip compression is automatically detected.
   */
  payloadFormat?: RecordDataFormat | undefined;
  /**
   * Maximum number of records per getRecords call
   */
  getRecordsLimit?: number | undefined;
  /**
   * Maximum number of records, across all shards, to pull down at once per Worker Process
   */
  getRecordsLimitTotal?: number | undefined;
  /**
   * The load-balancing algorithm to use for spreading out shards across Workers and Worker Processes
   */
  loadBalancingAlgorithm?: ShardLoadBalancing | undefined;
  /**
   * AWS authentication method. Choose Auto to use IAM roles.
   */
  awsAuthenticationMethod?: string | undefined;
  awsSecretKey?: string | undefined;
  /**
   * Region where the Kinesis stream is located
   */
  region: string;
  /**
   * Kinesis stream service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Kinesis stream-compatible endpoint.
   */
  endpoint?: string | undefined;
  /**
   * Signature version to use for signing Kinesis stream requests
   */
  signatureVersion?: models.SignatureVersionOptions2 | undefined;
  /**
   * Reuse connections between requests, which can improve performance
   */
  reuseConnections?: boolean | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * Use Assume Role credentials to access Kinesis stream
   */
  enableAssumeRole?: boolean | undefined;
  /**
   * Amazon Resource Name (ARN) of the role to assume
   */
  assumeRoleArn?: string | undefined;
  /**
   * External ID to use when assuming role
   */
  assumeRoleExternalId?: string | undefined;
  /**
   * Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
   */
  durationSeconds?: number | undefined;
  /**
   * Verify Kinesis Producer Library (KPL) event checksums
   */
  verifyKPLCheckSums?: boolean | undefined;
  /**
   * When resuming streaming from a stored state, Stream will read the next available record, rather than rereading the last-read record. Enabling this setting can cause data loss after a Worker Node's unexpected shutdown or restart.
   */
  avoidDuplicates?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  /**
   * Select or create a stored secret that references your access key and secret key
   */
  awsSecret?: string | undefined;
};

export type InputKinesisPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputKinesisType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Kinesis Data Stream to read data from
   */
  streamName: string;
  /**
   * Time interval in minutes between consecutive service calls
   */
  serviceInterval?: number | undefined;
  /**
   * A JavaScript expression to be called with each shardId for the stream. If the expression evaluates to a truthy value, the shard will be processed.
   */
  shardExpr?: string | undefined;
  /**
   * Location at which to start reading a shard for the first time
   */
  shardIteratorType?: ShardIteratorStart | undefined;
  /**
   * Format of data inside the Kinesis Stream records. Gzip compression is automatically detected.
   */
  payloadFormat?: RecordDataFormat | undefined;
  /**
   * Maximum number of records per getRecords call
   */
  getRecordsLimit?: number | undefined;
  /**
   * Maximum number of records, across all shards, to pull down at once per Worker Process
   */
  getRecordsLimitTotal?: number | undefined;
  /**
   * The load-balancing algorithm to use for spreading out shards across Workers and Worker Processes
   */
  loadBalancingAlgorithm?: ShardLoadBalancing | undefined;
  /**
   * AWS authentication method. Choose Auto to use IAM roles.
   */
  awsAuthenticationMethod?: string | undefined;
  awsSecretKey?: string | undefined;
  /**
   * Region where the Kinesis stream is located
   */
  region: string;
  /**
   * Kinesis stream service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Kinesis stream-compatible endpoint.
   */
  endpoint?: string | undefined;
  /**
   * Signature version to use for signing Kinesis stream requests
   */
  signatureVersion?: models.SignatureVersionOptions2 | undefined;
  /**
   * Reuse connections between requests, which can improve performance
   */
  reuseConnections?: boolean | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * Use Assume Role credentials to access Kinesis stream
   */
  enableAssumeRole?: boolean | undefined;
  /**
   * Amazon Resource Name (ARN) of the role to assume
   */
  assumeRoleArn?: string | undefined;
  /**
   * External ID to use when assuming role
   */
  assumeRoleExternalId?: string | undefined;
  /**
   * Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
   */
  durationSeconds?: number | undefined;
  /**
   * Verify Kinesis Producer Library (KPL) event checksums
   */
  verifyKPLCheckSums?: boolean | undefined;
  /**
   * When resuming streaming from a stored state, Stream will read the next available record, rather than rereading the last-read record. Enabling this setting can cause data loss after a Worker Node's unexpected shutdown or restart.
   */
  avoidDuplicates?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  /**
   * Select or create a stored secret that references your access key and secret key
   */
  awsSecret?: string | undefined;
};

export type InputKinesisSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputKinesisType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Kinesis Data Stream to read data from
   */
  streamName: string;
  /**
   * Time interval in minutes between consecutive service calls
   */
  serviceInterval?: number | undefined;
  /**
   * A JavaScript expression to be called with each shardId for the stream. If the expression evaluates to a truthy value, the shard will be processed.
   */
  shardExpr?: string | undefined;
  /**
   * Location at which to start reading a shard for the first time
   */
  shardIteratorType?: ShardIteratorStart | undefined;
  /**
   * Format of data inside the Kinesis Stream records. Gzip compression is automatically detected.
   */
  payloadFormat?: RecordDataFormat | undefined;
  /**
   * Maximum number of records per getRecords call
   */
  getRecordsLimit?: number | undefined;
  /**
   * Maximum number of records, across all shards, to pull down at once per Worker Process
   */
  getRecordsLimitTotal?: number | undefined;
  /**
   * The load-balancing algorithm to use for spreading out shards across Workers and Worker Processes
   */
  loadBalancingAlgorithm?: ShardLoadBalancing | undefined;
  /**
   * AWS authentication method. Choose Auto to use IAM roles.
   */
  awsAuthenticationMethod?: string | undefined;
  awsSecretKey?: string | undefined;
  /**
   * Region where the Kinesis stream is located
   */
  region: string;
  /**
   * Kinesis stream service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Kinesis stream-compatible endpoint.
   */
  endpoint?: string | undefined;
  /**
   * Signature version to use for signing Kinesis stream requests
   */
  signatureVersion?: models.SignatureVersionOptions2 | undefined;
  /**
   * Reuse connections between requests, which can improve performance
   */
  reuseConnections?: boolean | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * Use Assume Role credentials to access Kinesis stream
   */
  enableAssumeRole?: boolean | undefined;
  /**
   * Amazon Resource Name (ARN) of the role to assume
   */
  assumeRoleArn?: string | undefined;
  /**
   * External ID to use when assuming role
   */
  assumeRoleExternalId?: string | undefined;
  /**
   * Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
   */
  durationSeconds?: number | undefined;
  /**
   * Verify Kinesis Producer Library (KPL) event checksums
   */
  verifyKPLCheckSums?: boolean | undefined;
  /**
   * When resuming streaming from a stored state, Stream will read the next available record, rather than rereading the last-read record. Enabling this setting can cause data loss after a Worker Node's unexpected shutdown or restart.
   */
  avoidDuplicates?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  /**
   * Select or create a stored secret that references your access key and secret key
   */
  awsSecret?: string | undefined;
};

export type InputKinesisSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputKinesisType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Kinesis Data Stream to read data from
   */
  streamName: string;
  /**
   * Time interval in minutes between consecutive service calls
   */
  serviceInterval?: number | undefined;
  /**
   * A JavaScript expression to be called with each shardId for the stream. If the expression evaluates to a truthy value, the shard will be processed.
   */
  shardExpr?: string | undefined;
  /**
   * Location at which to start reading a shard for the first time
   */
  shardIteratorType?: ShardIteratorStart | undefined;
  /**
   * Format of data inside the Kinesis Stream records. Gzip compression is automatically detected.
   */
  payloadFormat?: RecordDataFormat | undefined;
  /**
   * Maximum number of records per getRecords call
   */
  getRecordsLimit?: number | undefined;
  /**
   * Maximum number of records, across all shards, to pull down at once per Worker Process
   */
  getRecordsLimitTotal?: number | undefined;
  /**
   * The load-balancing algorithm to use for spreading out shards across Workers and Worker Processes
   */
  loadBalancingAlgorithm?: ShardLoadBalancing | undefined;
  /**
   * AWS authentication method. Choose Auto to use IAM roles.
   */
  awsAuthenticationMethod?: string | undefined;
  awsSecretKey?: string | undefined;
  /**
   * Region where the Kinesis stream is located
   */
  region: string;
  /**
   * Kinesis stream service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Kinesis stream-compatible endpoint.
   */
  endpoint?: string | undefined;
  /**
   * Signature version to use for signing Kinesis stream requests
   */
  signatureVersion?: models.SignatureVersionOptions2 | undefined;
  /**
   * Reuse connections between requests, which can improve performance
   */
  reuseConnections?: boolean | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * Use Assume Role credentials to access Kinesis stream
   */
  enableAssumeRole?: boolean | undefined;
  /**
   * Amazon Resource Name (ARN) of the role to assume
   */
  assumeRoleArn?: string | undefined;
  /**
   * External ID to use when assuming role
   */
  assumeRoleExternalId?: string | undefined;
  /**
   * Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
   */
  durationSeconds?: number | undefined;
  /**
   * Verify Kinesis Producer Library (KPL) event checksums
   */
  verifyKPLCheckSums?: boolean | undefined;
  /**
   * When resuming streaming from a stored state, Stream will read the next available record, rather than rereading the last-read record. Enabling this setting can cause data loss after a Worker Node's unexpected shutdown or restart.
   */
  avoidDuplicates?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  /**
   * Select or create a stored secret that references your access key and secret key
   */
  awsSecret?: string | undefined;
};

export type InputKinesis =
  | InputKinesisSendToRoutesTrueWithConnectionsConstraint
  | InputKinesisSendToRoutesFalseWithConnectionsConstraint
  | InputKinesisPqEnabledFalseWithPqConstraint
  | InputKinesisPqEnabledTrueWithPqConstraint;

export const InputHttpRawType = {
  HttpRaw: "http_raw",
} as const;
export type InputHttpRawType = ClosedEnum<typeof InputHttpRawType>;

export type InputHttpRawPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputHttpRawType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokens?: Array<string> | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * List of URI paths accepted by this input, wildcards are supported, e.g /api/v* /hook. Defaults to allow all.
   */
  allowedPaths?: Array<string> | undefined;
  /**
   * List of HTTP methods accepted by this input. Wildcards are supported (such as P*, GET). Defaults to allow all.
   */
  allowedMethods?: Array<string> | undefined;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokensExt?: Array<models.ItemsTypeAuthTokensExt> | undefined;
  description?: string | undefined;
};

export type InputHttpRawPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputHttpRawType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokens?: Array<string> | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * List of URI paths accepted by this input, wildcards are supported, e.g /api/v* /hook. Defaults to allow all.
   */
  allowedPaths?: Array<string> | undefined;
  /**
   * List of HTTP methods accepted by this input. Wildcards are supported (such as P*, GET). Defaults to allow all.
   */
  allowedMethods?: Array<string> | undefined;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokensExt?: Array<models.ItemsTypeAuthTokensExt> | undefined;
  description?: string | undefined;
};

export type InputHttpRawSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputHttpRawType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokens?: Array<string> | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * List of URI paths accepted by this input, wildcards are supported, e.g /api/v* /hook. Defaults to allow all.
   */
  allowedPaths?: Array<string> | undefined;
  /**
   * List of HTTP methods accepted by this input. Wildcards are supported (such as P*, GET). Defaults to allow all.
   */
  allowedMethods?: Array<string> | undefined;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokensExt?: Array<models.ItemsTypeAuthTokensExt> | undefined;
  description?: string | undefined;
};

export type InputHttpRawSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputHttpRawType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokens?: Array<string> | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * List of URI paths accepted by this input, wildcards are supported, e.g /api/v* /hook. Defaults to allow all.
   */
  allowedPaths?: Array<string> | undefined;
  /**
   * List of HTTP methods accepted by this input. Wildcards are supported (such as P*, GET). Defaults to allow all.
   */
  allowedMethods?: Array<string> | undefined;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokensExt?: Array<models.ItemsTypeAuthTokensExt> | undefined;
  description?: string | undefined;
};

export type InputHttpRaw =
  | InputHttpRawSendToRoutesTrueWithConnectionsConstraint
  | InputHttpRawSendToRoutesFalseWithConnectionsConstraint
  | InputHttpRawPqEnabledFalseWithPqConstraint
  | InputHttpRawPqEnabledTrueWithPqConstraint;

export const InputDatagenType = {
  Datagen: "datagen",
} as const;
export type InputDatagenType = ClosedEnum<typeof InputDatagenType>;

export type Sample = {
  sample: string;
  /**
   * Maximum number of events to generate per second per Worker Node. Defaults to 10.
   */
  eventsPerSec?: number | undefined;
};

export type InputDatagenPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputDatagenType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  samples: Array<Sample>;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputDatagenPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputDatagenType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  samples: Array<Sample>;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputDatagenSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputDatagenType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  samples: Array<Sample>;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputDatagenSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputDatagenType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  samples: Array<Sample>;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputDatagen =
  | InputDatagenSendToRoutesTrueWithConnectionsConstraint
  | InputDatagenSendToRoutesFalseWithConnectionsConstraint
  | InputDatagenPqEnabledFalseWithPqConstraint
  | InputDatagenPqEnabledTrueWithPqConstraint;

export const InputDatadogAgentType = {
  DatadogAgent: "datadog_agent",
} as const;
export type InputDatadogAgentType = ClosedEnum<typeof InputDatadogAgentType>;

export type InputDatadogAgentProxyMode = {
  /**
   * Toggle to Yes to send key validation requests from Datadog Agent to the Datadog API. If toggled to No (the default), Stream handles key validation requests by always responding that the key is valid.
   */
  enabled?: boolean | undefined;
  /**
   * Whether to reject certificates that cannot be verified against a valid CA (e.g., self-signed certificates).
   */
  rejectUnauthorized?: boolean | undefined;
};

export type InputDatadogAgentPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputDatadogAgentType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Toggle to Yes to extract each incoming metric to multiple events, one per data point. This works well when sending metrics to a statsd-type output. If sending metrics to DatadogHQ or any destination that accepts arbitrary JSON, leave toggled to No (the default).
   */
  extractMetrics?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  proxyMode?: InputDatadogAgentProxyMode | undefined;
  description?: string | undefined;
};

export type InputDatadogAgentPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputDatadogAgentType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Toggle to Yes to extract each incoming metric to multiple events, one per data point. This works well when sending metrics to a statsd-type output. If sending metrics to DatadogHQ or any destination that accepts arbitrary JSON, leave toggled to No (the default).
   */
  extractMetrics?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  proxyMode?: InputDatadogAgentProxyMode | undefined;
  description?: string | undefined;
};

export type InputDatadogAgentSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputDatadogAgentType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Toggle to Yes to extract each incoming metric to multiple events, one per data point. This works well when sending metrics to a statsd-type output. If sending metrics to DatadogHQ or any destination that accepts arbitrary JSON, leave toggled to No (the default).
   */
  extractMetrics?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  proxyMode?: InputDatadogAgentProxyMode | undefined;
  description?: string | undefined;
};

export type InputDatadogAgentSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputDatadogAgentType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Toggle to Yes to extract each incoming metric to multiple events, one per data point. This works well when sending metrics to a statsd-type output. If sending metrics to DatadogHQ or any destination that accepts arbitrary JSON, leave toggled to No (the default).
   */
  extractMetrics?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  proxyMode?: InputDatadogAgentProxyMode | undefined;
  description?: string | undefined;
};

export type InputDatadogAgent =
  | InputDatadogAgentSendToRoutesTrueWithConnectionsConstraint
  | InputDatadogAgentSendToRoutesFalseWithConnectionsConstraint
  | InputDatadogAgentPqEnabledFalseWithPqConstraint
  | InputDatadogAgentPqEnabledTrueWithPqConstraint;

export const InputCrowdstrikeType = {
  Crowdstrike: "crowdstrike",
} as const;
export type InputCrowdstrikeType = ClosedEnum<typeof InputCrowdstrikeType>;

export type InputCrowdstrikePqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputCrowdstrikeType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.
   */
  queueName: string;
  /**
   * Regex matching file names to download and process. Defaults to: .*
   */
  fileFilter?: string | undefined;
  /**
   * SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.
   */
  awsAccountId?: string | undefined;
  /**
   * AWS authentication method. Choose Auto to use IAM roles.
   */
  awsAuthenticationMethod?: string | undefined;
  awsSecretKey?: string | undefined;
  /**
   * AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region.
   */
  region?: string | undefined;
  /**
   * S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.
   */
  endpoint?: string | undefined;
  /**
   * Signature version to use for signing S3 requests
   */
  signatureVersion?: models.SignatureVersionOptionsS3CollectorConf | undefined;
  /**
   * Reuse connections between requests, which can improve performance
   */
  reuseConnections?: boolean | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.
   */
  maxMessages?: number | undefined;
  /**
   * After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).
   */
  visibilityTimeout?: number | undefined;
  /**
   * How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.
   */
  numReceivers?: number | undefined;
  /**
   * Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure.
   */
  socketTimeout?: number | undefined;
  /**
   * Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.
   */
  skipOnError?: boolean | undefined;
  /**
   * Attach SQS notification metadata to a __sqsMetadata field on each event
   */
  includeSqsMetadata?: boolean | undefined;
  /**
   * Use Assume Role credentials to access Amazon S3
   */
  enableAssumeRole?: boolean | undefined;
  /**
   * Amazon Resource Name (ARN) of the role to assume
   */
  assumeRoleArn?: string | undefined;
  /**
   * External ID to use when assuming role
   */
  assumeRoleExternalId?: string | undefined;
  /**
   * Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
   */
  durationSeconds?: number | undefined;
  /**
   * Use Assume Role credentials when accessing Amazon SQS
   */
  enableSQSAssumeRole?: boolean | undefined;
  preprocess?: models.PreprocessTypeSavedJobCollectionInput | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  checkpointing?: models.CheckpointingType | undefined;
  /**
   * How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.
   */
  pollTimeout?: number | undefined;
  /**
   * Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters.
   */
  encoding?: string | undefined;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  /**
   * Select or create a stored secret that references your access key and secret key
   */
  awsSecret?: string | undefined;
  tagAfterProcessing?: models.TagAfterProcessingOptions | undefined;
  /**
   * The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.
   */
  processedTagKey?: string | undefined;
  /**
   * The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.
   */
  processedTagValue?: string | undefined;
};

export type InputCrowdstrikePqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputCrowdstrikeType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.
   */
  queueName: string;
  /**
   * Regex matching file names to download and process. Defaults to: .*
   */
  fileFilter?: string | undefined;
  /**
   * SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.
   */
  awsAccountId?: string | undefined;
  /**
   * AWS authentication method. Choose Auto to use IAM roles.
   */
  awsAuthenticationMethod?: string | undefined;
  awsSecretKey?: string | undefined;
  /**
   * AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region.
   */
  region?: string | undefined;
  /**
   * S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.
   */
  endpoint?: string | undefined;
  /**
   * Signature version to use for signing S3 requests
   */
  signatureVersion?: models.SignatureVersionOptionsS3CollectorConf | undefined;
  /**
   * Reuse connections between requests, which can improve performance
   */
  reuseConnections?: boolean | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.
   */
  maxMessages?: number | undefined;
  /**
   * After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).
   */
  visibilityTimeout?: number | undefined;
  /**
   * How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.
   */
  numReceivers?: number | undefined;
  /**
   * Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure.
   */
  socketTimeout?: number | undefined;
  /**
   * Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.
   */
  skipOnError?: boolean | undefined;
  /**
   * Attach SQS notification metadata to a __sqsMetadata field on each event
   */
  includeSqsMetadata?: boolean | undefined;
  /**
   * Use Assume Role credentials to access Amazon S3
   */
  enableAssumeRole?: boolean | undefined;
  /**
   * Amazon Resource Name (ARN) of the role to assume
   */
  assumeRoleArn?: string | undefined;
  /**
   * External ID to use when assuming role
   */
  assumeRoleExternalId?: string | undefined;
  /**
   * Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
   */
  durationSeconds?: number | undefined;
  /**
   * Use Assume Role credentials when accessing Amazon SQS
   */
  enableSQSAssumeRole?: boolean | undefined;
  preprocess?: models.PreprocessTypeSavedJobCollectionInput | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  checkpointing?: models.CheckpointingType | undefined;
  /**
   * How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.
   */
  pollTimeout?: number | undefined;
  /**
   * Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters.
   */
  encoding?: string | undefined;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  /**
   * Select or create a stored secret that references your access key and secret key
   */
  awsSecret?: string | undefined;
  tagAfterProcessing?: models.TagAfterProcessingOptions | undefined;
  /**
   * The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.
   */
  processedTagKey?: string | undefined;
  /**
   * The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.
   */
  processedTagValue?: string | undefined;
};

export type InputCrowdstrikeSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputCrowdstrikeType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.
   */
  queueName: string;
  /**
   * Regex matching file names to download and process. Defaults to: .*
   */
  fileFilter?: string | undefined;
  /**
   * SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.
   */
  awsAccountId?: string | undefined;
  /**
   * AWS authentication method. Choose Auto to use IAM roles.
   */
  awsAuthenticationMethod?: string | undefined;
  awsSecretKey?: string | undefined;
  /**
   * AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region.
   */
  region?: string | undefined;
  /**
   * S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.
   */
  endpoint?: string | undefined;
  /**
   * Signature version to use for signing S3 requests
   */
  signatureVersion?: models.SignatureVersionOptionsS3CollectorConf | undefined;
  /**
   * Reuse connections between requests, which can improve performance
   */
  reuseConnections?: boolean | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.
   */
  maxMessages?: number | undefined;
  /**
   * After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).
   */
  visibilityTimeout?: number | undefined;
  /**
   * How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.
   */
  numReceivers?: number | undefined;
  /**
   * Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure.
   */
  socketTimeout?: number | undefined;
  /**
   * Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.
   */
  skipOnError?: boolean | undefined;
  /**
   * Attach SQS notification metadata to a __sqsMetadata field on each event
   */
  includeSqsMetadata?: boolean | undefined;
  /**
   * Use Assume Role credentials to access Amazon S3
   */
  enableAssumeRole?: boolean | undefined;
  /**
   * Amazon Resource Name (ARN) of the role to assume
   */
  assumeRoleArn?: string | undefined;
  /**
   * External ID to use when assuming role
   */
  assumeRoleExternalId?: string | undefined;
  /**
   * Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
   */
  durationSeconds?: number | undefined;
  /**
   * Use Assume Role credentials when accessing Amazon SQS
   */
  enableSQSAssumeRole?: boolean | undefined;
  preprocess?: models.PreprocessTypeSavedJobCollectionInput | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  checkpointing?: models.CheckpointingType | undefined;
  /**
   * How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.
   */
  pollTimeout?: number | undefined;
  /**
   * Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters.
   */
  encoding?: string | undefined;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  /**
   * Select or create a stored secret that references your access key and secret key
   */
  awsSecret?: string | undefined;
  tagAfterProcessing?: models.TagAfterProcessingOptions | undefined;
  /**
   * The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.
   */
  processedTagKey?: string | undefined;
  /**
   * The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.
   */
  processedTagValue?: string | undefined;
};

export type InputCrowdstrikeSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputCrowdstrikeType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * The name, URL, or ARN of the SQS queue to read notifications from. When a non-AWS URL is specified, format must be: '{url}/myQueueName'. Example: 'https://host:port/myQueueName'. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at init time. Example referencing a Global Variable: `https://host:port/myQueue-${C.vars.myVar}`.
   */
  queueName: string;
  /**
   * Regex matching file names to download and process. Defaults to: .*
   */
  fileFilter?: string | undefined;
  /**
   * SQS queue owner's AWS account ID. Leave empty if SQS queue is in same AWS account.
   */
  awsAccountId?: string | undefined;
  /**
   * AWS authentication method. Choose Auto to use IAM roles.
   */
  awsAuthenticationMethod?: string | undefined;
  awsSecretKey?: string | undefined;
  /**
   * AWS Region where the S3 bucket and SQS queue are located. Required, unless the Queue entry is a URL or ARN that includes a Region.
   */
  region?: string | undefined;
  /**
   * S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.
   */
  endpoint?: string | undefined;
  /**
   * Signature version to use for signing S3 requests
   */
  signatureVersion?: models.SignatureVersionOptionsS3CollectorConf | undefined;
  /**
   * Reuse connections between requests, which can improve performance
   */
  reuseConnections?: boolean | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * The maximum number of messages SQS should return in a poll request. Amazon SQS never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 10.
   */
  maxMessages?: number | undefined;
  /**
   * After messages are retrieved by a ReceiveMessage request, @{product} will hide them from subsequent retrieve requests for at least this duration. You can set this as high as 43200 sec. (12 hours).
   */
  visibilityTimeout?: number | undefined;
  /**
   * How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.
   */
  numReceivers?: number | undefined;
  /**
   * Socket inactivity timeout (in seconds). Increase this value if timeouts occur due to backpressure.
   */
  socketTimeout?: number | undefined;
  /**
   * Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.
   */
  skipOnError?: boolean | undefined;
  /**
   * Attach SQS notification metadata to a __sqsMetadata field on each event
   */
  includeSqsMetadata?: boolean | undefined;
  /**
   * Use Assume Role credentials to access Amazon S3
   */
  enableAssumeRole?: boolean | undefined;
  /**
   * Amazon Resource Name (ARN) of the role to assume
   */
  assumeRoleArn?: string | undefined;
  /**
   * External ID to use when assuming role
   */
  assumeRoleExternalId?: string | undefined;
  /**
   * Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
   */
  durationSeconds?: number | undefined;
  /**
   * Use Assume Role credentials when accessing Amazon SQS
   */
  enableSQSAssumeRole?: boolean | undefined;
  preprocess?: models.PreprocessTypeSavedJobCollectionInput | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  checkpointing?: models.CheckpointingType | undefined;
  /**
   * How long to wait for events before trying polling again. The lower the number the higher the AWS bill. The higher the number the longer it will take for the source to react to configuration changes and system restarts.
   */
  pollTimeout?: number | undefined;
  /**
   * Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters.
   */
  encoding?: string | undefined;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  /**
   * Select or create a stored secret that references your access key and secret key
   */
  awsSecret?: string | undefined;
  tagAfterProcessing?: models.TagAfterProcessingOptions | undefined;
  /**
   * The key for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.
   */
  processedTagKey?: string | undefined;
  /**
   * The value for the S3 object tag applied after processing. This field accepts an expression for dynamic generation.
   */
  processedTagValue?: string | undefined;
};

export type InputCrowdstrike =
  | InputCrowdstrikeSendToRoutesTrueWithConnectionsConstraint
  | InputCrowdstrikeSendToRoutesFalseWithConnectionsConstraint
  | InputCrowdstrikePqEnabledFalseWithPqConstraint
  | InputCrowdstrikePqEnabledTrueWithPqConstraint;

export const InputWindowsMetricsType = {
  WindowsMetrics: "windows_metrics",
} as const;
export type InputWindowsMetricsType = ClosedEnum<
  typeof InputWindowsMetricsType
>;

/**
 * Select the level of details for system metrics
 */
export const InputWindowsMetricsSystemMode = {
  /**
   * Basic
   */
  Basic: "basic",
  /**
   * All
   */
  All: "all",
  /**
   * Custom
   */
  Custom: "custom",
  /**
   * Disabled
   */
  Disabled: "disabled",
} as const;
/**
 * Select the level of details for system metrics
 */
export type InputWindowsMetricsSystemMode = OpenEnum<
  typeof InputWindowsMetricsSystemMode
>;

export type InputWindowsMetricsSystem = {
  /**
   * Select the level of details for system metrics
   */
  mode?: InputWindowsMetricsSystemMode | undefined;
  /**
   * Generate metrics for all system information
   */
  detail?: boolean | undefined;
};

/**
 * Select the level of details for CPU metrics
 */
export const InputWindowsMetricsCpuMode = {
  /**
   * Basic
   */
  Basic: "basic",
  /**
   * All
   */
  All: "all",
  /**
   * Custom
   */
  Custom: "custom",
  /**
   * Disabled
   */
  Disabled: "disabled",
} as const;
/**
 * Select the level of details for CPU metrics
 */
export type InputWindowsMetricsCpuMode = OpenEnum<
  typeof InputWindowsMetricsCpuMode
>;

export type InputWindowsMetricsCpu = {
  /**
   * Select the level of details for CPU metrics
   */
  mode?: InputWindowsMetricsCpuMode | undefined;
  /**
   * Generate metrics for each CPU
   */
  perCpu?: boolean | undefined;
  /**
   * Generate metrics for all CPU states
   */
  detail?: boolean | undefined;
  /**
   * Generate raw, monotonic CPU time counters
   */
  time?: boolean | undefined;
};

/**
 * Select the level of details for memory metrics
 */
export const InputWindowsMetricsMemoryMode = {
  /**
   * Basic
   */
  Basic: "basic",
  /**
   * All
   */
  All: "all",
  /**
   * Custom
   */
  Custom: "custom",
  /**
   * Disabled
   */
  Disabled: "disabled",
} as const;
/**
 * Select the level of details for memory metrics
 */
export type InputWindowsMetricsMemoryMode = OpenEnum<
  typeof InputWindowsMetricsMemoryMode
>;

export type InputWindowsMetricsMemory = {
  /**
   * Select the level of details for memory metrics
   */
  mode?: InputWindowsMetricsMemoryMode | undefined;
  /**
   * Generate metrics for all memory states
   */
  detail?: boolean | undefined;
};

/**
 * Select the level of details for network metrics
 */
export const InputWindowsMetricsNetworkMode = {
  /**
   * Basic
   */
  Basic: "basic",
  /**
   * All
   */
  All: "all",
  /**
   * Custom
   */
  Custom: "custom",
  /**
   * Disabled
   */
  Disabled: "disabled",
} as const;
/**
 * Select the level of details for network metrics
 */
export type InputWindowsMetricsNetworkMode = OpenEnum<
  typeof InputWindowsMetricsNetworkMode
>;

export type InputWindowsMetricsNetwork = {
  /**
   * Select the level of details for network metrics
   */
  mode?: InputWindowsMetricsNetworkMode | undefined;
  /**
   * Generate full network metrics
   */
  detail?: boolean | undefined;
  /**
   * Generate protocol metrics for ICMP, ICMPMsg, IP, TCP, UDP and UDPLite
   */
  protocols?: boolean | undefined;
  /**
   * Network interfaces to include/exclude. All interfaces are included if this list is empty.
   */
  devices?: Array<string> | undefined;
  /**
   * Generate separate metrics for each interface
   */
  perInterface?: boolean | undefined;
};

/**
 * Select the level of details for disk metrics
 */
export const InputWindowsMetricsDiskMode = {
  /**
   * Basic
   */
  Basic: "basic",
  /**
   * All
   */
  All: "all",
  /**
   * Custom
   */
  Custom: "custom",
  /**
   * Disabled
   */
  Disabled: "disabled",
} as const;
/**
 * Select the level of details for disk metrics
 */
export type InputWindowsMetricsDiskMode = OpenEnum<
  typeof InputWindowsMetricsDiskMode
>;

export type InputWindowsMetricsDisk = {
  /**
   * Select the level of details for disk metrics
   */
  mode?: InputWindowsMetricsDiskMode | undefined;
  /**
   * Generate separate metrics for each volume
   */
  perVolume?: boolean | undefined;
  /**
   * Generate full disk metrics
   */
  detail?: boolean | undefined;
  /**
   * Windows volumes to include/exclude. E.g.: C:, !E:, etc. Wildcards and ! (not) operators are supported. All volumes are included if this list is empty.
   */
  volumes?: Array<string> | undefined;
};

export type InputWindowsMetricsCustom = {
  system?: InputWindowsMetricsSystem | undefined;
  cpu?: InputWindowsMetricsCpu | undefined;
  memory?: InputWindowsMetricsMemory | undefined;
  network?: InputWindowsMetricsNetwork | undefined;
  disk?: InputWindowsMetricsDisk | undefined;
};

export type InputWindowsMetricsHost = {
  /**
   * Select level of detail for host metrics
   */
  mode?: models.ModeOptionsHost | undefined;
  custom?: InputWindowsMetricsCustom | undefined;
};

export type InputWindowsMetricsPersistence = {
  /**
   * Spool metrics to disk for Cribl Edge and Search
   */
  enable?: boolean | undefined;
  /**
   * Time span for each file bucket
   */
  timeWindow?: string | undefined;
  /**
   * Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.
   */
  maxDataSize?: string | undefined;
  /**
   * Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.
   */
  maxDataTime?: string | undefined;
  compress?: models.DataCompressionFormatOptionsPersistence | undefined;
  /**
   * Path to use to write metrics. Defaults to $CRIBL_HOME/state/windows_metrics
   */
  destPath?: string | undefined;
};

export type InputWindowsMetricsPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputWindowsMetricsType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Time, in seconds, between consecutive metric collections. Default is 10 seconds.
   */
  interval?: number | undefined;
  host?: InputWindowsMetricsHost | undefined;
  process?: models.ProcessType | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  persistence?: InputWindowsMetricsPersistence | undefined;
  /**
   * Enable to use built-in tools (PowerShell) to collect metrics instead of native API (default) [Learn more](https://docs.cribl.io/edge/sources-windows-metrics/#advanced-tab)
   */
  disableNativeModule?: boolean | undefined;
  description?: string | undefined;
};

export type InputWindowsMetricsPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputWindowsMetricsType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Time, in seconds, between consecutive metric collections. Default is 10 seconds.
   */
  interval?: number | undefined;
  host?: InputWindowsMetricsHost | undefined;
  process?: models.ProcessType | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  persistence?: InputWindowsMetricsPersistence | undefined;
  /**
   * Enable to use built-in tools (PowerShell) to collect metrics instead of native API (default) [Learn more](https://docs.cribl.io/edge/sources-windows-metrics/#advanced-tab)
   */
  disableNativeModule?: boolean | undefined;
  description?: string | undefined;
};

export type InputWindowsMetricsSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputWindowsMetricsType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Time, in seconds, between consecutive metric collections. Default is 10 seconds.
   */
  interval?: number | undefined;
  host?: InputWindowsMetricsHost | undefined;
  process?: models.ProcessType | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  persistence?: InputWindowsMetricsPersistence | undefined;
  /**
   * Enable to use built-in tools (PowerShell) to collect metrics instead of native API (default) [Learn more](https://docs.cribl.io/edge/sources-windows-metrics/#advanced-tab)
   */
  disableNativeModule?: boolean | undefined;
  description?: string | undefined;
};

export type InputWindowsMetricsSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputWindowsMetricsType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Time, in seconds, between consecutive metric collections. Default is 10 seconds.
   */
  interval?: number | undefined;
  host?: InputWindowsMetricsHost | undefined;
  process?: models.ProcessType | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  persistence?: InputWindowsMetricsPersistence | undefined;
  /**
   * Enable to use built-in tools (PowerShell) to collect metrics instead of native API (default) [Learn more](https://docs.cribl.io/edge/sources-windows-metrics/#advanced-tab)
   */
  disableNativeModule?: boolean | undefined;
  description?: string | undefined;
};

export type InputWindowsMetrics =
  | InputWindowsMetricsSendToRoutesTrueWithConnectionsConstraint
  | InputWindowsMetricsSendToRoutesFalseWithConnectionsConstraint
  | InputWindowsMetricsPqEnabledFalseWithPqConstraint
  | InputWindowsMetricsPqEnabledTrueWithPqConstraint;

export const InputKubeEventsType = {
  KubeEvents: "kube_events",
} as const;
export type InputKubeEventsType = ClosedEnum<typeof InputKubeEventsType>;

export type InputKubeEventsPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputKubeEventsType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Filtering on event fields
   */
  rules?: Array<models.ItemsTypeRules> | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputKubeEventsPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputKubeEventsType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Filtering on event fields
   */
  rules?: Array<models.ItemsTypeRules> | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputKubeEventsSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputKubeEventsType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Filtering on event fields
   */
  rules?: Array<models.ItemsTypeRules> | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputKubeEventsSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputKubeEventsType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Filtering on event fields
   */
  rules?: Array<models.ItemsTypeRules> | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputKubeEvents =
  | InputKubeEventsSendToRoutesTrueWithConnectionsConstraint
  | InputKubeEventsSendToRoutesFalseWithConnectionsConstraint
  | InputKubeEventsPqEnabledFalseWithPqConstraint
  | InputKubeEventsPqEnabledTrueWithPqConstraint;

export const InputKubeLogsType = {
  KubeLogs: "kube_logs",
} as const;
export type InputKubeLogsType = ClosedEnum<typeof InputKubeLogsType>;

export type InputKubeLogsRule = {
  /**
   * JavaScript expression applied to Pod objects. Return 'true' to include it.
   */
  filter: string;
  /**
   * Optional description of this rule's purpose
   */
  description?: string | undefined;
};

export type InputKubeLogsPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputKubeLogsType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Time, in seconds, between checks for new containers. Default is 15 secs.
   */
  interval?: number | undefined;
  /**
   * Add rules to decide which Pods to collect logs from. Logs are collected if no rules are given or if all the rules' expressions evaluate to true.
   */
  rules?: Array<InputKubeLogsRule> | undefined;
  /**
   * For use when containers do not emit a timestamp, prefix each line of output with a timestamp. If you enable this setting, you can use the Kubernetes Logs Event Breaker and the kubernetes_logs Pre-processing Pipeline to remove them from the events after the timestamps are extracted.
   */
  timestamps?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  persistence?: models.DiskSpoolingType | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * Load balance traffic across all Worker Processes
   */
  enableLoadBalancing?: boolean | undefined;
  description?: string | undefined;
};

export type InputKubeLogsPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputKubeLogsType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Time, in seconds, between checks for new containers. Default is 15 secs.
   */
  interval?: number | undefined;
  /**
   * Add rules to decide which Pods to collect logs from. Logs are collected if no rules are given or if all the rules' expressions evaluate to true.
   */
  rules?: Array<InputKubeLogsRule> | undefined;
  /**
   * For use when containers do not emit a timestamp, prefix each line of output with a timestamp. If you enable this setting, you can use the Kubernetes Logs Event Breaker and the kubernetes_logs Pre-processing Pipeline to remove them from the events after the timestamps are extracted.
   */
  timestamps?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  persistence?: models.DiskSpoolingType | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * Load balance traffic across all Worker Processes
   */
  enableLoadBalancing?: boolean | undefined;
  description?: string | undefined;
};

export type InputKubeLogsSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputKubeLogsType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Time, in seconds, between checks for new containers. Default is 15 secs.
   */
  interval?: number | undefined;
  /**
   * Add rules to decide which Pods to collect logs from. Logs are collected if no rules are given or if all the rules' expressions evaluate to true.
   */
  rules?: Array<InputKubeLogsRule> | undefined;
  /**
   * For use when containers do not emit a timestamp, prefix each line of output with a timestamp. If you enable this setting, you can use the Kubernetes Logs Event Breaker and the kubernetes_logs Pre-processing Pipeline to remove them from the events after the timestamps are extracted.
   */
  timestamps?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  persistence?: models.DiskSpoolingType | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * Load balance traffic across all Worker Processes
   */
  enableLoadBalancing?: boolean | undefined;
  description?: string | undefined;
};

export type InputKubeLogsSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputKubeLogsType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Time, in seconds, between checks for new containers. Default is 15 secs.
   */
  interval?: number | undefined;
  /**
   * Add rules to decide which Pods to collect logs from. Logs are collected if no rules are given or if all the rules' expressions evaluate to true.
   */
  rules?: Array<InputKubeLogsRule> | undefined;
  /**
   * For use when containers do not emit a timestamp, prefix each line of output with a timestamp. If you enable this setting, you can use the Kubernetes Logs Event Breaker and the kubernetes_logs Pre-processing Pipeline to remove them from the events after the timestamps are extracted.
   */
  timestamps?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  persistence?: models.DiskSpoolingType | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * Load balance traffic across all Worker Processes
   */
  enableLoadBalancing?: boolean | undefined;
  description?: string | undefined;
};

export type InputKubeLogs =
  | InputKubeLogsSendToRoutesTrueWithConnectionsConstraint
  | InputKubeLogsSendToRoutesFalseWithConnectionsConstraint
  | InputKubeLogsPqEnabledFalseWithPqConstraint
  | InputKubeLogsPqEnabledTrueWithPqConstraint;

export const InputKubeMetricsType = {
  KubeMetrics: "kube_metrics",
} as const;
export type InputKubeMetricsType = ClosedEnum<typeof InputKubeMetricsType>;

export type InputKubeMetricsPersistence = {
  /**
   * Spool metrics on disk for Cribl Search
   */
  enable?: boolean | undefined;
  /**
   * Time span for each file bucket
   */
  timeWindow?: string | undefined;
  /**
   * Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.
   */
  maxDataSize?: string | undefined;
  /**
   * Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.
   */
  maxDataTime?: string | undefined;
  compress?: models.DataCompressionFormatOptionsPersistence | undefined;
  /**
   * Path to use to write metrics. Defaults to $CRIBL_HOME/state/<id>
   */
  destPath?: string | undefined;
};

export type InputKubeMetricsPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputKubeMetricsType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Time, in seconds, between consecutive metrics collections. Default is 15 secs.
   */
  interval?: number | undefined;
  /**
   * Add rules to decide which Kubernetes objects to generate metrics for. Events are generated if no rules are given or of all the rules' expressions evaluate to true.
   */
  rules?: Array<models.ItemsTypeRules> | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  persistence?: InputKubeMetricsPersistence | undefined;
  description?: string | undefined;
};

export type InputKubeMetricsPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputKubeMetricsType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Time, in seconds, between consecutive metrics collections. Default is 15 secs.
   */
  interval?: number | undefined;
  /**
   * Add rules to decide which Kubernetes objects to generate metrics for. Events are generated if no rules are given or of all the rules' expressions evaluate to true.
   */
  rules?: Array<models.ItemsTypeRules> | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  persistence?: InputKubeMetricsPersistence | undefined;
  description?: string | undefined;
};

export type InputKubeMetricsSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputKubeMetricsType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Time, in seconds, between consecutive metrics collections. Default is 15 secs.
   */
  interval?: number | undefined;
  /**
   * Add rules to decide which Kubernetes objects to generate metrics for. Events are generated if no rules are given or of all the rules' expressions evaluate to true.
   */
  rules?: Array<models.ItemsTypeRules> | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  persistence?: InputKubeMetricsPersistence | undefined;
  description?: string | undefined;
};

export type InputKubeMetricsSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputKubeMetricsType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Time, in seconds, between consecutive metrics collections. Default is 15 secs.
   */
  interval?: number | undefined;
  /**
   * Add rules to decide which Kubernetes objects to generate metrics for. Events are generated if no rules are given or of all the rules' expressions evaluate to true.
   */
  rules?: Array<models.ItemsTypeRules> | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  persistence?: InputKubeMetricsPersistence | undefined;
  description?: string | undefined;
};

export type InputKubeMetrics =
  | InputKubeMetricsSendToRoutesTrueWithConnectionsConstraint
  | InputKubeMetricsSendToRoutesFalseWithConnectionsConstraint
  | InputKubeMetricsPqEnabledFalseWithPqConstraint
  | InputKubeMetricsPqEnabledTrueWithPqConstraint;

export const InputSystemStateType = {
  SystemState: "system_state",
} as const;
export type InputSystemStateType = ClosedEnum<typeof InputSystemStateType>;

/**
 * Creates events based on entries collected from the hosts file
 */
export type HostsFile = {
  enable?: boolean | undefined;
};

/**
 * Creates events for each of the hosts network interfaces
 */
export type Interfaces = {
  enable?: boolean | undefined;
};

/**
 * Creates events for physical disks, partitions, and file systems
 */
export type DisksAndFileSystems = {
  enable?: boolean | undefined;
};

/**
 * Creates events based on the host systems current state
 */
export type HostInfo = {
  enable?: boolean | undefined;
};

/**
 * Creates events based on entries collected from the hosts network routes
 */
export type Routes = {
  enable?: boolean | undefined;
};

/**
 * Creates events for DNS resolvers and search entries
 */
export type Dns = {
  enable?: boolean | undefined;
};

/**
 * Creates events for local users and groups
 */
export type UsersAndGroups = {
  enable?: boolean | undefined;
};

/**
 * Creates events for Firewall rules entries
 */
export type Firewall = {
  enable?: boolean | undefined;
};

/**
 * Creates events from the list of services
 */
export type Services = {
  enable?: boolean | undefined;
};

/**
 * Creates events from list of listening ports
 */
export type ListeningPorts = {
  enable?: boolean | undefined;
};

/**
 * Creates events from list of logged-in users
 */
export type LoggedInUsers = {
  enable?: boolean | undefined;
};

export type Collectors = {
  /**
   * Creates events based on entries collected from the hosts file
   */
  hostsfile?: HostsFile | undefined;
  /**
   * Creates events for each of the hosts network interfaces
   */
  interfaces?: Interfaces | undefined;
  /**
   * Creates events for physical disks, partitions, and file systems
   */
  disk?: DisksAndFileSystems | undefined;
  /**
   * Creates events based on the host systems current state
   */
  metadata?: HostInfo | undefined;
  /**
   * Creates events based on entries collected from the hosts network routes
   */
  routes?: Routes | undefined;
  /**
   * Creates events for DNS resolvers and search entries
   */
  dns?: Dns | undefined;
  /**
   * Creates events for local users and groups
   */
  user?: UsersAndGroups | undefined;
  /**
   * Creates events for Firewall rules entries
   */
  firewall?: Firewall | undefined;
  /**
   * Creates events from the list of services
   */
  services?: Services | undefined;
  /**
   * Creates events from list of listening ports
   */
  ports?: ListeningPorts | undefined;
  /**
   * Creates events from list of logged-in users
   */
  loginUsers?: LoggedInUsers | undefined;
};

export const DataCompressionFormat = {
  None: "none",
  Gzip: "gzip",
} as const;
export type DataCompressionFormat = OpenEnum<typeof DataCompressionFormat>;

export type InputSystemStatePersistence = {
  /**
   * Spool metrics to disk for Cribl Edge and Search
   */
  enable?: boolean | undefined;
  /**
   * Time span for each file bucket
   */
  timeWindow?: string | undefined;
  /**
   * Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.
   */
  maxDataSize?: string | undefined;
  /**
   * Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.
   */
  maxDataTime?: string | undefined;
  compress?: DataCompressionFormat | undefined;
  /**
   * Path to use to write metrics. Defaults to $CRIBL_HOME/state/system_state
   */
  destPath?: string | undefined;
};

export type InputSystemStatePqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputSystemStateType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Time, in seconds, between consecutive state collections. Default is 300 seconds (5 minutes).
   */
  interval?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  collectors?: Collectors | undefined;
  persistence?: InputSystemStatePersistence | undefined;
  /**
   * Enable to use built-in tools (PowerShell) to collect events instead of native API (default) [Learn more](https://docs.cribl.io/edge/sources-system-state/#advanced-tab)
   */
  disableNativeModule?: boolean | undefined;
  description?: string | undefined;
};

export type InputSystemStatePqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputSystemStateType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Time, in seconds, between consecutive state collections. Default is 300 seconds (5 minutes).
   */
  interval?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  collectors?: Collectors | undefined;
  persistence?: InputSystemStatePersistence | undefined;
  /**
   * Enable to use built-in tools (PowerShell) to collect events instead of native API (default) [Learn more](https://docs.cribl.io/edge/sources-system-state/#advanced-tab)
   */
  disableNativeModule?: boolean | undefined;
  description?: string | undefined;
};

export type InputSystemStateSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputSystemStateType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Time, in seconds, between consecutive state collections. Default is 300 seconds (5 minutes).
   */
  interval?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  collectors?: Collectors | undefined;
  persistence?: InputSystemStatePersistence | undefined;
  /**
   * Enable to use built-in tools (PowerShell) to collect events instead of native API (default) [Learn more](https://docs.cribl.io/edge/sources-system-state/#advanced-tab)
   */
  disableNativeModule?: boolean | undefined;
  description?: string | undefined;
};

export type InputSystemStateSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputSystemStateType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Time, in seconds, between consecutive state collections. Default is 300 seconds (5 minutes).
   */
  interval?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  collectors?: Collectors | undefined;
  persistence?: InputSystemStatePersistence | undefined;
  /**
   * Enable to use built-in tools (PowerShell) to collect events instead of native API (default) [Learn more](https://docs.cribl.io/edge/sources-system-state/#advanced-tab)
   */
  disableNativeModule?: boolean | undefined;
  description?: string | undefined;
};

export type InputSystemState =
  | InputSystemStateSendToRoutesTrueWithConnectionsConstraint
  | InputSystemStateSendToRoutesFalseWithConnectionsConstraint
  | InputSystemStatePqEnabledFalseWithPqConstraint
  | InputSystemStatePqEnabledTrueWithPqConstraint;

export const InputSystemMetricsType = {
  SystemMetrics: "system_metrics",
} as const;
export type InputSystemMetricsType = ClosedEnum<typeof InputSystemMetricsType>;

/**
 * Select the level of detail for system metrics
 */
export const InputSystemMetricsSystemMode = {
  /**
   * Basic
   */
  Basic: "basic",
  /**
   * All
   */
  All: "all",
  /**
   * Custom
   */
  Custom: "custom",
  /**
   * Disabled
   */
  Disabled: "disabled",
} as const;
/**
 * Select the level of detail for system metrics
 */
export type InputSystemMetricsSystemMode = OpenEnum<
  typeof InputSystemMetricsSystemMode
>;

export type InputSystemMetricsSystem = {
  /**
   * Select the level of detail for system metrics
   */
  mode?: InputSystemMetricsSystemMode | undefined;
  /**
   * Generate metrics for the numbers of processes in various states
   */
  processes?: boolean | undefined;
};

/**
 * Select the level of detail for CPU metrics
 */
export const InputSystemMetricsCpuMode = {
  /**
   * Basic
   */
  Basic: "basic",
  /**
   * All
   */
  All: "all",
  /**
   * Custom
   */
  Custom: "custom",
  /**
   * Disabled
   */
  Disabled: "disabled",
} as const;
/**
 * Select the level of detail for CPU metrics
 */
export type InputSystemMetricsCpuMode = OpenEnum<
  typeof InputSystemMetricsCpuMode
>;

export type InputSystemMetricsCpu = {
  /**
   * Select the level of detail for CPU metrics
   */
  mode?: InputSystemMetricsCpuMode | undefined;
  /**
   * Generate metrics for each CPU
   */
  perCpu?: boolean | undefined;
  /**
   * Generate metrics for all CPU states
   */
  detail?: boolean | undefined;
  /**
   * Generate raw, monotonic CPU time counters
   */
  time?: boolean | undefined;
};

/**
 * Select the level of detail for memory metrics
 */
export const InputSystemMetricsMemoryMode = {
  /**
   * Basic
   */
  Basic: "basic",
  /**
   * All
   */
  All: "all",
  /**
   * Custom
   */
  Custom: "custom",
  /**
   * Disabled
   */
  Disabled: "disabled",
} as const;
/**
 * Select the level of detail for memory metrics
 */
export type InputSystemMetricsMemoryMode = OpenEnum<
  typeof InputSystemMetricsMemoryMode
>;

export type InputSystemMetricsMemory = {
  /**
   * Select the level of detail for memory metrics
   */
  mode?: InputSystemMetricsMemoryMode | undefined;
  /**
   * Generate metrics for all memory states
   */
  detail?: boolean | undefined;
};

/**
 * Select the level of detail for network metrics
 */
export const InputSystemMetricsNetworkMode = {
  /**
   * Basic
   */
  Basic: "basic",
  /**
   * All
   */
  All: "all",
  /**
   * Custom
   */
  Custom: "custom",
  /**
   * Disabled
   */
  Disabled: "disabled",
} as const;
/**
 * Select the level of detail for network metrics
 */
export type InputSystemMetricsNetworkMode = OpenEnum<
  typeof InputSystemMetricsNetworkMode
>;

export type InputSystemMetricsNetwork = {
  /**
   * Select the level of detail for network metrics
   */
  mode?: InputSystemMetricsNetworkMode | undefined;
  /**
   * Generate full network metrics
   */
  detail?: boolean | undefined;
  /**
   * Generate protocol metrics for ICMP, ICMPMsg, IP, TCP, UDP and UDPLite
   */
  protocols?: boolean | undefined;
  /**
   * Network interfaces to include/exclude. Examples: eth0, !lo. All interfaces are included if this list is empty.
   */
  devices?: Array<string> | undefined;
  /**
   * Generate separate metrics for each interface
   */
  perInterface?: boolean | undefined;
};

/**
 * Select the level of detail for disk metrics
 */
export const InputSystemMetricsDiskMode = {
  /**
   * Basic
   */
  Basic: "basic",
  /**
   * All
   */
  All: "all",
  /**
   * Custom
   */
  Custom: "custom",
  /**
   * Disabled
   */
  Disabled: "disabled",
} as const;
/**
 * Select the level of detail for disk metrics
 */
export type InputSystemMetricsDiskMode = OpenEnum<
  typeof InputSystemMetricsDiskMode
>;

export type InputSystemMetricsDisk = {
  /**
   * Select the level of detail for disk metrics
   */
  mode?: InputSystemMetricsDiskMode | undefined;
  /**
   * Generate full disk metrics
   */
  detail?: boolean | undefined;
  /**
   * Generate filesystem inode metrics
   */
  inodes?: boolean | undefined;
  /**
   * Block devices to include/exclude. Examples: sda*, !loop*. Wildcards and ! (not) operators are supported. All devices are included if this list is empty.
   */
  devices?: Array<string> | undefined;
  /**
   * Filesystem mountpoints to include/exclude. Examples: /, /home, !/proc*, !/tmp. Wildcards and ! (not) operators are supported. All mountpoints are included if this list is empty.
   */
  mountpoints?: Array<string> | undefined;
  /**
   * Filesystem types to include/exclude. Examples: ext4, !*tmpfs, !squashfs. Wildcards and ! (not) operators are supported. All types are included if this list is empty.
   */
  fstypes?: Array<string> | undefined;
  /**
   * Generate separate metrics for each device
   */
  perDevice?: boolean | undefined;
};

export type InputSystemMetricsCustom = {
  system?: InputSystemMetricsSystem | undefined;
  cpu?: InputSystemMetricsCpu | undefined;
  memory?: InputSystemMetricsMemory | undefined;
  network?: InputSystemMetricsNetwork | undefined;
  disk?: InputSystemMetricsDisk | undefined;
};

export type InputSystemMetricsHost = {
  /**
   * Select level of detail for host metrics
   */
  mode?: models.ModeOptionsHost | undefined;
  custom?: InputSystemMetricsCustom | undefined;
};

/**
 * Select the level of detail for container metrics
 */
export const ContainerMode = {
  /**
   * Basic
   */
  Basic: "basic",
  /**
   * All
   */
  All: "all",
  /**
   * Custom
   */
  Custom: "custom",
  /**
   * Disabled
   */
  Disabled: "disabled",
} as const;
/**
 * Select the level of detail for container metrics
 */
export type ContainerMode = OpenEnum<typeof ContainerMode>;

export type InputSystemMetricsFilter = {
  expr: string;
};

export type Container = {
  /**
   * Select the level of detail for container metrics
   */
  mode?: ContainerMode | undefined;
  /**
   * Full paths for Docker's UNIX-domain socket
   */
  dockerSocket?: Array<string> | undefined;
  /**
   * Timeout, in seconds, for the Docker API
   */
  dockerTimeout?: number | undefined;
  /**
   * Containers matching any of these will be included. All are included if no filters are added.
   */
  filters?: Array<InputSystemMetricsFilter> | undefined;
  /**
   * Include stopped and paused containers
   */
  allContainers?: boolean | undefined;
  /**
   * Generate separate metrics for each device
   */
  perDevice?: boolean | undefined;
  /**
   * Generate full container metrics
   */
  detail?: boolean | undefined;
};

export type InputSystemMetricsPersistence = {
  /**
   * Spool metrics to disk for Cribl Edge and Search
   */
  enable?: boolean | undefined;
  /**
   * Time span for each file bucket
   */
  timeWindow?: string | undefined;
  /**
   * Maximum disk space allowed to be consumed (examples: 420MB, 4GB). When limit is reached, older data will be deleted.
   */
  maxDataSize?: string | undefined;
  /**
   * Maximum amount of time to retain data (examples: 2h, 4d). When limit is reached, older data will be deleted.
   */
  maxDataTime?: string | undefined;
  compress?: models.DataCompressionFormatOptionsPersistence | undefined;
  /**
   * Path to use to write metrics. Defaults to $CRIBL_HOME/state/system_metrics
   */
  destPath?: string | undefined;
};

export type InputSystemMetricsPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputSystemMetricsType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Time, in seconds, between consecutive metric collections. Default is 10 seconds.
   */
  interval?: number | undefined;
  host?: InputSystemMetricsHost | undefined;
  process?: models.ProcessType | undefined;
  container?: Container | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  persistence?: InputSystemMetricsPersistence | undefined;
  description?: string | undefined;
};

export type InputSystemMetricsPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputSystemMetricsType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Time, in seconds, between consecutive metric collections. Default is 10 seconds.
   */
  interval?: number | undefined;
  host?: InputSystemMetricsHost | undefined;
  process?: models.ProcessType | undefined;
  container?: Container | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  persistence?: InputSystemMetricsPersistence | undefined;
  description?: string | undefined;
};

export type InputSystemMetricsSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputSystemMetricsType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Time, in seconds, between consecutive metric collections. Default is 10 seconds.
   */
  interval?: number | undefined;
  host?: InputSystemMetricsHost | undefined;
  process?: models.ProcessType | undefined;
  container?: Container | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  persistence?: InputSystemMetricsPersistence | undefined;
  description?: string | undefined;
};

export type InputSystemMetricsSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputSystemMetricsType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Time, in seconds, between consecutive metric collections. Default is 10 seconds.
   */
  interval?: number | undefined;
  host?: InputSystemMetricsHost | undefined;
  process?: models.ProcessType | undefined;
  container?: Container | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  persistence?: InputSystemMetricsPersistence | undefined;
  description?: string | undefined;
};

export type InputSystemMetrics =
  | InputSystemMetricsSendToRoutesTrueWithConnectionsConstraint
  | InputSystemMetricsSendToRoutesFalseWithConnectionsConstraint
  | InputSystemMetricsPqEnabledFalseWithPqConstraint
  | InputSystemMetricsPqEnabledTrueWithPqConstraint;

export const InputTcpjsonType = {
  Tcpjson: "tcpjson",
} as const;
export type InputTcpjsonType = ClosedEnum<typeof InputTcpjsonType>;

export type InputTcpjsonPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputTcpjsonType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Regex matching IP addresses that are allowed to establish a connection
   */
  ipWhitelistRegex?: string | undefined;
  /**
   * Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.
   */
  maxActiveCxn?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.
   */
  socketIdleTimeout?: number | undefined;
  /**
   * How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.
   */
  socketEndingMaxWait?: number | undefined;
  /**
   * The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.
   */
  socketMaxLifespan?: number | undefined;
  /**
   * Enable if the connection is proxied by a device that supports proxy protocol v1 or v2
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Load balance traffic across all Worker Processes
   */
  enableLoadBalancing?: boolean | undefined;
  /**
   * Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
   */
  authType?: models.AuthenticationMethodOptionsAuthTokensItems | undefined;
  description?: string | undefined;
  /**
   * Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted.
   */
  authToken?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
};

export type InputTcpjsonPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputTcpjsonType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Regex matching IP addresses that are allowed to establish a connection
   */
  ipWhitelistRegex?: string | undefined;
  /**
   * Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.
   */
  maxActiveCxn?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.
   */
  socketIdleTimeout?: number | undefined;
  /**
   * How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.
   */
  socketEndingMaxWait?: number | undefined;
  /**
   * The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.
   */
  socketMaxLifespan?: number | undefined;
  /**
   * Enable if the connection is proxied by a device that supports proxy protocol v1 or v2
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Load balance traffic across all Worker Processes
   */
  enableLoadBalancing?: boolean | undefined;
  /**
   * Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
   */
  authType?: models.AuthenticationMethodOptionsAuthTokensItems | undefined;
  description?: string | undefined;
  /**
   * Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted.
   */
  authToken?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
};

export type InputTcpjsonSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputTcpjsonType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Regex matching IP addresses that are allowed to establish a connection
   */
  ipWhitelistRegex?: string | undefined;
  /**
   * Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.
   */
  maxActiveCxn?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.
   */
  socketIdleTimeout?: number | undefined;
  /**
   * How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.
   */
  socketEndingMaxWait?: number | undefined;
  /**
   * The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.
   */
  socketMaxLifespan?: number | undefined;
  /**
   * Enable if the connection is proxied by a device that supports proxy protocol v1 or v2
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Load balance traffic across all Worker Processes
   */
  enableLoadBalancing?: boolean | undefined;
  /**
   * Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
   */
  authType?: models.AuthenticationMethodOptionsAuthTokensItems | undefined;
  description?: string | undefined;
  /**
   * Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted.
   */
  authToken?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
};

export type InputTcpjsonSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputTcpjsonType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Regex matching IP addresses that are allowed to establish a connection
   */
  ipWhitelistRegex?: string | undefined;
  /**
   * Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.
   */
  maxActiveCxn?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.
   */
  socketIdleTimeout?: number | undefined;
  /**
   * How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.
   */
  socketEndingMaxWait?: number | undefined;
  /**
   * The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.
   */
  socketMaxLifespan?: number | undefined;
  /**
   * Enable if the connection is proxied by a device that supports proxy protocol v1 or v2
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Load balance traffic across all Worker Processes
   */
  enableLoadBalancing?: boolean | undefined;
  /**
   * Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
   */
  authType?: models.AuthenticationMethodOptionsAuthTokensItems | undefined;
  description?: string | undefined;
  /**
   * Shared secret to be provided by any client (in authToken header field). If empty, unauthorized access is permitted.
   */
  authToken?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
};

export type InputTcpjson =
  | InputTcpjsonSendToRoutesTrueWithConnectionsConstraint
  | InputTcpjsonSendToRoutesFalseWithConnectionsConstraint
  | InputTcpjsonPqEnabledFalseWithPqConstraint
  | InputTcpjsonPqEnabledTrueWithPqConstraint;

export const InputCriblLakeHttpType = {
  CriblLakeHttp: "cribl_lake_http",
} as const;
export type InputCriblLakeHttpType = ClosedEnum<typeof InputCriblLakeHttpType>;

export type SplunkHecMetadata = {
  enabled?: boolean | undefined;
  defaultDataset?: string | undefined;
  allowedIndexesAtToken?: Array<string> | undefined;
};

export type ElasticsearchMetadata = {
  enabled?: boolean | undefined;
  defaultDataset?: string | undefined;
};

export type AuthTokensExt = {
  token: string;
  description?: string | undefined;
  /**
   * Fields to add to events referencing this token
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  splunkHecMetadata?: SplunkHecMetadata | undefined;
  elasticsearchMetadata?: ElasticsearchMetadata | undefined;
};

export type InputCriblLakeHttpPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputCriblLakeHttpType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokens?: Array<string> | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Absolute path on which to listen for the Cribl HTTP API requests. Only _bulk (default /cribl/_bulk) is available. Use empty string to disable.
   */
  criblAPI?: string | undefined;
  /**
   * Absolute path on which to listen for the Elasticsearch API requests. Only _bulk (default /elastic/_bulk) is available. Use empty string to disable.
   */
  elasticAPI?: string | undefined;
  /**
   * Absolute path on which listen for the Splunk HTTP Event Collector API requests. Use empty string to disable.
   */
  splunkHecAPI?: string | undefined;
  splunkHecAcks?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  authTokensExt?: Array<AuthTokensExt> | undefined;
  description?: string | undefined;
};

export type InputCriblLakeHttpPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputCriblLakeHttpType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokens?: Array<string> | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Absolute path on which to listen for the Cribl HTTP API requests. Only _bulk (default /cribl/_bulk) is available. Use empty string to disable.
   */
  criblAPI?: string | undefined;
  /**
   * Absolute path on which to listen for the Elasticsearch API requests. Only _bulk (default /elastic/_bulk) is available. Use empty string to disable.
   */
  elasticAPI?: string | undefined;
  /**
   * Absolute path on which listen for the Splunk HTTP Event Collector API requests. Use empty string to disable.
   */
  splunkHecAPI?: string | undefined;
  splunkHecAcks?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  authTokensExt?: Array<AuthTokensExt> | undefined;
  description?: string | undefined;
};

export type InputCriblLakeHttpSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputCriblLakeHttpType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokens?: Array<string> | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Absolute path on which to listen for the Cribl HTTP API requests. Only _bulk (default /cribl/_bulk) is available. Use empty string to disable.
   */
  criblAPI?: string | undefined;
  /**
   * Absolute path on which to listen for the Elasticsearch API requests. Only _bulk (default /elastic/_bulk) is available. Use empty string to disable.
   */
  elasticAPI?: string | undefined;
  /**
   * Absolute path on which listen for the Splunk HTTP Event Collector API requests. Use empty string to disable.
   */
  splunkHecAPI?: string | undefined;
  splunkHecAcks?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  authTokensExt?: Array<AuthTokensExt> | undefined;
  description?: string | undefined;
};

export type InputCriblLakeHttpSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputCriblLakeHttpType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokens?: Array<string> | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Absolute path on which to listen for the Cribl HTTP API requests. Only _bulk (default /cribl/_bulk) is available. Use empty string to disable.
   */
  criblAPI?: string | undefined;
  /**
   * Absolute path on which to listen for the Elasticsearch API requests. Only _bulk (default /elastic/_bulk) is available. Use empty string to disable.
   */
  elasticAPI?: string | undefined;
  /**
   * Absolute path on which listen for the Splunk HTTP Event Collector API requests. Use empty string to disable.
   */
  splunkHecAPI?: string | undefined;
  splunkHecAcks?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  authTokensExt?: Array<AuthTokensExt> | undefined;
  description?: string | undefined;
};

export type InputCriblLakeHttp =
  | InputCriblLakeHttpSendToRoutesTrueWithConnectionsConstraint
  | InputCriblLakeHttpSendToRoutesFalseWithConnectionsConstraint
  | InputCriblLakeHttpPqEnabledFalseWithPqConstraint
  | InputCriblLakeHttpPqEnabledTrueWithPqConstraint;

export const InputCriblHttpType = {
  CriblHttp: "cribl_http",
} as const;
export type InputCriblHttpType = ClosedEnum<typeof InputCriblHttpType>;

export type InputCriblHttpPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputCriblHttpType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  /**
   * Shared secrets to be used by connected environments to authorize connections. These tokens should be installed in Cribl HTTP destinations in connected environments.
   */
  authTokens?: Array<models.ItemsTypeAuthTokens> | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputCriblHttpPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputCriblHttpType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  /**
   * Shared secrets to be used by connected environments to authorize connections. These tokens should be installed in Cribl HTTP destinations in connected environments.
   */
  authTokens?: Array<models.ItemsTypeAuthTokens> | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputCriblHttpSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputCriblHttpType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  /**
   * Shared secrets to be used by connected environments to authorize connections. These tokens should be installed in Cribl HTTP destinations in connected environments.
   */
  authTokens?: Array<models.ItemsTypeAuthTokens> | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputCriblHttpSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputCriblHttpType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  /**
   * Shared secrets to be used by connected environments to authorize connections. These tokens should be installed in Cribl HTTP destinations in connected environments.
   */
  authTokens?: Array<models.ItemsTypeAuthTokens> | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputCriblHttp =
  | InputCriblHttpSendToRoutesTrueWithConnectionsConstraint
  | InputCriblHttpSendToRoutesFalseWithConnectionsConstraint
  | InputCriblHttpPqEnabledFalseWithPqConstraint
  | InputCriblHttpPqEnabledTrueWithPqConstraint;

export const InputCriblTcpType = {
  CriblTcp: "cribl_tcp",
} as const;
export type InputCriblTcpType = ClosedEnum<typeof InputCriblTcpType>;

export type InputCriblTcpPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputCriblTcpType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.
   */
  maxActiveCxn?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.
   */
  socketIdleTimeout?: number | undefined;
  /**
   * How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.
   */
  socketEndingMaxWait?: number | undefined;
  /**
   * The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.
   */
  socketMaxLifespan?: number | undefined;
  /**
   * Enable if the connection is proxied by a device that supports proxy protocol v1 or v2
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Load balance traffic across all Worker Processes
   */
  enableLoadBalancing?: boolean | undefined;
  /**
   * Shared secrets to be used by connected environments to authorize connections. These tokens should be installed in Cribl TCP destinations in connected environments.
   */
  authTokens?: Array<models.ItemsTypeAuthTokens> | undefined;
  description?: string | undefined;
};

export type InputCriblTcpPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputCriblTcpType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.
   */
  maxActiveCxn?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.
   */
  socketIdleTimeout?: number | undefined;
  /**
   * How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.
   */
  socketEndingMaxWait?: number | undefined;
  /**
   * The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.
   */
  socketMaxLifespan?: number | undefined;
  /**
   * Enable if the connection is proxied by a device that supports proxy protocol v1 or v2
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Load balance traffic across all Worker Processes
   */
  enableLoadBalancing?: boolean | undefined;
  /**
   * Shared secrets to be used by connected environments to authorize connections. These tokens should be installed in Cribl TCP destinations in connected environments.
   */
  authTokens?: Array<models.ItemsTypeAuthTokens> | undefined;
  description?: string | undefined;
};

export type InputCriblTcpSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputCriblTcpType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.
   */
  maxActiveCxn?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.
   */
  socketIdleTimeout?: number | undefined;
  /**
   * How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.
   */
  socketEndingMaxWait?: number | undefined;
  /**
   * The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.
   */
  socketMaxLifespan?: number | undefined;
  /**
   * Enable if the connection is proxied by a device that supports proxy protocol v1 or v2
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Load balance traffic across all Worker Processes
   */
  enableLoadBalancing?: boolean | undefined;
  /**
   * Shared secrets to be used by connected environments to authorize connections. These tokens should be installed in Cribl TCP destinations in connected environments.
   */
  authTokens?: Array<models.ItemsTypeAuthTokens> | undefined;
  description?: string | undefined;
};

export type InputCriblTcpSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputCriblTcpType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.
   */
  maxActiveCxn?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.
   */
  socketIdleTimeout?: number | undefined;
  /**
   * How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.
   */
  socketEndingMaxWait?: number | undefined;
  /**
   * The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.
   */
  socketMaxLifespan?: number | undefined;
  /**
   * Enable if the connection is proxied by a device that supports proxy protocol v1 or v2
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Load balance traffic across all Worker Processes
   */
  enableLoadBalancing?: boolean | undefined;
  /**
   * Shared secrets to be used by connected environments to authorize connections. These tokens should be installed in Cribl TCP destinations in connected environments.
   */
  authTokens?: Array<models.ItemsTypeAuthTokens> | undefined;
  description?: string | undefined;
};

export type InputCriblTcp =
  | InputCriblTcpSendToRoutesTrueWithConnectionsConstraint
  | InputCriblTcpSendToRoutesFalseWithConnectionsConstraint
  | InputCriblTcpPqEnabledFalseWithPqConstraint
  | InputCriblTcpPqEnabledTrueWithPqConstraint;

export const InputCriblType = {
  Cribl: "cribl",
} as const;
export type InputCriblType = ClosedEnum<typeof InputCriblType>;

export type InputCriblPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputCriblType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  filter?: string | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputCriblPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputCriblType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  filter?: string | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputCriblSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputCriblType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  filter?: string | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputCriblSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputCriblType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  filter?: string | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputCribl =
  | InputCriblSendToRoutesTrueWithConnectionsConstraint
  | InputCriblSendToRoutesFalseWithConnectionsConstraint
  | InputCriblPqEnabledFalseWithPqConstraint
  | InputCriblPqEnabledTrueWithPqConstraint;

export const InputGooglePubsubType = {
  GooglePubsub: "google_pubsub",
} as const;
export type InputGooglePubsubType = ClosedEnum<typeof InputGooglePubsubType>;

export type InputGooglePubsubPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputGooglePubsubType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * ID of the topic to receive events from. When Monitor subscription is enabled, any value may be entered.
   */
  topicName?: string | undefined;
  /**
   * ID of the subscription to use when receiving events. When Monitor subscription is enabled, the fully qualified subscription name must be entered. Example: projects/myProject/subscriptions/mySubscription
   */
  subscriptionName: string;
  /**
   * Use when the subscription is not created by this Source and topic is not known
   */
  monitorSubscription?: boolean | undefined;
  /**
   * Create topic if it does not exist
   */
  createTopic?: boolean | undefined;
  /**
   * Create subscription if it does not exist
   */
  createSubscription?: boolean | undefined;
  /**
   * Region to retrieve messages from. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy.
   */
  region?: string | undefined;
  /**
   * Choose Auto to use Google Application Default Credentials (ADC), Manual to enter Google service account credentials directly, or Secret to select or create a stored secret that references Google service account credentials.
   */
  googleAuthMethod?: models.GoogleAuthenticationMethodOptions | undefined;
  /**
   * Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.
   */
  serviceAccountCredentials?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  secret?: string | undefined;
  /**
   * If Destination exerts backpressure, this setting limits how many inbound events Stream will queue for processing before it stops retrieving events
   */
  maxBacklog?: number | undefined;
  /**
   * How many streams to pull messages from at one time. Doubling the value doubles the number of messages this Source pulls from the topic (if available), while consuming more CPU and memory. Defaults to 5.
   */
  concurrency?: number | undefined;
  /**
   * Pull request timeout, in milliseconds
   */
  requestTimeout?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
  /**
   * Receive events in the order they were added to the queue. The process sending events must have ordering enabled.
   */
  orderedDelivery?: boolean | undefined;
};

export type InputGooglePubsubPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputGooglePubsubType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * ID of the topic to receive events from. When Monitor subscription is enabled, any value may be entered.
   */
  topicName?: string | undefined;
  /**
   * ID of the subscription to use when receiving events. When Monitor subscription is enabled, the fully qualified subscription name must be entered. Example: projects/myProject/subscriptions/mySubscription
   */
  subscriptionName: string;
  /**
   * Use when the subscription is not created by this Source and topic is not known
   */
  monitorSubscription?: boolean | undefined;
  /**
   * Create topic if it does not exist
   */
  createTopic?: boolean | undefined;
  /**
   * Create subscription if it does not exist
   */
  createSubscription?: boolean | undefined;
  /**
   * Region to retrieve messages from. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy.
   */
  region?: string | undefined;
  /**
   * Choose Auto to use Google Application Default Credentials (ADC), Manual to enter Google service account credentials directly, or Secret to select or create a stored secret that references Google service account credentials.
   */
  googleAuthMethod?: models.GoogleAuthenticationMethodOptions | undefined;
  /**
   * Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.
   */
  serviceAccountCredentials?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  secret?: string | undefined;
  /**
   * If Destination exerts backpressure, this setting limits how many inbound events Stream will queue for processing before it stops retrieving events
   */
  maxBacklog?: number | undefined;
  /**
   * How many streams to pull messages from at one time. Doubling the value doubles the number of messages this Source pulls from the topic (if available), while consuming more CPU and memory. Defaults to 5.
   */
  concurrency?: number | undefined;
  /**
   * Pull request timeout, in milliseconds
   */
  requestTimeout?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
  /**
   * Receive events in the order they were added to the queue. The process sending events must have ordering enabled.
   */
  orderedDelivery?: boolean | undefined;
};

export type InputGooglePubsubSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputGooglePubsubType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * ID of the topic to receive events from. When Monitor subscription is enabled, any value may be entered.
   */
  topicName?: string | undefined;
  /**
   * ID of the subscription to use when receiving events. When Monitor subscription is enabled, the fully qualified subscription name must be entered. Example: projects/myProject/subscriptions/mySubscription
   */
  subscriptionName: string;
  /**
   * Use when the subscription is not created by this Source and topic is not known
   */
  monitorSubscription?: boolean | undefined;
  /**
   * Create topic if it does not exist
   */
  createTopic?: boolean | undefined;
  /**
   * Create subscription if it does not exist
   */
  createSubscription?: boolean | undefined;
  /**
   * Region to retrieve messages from. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy.
   */
  region?: string | undefined;
  /**
   * Choose Auto to use Google Application Default Credentials (ADC), Manual to enter Google service account credentials directly, or Secret to select or create a stored secret that references Google service account credentials.
   */
  googleAuthMethod?: models.GoogleAuthenticationMethodOptions | undefined;
  /**
   * Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.
   */
  serviceAccountCredentials?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  secret?: string | undefined;
  /**
   * If Destination exerts backpressure, this setting limits how many inbound events Stream will queue for processing before it stops retrieving events
   */
  maxBacklog?: number | undefined;
  /**
   * How many streams to pull messages from at one time. Doubling the value doubles the number of messages this Source pulls from the topic (if available), while consuming more CPU and memory. Defaults to 5.
   */
  concurrency?: number | undefined;
  /**
   * Pull request timeout, in milliseconds
   */
  requestTimeout?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
  /**
   * Receive events in the order they were added to the queue. The process sending events must have ordering enabled.
   */
  orderedDelivery?: boolean | undefined;
};

export type InputGooglePubsubSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputGooglePubsubType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * ID of the topic to receive events from. When Monitor subscription is enabled, any value may be entered.
   */
  topicName?: string | undefined;
  /**
   * ID of the subscription to use when receiving events. When Monitor subscription is enabled, the fully qualified subscription name must be entered. Example: projects/myProject/subscriptions/mySubscription
   */
  subscriptionName: string;
  /**
   * Use when the subscription is not created by this Source and topic is not known
   */
  monitorSubscription?: boolean | undefined;
  /**
   * Create topic if it does not exist
   */
  createTopic?: boolean | undefined;
  /**
   * Create subscription if it does not exist
   */
  createSubscription?: boolean | undefined;
  /**
   * Region to retrieve messages from. Select 'default' to allow Google to auto-select the nearest region. When using ordered delivery, the selected region must be allowed by message storage policy.
   */
  region?: string | undefined;
  /**
   * Choose Auto to use Google Application Default Credentials (ADC), Manual to enter Google service account credentials directly, or Secret to select or create a stored secret that references Google service account credentials.
   */
  googleAuthMethod?: models.GoogleAuthenticationMethodOptions | undefined;
  /**
   * Contents of service account credentials (JSON keys) file downloaded from Google Cloud. To upload a file, click the upload button at this field's upper right.
   */
  serviceAccountCredentials?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  secret?: string | undefined;
  /**
   * If Destination exerts backpressure, this setting limits how many inbound events Stream will queue for processing before it stops retrieving events
   */
  maxBacklog?: number | undefined;
  /**
   * How many streams to pull messages from at one time. Doubling the value doubles the number of messages this Source pulls from the topic (if available), while consuming more CPU and memory. Defaults to 5.
   */
  concurrency?: number | undefined;
  /**
   * Pull request timeout, in milliseconds
   */
  requestTimeout?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
  /**
   * Receive events in the order they were added to the queue. The process sending events must have ordering enabled.
   */
  orderedDelivery?: boolean | undefined;
};

export type InputGooglePubsub =
  | InputGooglePubsubSendToRoutesTrueWithConnectionsConstraint
  | InputGooglePubsubSendToRoutesFalseWithConnectionsConstraint
  | InputGooglePubsubPqEnabledFalseWithPqConstraint
  | InputGooglePubsubPqEnabledTrueWithPqConstraint;

export const InputFirehoseType = {
  Firehose: "firehose",
} as const;
export type InputFirehoseType = ClosedEnum<typeof InputFirehoseType>;

export type InputFirehosePqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputFirehoseType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokens?: Array<string> | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputFirehosePqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputFirehoseType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokens?: Array<string> | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputFirehoseSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputFirehoseType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokens?: Array<string> | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputFirehoseSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputFirehoseType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokens?: Array<string> | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputFirehose =
  | InputFirehoseSendToRoutesTrueWithConnectionsConstraint
  | InputFirehoseSendToRoutesFalseWithConnectionsConstraint
  | InputFirehosePqEnabledFalseWithPqConstraint
  | InputFirehosePqEnabledTrueWithPqConstraint;

export const PqEnabledTrueWithPqConstraintInputExecType = {
  Exec: "exec",
} as const;
export type PqEnabledTrueWithPqConstraintInputExecType = ClosedEnum<
  typeof PqEnabledTrueWithPqConstraintInputExecType
>;

/**
 * Select a schedule type; either an interval (in seconds) or a cron-style schedule.
 */
export const PqEnabledTrueWithPqConstraintScheduleType = {
  Interval: "interval",
  CronSchedule: "cronSchedule",
} as const;
/**
 * Select a schedule type; either an interval (in seconds) or a cron-style schedule.
 */
export type PqEnabledTrueWithPqConstraintScheduleType = OpenEnum<
  typeof PqEnabledTrueWithPqConstraintScheduleType
>;

export type InputExecPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: PqEnabledTrueWithPqConstraintInputExecType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Command to execute; supports Bourne shell (or CMD on Windows) syntax
   */
  command: string;
  /**
   * Maximum number of retry attempts in the event that the command fails
   */
  retries?: number | undefined;
  /**
   * Select a schedule type; either an interval (in seconds) or a cron-style schedule.
   */
  scheduleType?: PqEnabledTrueWithPqConstraintScheduleType | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
  /**
   * Interval between command executions in seconds.
   */
  interval?: number | undefined;
  /**
   * Cron schedule to execute the command on.
   */
  cronSchedule?: string | undefined;
};

export const PqEnabledFalseWithPqConstraintInputExecType = {
  Exec: "exec",
} as const;
export type PqEnabledFalseWithPqConstraintInputExecType = ClosedEnum<
  typeof PqEnabledFalseWithPqConstraintInputExecType
>;

/**
 * Select a schedule type; either an interval (in seconds) or a cron-style schedule.
 */
export const PqEnabledFalseWithPqConstraintScheduleType = {
  Interval: "interval",
  CronSchedule: "cronSchedule",
} as const;
/**
 * Select a schedule type; either an interval (in seconds) or a cron-style schedule.
 */
export type PqEnabledFalseWithPqConstraintScheduleType = OpenEnum<
  typeof PqEnabledFalseWithPqConstraintScheduleType
>;

export type InputExecPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: PqEnabledFalseWithPqConstraintInputExecType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Command to execute; supports Bourne shell (or CMD on Windows) syntax
   */
  command: string;
  /**
   * Maximum number of retry attempts in the event that the command fails
   */
  retries?: number | undefined;
  /**
   * Select a schedule type; either an interval (in seconds) or a cron-style schedule.
   */
  scheduleType?: PqEnabledFalseWithPqConstraintScheduleType | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
  /**
   * Interval between command executions in seconds.
   */
  interval?: number | undefined;
  /**
   * Cron schedule to execute the command on.
   */
  cronSchedule?: string | undefined;
};

export const SendToRoutesFalseWithConnectionsConstraintInputExecType = {
  Exec: "exec",
} as const;
export type SendToRoutesFalseWithConnectionsConstraintInputExecType =
  ClosedEnum<typeof SendToRoutesFalseWithConnectionsConstraintInputExecType>;

/**
 * Select a schedule type; either an interval (in seconds) or a cron-style schedule.
 */
export const SendToRoutesFalseWithConnectionsConstraintScheduleType = {
  Interval: "interval",
  CronSchedule: "cronSchedule",
} as const;
/**
 * Select a schedule type; either an interval (in seconds) or a cron-style schedule.
 */
export type SendToRoutesFalseWithConnectionsConstraintScheduleType = OpenEnum<
  typeof SendToRoutesFalseWithConnectionsConstraintScheduleType
>;

export type InputExecSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: SendToRoutesFalseWithConnectionsConstraintInputExecType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Command to execute; supports Bourne shell (or CMD on Windows) syntax
   */
  command: string;
  /**
   * Maximum number of retry attempts in the event that the command fails
   */
  retries?: number | undefined;
  /**
   * Select a schedule type; either an interval (in seconds) or a cron-style schedule.
   */
  scheduleType?:
    | SendToRoutesFalseWithConnectionsConstraintScheduleType
    | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
  /**
   * Interval between command executions in seconds.
   */
  interval?: number | undefined;
  /**
   * Cron schedule to execute the command on.
   */
  cronSchedule?: string | undefined;
};

export const SendToRoutesTrueWithConnectionsConstraintInputExecType = {
  Exec: "exec",
} as const;
export type SendToRoutesTrueWithConnectionsConstraintInputExecType = ClosedEnum<
  typeof SendToRoutesTrueWithConnectionsConstraintInputExecType
>;

/**
 * Select a schedule type; either an interval (in seconds) or a cron-style schedule.
 */
export const SendToRoutesTrueWithConnectionsConstraintScheduleType = {
  Interval: "interval",
  CronSchedule: "cronSchedule",
} as const;
/**
 * Select a schedule type; either an interval (in seconds) or a cron-style schedule.
 */
export type SendToRoutesTrueWithConnectionsConstraintScheduleType = OpenEnum<
  typeof SendToRoutesTrueWithConnectionsConstraintScheduleType
>;

export type InputExecSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: SendToRoutesTrueWithConnectionsConstraintInputExecType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Command to execute; supports Bourne shell (or CMD on Windows) syntax
   */
  command: string;
  /**
   * Maximum number of retry attempts in the event that the command fails
   */
  retries?: number | undefined;
  /**
   * Select a schedule type; either an interval (in seconds) or a cron-style schedule.
   */
  scheduleType?:
    | SendToRoutesTrueWithConnectionsConstraintScheduleType
    | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
  /**
   * Interval between command executions in seconds.
   */
  interval?: number | undefined;
  /**
   * Cron schedule to execute the command on.
   */
  cronSchedule?: string | undefined;
};

export type InputExec =
  | InputExecSendToRoutesTrueWithConnectionsConstraint
  | InputExecSendToRoutesFalseWithConnectionsConstraint
  | InputExecPqEnabledFalseWithPqConstraint
  | InputExecPqEnabledTrueWithPqConstraint;

export const InputEventhubType = {
  Eventhub: "eventhub",
} as const;
export type InputEventhubType = ClosedEnum<typeof InputEventhubType>;

export type InputEventhubPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputEventhubType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * List of Event Hubs Kafka brokers to connect to (example: yourdomain.servicebus.windows.net:9093). The hostname can be found in the host portion of the primary or secondary connection string in Shared Access Policies.
   */
  brokers: Array<string>;
  /**
   * The name of the Event Hub (Kafka topic) to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Event Hubs Source to only a single topic.
   */
  topics: Array<string>;
  /**
   * The consumer group this instance belongs to. Default is 'Cribl'.
   */
  groupId?: string | undefined;
  /**
   * Start reading from earliest available data; relevant only during initial subscription
   */
  fromBeginning?: boolean | undefined;
  /**
   * Maximum time to wait for a connection to complete successfully
   */
  connectionTimeout?: number | undefined;
  /**
   * Maximum time to wait for Kafka to respond to a request
   */
  requestTimeout?: number | undefined;
  /**
   * If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data
   */
  maxRetries?: number | undefined;
  /**
   * The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).
   */
  maxBackOff?: number | undefined;
  /**
   * Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).
   */
  initialBackoff?: number | undefined;
  /**
   * Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.
   */
  backoffRate?: number | undefined;
  /**
   * Maximum time to wait for Kafka to respond to an authentication request
   */
  authenticationTimeout?: number | undefined;
  /**
   * Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.
   */
  reauthenticationThreshold?: number | undefined;
  /**
   * Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.
   */
  sasl?: models.AuthenticationType1 | undefined;
  tls?: models.TlsSettingsClientSideType | undefined;
  /**
   *       Timeout (session.timeout.ms in Kafka domain) used to detect client failures when using Kafka's group-management facilities.
   *
   * @remarks
   *       If the client sends no heartbeats to the broker before the timeout expires, the broker will remove the client from the group and initiate a rebalance.
   *       Value must be lower than rebalanceTimeout.
   *       See details [here](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).
   */
  sessionTimeout?: number | undefined;
  /**
   *       Maximum allowed time (rebalance.timeout.ms in Kafka domain) for each worker to join the group after a rebalance begins.
   *
   * @remarks
   *       If the timeout is exceeded, the coordinator broker will remove the worker from the group.
   *       See [Recommended configurations](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).
   */
  rebalanceTimeout?: number | undefined;
  /**
   *       Expected time (heartbeat.interval.ms in Kafka domain) between heartbeats to the consumer coordinator when using Kafka's group-management facilities.
   *
   * @remarks
   *       Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.
   *       See [Recommended configurations](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).
   */
  heartbeatInterval?: number | undefined;
  /**
   * How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.
   */
  autoCommitInterval?: number | undefined;
  /**
   * How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.
   */
  autoCommitThreshold?: number | undefined;
  /**
   * Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).
   */
  maxBytesPerPartition?: number | undefined;
  /**
   * Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).
   */
  maxBytes?: number | undefined;
  /**
   * Maximum number of network errors before the consumer re-creates a socket
   */
  maxSocketErrors?: number | undefined;
  /**
   * Minimize duplicate events by starting only one consumer for each topic partition
   */
  minimizeDuplicates?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputEventhubPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputEventhubType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * List of Event Hubs Kafka brokers to connect to (example: yourdomain.servicebus.windows.net:9093). The hostname can be found in the host portion of the primary or secondary connection string in Shared Access Policies.
   */
  brokers: Array<string>;
  /**
   * The name of the Event Hub (Kafka topic) to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Event Hubs Source to only a single topic.
   */
  topics: Array<string>;
  /**
   * The consumer group this instance belongs to. Default is 'Cribl'.
   */
  groupId?: string | undefined;
  /**
   * Start reading from earliest available data; relevant only during initial subscription
   */
  fromBeginning?: boolean | undefined;
  /**
   * Maximum time to wait for a connection to complete successfully
   */
  connectionTimeout?: number | undefined;
  /**
   * Maximum time to wait for Kafka to respond to a request
   */
  requestTimeout?: number | undefined;
  /**
   * If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data
   */
  maxRetries?: number | undefined;
  /**
   * The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).
   */
  maxBackOff?: number | undefined;
  /**
   * Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).
   */
  initialBackoff?: number | undefined;
  /**
   * Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.
   */
  backoffRate?: number | undefined;
  /**
   * Maximum time to wait for Kafka to respond to an authentication request
   */
  authenticationTimeout?: number | undefined;
  /**
   * Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.
   */
  reauthenticationThreshold?: number | undefined;
  /**
   * Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.
   */
  sasl?: models.AuthenticationType1 | undefined;
  tls?: models.TlsSettingsClientSideType | undefined;
  /**
   *       Timeout (session.timeout.ms in Kafka domain) used to detect client failures when using Kafka's group-management facilities.
   *
   * @remarks
   *       If the client sends no heartbeats to the broker before the timeout expires, the broker will remove the client from the group and initiate a rebalance.
   *       Value must be lower than rebalanceTimeout.
   *       See details [here](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).
   */
  sessionTimeout?: number | undefined;
  /**
   *       Maximum allowed time (rebalance.timeout.ms in Kafka domain) for each worker to join the group after a rebalance begins.
   *
   * @remarks
   *       If the timeout is exceeded, the coordinator broker will remove the worker from the group.
   *       See [Recommended configurations](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).
   */
  rebalanceTimeout?: number | undefined;
  /**
   *       Expected time (heartbeat.interval.ms in Kafka domain) between heartbeats to the consumer coordinator when using Kafka's group-management facilities.
   *
   * @remarks
   *       Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.
   *       See [Recommended configurations](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).
   */
  heartbeatInterval?: number | undefined;
  /**
   * How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.
   */
  autoCommitInterval?: number | undefined;
  /**
   * How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.
   */
  autoCommitThreshold?: number | undefined;
  /**
   * Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).
   */
  maxBytesPerPartition?: number | undefined;
  /**
   * Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).
   */
  maxBytes?: number | undefined;
  /**
   * Maximum number of network errors before the consumer re-creates a socket
   */
  maxSocketErrors?: number | undefined;
  /**
   * Minimize duplicate events by starting only one consumer for each topic partition
   */
  minimizeDuplicates?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputEventhubSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputEventhubType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * List of Event Hubs Kafka brokers to connect to (example: yourdomain.servicebus.windows.net:9093). The hostname can be found in the host portion of the primary or secondary connection string in Shared Access Policies.
   */
  brokers: Array<string>;
  /**
   * The name of the Event Hub (Kafka topic) to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Event Hubs Source to only a single topic.
   */
  topics: Array<string>;
  /**
   * The consumer group this instance belongs to. Default is 'Cribl'.
   */
  groupId?: string | undefined;
  /**
   * Start reading from earliest available data; relevant only during initial subscription
   */
  fromBeginning?: boolean | undefined;
  /**
   * Maximum time to wait for a connection to complete successfully
   */
  connectionTimeout?: number | undefined;
  /**
   * Maximum time to wait for Kafka to respond to a request
   */
  requestTimeout?: number | undefined;
  /**
   * If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data
   */
  maxRetries?: number | undefined;
  /**
   * The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).
   */
  maxBackOff?: number | undefined;
  /**
   * Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).
   */
  initialBackoff?: number | undefined;
  /**
   * Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.
   */
  backoffRate?: number | undefined;
  /**
   * Maximum time to wait for Kafka to respond to an authentication request
   */
  authenticationTimeout?: number | undefined;
  /**
   * Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.
   */
  reauthenticationThreshold?: number | undefined;
  /**
   * Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.
   */
  sasl?: models.AuthenticationType1 | undefined;
  tls?: models.TlsSettingsClientSideType | undefined;
  /**
   *       Timeout (session.timeout.ms in Kafka domain) used to detect client failures when using Kafka's group-management facilities.
   *
   * @remarks
   *       If the client sends no heartbeats to the broker before the timeout expires, the broker will remove the client from the group and initiate a rebalance.
   *       Value must be lower than rebalanceTimeout.
   *       See details [here](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).
   */
  sessionTimeout?: number | undefined;
  /**
   *       Maximum allowed time (rebalance.timeout.ms in Kafka domain) for each worker to join the group after a rebalance begins.
   *
   * @remarks
   *       If the timeout is exceeded, the coordinator broker will remove the worker from the group.
   *       See [Recommended configurations](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).
   */
  rebalanceTimeout?: number | undefined;
  /**
   *       Expected time (heartbeat.interval.ms in Kafka domain) between heartbeats to the consumer coordinator when using Kafka's group-management facilities.
   *
   * @remarks
   *       Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.
   *       See [Recommended configurations](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).
   */
  heartbeatInterval?: number | undefined;
  /**
   * How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.
   */
  autoCommitInterval?: number | undefined;
  /**
   * How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.
   */
  autoCommitThreshold?: number | undefined;
  /**
   * Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).
   */
  maxBytesPerPartition?: number | undefined;
  /**
   * Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).
   */
  maxBytes?: number | undefined;
  /**
   * Maximum number of network errors before the consumer re-creates a socket
   */
  maxSocketErrors?: number | undefined;
  /**
   * Minimize duplicate events by starting only one consumer for each topic partition
   */
  minimizeDuplicates?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputEventhubSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputEventhubType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * List of Event Hubs Kafka brokers to connect to (example: yourdomain.servicebus.windows.net:9093). The hostname can be found in the host portion of the primary or secondary connection string in Shared Access Policies.
   */
  brokers: Array<string>;
  /**
   * The name of the Event Hub (Kafka topic) to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Event Hubs Source to only a single topic.
   */
  topics: Array<string>;
  /**
   * The consumer group this instance belongs to. Default is 'Cribl'.
   */
  groupId?: string | undefined;
  /**
   * Start reading from earliest available data; relevant only during initial subscription
   */
  fromBeginning?: boolean | undefined;
  /**
   * Maximum time to wait for a connection to complete successfully
   */
  connectionTimeout?: number | undefined;
  /**
   * Maximum time to wait for Kafka to respond to a request
   */
  requestTimeout?: number | undefined;
  /**
   * If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data
   */
  maxRetries?: number | undefined;
  /**
   * The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).
   */
  maxBackOff?: number | undefined;
  /**
   * Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).
   */
  initialBackoff?: number | undefined;
  /**
   * Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.
   */
  backoffRate?: number | undefined;
  /**
   * Maximum time to wait for Kafka to respond to an authentication request
   */
  authenticationTimeout?: number | undefined;
  /**
   * Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.
   */
  reauthenticationThreshold?: number | undefined;
  /**
   * Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.
   */
  sasl?: models.AuthenticationType1 | undefined;
  tls?: models.TlsSettingsClientSideType | undefined;
  /**
   *       Timeout (session.timeout.ms in Kafka domain) used to detect client failures when using Kafka's group-management facilities.
   *
   * @remarks
   *       If the client sends no heartbeats to the broker before the timeout expires, the broker will remove the client from the group and initiate a rebalance.
   *       Value must be lower than rebalanceTimeout.
   *       See details [here](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).
   */
  sessionTimeout?: number | undefined;
  /**
   *       Maximum allowed time (rebalance.timeout.ms in Kafka domain) for each worker to join the group after a rebalance begins.
   *
   * @remarks
   *       If the timeout is exceeded, the coordinator broker will remove the worker from the group.
   *       See [Recommended configurations](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).
   */
  rebalanceTimeout?: number | undefined;
  /**
   *       Expected time (heartbeat.interval.ms in Kafka domain) between heartbeats to the consumer coordinator when using Kafka's group-management facilities.
   *
   * @remarks
   *       Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.
   *       See [Recommended configurations](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).
   */
  heartbeatInterval?: number | undefined;
  /**
   * How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.
   */
  autoCommitInterval?: number | undefined;
  /**
   * How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.
   */
  autoCommitThreshold?: number | undefined;
  /**
   * Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).
   */
  maxBytesPerPartition?: number | undefined;
  /**
   * Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).
   */
  maxBytes?: number | undefined;
  /**
   * Maximum number of network errors before the consumer re-creates a socket
   */
  maxSocketErrors?: number | undefined;
  /**
   * Minimize duplicate events by starting only one consumer for each topic partition
   */
  minimizeDuplicates?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputEventhub =
  | InputEventhubSendToRoutesTrueWithConnectionsConstraint
  | InputEventhubSendToRoutesFalseWithConnectionsConstraint
  | InputEventhubPqEnabledFalseWithPqConstraint
  | InputEventhubPqEnabledTrueWithPqConstraint;

export const InputOffice365MsgTraceType = {
  Office365MsgTrace: "office365_msg_trace",
} as const;
export type InputOffice365MsgTraceType = ClosedEnum<
  typeof InputOffice365MsgTraceType
>;

/**
 * Select authentication method.
 */
export const InputOffice365MsgTraceAuthenticationMethod = {
  Manual: "manual",
  Secret: "secret",
  Oauth: "oauth",
  OauthSecret: "oauthSecret",
  OauthCert: "oauthCert",
} as const;
/**
 * Select authentication method.
 */
export type InputOffice365MsgTraceAuthenticationMethod = OpenEnum<
  typeof InputOffice365MsgTraceAuthenticationMethod
>;

/**
 * Log Level (verbosity) for collection runtime behavior.
 */
export const InputOffice365MsgTraceLogLevel = {
  Error: "error",
  Warn: "warn",
  Info: "info",
  Debug: "debug",
  Silly: "silly",
} as const;
/**
 * Log Level (verbosity) for collection runtime behavior.
 */
export type InputOffice365MsgTraceLogLevel = OpenEnum<
  typeof InputOffice365MsgTraceLogLevel
>;

export type CertOptions = {
  /**
   * The name of the predefined certificate.
   */
  certificateName?: string | undefined;
  /**
   * Path to the private key to use. Key should be in PEM format. Can reference $ENV_VARS.
   */
  privKeyPath: string;
  /**
   * Passphrase to use to decrypt the private key.
   */
  passphrase?: string | undefined;
  /**
   * Path to the certificate to use. Certificate should be in PEM format. Can reference $ENV_VARS.
   */
  certPath: string;
};

export type InputOffice365MsgTracePqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputOffice365MsgTraceType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * URL to use when retrieving report data.
   */
  url?: string | undefined;
  /**
   * How often (in minutes) to run the report. Must divide evenly into 60 minutes to create a predictable schedule, or Save will fail.
   */
  interval?: number | undefined;
  /**
   * Backward offset for the search range's head. (E.g.: -3h@h) Message Trace data is delayed; this parameter (with Date range end) compensates for delay and gaps.
   */
  startDate?: string | undefined;
  /**
   * Backward offset for the search range's tail. (E.g.: -2h@h) Message Trace data is delayed; this parameter (with Date range start) compensates for delay and gaps.
   */
  endDate?: string | undefined;
  /**
   * HTTP request inactivity timeout. Maximum is 2400 (40 minutes); enter 0 to wait indefinitely.
   */
  timeout?: number | undefined;
  /**
   * Disables time filtering of events when a date range is specified.
   */
  disableTimeFilter?: boolean | undefined;
  /**
   * Select authentication method.
   */
  authType?: InputOffice365MsgTraceAuthenticationMethod | undefined;
  /**
   * Reschedule tasks that failed with non-fatal errors
   */
  rescheduleDroppedTasks?: boolean | undefined;
  /**
   * Maximum number of times a task can be rescheduled
   */
  maxTaskReschedule?: number | undefined;
  /**
   * Log Level (verbosity) for collection runtime behavior.
   */
  logLevel?: InputOffice365MsgTraceLogLevel | undefined;
  /**
   * Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.
   */
  jobTimeout?: string | undefined;
  /**
   * How often workers should check in with the scheduler to keep job subscription alive
   */
  keepAliveTime?: number | undefined;
  /**
   * The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.
   */
  maxMissedKeepAlives?: number | undefined;
  /**
   * Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.
   */
  ttl?: string | undefined;
  /**
   * When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.
   */
  ignoreGroupJobsLimit?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  retryRules?: models.RetryRulesType1 | undefined;
  description?: string | undefined;
  /**
   * Username to run Message Trace API call.
   */
  username?: string | undefined;
  /**
   * Password to run Message Trace API call.
   */
  password?: string | undefined;
  /**
   * Select or create a secret that references your credentials.
   */
  credentialsSecret?: string | undefined;
  /**
   * client_secret to pass in the OAuth request parameter.
   */
  clientSecret?: string | undefined;
  /**
   * Directory ID (tenant identifier) in Azure Active Directory.
   */
  tenantId?: string | undefined;
  /**
   * client_id to pass in the OAuth request parameter.
   */
  clientId?: string | undefined;
  /**
   * Resource to pass in the OAuth request parameter.
   */
  resource?: string | undefined;
  /**
   * Office 365 subscription plan for your organization, typically Office 365 Enterprise
   */
  planType?: models.SubscriptionPlanOptions | undefined;
  /**
   * Select or create a secret that references your client_secret to pass in the OAuth request parameter.
   */
  textSecret?: string | undefined;
  certOptions?: CertOptions | undefined;
};

export type InputOffice365MsgTracePqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputOffice365MsgTraceType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * URL to use when retrieving report data.
   */
  url?: string | undefined;
  /**
   * How often (in minutes) to run the report. Must divide evenly into 60 minutes to create a predictable schedule, or Save will fail.
   */
  interval?: number | undefined;
  /**
   * Backward offset for the search range's head. (E.g.: -3h@h) Message Trace data is delayed; this parameter (with Date range end) compensates for delay and gaps.
   */
  startDate?: string | undefined;
  /**
   * Backward offset for the search range's tail. (E.g.: -2h@h) Message Trace data is delayed; this parameter (with Date range start) compensates for delay and gaps.
   */
  endDate?: string | undefined;
  /**
   * HTTP request inactivity timeout. Maximum is 2400 (40 minutes); enter 0 to wait indefinitely.
   */
  timeout?: number | undefined;
  /**
   * Disables time filtering of events when a date range is specified.
   */
  disableTimeFilter?: boolean | undefined;
  /**
   * Select authentication method.
   */
  authType?: InputOffice365MsgTraceAuthenticationMethod | undefined;
  /**
   * Reschedule tasks that failed with non-fatal errors
   */
  rescheduleDroppedTasks?: boolean | undefined;
  /**
   * Maximum number of times a task can be rescheduled
   */
  maxTaskReschedule?: number | undefined;
  /**
   * Log Level (verbosity) for collection runtime behavior.
   */
  logLevel?: InputOffice365MsgTraceLogLevel | undefined;
  /**
   * Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.
   */
  jobTimeout?: string | undefined;
  /**
   * How often workers should check in with the scheduler to keep job subscription alive
   */
  keepAliveTime?: number | undefined;
  /**
   * The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.
   */
  maxMissedKeepAlives?: number | undefined;
  /**
   * Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.
   */
  ttl?: string | undefined;
  /**
   * When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.
   */
  ignoreGroupJobsLimit?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  retryRules?: models.RetryRulesType1 | undefined;
  description?: string | undefined;
  /**
   * Username to run Message Trace API call.
   */
  username?: string | undefined;
  /**
   * Password to run Message Trace API call.
   */
  password?: string | undefined;
  /**
   * Select or create a secret that references your credentials.
   */
  credentialsSecret?: string | undefined;
  /**
   * client_secret to pass in the OAuth request parameter.
   */
  clientSecret?: string | undefined;
  /**
   * Directory ID (tenant identifier) in Azure Active Directory.
   */
  tenantId?: string | undefined;
  /**
   * client_id to pass in the OAuth request parameter.
   */
  clientId?: string | undefined;
  /**
   * Resource to pass in the OAuth request parameter.
   */
  resource?: string | undefined;
  /**
   * Office 365 subscription plan for your organization, typically Office 365 Enterprise
   */
  planType?: models.SubscriptionPlanOptions | undefined;
  /**
   * Select or create a secret that references your client_secret to pass in the OAuth request parameter.
   */
  textSecret?: string | undefined;
  certOptions?: CertOptions | undefined;
};

export type InputOffice365MsgTraceSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputOffice365MsgTraceType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * URL to use when retrieving report data.
   */
  url?: string | undefined;
  /**
   * How often (in minutes) to run the report. Must divide evenly into 60 minutes to create a predictable schedule, or Save will fail.
   */
  interval?: number | undefined;
  /**
   * Backward offset for the search range's head. (E.g.: -3h@h) Message Trace data is delayed; this parameter (with Date range end) compensates for delay and gaps.
   */
  startDate?: string | undefined;
  /**
   * Backward offset for the search range's tail. (E.g.: -2h@h) Message Trace data is delayed; this parameter (with Date range start) compensates for delay and gaps.
   */
  endDate?: string | undefined;
  /**
   * HTTP request inactivity timeout. Maximum is 2400 (40 minutes); enter 0 to wait indefinitely.
   */
  timeout?: number | undefined;
  /**
   * Disables time filtering of events when a date range is specified.
   */
  disableTimeFilter?: boolean | undefined;
  /**
   * Select authentication method.
   */
  authType?: InputOffice365MsgTraceAuthenticationMethod | undefined;
  /**
   * Reschedule tasks that failed with non-fatal errors
   */
  rescheduleDroppedTasks?: boolean | undefined;
  /**
   * Maximum number of times a task can be rescheduled
   */
  maxTaskReschedule?: number | undefined;
  /**
   * Log Level (verbosity) for collection runtime behavior.
   */
  logLevel?: InputOffice365MsgTraceLogLevel | undefined;
  /**
   * Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.
   */
  jobTimeout?: string | undefined;
  /**
   * How often workers should check in with the scheduler to keep job subscription alive
   */
  keepAliveTime?: number | undefined;
  /**
   * The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.
   */
  maxMissedKeepAlives?: number | undefined;
  /**
   * Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.
   */
  ttl?: string | undefined;
  /**
   * When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.
   */
  ignoreGroupJobsLimit?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  retryRules?: models.RetryRulesType1 | undefined;
  description?: string | undefined;
  /**
   * Username to run Message Trace API call.
   */
  username?: string | undefined;
  /**
   * Password to run Message Trace API call.
   */
  password?: string | undefined;
  /**
   * Select or create a secret that references your credentials.
   */
  credentialsSecret?: string | undefined;
  /**
   * client_secret to pass in the OAuth request parameter.
   */
  clientSecret?: string | undefined;
  /**
   * Directory ID (tenant identifier) in Azure Active Directory.
   */
  tenantId?: string | undefined;
  /**
   * client_id to pass in the OAuth request parameter.
   */
  clientId?: string | undefined;
  /**
   * Resource to pass in the OAuth request parameter.
   */
  resource?: string | undefined;
  /**
   * Office 365 subscription plan for your organization, typically Office 365 Enterprise
   */
  planType?: models.SubscriptionPlanOptions | undefined;
  /**
   * Select or create a secret that references your client_secret to pass in the OAuth request parameter.
   */
  textSecret?: string | undefined;
  certOptions?: CertOptions | undefined;
};

export type InputOffice365MsgTraceSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputOffice365MsgTraceType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * URL to use when retrieving report data.
   */
  url?: string | undefined;
  /**
   * How often (in minutes) to run the report. Must divide evenly into 60 minutes to create a predictable schedule, or Save will fail.
   */
  interval?: number | undefined;
  /**
   * Backward offset for the search range's head. (E.g.: -3h@h) Message Trace data is delayed; this parameter (with Date range end) compensates for delay and gaps.
   */
  startDate?: string | undefined;
  /**
   * Backward offset for the search range's tail. (E.g.: -2h@h) Message Trace data is delayed; this parameter (with Date range start) compensates for delay and gaps.
   */
  endDate?: string | undefined;
  /**
   * HTTP request inactivity timeout. Maximum is 2400 (40 minutes); enter 0 to wait indefinitely.
   */
  timeout?: number | undefined;
  /**
   * Disables time filtering of events when a date range is specified.
   */
  disableTimeFilter?: boolean | undefined;
  /**
   * Select authentication method.
   */
  authType?: InputOffice365MsgTraceAuthenticationMethod | undefined;
  /**
   * Reschedule tasks that failed with non-fatal errors
   */
  rescheduleDroppedTasks?: boolean | undefined;
  /**
   * Maximum number of times a task can be rescheduled
   */
  maxTaskReschedule?: number | undefined;
  /**
   * Log Level (verbosity) for collection runtime behavior.
   */
  logLevel?: InputOffice365MsgTraceLogLevel | undefined;
  /**
   * Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.
   */
  jobTimeout?: string | undefined;
  /**
   * How often workers should check in with the scheduler to keep job subscription alive
   */
  keepAliveTime?: number | undefined;
  /**
   * The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.
   */
  maxMissedKeepAlives?: number | undefined;
  /**
   * Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.
   */
  ttl?: string | undefined;
  /**
   * When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.
   */
  ignoreGroupJobsLimit?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  retryRules?: models.RetryRulesType1 | undefined;
  description?: string | undefined;
  /**
   * Username to run Message Trace API call.
   */
  username?: string | undefined;
  /**
   * Password to run Message Trace API call.
   */
  password?: string | undefined;
  /**
   * Select or create a secret that references your credentials.
   */
  credentialsSecret?: string | undefined;
  /**
   * client_secret to pass in the OAuth request parameter.
   */
  clientSecret?: string | undefined;
  /**
   * Directory ID (tenant identifier) in Azure Active Directory.
   */
  tenantId?: string | undefined;
  /**
   * client_id to pass in the OAuth request parameter.
   */
  clientId?: string | undefined;
  /**
   * Resource to pass in the OAuth request parameter.
   */
  resource?: string | undefined;
  /**
   * Office 365 subscription plan for your organization, typically Office 365 Enterprise
   */
  planType?: models.SubscriptionPlanOptions | undefined;
  /**
   * Select or create a secret that references your client_secret to pass in the OAuth request parameter.
   */
  textSecret?: string | undefined;
  certOptions?: CertOptions | undefined;
};

export type InputOffice365MsgTrace =
  | InputOffice365MsgTraceSendToRoutesTrueWithConnectionsConstraint
  | InputOffice365MsgTraceSendToRoutesFalseWithConnectionsConstraint
  | InputOffice365MsgTracePqEnabledFalseWithPqConstraint
  | InputOffice365MsgTracePqEnabledTrueWithPqConstraint;

export const InputOffice365ServiceType = {
  Office365Service: "office365_service",
} as const;
export type InputOffice365ServiceType = ClosedEnum<
  typeof InputOffice365ServiceType
>;

export type InputOffice365ServiceContentConfig = {
  /**
   * Office 365 Services API Content Type
   */
  contentType?: string | undefined;
  /**
   * If interval type is minutes the value entered must evenly divisible by 60 or save will fail
   */
  description?: string | undefined;
  interval?: number | undefined;
  /**
   * Collector runtime Log Level
   */
  logLevel?: models.LogLevelOptionsContentConfigItems | undefined;
  enabled?: boolean | undefined;
};

export type InputOffice365ServicePqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputOffice365ServiceType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Office 365 subscription plan for your organization, typically Office 365 Enterprise
   */
  planType?: models.SubscriptionPlanOptions | undefined;
  /**
   * Office 365 Azure Tenant ID
   */
  tenantId: string;
  /**
   * Office 365 Azure Application ID
   */
  appId: string;
  /**
   * HTTP request inactivity timeout, use 0 to disable
   */
  timeout?: number | undefined;
  /**
   * How often workers should check in with the scheduler to keep job subscription alive
   */
  keepAliveTime?: number | undefined;
  /**
   * Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.
   */
  jobTimeout?: string | undefined;
  /**
   * The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.
   */
  maxMissedKeepAlives?: number | undefined;
  /**
   * Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.
   */
  ttl?: string | undefined;
  /**
   * When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.
   */
  ignoreGroupJobsLimit?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Enable Office 365 Service Communication API content types and polling intervals. Polling intervals are used to set up search date range and cron schedule, e.g.: * /${interval} * * * *. Because of this, intervals entered for current and historical status must be evenly divisible by 60 to give a predictable schedule.
   */
  contentConfig?: Array<InputOffice365ServiceContentConfig> | undefined;
  retryRules?: models.RetryRulesType1 | undefined;
  /**
   * Enter client secret directly, or select a stored secret
   */
  authType?: models.AuthenticationMethodOptions1 | undefined;
  description?: string | undefined;
  /**
   * Office 365 Azure client secret
   */
  clientSecret?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
};

export type InputOffice365ServicePqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputOffice365ServiceType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Office 365 subscription plan for your organization, typically Office 365 Enterprise
   */
  planType?: models.SubscriptionPlanOptions | undefined;
  /**
   * Office 365 Azure Tenant ID
   */
  tenantId: string;
  /**
   * Office 365 Azure Application ID
   */
  appId: string;
  /**
   * HTTP request inactivity timeout, use 0 to disable
   */
  timeout?: number | undefined;
  /**
   * How often workers should check in with the scheduler to keep job subscription alive
   */
  keepAliveTime?: number | undefined;
  /**
   * Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.
   */
  jobTimeout?: string | undefined;
  /**
   * The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.
   */
  maxMissedKeepAlives?: number | undefined;
  /**
   * Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.
   */
  ttl?: string | undefined;
  /**
   * When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.
   */
  ignoreGroupJobsLimit?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Enable Office 365 Service Communication API content types and polling intervals. Polling intervals are used to set up search date range and cron schedule, e.g.: * /${interval} * * * *. Because of this, intervals entered for current and historical status must be evenly divisible by 60 to give a predictable schedule.
   */
  contentConfig?: Array<InputOffice365ServiceContentConfig> | undefined;
  retryRules?: models.RetryRulesType1 | undefined;
  /**
   * Enter client secret directly, or select a stored secret
   */
  authType?: models.AuthenticationMethodOptions1 | undefined;
  description?: string | undefined;
  /**
   * Office 365 Azure client secret
   */
  clientSecret?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
};

export type InputOffice365ServiceSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputOffice365ServiceType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Office 365 subscription plan for your organization, typically Office 365 Enterprise
   */
  planType?: models.SubscriptionPlanOptions | undefined;
  /**
   * Office 365 Azure Tenant ID
   */
  tenantId: string;
  /**
   * Office 365 Azure Application ID
   */
  appId: string;
  /**
   * HTTP request inactivity timeout, use 0 to disable
   */
  timeout?: number | undefined;
  /**
   * How often workers should check in with the scheduler to keep job subscription alive
   */
  keepAliveTime?: number | undefined;
  /**
   * Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.
   */
  jobTimeout?: string | undefined;
  /**
   * The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.
   */
  maxMissedKeepAlives?: number | undefined;
  /**
   * Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.
   */
  ttl?: string | undefined;
  /**
   * When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.
   */
  ignoreGroupJobsLimit?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Enable Office 365 Service Communication API content types and polling intervals. Polling intervals are used to set up search date range and cron schedule, e.g.: * /${interval} * * * *. Because of this, intervals entered for current and historical status must be evenly divisible by 60 to give a predictable schedule.
   */
  contentConfig?: Array<InputOffice365ServiceContentConfig> | undefined;
  retryRules?: models.RetryRulesType1 | undefined;
  /**
   * Enter client secret directly, or select a stored secret
   */
  authType?: models.AuthenticationMethodOptions1 | undefined;
  description?: string | undefined;
  /**
   * Office 365 Azure client secret
   */
  clientSecret?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
};

export type InputOffice365ServiceSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputOffice365ServiceType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Office 365 subscription plan for your organization, typically Office 365 Enterprise
   */
  planType?: models.SubscriptionPlanOptions | undefined;
  /**
   * Office 365 Azure Tenant ID
   */
  tenantId: string;
  /**
   * Office 365 Azure Application ID
   */
  appId: string;
  /**
   * HTTP request inactivity timeout, use 0 to disable
   */
  timeout?: number | undefined;
  /**
   * How often workers should check in with the scheduler to keep job subscription alive
   */
  keepAliveTime?: number | undefined;
  /**
   * Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.
   */
  jobTimeout?: string | undefined;
  /**
   * The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.
   */
  maxMissedKeepAlives?: number | undefined;
  /**
   * Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.
   */
  ttl?: string | undefined;
  /**
   * When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.
   */
  ignoreGroupJobsLimit?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Enable Office 365 Service Communication API content types and polling intervals. Polling intervals are used to set up search date range and cron schedule, e.g.: * /${interval} * * * *. Because of this, intervals entered for current and historical status must be evenly divisible by 60 to give a predictable schedule.
   */
  contentConfig?: Array<InputOffice365ServiceContentConfig> | undefined;
  retryRules?: models.RetryRulesType1 | undefined;
  /**
   * Enter client secret directly, or select a stored secret
   */
  authType?: models.AuthenticationMethodOptions1 | undefined;
  description?: string | undefined;
  /**
   * Office 365 Azure client secret
   */
  clientSecret?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
};

export type InputOffice365Service =
  | InputOffice365ServiceSendToRoutesTrueWithConnectionsConstraint
  | InputOffice365ServiceSendToRoutesFalseWithConnectionsConstraint
  | InputOffice365ServicePqEnabledFalseWithPqConstraint
  | InputOffice365ServicePqEnabledTrueWithPqConstraint;

export const InputOffice365MgmtType = {
  Office365Mgmt: "office365_mgmt",
} as const;
export type InputOffice365MgmtType = ClosedEnum<typeof InputOffice365MgmtType>;

export type InputOffice365MgmtContentConfig = {
  /**
   * Office 365 Management Activity API Content Type
   */
  contentType?: string | undefined;
  /**
   * If interval type is minutes the value entered must evenly divisible by 60 or save will fail
   */
  description?: string | undefined;
  interval?: number | undefined;
  /**
   * Collector runtime Log Level
   */
  logLevel?: models.LogLevelOptionsContentConfigItems | undefined;
  enabled?: boolean | undefined;
};

export type InputOffice365MgmtPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputOffice365MgmtType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Office 365 subscription plan for your organization, typically Office 365 Enterprise
   */
  planType?: models.SubscriptionPlanOptions | undefined;
  /**
   * Office 365 Azure Tenant ID
   */
  tenantId: string;
  /**
   * Office 365 Azure Application ID
   */
  appId: string;
  /**
   * HTTP request inactivity timeout, use 0 to disable
   */
  timeout?: number | undefined;
  /**
   * How often workers should check in with the scheduler to keep job subscription alive
   */
  keepAliveTime?: number | undefined;
  /**
   * Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.
   */
  jobTimeout?: string | undefined;
  /**
   * The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.
   */
  maxMissedKeepAlives?: number | undefined;
  /**
   * Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.
   */
  ttl?: string | undefined;
  /**
   * When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.
   */
  ignoreGroupJobsLimit?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Optional Publisher Identifier to use in API requests, defaults to tenant id if not defined. For more information see [here](https://docs.microsoft.com/en-us/office/office-365-management-api/office-365-management-activity-api-reference#start-a-subscription)
   */
  publisherIdentifier?: string | undefined;
  /**
   * Enable Office 365 Management Activity API content types and polling intervals. Polling intervals are used to set up search date range and cron schedule, e.g.: * /${interval} * * * *. Because of this, intervals entered must be evenly divisible by 60 to give a predictable schedule.
   */
  contentConfig?: Array<InputOffice365MgmtContentConfig> | undefined;
  /**
   * Use this setting to account for ingestion lag. This is necessary because there can be a lag of 60 - 90 minutes (or longer) before Office 365 events are available for retrieval.
   */
  ingestionLag?: number | undefined;
  retryRules?: models.RetryRulesType1 | undefined;
  /**
   * Enter client secret directly, or select a stored secret
   */
  authType?: models.AuthenticationMethodOptions1 | undefined;
  description?: string | undefined;
  /**
   * Office 365 Azure client secret
   */
  clientSecret?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
};

export type InputOffice365MgmtPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputOffice365MgmtType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Office 365 subscription plan for your organization, typically Office 365 Enterprise
   */
  planType?: models.SubscriptionPlanOptions | undefined;
  /**
   * Office 365 Azure Tenant ID
   */
  tenantId: string;
  /**
   * Office 365 Azure Application ID
   */
  appId: string;
  /**
   * HTTP request inactivity timeout, use 0 to disable
   */
  timeout?: number | undefined;
  /**
   * How often workers should check in with the scheduler to keep job subscription alive
   */
  keepAliveTime?: number | undefined;
  /**
   * Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.
   */
  jobTimeout?: string | undefined;
  /**
   * The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.
   */
  maxMissedKeepAlives?: number | undefined;
  /**
   * Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.
   */
  ttl?: string | undefined;
  /**
   * When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.
   */
  ignoreGroupJobsLimit?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Optional Publisher Identifier to use in API requests, defaults to tenant id if not defined. For more information see [here](https://docs.microsoft.com/en-us/office/office-365-management-api/office-365-management-activity-api-reference#start-a-subscription)
   */
  publisherIdentifier?: string | undefined;
  /**
   * Enable Office 365 Management Activity API content types and polling intervals. Polling intervals are used to set up search date range and cron schedule, e.g.: * /${interval} * * * *. Because of this, intervals entered must be evenly divisible by 60 to give a predictable schedule.
   */
  contentConfig?: Array<InputOffice365MgmtContentConfig> | undefined;
  /**
   * Use this setting to account for ingestion lag. This is necessary because there can be a lag of 60 - 90 minutes (or longer) before Office 365 events are available for retrieval.
   */
  ingestionLag?: number | undefined;
  retryRules?: models.RetryRulesType1 | undefined;
  /**
   * Enter client secret directly, or select a stored secret
   */
  authType?: models.AuthenticationMethodOptions1 | undefined;
  description?: string | undefined;
  /**
   * Office 365 Azure client secret
   */
  clientSecret?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
};

export type InputOffice365MgmtSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputOffice365MgmtType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Office 365 subscription plan for your organization, typically Office 365 Enterprise
   */
  planType?: models.SubscriptionPlanOptions | undefined;
  /**
   * Office 365 Azure Tenant ID
   */
  tenantId: string;
  /**
   * Office 365 Azure Application ID
   */
  appId: string;
  /**
   * HTTP request inactivity timeout, use 0 to disable
   */
  timeout?: number | undefined;
  /**
   * How often workers should check in with the scheduler to keep job subscription alive
   */
  keepAliveTime?: number | undefined;
  /**
   * Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.
   */
  jobTimeout?: string | undefined;
  /**
   * The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.
   */
  maxMissedKeepAlives?: number | undefined;
  /**
   * Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.
   */
  ttl?: string | undefined;
  /**
   * When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.
   */
  ignoreGroupJobsLimit?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Optional Publisher Identifier to use in API requests, defaults to tenant id if not defined. For more information see [here](https://docs.microsoft.com/en-us/office/office-365-management-api/office-365-management-activity-api-reference#start-a-subscription)
   */
  publisherIdentifier?: string | undefined;
  /**
   * Enable Office 365 Management Activity API content types and polling intervals. Polling intervals are used to set up search date range and cron schedule, e.g.: * /${interval} * * * *. Because of this, intervals entered must be evenly divisible by 60 to give a predictable schedule.
   */
  contentConfig?: Array<InputOffice365MgmtContentConfig> | undefined;
  /**
   * Use this setting to account for ingestion lag. This is necessary because there can be a lag of 60 - 90 minutes (or longer) before Office 365 events are available for retrieval.
   */
  ingestionLag?: number | undefined;
  retryRules?: models.RetryRulesType1 | undefined;
  /**
   * Enter client secret directly, or select a stored secret
   */
  authType?: models.AuthenticationMethodOptions1 | undefined;
  description?: string | undefined;
  /**
   * Office 365 Azure client secret
   */
  clientSecret?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
};

export type InputOffice365MgmtSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputOffice365MgmtType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Office 365 subscription plan for your organization, typically Office 365 Enterprise
   */
  planType?: models.SubscriptionPlanOptions | undefined;
  /**
   * Office 365 Azure Tenant ID
   */
  tenantId: string;
  /**
   * Office 365 Azure Application ID
   */
  appId: string;
  /**
   * HTTP request inactivity timeout, use 0 to disable
   */
  timeout?: number | undefined;
  /**
   * How often workers should check in with the scheduler to keep job subscription alive
   */
  keepAliveTime?: number | undefined;
  /**
   * Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.
   */
  jobTimeout?: string | undefined;
  /**
   * The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.
   */
  maxMissedKeepAlives?: number | undefined;
  /**
   * Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.
   */
  ttl?: string | undefined;
  /**
   * When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.
   */
  ignoreGroupJobsLimit?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Optional Publisher Identifier to use in API requests, defaults to tenant id if not defined. For more information see [here](https://docs.microsoft.com/en-us/office/office-365-management-api/office-365-management-activity-api-reference#start-a-subscription)
   */
  publisherIdentifier?: string | undefined;
  /**
   * Enable Office 365 Management Activity API content types and polling intervals. Polling intervals are used to set up search date range and cron schedule, e.g.: * /${interval} * * * *. Because of this, intervals entered must be evenly divisible by 60 to give a predictable schedule.
   */
  contentConfig?: Array<InputOffice365MgmtContentConfig> | undefined;
  /**
   * Use this setting to account for ingestion lag. This is necessary because there can be a lag of 60 - 90 minutes (or longer) before Office 365 events are available for retrieval.
   */
  ingestionLag?: number | undefined;
  retryRules?: models.RetryRulesType1 | undefined;
  /**
   * Enter client secret directly, or select a stored secret
   */
  authType?: models.AuthenticationMethodOptions1 | undefined;
  description?: string | undefined;
  /**
   * Office 365 Azure client secret
   */
  clientSecret?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
};

export type InputOffice365Mgmt =
  | InputOffice365MgmtSendToRoutesTrueWithConnectionsConstraint
  | InputOffice365MgmtSendToRoutesFalseWithConnectionsConstraint
  | InputOffice365MgmtPqEnabledFalseWithPqConstraint
  | InputOffice365MgmtPqEnabledTrueWithPqConstraint;

export const InputEdgePrometheusType = {
  EdgePrometheus: "edge_prometheus",
} as const;
export type InputEdgePrometheusType = ClosedEnum<
  typeof InputEdgePrometheusType
>;

/**
 * Target discovery mechanism. Use static to manually enter a list of targets.
 */
export const InputEdgePrometheusDiscoveryType = {
  /**
   * Static
   */
  Static: "static",
  /**
   * DNS
   */
  Dns: "dns",
  /**
   * AWS EC2
   */
  Ec2: "ec2",
  /**
   * Kubernetes Node
   */
  K8sNode: "k8s-node",
  /**
   * Kubernetes Pods
   */
  K8sPods: "k8s-pods",
} as const;
/**
 * Target discovery mechanism. Use static to manually enter a list of targets.
 */
export type InputEdgePrometheusDiscoveryType = OpenEnum<
  typeof InputEdgePrometheusDiscoveryType
>;

/**
 * Enter credentials directly, or select a stored secret
 */
export const InputEdgePrometheusAuthenticationMethod = {
  Manual: "manual",
  Secret: "secret",
  Kubernetes: "kubernetes",
} as const;
/**
 * Enter credentials directly, or select a stored secret
 */
export type InputEdgePrometheusAuthenticationMethod = OpenEnum<
  typeof InputEdgePrometheusAuthenticationMethod
>;

export type Target = {
  /**
   * Protocol to use when collecting metrics
   */
  protocol?: models.ProtocolOptionsTargetsItems | undefined;
  /**
   * Name of host from which to pull metrics.
   */
  host: string;
  /**
   * The port number in the metrics URL for discovered targets.
   */
  port?: number | undefined;
  /**
   * Path to use when collecting metrics from discovered targets
   */
  path?: string | undefined;
};

export type PodFilter = {
  /**
   * JavaScript expression applied to pods objects. Return 'true' to include it.
   */
  filter: string;
  /**
   * Optional description of this rule's purpose
   */
  description?: string | undefined;
};

export type InputEdgePrometheusPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputEdgePrometheusType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Other dimensions to include in events
   */
  dimensionList?: Array<string> | undefined;
  /**
   * Target discovery mechanism. Use static to manually enter a list of targets.
   */
  discoveryType?: InputEdgePrometheusDiscoveryType | undefined;
  /**
   * How often in seconds to scrape targets for metrics.
   */
  interval?: number | undefined;
  /**
   * Timeout, in milliseconds, before aborting HTTP connection attempts; 1-60000 or 0 to disable
   */
  timeout?: number | undefined;
  persistence?: models.DiskSpoolingType | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Enter credentials directly, or select a stored secret
   */
  authType?: InputEdgePrometheusAuthenticationMethod | undefined;
  description?: string | undefined;
  targets?: Array<Target> | undefined;
  /**
   * DNS record type to resolve
   */
  recordType?: models.RecordTypeOptions | undefined;
  /**
   * The port number in the metrics URL for discovered targets.
   */
  scrapePort?: number | undefined;
  /**
   * List of DNS names to resolve
   */
  nameList?: Array<string> | undefined;
  /**
   * Protocol to use when collecting metrics
   */
  scrapeProtocol?: models.ProtocolOptionsTargetsItems | undefined;
  /**
   * Path to use when collecting metrics from discovered targets
   */
  scrapePath?: string | undefined;
  /**
   * AWS authentication method. Choose Auto to use IAM roles.
   */
  awsAuthenticationMethod?: string | undefined;
  awsApiKey?: string | undefined;
  /**
   * Select or create a stored secret that references your access key and secret key
   */
  awsSecret?: string | undefined;
  /**
   * Use public IP address for discovered targets. Disable to use the private IP address.
   */
  usePublicIp?: boolean | undefined;
  /**
   * Filter to apply when searching for EC2 instances
   */
  searchFilter?: Array<models.ItemsTypeSearchFilter> | undefined;
  awsSecretKey?: string | undefined;
  /**
   * Region where the EC2 is located
   */
  region?: string | undefined;
  /**
   * EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint.
   */
  endpoint?: string | undefined;
  /**
   * Signature version to use for signing EC2 requests
   */
  signatureVersion?: models.SignatureVersionOptions1 | undefined;
  /**
   * Reuse connections between requests, which can improve performance
   */
  reuseConnections?: boolean | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * Use Assume Role credentials to access EC2
   */
  enableAssumeRole?: boolean | undefined;
  /**
   * Amazon Resource Name (ARN) of the role to assume
   */
  assumeRoleArn?: string | undefined;
  /**
   * External ID to use when assuming role
   */
  assumeRoleExternalId?: string | undefined;
  /**
   * Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
   */
  durationSeconds?: number | undefined;
  /**
   * Protocol to use when collecting metrics
   */
  scrapeProtocolExpr?: string | undefined;
  /**
   * The port number in the metrics URL for discovered targets.
   */
  scrapePortExpr?: string | undefined;
  /**
   * Path to use when collecting metrics from discovered targets
   */
  scrapePathExpr?: string | undefined;
  /**
   *   Add rules to decide which pods to discover for metrics.
   *
   * @remarks
   *   Pods are searched if no rules are given or of all the rules'
   *   expressions evaluate to true.
   */
  podFilter?: Array<PodFilter> | undefined;
  /**
   * Username for Prometheus Basic authentication
   */
  username?: string | undefined;
  /**
   * Password for Prometheus Basic authentication
   */
  password?: string | undefined;
  /**
   * Select or create a secret that references your credentials
   */
  credentialsSecret?: string | undefined;
};

export type InputEdgePrometheusPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputEdgePrometheusType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Other dimensions to include in events
   */
  dimensionList?: Array<string> | undefined;
  /**
   * Target discovery mechanism. Use static to manually enter a list of targets.
   */
  discoveryType?: InputEdgePrometheusDiscoveryType | undefined;
  /**
   * How often in seconds to scrape targets for metrics.
   */
  interval?: number | undefined;
  /**
   * Timeout, in milliseconds, before aborting HTTP connection attempts; 1-60000 or 0 to disable
   */
  timeout?: number | undefined;
  persistence?: models.DiskSpoolingType | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Enter credentials directly, or select a stored secret
   */
  authType?: InputEdgePrometheusAuthenticationMethod | undefined;
  description?: string | undefined;
  targets?: Array<Target> | undefined;
  /**
   * DNS record type to resolve
   */
  recordType?: models.RecordTypeOptions | undefined;
  /**
   * The port number in the metrics URL for discovered targets.
   */
  scrapePort?: number | undefined;
  /**
   * List of DNS names to resolve
   */
  nameList?: Array<string> | undefined;
  /**
   * Protocol to use when collecting metrics
   */
  scrapeProtocol?: models.ProtocolOptionsTargetsItems | undefined;
  /**
   * Path to use when collecting metrics from discovered targets
   */
  scrapePath?: string | undefined;
  /**
   * AWS authentication method. Choose Auto to use IAM roles.
   */
  awsAuthenticationMethod?: string | undefined;
  awsApiKey?: string | undefined;
  /**
   * Select or create a stored secret that references your access key and secret key
   */
  awsSecret?: string | undefined;
  /**
   * Use public IP address for discovered targets. Disable to use the private IP address.
   */
  usePublicIp?: boolean | undefined;
  /**
   * Filter to apply when searching for EC2 instances
   */
  searchFilter?: Array<models.ItemsTypeSearchFilter> | undefined;
  awsSecretKey?: string | undefined;
  /**
   * Region where the EC2 is located
   */
  region?: string | undefined;
  /**
   * EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint.
   */
  endpoint?: string | undefined;
  /**
   * Signature version to use for signing EC2 requests
   */
  signatureVersion?: models.SignatureVersionOptions1 | undefined;
  /**
   * Reuse connections between requests, which can improve performance
   */
  reuseConnections?: boolean | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * Use Assume Role credentials to access EC2
   */
  enableAssumeRole?: boolean | undefined;
  /**
   * Amazon Resource Name (ARN) of the role to assume
   */
  assumeRoleArn?: string | undefined;
  /**
   * External ID to use when assuming role
   */
  assumeRoleExternalId?: string | undefined;
  /**
   * Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
   */
  durationSeconds?: number | undefined;
  /**
   * Protocol to use when collecting metrics
   */
  scrapeProtocolExpr?: string | undefined;
  /**
   * The port number in the metrics URL for discovered targets.
   */
  scrapePortExpr?: string | undefined;
  /**
   * Path to use when collecting metrics from discovered targets
   */
  scrapePathExpr?: string | undefined;
  /**
   *   Add rules to decide which pods to discover for metrics.
   *
   * @remarks
   *   Pods are searched if no rules are given or of all the rules'
   *   expressions evaluate to true.
   */
  podFilter?: Array<PodFilter> | undefined;
  /**
   * Username for Prometheus Basic authentication
   */
  username?: string | undefined;
  /**
   * Password for Prometheus Basic authentication
   */
  password?: string | undefined;
  /**
   * Select or create a secret that references your credentials
   */
  credentialsSecret?: string | undefined;
};

export type InputEdgePrometheusSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputEdgePrometheusType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Other dimensions to include in events
   */
  dimensionList?: Array<string> | undefined;
  /**
   * Target discovery mechanism. Use static to manually enter a list of targets.
   */
  discoveryType?: InputEdgePrometheusDiscoveryType | undefined;
  /**
   * How often in seconds to scrape targets for metrics.
   */
  interval?: number | undefined;
  /**
   * Timeout, in milliseconds, before aborting HTTP connection attempts; 1-60000 or 0 to disable
   */
  timeout?: number | undefined;
  persistence?: models.DiskSpoolingType | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Enter credentials directly, or select a stored secret
   */
  authType?: InputEdgePrometheusAuthenticationMethod | undefined;
  description?: string | undefined;
  targets?: Array<Target> | undefined;
  /**
   * DNS record type to resolve
   */
  recordType?: models.RecordTypeOptions | undefined;
  /**
   * The port number in the metrics URL for discovered targets.
   */
  scrapePort?: number | undefined;
  /**
   * List of DNS names to resolve
   */
  nameList?: Array<string> | undefined;
  /**
   * Protocol to use when collecting metrics
   */
  scrapeProtocol?: models.ProtocolOptionsTargetsItems | undefined;
  /**
   * Path to use when collecting metrics from discovered targets
   */
  scrapePath?: string | undefined;
  /**
   * AWS authentication method. Choose Auto to use IAM roles.
   */
  awsAuthenticationMethod?: string | undefined;
  awsApiKey?: string | undefined;
  /**
   * Select or create a stored secret that references your access key and secret key
   */
  awsSecret?: string | undefined;
  /**
   * Use public IP address for discovered targets. Disable to use the private IP address.
   */
  usePublicIp?: boolean | undefined;
  /**
   * Filter to apply when searching for EC2 instances
   */
  searchFilter?: Array<models.ItemsTypeSearchFilter> | undefined;
  awsSecretKey?: string | undefined;
  /**
   * Region where the EC2 is located
   */
  region?: string | undefined;
  /**
   * EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint.
   */
  endpoint?: string | undefined;
  /**
   * Signature version to use for signing EC2 requests
   */
  signatureVersion?: models.SignatureVersionOptions1 | undefined;
  /**
   * Reuse connections between requests, which can improve performance
   */
  reuseConnections?: boolean | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * Use Assume Role credentials to access EC2
   */
  enableAssumeRole?: boolean | undefined;
  /**
   * Amazon Resource Name (ARN) of the role to assume
   */
  assumeRoleArn?: string | undefined;
  /**
   * External ID to use when assuming role
   */
  assumeRoleExternalId?: string | undefined;
  /**
   * Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
   */
  durationSeconds?: number | undefined;
  /**
   * Protocol to use when collecting metrics
   */
  scrapeProtocolExpr?: string | undefined;
  /**
   * The port number in the metrics URL for discovered targets.
   */
  scrapePortExpr?: string | undefined;
  /**
   * Path to use when collecting metrics from discovered targets
   */
  scrapePathExpr?: string | undefined;
  /**
   *   Add rules to decide which pods to discover for metrics.
   *
   * @remarks
   *   Pods are searched if no rules are given or of all the rules'
   *   expressions evaluate to true.
   */
  podFilter?: Array<PodFilter> | undefined;
  /**
   * Username for Prometheus Basic authentication
   */
  username?: string | undefined;
  /**
   * Password for Prometheus Basic authentication
   */
  password?: string | undefined;
  /**
   * Select or create a secret that references your credentials
   */
  credentialsSecret?: string | undefined;
};

export type InputEdgePrometheusSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputEdgePrometheusType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Other dimensions to include in events
   */
  dimensionList?: Array<string> | undefined;
  /**
   * Target discovery mechanism. Use static to manually enter a list of targets.
   */
  discoveryType?: InputEdgePrometheusDiscoveryType | undefined;
  /**
   * How often in seconds to scrape targets for metrics.
   */
  interval?: number | undefined;
  /**
   * Timeout, in milliseconds, before aborting HTTP connection attempts; 1-60000 or 0 to disable
   */
  timeout?: number | undefined;
  persistence?: models.DiskSpoolingType | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Enter credentials directly, or select a stored secret
   */
  authType?: InputEdgePrometheusAuthenticationMethod | undefined;
  description?: string | undefined;
  targets?: Array<Target> | undefined;
  /**
   * DNS record type to resolve
   */
  recordType?: models.RecordTypeOptions | undefined;
  /**
   * The port number in the metrics URL for discovered targets.
   */
  scrapePort?: number | undefined;
  /**
   * List of DNS names to resolve
   */
  nameList?: Array<string> | undefined;
  /**
   * Protocol to use when collecting metrics
   */
  scrapeProtocol?: models.ProtocolOptionsTargetsItems | undefined;
  /**
   * Path to use when collecting metrics from discovered targets
   */
  scrapePath?: string | undefined;
  /**
   * AWS authentication method. Choose Auto to use IAM roles.
   */
  awsAuthenticationMethod?: string | undefined;
  awsApiKey?: string | undefined;
  /**
   * Select or create a stored secret that references your access key and secret key
   */
  awsSecret?: string | undefined;
  /**
   * Use public IP address for discovered targets. Disable to use the private IP address.
   */
  usePublicIp?: boolean | undefined;
  /**
   * Filter to apply when searching for EC2 instances
   */
  searchFilter?: Array<models.ItemsTypeSearchFilter> | undefined;
  awsSecretKey?: string | undefined;
  /**
   * Region where the EC2 is located
   */
  region?: string | undefined;
  /**
   * EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint.
   */
  endpoint?: string | undefined;
  /**
   * Signature version to use for signing EC2 requests
   */
  signatureVersion?: models.SignatureVersionOptions1 | undefined;
  /**
   * Reuse connections between requests, which can improve performance
   */
  reuseConnections?: boolean | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * Use Assume Role credentials to access EC2
   */
  enableAssumeRole?: boolean | undefined;
  /**
   * Amazon Resource Name (ARN) of the role to assume
   */
  assumeRoleArn?: string | undefined;
  /**
   * External ID to use when assuming role
   */
  assumeRoleExternalId?: string | undefined;
  /**
   * Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
   */
  durationSeconds?: number | undefined;
  /**
   * Protocol to use when collecting metrics
   */
  scrapeProtocolExpr?: string | undefined;
  /**
   * The port number in the metrics URL for discovered targets.
   */
  scrapePortExpr?: string | undefined;
  /**
   * Path to use when collecting metrics from discovered targets
   */
  scrapePathExpr?: string | undefined;
  /**
   *   Add rules to decide which pods to discover for metrics.
   *
   * @remarks
   *   Pods are searched if no rules are given or of all the rules'
   *   expressions evaluate to true.
   */
  podFilter?: Array<PodFilter> | undefined;
  /**
   * Username for Prometheus Basic authentication
   */
  username?: string | undefined;
  /**
   * Password for Prometheus Basic authentication
   */
  password?: string | undefined;
  /**
   * Select or create a secret that references your credentials
   */
  credentialsSecret?: string | undefined;
};

export type InputEdgePrometheus =
  | InputEdgePrometheusSendToRoutesTrueWithConnectionsConstraint
  | InputEdgePrometheusSendToRoutesFalseWithConnectionsConstraint
  | InputEdgePrometheusPqEnabledFalseWithPqConstraint
  | InputEdgePrometheusPqEnabledTrueWithPqConstraint;

export const InputPrometheusType = {
  Prometheus: "prometheus",
} as const;
export type InputPrometheusType = ClosedEnum<typeof InputPrometheusType>;

/**
 * Target discovery mechanism. Use static to manually enter a list of targets.
 */
export const InputPrometheusDiscoveryType = {
  /**
   * Static
   */
  Static: "static",
  /**
   * DNS
   */
  Dns: "dns",
  /**
   * AWS EC2
   */
  Ec2: "ec2",
} as const;
/**
 * Target discovery mechanism. Use static to manually enter a list of targets.
 */
export type InputPrometheusDiscoveryType = OpenEnum<
  typeof InputPrometheusDiscoveryType
>;

/**
 * Collector runtime log level
 */
export const InputPrometheusLogLevel = {
  Error: "error",
  Warn: "warn",
  Info: "info",
  Debug: "debug",
} as const;
/**
 * Collector runtime log level
 */
export type InputPrometheusLogLevel = OpenEnum<typeof InputPrometheusLogLevel>;

/**
 * Protocol to use when collecting metrics
 */
export const MetricsProtocol = {
  Http: "http",
  Https: "https",
} as const;
/**
 * Protocol to use when collecting metrics
 */
export type MetricsProtocol = OpenEnum<typeof MetricsProtocol>;

export type InputPrometheusPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputPrometheusType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Other dimensions to include in events
   */
  dimensionList?: Array<string> | undefined;
  /**
   * Target discovery mechanism. Use static to manually enter a list of targets.
   */
  discoveryType?: InputPrometheusDiscoveryType | undefined;
  /**
   * How often, in minutes, to scrape targets for metrics. Maximum of 60 minutes. 60 must be evenly divisible by the value you enter.
   */
  interval?: number | undefined;
  /**
   * Collector runtime log level
   */
  logLevel?: InputPrometheusLogLevel | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * Time, in seconds, before aborting HTTP connection attempts; use 0 for no timeout
   */
  timeout?: number | undefined;
  /**
   * How often workers should check in with the scheduler to keep job subscription alive
   */
  keepAliveTime?: number | undefined;
  /**
   * Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.
   */
  jobTimeout?: string | undefined;
  /**
   * The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.
   */
  maxMissedKeepAlives?: number | undefined;
  /**
   * Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.
   */
  ttl?: string | undefined;
  /**
   * When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.
   */
  ignoreGroupJobsLimit?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Enter credentials directly, or select a stored secret
   */
  authType?: models.AuthenticationMethodOptionsSasl | undefined;
  description?: string | undefined;
  /**
   * List of Prometheus targets to pull metrics from. Values can be in URL or host[:port] format. For example: http://localhost:9090/metrics, localhost:9090, or localhost. In cases where just host[:port] is specified, the endpoint will resolve to 'http://host[:port]/metrics'.
   */
  targetList?: Array<string> | undefined;
  /**
   * DNS record type to resolve
   */
  recordType?: models.RecordTypeOptions | undefined;
  /**
   * The port number in the metrics URL for discovered targets
   */
  scrapePort?: number | undefined;
  /**
   * List of DNS names to resolve
   */
  nameList?: Array<string> | undefined;
  /**
   * Protocol to use when collecting metrics
   */
  scrapeProtocol?: MetricsProtocol | undefined;
  /**
   * Path to use when collecting metrics from discovered targets
   */
  scrapePath?: string | undefined;
  /**
   * AWS authentication method. Choose Auto to use IAM roles.
   */
  awsAuthenticationMethod?: string | undefined;
  awsApiKey?: string | undefined;
  /**
   * Select or create a stored secret that references your access key and secret key
   */
  awsSecret?: string | undefined;
  /**
   * Use public IP address for discovered targets. Disable to use the private IP address.
   */
  usePublicIp?: boolean | undefined;
  /**
   * Filter to apply when searching for EC2 instances
   */
  searchFilter?: Array<models.ItemsTypeSearchFilter> | undefined;
  awsSecretKey?: string | undefined;
  /**
   * Region where the EC2 is located
   */
  region?: string | undefined;
  /**
   * EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint.
   */
  endpoint?: string | undefined;
  /**
   * Signature version to use for signing EC2 requests
   */
  signatureVersion?: models.SignatureVersionOptions1 | undefined;
  /**
   * Reuse connections between requests, which can improve performance
   */
  reuseConnections?: boolean | undefined;
  /**
   * Use Assume Role credentials to access EC2
   */
  enableAssumeRole?: boolean | undefined;
  /**
   * Amazon Resource Name (ARN) of the role to assume
   */
  assumeRoleArn?: string | undefined;
  /**
   * External ID to use when assuming role
   */
  assumeRoleExternalId?: string | undefined;
  /**
   * Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
   */
  durationSeconds?: number | undefined;
  /**
   * Username for Prometheus Basic authentication
   */
  username?: string | undefined;
  /**
   * Password for Prometheus Basic authentication
   */
  password?: string | undefined;
  /**
   * Select or create a secret that references your credentials
   */
  credentialsSecret?: string | undefined;
};

export type InputPrometheusPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputPrometheusType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Other dimensions to include in events
   */
  dimensionList?: Array<string> | undefined;
  /**
   * Target discovery mechanism. Use static to manually enter a list of targets.
   */
  discoveryType?: InputPrometheusDiscoveryType | undefined;
  /**
   * How often, in minutes, to scrape targets for metrics. Maximum of 60 minutes. 60 must be evenly divisible by the value you enter.
   */
  interval?: number | undefined;
  /**
   * Collector runtime log level
   */
  logLevel?: InputPrometheusLogLevel | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * Time, in seconds, before aborting HTTP connection attempts; use 0 for no timeout
   */
  timeout?: number | undefined;
  /**
   * How often workers should check in with the scheduler to keep job subscription alive
   */
  keepAliveTime?: number | undefined;
  /**
   * Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.
   */
  jobTimeout?: string | undefined;
  /**
   * The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.
   */
  maxMissedKeepAlives?: number | undefined;
  /**
   * Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.
   */
  ttl?: string | undefined;
  /**
   * When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.
   */
  ignoreGroupJobsLimit?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Enter credentials directly, or select a stored secret
   */
  authType?: models.AuthenticationMethodOptionsSasl | undefined;
  description?: string | undefined;
  /**
   * List of Prometheus targets to pull metrics from. Values can be in URL or host[:port] format. For example: http://localhost:9090/metrics, localhost:9090, or localhost. In cases where just host[:port] is specified, the endpoint will resolve to 'http://host[:port]/metrics'.
   */
  targetList?: Array<string> | undefined;
  /**
   * DNS record type to resolve
   */
  recordType?: models.RecordTypeOptions | undefined;
  /**
   * The port number in the metrics URL for discovered targets
   */
  scrapePort?: number | undefined;
  /**
   * List of DNS names to resolve
   */
  nameList?: Array<string> | undefined;
  /**
   * Protocol to use when collecting metrics
   */
  scrapeProtocol?: MetricsProtocol | undefined;
  /**
   * Path to use when collecting metrics from discovered targets
   */
  scrapePath?: string | undefined;
  /**
   * AWS authentication method. Choose Auto to use IAM roles.
   */
  awsAuthenticationMethod?: string | undefined;
  awsApiKey?: string | undefined;
  /**
   * Select or create a stored secret that references your access key and secret key
   */
  awsSecret?: string | undefined;
  /**
   * Use public IP address for discovered targets. Disable to use the private IP address.
   */
  usePublicIp?: boolean | undefined;
  /**
   * Filter to apply when searching for EC2 instances
   */
  searchFilter?: Array<models.ItemsTypeSearchFilter> | undefined;
  awsSecretKey?: string | undefined;
  /**
   * Region where the EC2 is located
   */
  region?: string | undefined;
  /**
   * EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint.
   */
  endpoint?: string | undefined;
  /**
   * Signature version to use for signing EC2 requests
   */
  signatureVersion?: models.SignatureVersionOptions1 | undefined;
  /**
   * Reuse connections between requests, which can improve performance
   */
  reuseConnections?: boolean | undefined;
  /**
   * Use Assume Role credentials to access EC2
   */
  enableAssumeRole?: boolean | undefined;
  /**
   * Amazon Resource Name (ARN) of the role to assume
   */
  assumeRoleArn?: string | undefined;
  /**
   * External ID to use when assuming role
   */
  assumeRoleExternalId?: string | undefined;
  /**
   * Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
   */
  durationSeconds?: number | undefined;
  /**
   * Username for Prometheus Basic authentication
   */
  username?: string | undefined;
  /**
   * Password for Prometheus Basic authentication
   */
  password?: string | undefined;
  /**
   * Select or create a secret that references your credentials
   */
  credentialsSecret?: string | undefined;
};

export type InputPrometheusSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputPrometheusType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Other dimensions to include in events
   */
  dimensionList?: Array<string> | undefined;
  /**
   * Target discovery mechanism. Use static to manually enter a list of targets.
   */
  discoveryType?: InputPrometheusDiscoveryType | undefined;
  /**
   * How often, in minutes, to scrape targets for metrics. Maximum of 60 minutes. 60 must be evenly divisible by the value you enter.
   */
  interval?: number | undefined;
  /**
   * Collector runtime log level
   */
  logLevel?: InputPrometheusLogLevel | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * Time, in seconds, before aborting HTTP connection attempts; use 0 for no timeout
   */
  timeout?: number | undefined;
  /**
   * How often workers should check in with the scheduler to keep job subscription alive
   */
  keepAliveTime?: number | undefined;
  /**
   * Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.
   */
  jobTimeout?: string | undefined;
  /**
   * The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.
   */
  maxMissedKeepAlives?: number | undefined;
  /**
   * Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.
   */
  ttl?: string | undefined;
  /**
   * When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.
   */
  ignoreGroupJobsLimit?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Enter credentials directly, or select a stored secret
   */
  authType?: models.AuthenticationMethodOptionsSasl | undefined;
  description?: string | undefined;
  /**
   * List of Prometheus targets to pull metrics from. Values can be in URL or host[:port] format. For example: http://localhost:9090/metrics, localhost:9090, or localhost. In cases where just host[:port] is specified, the endpoint will resolve to 'http://host[:port]/metrics'.
   */
  targetList?: Array<string> | undefined;
  /**
   * DNS record type to resolve
   */
  recordType?: models.RecordTypeOptions | undefined;
  /**
   * The port number in the metrics URL for discovered targets
   */
  scrapePort?: number | undefined;
  /**
   * List of DNS names to resolve
   */
  nameList?: Array<string> | undefined;
  /**
   * Protocol to use when collecting metrics
   */
  scrapeProtocol?: MetricsProtocol | undefined;
  /**
   * Path to use when collecting metrics from discovered targets
   */
  scrapePath?: string | undefined;
  /**
   * AWS authentication method. Choose Auto to use IAM roles.
   */
  awsAuthenticationMethod?: string | undefined;
  awsApiKey?: string | undefined;
  /**
   * Select or create a stored secret that references your access key and secret key
   */
  awsSecret?: string | undefined;
  /**
   * Use public IP address for discovered targets. Disable to use the private IP address.
   */
  usePublicIp?: boolean | undefined;
  /**
   * Filter to apply when searching for EC2 instances
   */
  searchFilter?: Array<models.ItemsTypeSearchFilter> | undefined;
  awsSecretKey?: string | undefined;
  /**
   * Region where the EC2 is located
   */
  region?: string | undefined;
  /**
   * EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint.
   */
  endpoint?: string | undefined;
  /**
   * Signature version to use for signing EC2 requests
   */
  signatureVersion?: models.SignatureVersionOptions1 | undefined;
  /**
   * Reuse connections between requests, which can improve performance
   */
  reuseConnections?: boolean | undefined;
  /**
   * Use Assume Role credentials to access EC2
   */
  enableAssumeRole?: boolean | undefined;
  /**
   * Amazon Resource Name (ARN) of the role to assume
   */
  assumeRoleArn?: string | undefined;
  /**
   * External ID to use when assuming role
   */
  assumeRoleExternalId?: string | undefined;
  /**
   * Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
   */
  durationSeconds?: number | undefined;
  /**
   * Username for Prometheus Basic authentication
   */
  username?: string | undefined;
  /**
   * Password for Prometheus Basic authentication
   */
  password?: string | undefined;
  /**
   * Select or create a secret that references your credentials
   */
  credentialsSecret?: string | undefined;
};

export type InputPrometheusSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputPrometheusType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Other dimensions to include in events
   */
  dimensionList?: Array<string> | undefined;
  /**
   * Target discovery mechanism. Use static to manually enter a list of targets.
   */
  discoveryType?: InputPrometheusDiscoveryType | undefined;
  /**
   * How often, in minutes, to scrape targets for metrics. Maximum of 60 minutes. 60 must be evenly divisible by the value you enter.
   */
  interval?: number | undefined;
  /**
   * Collector runtime log level
   */
  logLevel?: InputPrometheusLogLevel | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * Time, in seconds, before aborting HTTP connection attempts; use 0 for no timeout
   */
  timeout?: number | undefined;
  /**
   * How often workers should check in with the scheduler to keep job subscription alive
   */
  keepAliveTime?: number | undefined;
  /**
   * Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.
   */
  jobTimeout?: string | undefined;
  /**
   * The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.
   */
  maxMissedKeepAlives?: number | undefined;
  /**
   * Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.
   */
  ttl?: string | undefined;
  /**
   * When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.
   */
  ignoreGroupJobsLimit?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Enter credentials directly, or select a stored secret
   */
  authType?: models.AuthenticationMethodOptionsSasl | undefined;
  description?: string | undefined;
  /**
   * List of Prometheus targets to pull metrics from. Values can be in URL or host[:port] format. For example: http://localhost:9090/metrics, localhost:9090, or localhost. In cases where just host[:port] is specified, the endpoint will resolve to 'http://host[:port]/metrics'.
   */
  targetList?: Array<string> | undefined;
  /**
   * DNS record type to resolve
   */
  recordType?: models.RecordTypeOptions | undefined;
  /**
   * The port number in the metrics URL for discovered targets
   */
  scrapePort?: number | undefined;
  /**
   * List of DNS names to resolve
   */
  nameList?: Array<string> | undefined;
  /**
   * Protocol to use when collecting metrics
   */
  scrapeProtocol?: MetricsProtocol | undefined;
  /**
   * Path to use when collecting metrics from discovered targets
   */
  scrapePath?: string | undefined;
  /**
   * AWS authentication method. Choose Auto to use IAM roles.
   */
  awsAuthenticationMethod?: string | undefined;
  awsApiKey?: string | undefined;
  /**
   * Select or create a stored secret that references your access key and secret key
   */
  awsSecret?: string | undefined;
  /**
   * Use public IP address for discovered targets. Disable to use the private IP address.
   */
  usePublicIp?: boolean | undefined;
  /**
   * Filter to apply when searching for EC2 instances
   */
  searchFilter?: Array<models.ItemsTypeSearchFilter> | undefined;
  awsSecretKey?: string | undefined;
  /**
   * Region where the EC2 is located
   */
  region?: string | undefined;
  /**
   * EC2 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to EC2-compatible endpoint.
   */
  endpoint?: string | undefined;
  /**
   * Signature version to use for signing EC2 requests
   */
  signatureVersion?: models.SignatureVersionOptions1 | undefined;
  /**
   * Reuse connections between requests, which can improve performance
   */
  reuseConnections?: boolean | undefined;
  /**
   * Use Assume Role credentials to access EC2
   */
  enableAssumeRole?: boolean | undefined;
  /**
   * Amazon Resource Name (ARN) of the role to assume
   */
  assumeRoleArn?: string | undefined;
  /**
   * External ID to use when assuming role
   */
  assumeRoleExternalId?: string | undefined;
  /**
   * Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
   */
  durationSeconds?: number | undefined;
  /**
   * Username for Prometheus Basic authentication
   */
  username?: string | undefined;
  /**
   * Password for Prometheus Basic authentication
   */
  password?: string | undefined;
  /**
   * Select or create a secret that references your credentials
   */
  credentialsSecret?: string | undefined;
};

export type InputPrometheus =
  | InputPrometheusSendToRoutesTrueWithConnectionsConstraint
  | InputPrometheusSendToRoutesFalseWithConnectionsConstraint
  | InputPrometheusPqEnabledFalseWithPqConstraint
  | InputPrometheusPqEnabledTrueWithPqConstraint;

export const InputPrometheusRwType = {
  PrometheusRw: "prometheus_rw",
} as const;
export type InputPrometheusRwType = ClosedEnum<typeof InputPrometheusRwType>;

export type InputPrometheusRwPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputPrometheusRwType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Absolute path on which to listen for Prometheus requests. Defaults to /write, which will expand as: http://<yourupstreamURL>:<yourport>/write.
   */
  prometheusAPI?: string | undefined;
  /**
   * Remote Write authentication type
   */
  authType?: models.AuthenticationTypeOptionsPrometheusAuth | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
  username?: string | undefined;
  password?: string | undefined;
  /**
   * Bearer token to include in the authorization header
   */
  token?: string | undefined;
  /**
   * Select or create a secret that references your credentials
   */
  credentialsSecret?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
  /**
   * URL for OAuth
   */
  loginUrl?: string | undefined;
  /**
   * Secret parameter name to pass in request body
   */
  secretParamName?: string | undefined;
  /**
   * Secret parameter value to pass in request body
   */
  secret?: string | undefined;
  /**
   * Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
   */
  tokenAttributeName?: string | undefined;
  /**
   * JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.
   */
  authHeaderExpr?: string | undefined;
  /**
   * How often the OAuth token should be refreshed.
   */
  tokenTimeoutSecs?: number | undefined;
  /**
   * Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
   */
  oauthParams?: Array<models.ItemsTypeOauthParams> | undefined;
  /**
   * Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
   */
  oauthHeaders?: Array<models.ItemsTypeOauthHeaders> | undefined;
};

export type InputPrometheusRwPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputPrometheusRwType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Absolute path on which to listen for Prometheus requests. Defaults to /write, which will expand as: http://<yourupstreamURL>:<yourport>/write.
   */
  prometheusAPI?: string | undefined;
  /**
   * Remote Write authentication type
   */
  authType?: models.AuthenticationTypeOptionsPrometheusAuth | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
  username?: string | undefined;
  password?: string | undefined;
  /**
   * Bearer token to include in the authorization header
   */
  token?: string | undefined;
  /**
   * Select or create a secret that references your credentials
   */
  credentialsSecret?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
  /**
   * URL for OAuth
   */
  loginUrl?: string | undefined;
  /**
   * Secret parameter name to pass in request body
   */
  secretParamName?: string | undefined;
  /**
   * Secret parameter value to pass in request body
   */
  secret?: string | undefined;
  /**
   * Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
   */
  tokenAttributeName?: string | undefined;
  /**
   * JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.
   */
  authHeaderExpr?: string | undefined;
  /**
   * How often the OAuth token should be refreshed.
   */
  tokenTimeoutSecs?: number | undefined;
  /**
   * Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
   */
  oauthParams?: Array<models.ItemsTypeOauthParams> | undefined;
  /**
   * Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
   */
  oauthHeaders?: Array<models.ItemsTypeOauthHeaders> | undefined;
};

export type InputPrometheusRwSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputPrometheusRwType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Absolute path on which to listen for Prometheus requests. Defaults to /write, which will expand as: http://<yourupstreamURL>:<yourport>/write.
   */
  prometheusAPI?: string | undefined;
  /**
   * Remote Write authentication type
   */
  authType?: models.AuthenticationTypeOptionsPrometheusAuth | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
  username?: string | undefined;
  password?: string | undefined;
  /**
   * Bearer token to include in the authorization header
   */
  token?: string | undefined;
  /**
   * Select or create a secret that references your credentials
   */
  credentialsSecret?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
  /**
   * URL for OAuth
   */
  loginUrl?: string | undefined;
  /**
   * Secret parameter name to pass in request body
   */
  secretParamName?: string | undefined;
  /**
   * Secret parameter value to pass in request body
   */
  secret?: string | undefined;
  /**
   * Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
   */
  tokenAttributeName?: string | undefined;
  /**
   * JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.
   */
  authHeaderExpr?: string | undefined;
  /**
   * How often the OAuth token should be refreshed.
   */
  tokenTimeoutSecs?: number | undefined;
  /**
   * Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
   */
  oauthParams?: Array<models.ItemsTypeOauthParams> | undefined;
  /**
   * Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
   */
  oauthHeaders?: Array<models.ItemsTypeOauthHeaders> | undefined;
};

export type InputPrometheusRwSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputPrometheusRwType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Absolute path on which to listen for Prometheus requests. Defaults to /write, which will expand as: http://<yourupstreamURL>:<yourport>/write.
   */
  prometheusAPI?: string | undefined;
  /**
   * Remote Write authentication type
   */
  authType?: models.AuthenticationTypeOptionsPrometheusAuth | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
  username?: string | undefined;
  password?: string | undefined;
  /**
   * Bearer token to include in the authorization header
   */
  token?: string | undefined;
  /**
   * Select or create a secret that references your credentials
   */
  credentialsSecret?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
  /**
   * URL for OAuth
   */
  loginUrl?: string | undefined;
  /**
   * Secret parameter name to pass in request body
   */
  secretParamName?: string | undefined;
  /**
   * Secret parameter value to pass in request body
   */
  secret?: string | undefined;
  /**
   * Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
   */
  tokenAttributeName?: string | undefined;
  /**
   * JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.
   */
  authHeaderExpr?: string | undefined;
  /**
   * How often the OAuth token should be refreshed.
   */
  tokenTimeoutSecs?: number | undefined;
  /**
   * Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
   */
  oauthParams?: Array<models.ItemsTypeOauthParams> | undefined;
  /**
   * Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
   */
  oauthHeaders?: Array<models.ItemsTypeOauthHeaders> | undefined;
};

export type InputPrometheusRw =
  | InputPrometheusRwSendToRoutesTrueWithConnectionsConstraint
  | InputPrometheusRwSendToRoutesFalseWithConnectionsConstraint
  | InputPrometheusRwPqEnabledFalseWithPqConstraint
  | InputPrometheusRwPqEnabledTrueWithPqConstraint;

export const InputLokiType = {
  Loki: "loki",
} as const;
export type InputLokiType = ClosedEnum<typeof InputLokiType>;

export type InputLokiPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputLokiType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Absolute path on which to listen for Loki logs requests. Defaults to /loki/api/v1/push, which will (in this example) expand as: 'http://<yourupstreamURL>:<yourport>/loki/api/v1/push'.
   */
  lokiAPI?: string | undefined;
  /**
   * Loki logs authentication type
   */
  authType?: models.AuthenticationTypeOptionsLokiAuth | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
  username?: string | undefined;
  password?: string | undefined;
  /**
   * Bearer token to include in the authorization header
   */
  token?: string | undefined;
  /**
   * Select or create a secret that references your credentials
   */
  credentialsSecret?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
  /**
   * URL for OAuth
   */
  loginUrl?: string | undefined;
  /**
   * Secret parameter name to pass in request body
   */
  secretParamName?: string | undefined;
  /**
   * Secret parameter value to pass in request body
   */
  secret?: string | undefined;
  /**
   * Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
   */
  tokenAttributeName?: string | undefined;
  /**
   * JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.
   */
  authHeaderExpr?: string | undefined;
  /**
   * How often the OAuth token should be refreshed.
   */
  tokenTimeoutSecs?: number | undefined;
  /**
   * Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
   */
  oauthParams?: Array<models.ItemsTypeOauthParams> | undefined;
  /**
   * Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
   */
  oauthHeaders?: Array<models.ItemsTypeOauthHeaders> | undefined;
};

export type InputLokiPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputLokiType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Absolute path on which to listen for Loki logs requests. Defaults to /loki/api/v1/push, which will (in this example) expand as: 'http://<yourupstreamURL>:<yourport>/loki/api/v1/push'.
   */
  lokiAPI?: string | undefined;
  /**
   * Loki logs authentication type
   */
  authType?: models.AuthenticationTypeOptionsLokiAuth | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
  username?: string | undefined;
  password?: string | undefined;
  /**
   * Bearer token to include in the authorization header
   */
  token?: string | undefined;
  /**
   * Select or create a secret that references your credentials
   */
  credentialsSecret?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
  /**
   * URL for OAuth
   */
  loginUrl?: string | undefined;
  /**
   * Secret parameter name to pass in request body
   */
  secretParamName?: string | undefined;
  /**
   * Secret parameter value to pass in request body
   */
  secret?: string | undefined;
  /**
   * Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
   */
  tokenAttributeName?: string | undefined;
  /**
   * JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.
   */
  authHeaderExpr?: string | undefined;
  /**
   * How often the OAuth token should be refreshed.
   */
  tokenTimeoutSecs?: number | undefined;
  /**
   * Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
   */
  oauthParams?: Array<models.ItemsTypeOauthParams> | undefined;
  /**
   * Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
   */
  oauthHeaders?: Array<models.ItemsTypeOauthHeaders> | undefined;
};

export type InputLokiSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputLokiType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Absolute path on which to listen for Loki logs requests. Defaults to /loki/api/v1/push, which will (in this example) expand as: 'http://<yourupstreamURL>:<yourport>/loki/api/v1/push'.
   */
  lokiAPI?: string | undefined;
  /**
   * Loki logs authentication type
   */
  authType?: models.AuthenticationTypeOptionsLokiAuth | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
  username?: string | undefined;
  password?: string | undefined;
  /**
   * Bearer token to include in the authorization header
   */
  token?: string | undefined;
  /**
   * Select or create a secret that references your credentials
   */
  credentialsSecret?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
  /**
   * URL for OAuth
   */
  loginUrl?: string | undefined;
  /**
   * Secret parameter name to pass in request body
   */
  secretParamName?: string | undefined;
  /**
   * Secret parameter value to pass in request body
   */
  secret?: string | undefined;
  /**
   * Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
   */
  tokenAttributeName?: string | undefined;
  /**
   * JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.
   */
  authHeaderExpr?: string | undefined;
  /**
   * How often the OAuth token should be refreshed.
   */
  tokenTimeoutSecs?: number | undefined;
  /**
   * Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
   */
  oauthParams?: Array<models.ItemsTypeOauthParams> | undefined;
  /**
   * Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
   */
  oauthHeaders?: Array<models.ItemsTypeOauthHeaders> | undefined;
};

export type InputLokiSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputLokiType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Absolute path on which to listen for Loki logs requests. Defaults to /loki/api/v1/push, which will (in this example) expand as: 'http://<yourupstreamURL>:<yourport>/loki/api/v1/push'.
   */
  lokiAPI?: string | undefined;
  /**
   * Loki logs authentication type
   */
  authType?: models.AuthenticationTypeOptionsLokiAuth | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
  username?: string | undefined;
  password?: string | undefined;
  /**
   * Bearer token to include in the authorization header
   */
  token?: string | undefined;
  /**
   * Select or create a secret that references your credentials
   */
  credentialsSecret?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
  /**
   * URL for OAuth
   */
  loginUrl?: string | undefined;
  /**
   * Secret parameter name to pass in request body
   */
  secretParamName?: string | undefined;
  /**
   * Secret parameter value to pass in request body
   */
  secret?: string | undefined;
  /**
   * Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
   */
  tokenAttributeName?: string | undefined;
  /**
   * JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.
   */
  authHeaderExpr?: string | undefined;
  /**
   * How often the OAuth token should be refreshed.
   */
  tokenTimeoutSecs?: number | undefined;
  /**
   * Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
   */
  oauthParams?: Array<models.ItemsTypeOauthParams> | undefined;
  /**
   * Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
   */
  oauthHeaders?: Array<models.ItemsTypeOauthHeaders> | undefined;
};

export type InputLoki =
  | InputLokiSendToRoutesTrueWithConnectionsConstraint
  | InputLokiSendToRoutesFalseWithConnectionsConstraint
  | InputLokiPqEnabledFalseWithPqConstraint
  | InputLokiPqEnabledTrueWithPqConstraint;

export const InputGrafanaType2 = {
  Grafana: "grafana",
} as const;
export type InputGrafanaType2 = ClosedEnum<typeof InputGrafanaType2>;

export type PrometheusAuth2 = {
  /**
   * Remote Write authentication type
   */
  authType?: models.AuthenticationTypeOptionsPrometheusAuth | undefined;
  username?: string | undefined;
  password?: string | undefined;
  /**
   * Bearer token to include in the authorization header
   */
  token?: string | undefined;
  /**
   * Select or create a secret that references your credentials
   */
  credentialsSecret?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
  /**
   * URL for OAuth
   */
  loginUrl?: string | undefined;
  /**
   * Secret parameter name to pass in request body
   */
  secretParamName?: string | undefined;
  /**
   * Secret parameter value to pass in request body
   */
  secret?: string | undefined;
  /**
   * Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
   */
  tokenAttributeName?: string | undefined;
  /**
   * JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.
   */
  authHeaderExpr?: string | undefined;
  /**
   * How often the OAuth token should be refreshed.
   */
  tokenTimeoutSecs?: number | undefined;
  /**
   * Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
   */
  oauthParams?: Array<models.ItemsTypeOauthParams> | undefined;
  /**
   * Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
   */
  oauthHeaders?: Array<models.ItemsTypeOauthHeaders> | undefined;
};

export type LokiAuth2 = {
  /**
   * Loki logs authentication type
   */
  authType?: models.AuthenticationTypeOptionsLokiAuth | undefined;
  username?: string | undefined;
  password?: string | undefined;
  /**
   * Bearer token to include in the authorization header
   */
  token?: string | undefined;
  /**
   * Select or create a secret that references your credentials
   */
  credentialsSecret?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
  /**
   * URL for OAuth
   */
  loginUrl?: string | undefined;
  /**
   * Secret parameter name to pass in request body
   */
  secretParamName?: string | undefined;
  /**
   * Secret parameter value to pass in request body
   */
  secret?: string | undefined;
  /**
   * Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
   */
  tokenAttributeName?: string | undefined;
  /**
   * JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.
   */
  authHeaderExpr?: string | undefined;
  /**
   * How often the OAuth token should be refreshed.
   */
  tokenTimeoutSecs?: number | undefined;
  /**
   * Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
   */
  oauthParams?: Array<models.ItemsTypeOauthParams> | undefined;
  /**
   * Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
   */
  oauthHeaders?: Array<models.ItemsTypeOauthHeaders> | undefined;
};

export type InputGrafanaGrafana2 = {
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputGrafanaType2;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * Maximum time to wait for additional data, after the last response was sent, before closing a socket connection. This can be very useful when Grafana Agent remote write's request frequency is high so, reusing connections, would help mitigating the cost of creating a new connection per request. Note that Grafana Agent's embedded Prometheus would attempt to keep connections open for up to 5 minutes.
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Absolute path on which to listen for Grafana Agent's Remote Write requests. Defaults to /api/prom/push, which will expand as: 'http://<yourupstreamURL>:<yourport>/api/prom/push'. Either this field or 'Logs API endpoint' must be configured.
   */
  prometheusAPI?: string | undefined;
  /**
   * Absolute path on which to listen for Loki logs requests. Defaults to /loki/api/v1/push, which will (in this example) expand as: 'http://<yourupstreamURL>:<yourport>/loki/api/v1/push'. Either this field or 'Remote Write API endpoint' must be configured.
   */
  lokiAPI?: string | undefined;
  prometheusAuth?: PrometheusAuth2 | undefined;
  lokiAuth?: LokiAuth2 | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export const InputGrafanaType1 = {
  Grafana: "grafana",
} as const;
export type InputGrafanaType1 = ClosedEnum<typeof InputGrafanaType1>;

export type PrometheusAuth1 = {
  /**
   * Remote Write authentication type
   */
  authType?: models.AuthenticationTypeOptionsPrometheusAuth | undefined;
  username?: string | undefined;
  password?: string | undefined;
  /**
   * Bearer token to include in the authorization header
   */
  token?: string | undefined;
  /**
   * Select or create a secret that references your credentials
   */
  credentialsSecret?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
  /**
   * URL for OAuth
   */
  loginUrl?: string | undefined;
  /**
   * Secret parameter name to pass in request body
   */
  secretParamName?: string | undefined;
  /**
   * Secret parameter value to pass in request body
   */
  secret?: string | undefined;
  /**
   * Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
   */
  tokenAttributeName?: string | undefined;
  /**
   * JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.
   */
  authHeaderExpr?: string | undefined;
  /**
   * How often the OAuth token should be refreshed.
   */
  tokenTimeoutSecs?: number | undefined;
  /**
   * Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
   */
  oauthParams?: Array<models.ItemsTypeOauthParams> | undefined;
  /**
   * Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
   */
  oauthHeaders?: Array<models.ItemsTypeOauthHeaders> | undefined;
};

export type LokiAuth1 = {
  /**
   * Loki logs authentication type
   */
  authType?: models.AuthenticationTypeOptionsLokiAuth | undefined;
  username?: string | undefined;
  password?: string | undefined;
  /**
   * Bearer token to include in the authorization header
   */
  token?: string | undefined;
  /**
   * Select or create a secret that references your credentials
   */
  credentialsSecret?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
  /**
   * URL for OAuth
   */
  loginUrl?: string | undefined;
  /**
   * Secret parameter name to pass in request body
   */
  secretParamName?: string | undefined;
  /**
   * Secret parameter value to pass in request body
   */
  secret?: string | undefined;
  /**
   * Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
   */
  tokenAttributeName?: string | undefined;
  /**
   * JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.
   */
  authHeaderExpr?: string | undefined;
  /**
   * How often the OAuth token should be refreshed.
   */
  tokenTimeoutSecs?: number | undefined;
  /**
   * Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
   */
  oauthParams?: Array<models.ItemsTypeOauthParams> | undefined;
  /**
   * Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
   */
  oauthHeaders?: Array<models.ItemsTypeOauthHeaders> | undefined;
};

export type InputGrafanaGrafana1 = {
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputGrafanaType1;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * Maximum time to wait for additional data, after the last response was sent, before closing a socket connection. This can be very useful when Grafana Agent remote write's request frequency is high so, reusing connections, would help mitigating the cost of creating a new connection per request. Note that Grafana Agent's embedded Prometheus would attempt to keep connections open for up to 5 minutes.
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Absolute path on which to listen for Grafana Agent's Remote Write requests. Defaults to /api/prom/push, which will expand as: 'http://<yourupstreamURL>:<yourport>/api/prom/push'. Either this field or 'Logs API endpoint' must be configured.
   */
  prometheusAPI?: string | undefined;
  /**
   * Absolute path on which to listen for Loki logs requests. Defaults to /loki/api/v1/push, which will (in this example) expand as: 'http://<yourupstreamURL>:<yourport>/loki/api/v1/push'. Either this field or 'Remote Write API endpoint' must be configured.
   */
  lokiAPI?: string | undefined;
  prometheusAuth?: PrometheusAuth1 | undefined;
  lokiAuth?: LokiAuth1 | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputGrafana = InputGrafanaGrafana1 | InputGrafanaGrafana2;

export const InputConfluentCloudType = {
  ConfluentCloud: "confluent_cloud",
} as const;
export type InputConfluentCloudType = ClosedEnum<
  typeof InputConfluentCloudType
>;

export type InputConfluentCloudPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputConfluentCloudType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * List of Confluent Cloud bootstrap servers to use, such as yourAccount.confluent.cloud:9092
   */
  brokers: Array<string>;
  tls?: models.TlsSettingsClientSideType1 | undefined;
  /**
   * Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only.
   */
  topics: Array<string>;
  /**
   * The consumer group to which this instance belongs. Defaults to 'Cribl'.
   */
  groupId?: string | undefined;
  /**
   * Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message
   */
  fromBeginning?: boolean | undefined;
  kafkaSchemaRegistry?:
    | models.KafkaSchemaRegistryAuthenticationType
    | undefined;
  /**
   * Maximum time to wait for a connection to complete successfully
   */
  connectionTimeout?: number | undefined;
  /**
   * Maximum time to wait for Kafka to respond to a request
   */
  requestTimeout?: number | undefined;
  /**
   * If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data
   */
  maxRetries?: number | undefined;
  /**
   * The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).
   */
  maxBackOff?: number | undefined;
  /**
   * Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).
   */
  initialBackoff?: number | undefined;
  /**
   * Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.
   */
  backoffRate?: number | undefined;
  /**
   * Maximum time to wait for Kafka to respond to an authentication request
   */
  authenticationTimeout?: number | undefined;
  /**
   * Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.
   */
  reauthenticationThreshold?: number | undefined;
  /**
   * Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.
   */
  sasl?: models.AuthenticationType | undefined;
  /**
   * @remarks
   *       Timeout used to detect client failures when using Kafka's group-management facilities.
   *       If the client sends no heartbeats to the broker before the timeout expires,
   *       the broker will remove the client from the group and initiate a rebalance.
   *       Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.
   */
  sessionTimeout?: number | undefined;
  /**
   *       Maximum allowed time for each worker to join the group after a rebalance begins.
   *
   * @remarks
   *       If the timeout is exceeded, the coordinator broker will remove the worker from the group.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.
   */
  rebalanceTimeout?: number | undefined;
  /**
   *       Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.
   *
   * @remarks
   *       Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.
   */
  heartbeatInterval?: number | undefined;
  /**
   * How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.
   */
  autoCommitInterval?: number | undefined;
  /**
   * How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.
   */
  autoCommitThreshold?: number | undefined;
  /**
   * Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).
   */
  maxBytesPerPartition?: number | undefined;
  /**
   * Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).
   */
  maxBytes?: number | undefined;
  /**
   * Maximum number of network errors before the consumer re-creates a socket
   */
  maxSocketErrors?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputConfluentCloudPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputConfluentCloudType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * List of Confluent Cloud bootstrap servers to use, such as yourAccount.confluent.cloud:9092
   */
  brokers: Array<string>;
  tls?: models.TlsSettingsClientSideType1 | undefined;
  /**
   * Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only.
   */
  topics: Array<string>;
  /**
   * The consumer group to which this instance belongs. Defaults to 'Cribl'.
   */
  groupId?: string | undefined;
  /**
   * Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message
   */
  fromBeginning?: boolean | undefined;
  kafkaSchemaRegistry?:
    | models.KafkaSchemaRegistryAuthenticationType
    | undefined;
  /**
   * Maximum time to wait for a connection to complete successfully
   */
  connectionTimeout?: number | undefined;
  /**
   * Maximum time to wait for Kafka to respond to a request
   */
  requestTimeout?: number | undefined;
  /**
   * If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data
   */
  maxRetries?: number | undefined;
  /**
   * The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).
   */
  maxBackOff?: number | undefined;
  /**
   * Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).
   */
  initialBackoff?: number | undefined;
  /**
   * Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.
   */
  backoffRate?: number | undefined;
  /**
   * Maximum time to wait for Kafka to respond to an authentication request
   */
  authenticationTimeout?: number | undefined;
  /**
   * Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.
   */
  reauthenticationThreshold?: number | undefined;
  /**
   * Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.
   */
  sasl?: models.AuthenticationType | undefined;
  /**
   * @remarks
   *       Timeout used to detect client failures when using Kafka's group-management facilities.
   *       If the client sends no heartbeats to the broker before the timeout expires,
   *       the broker will remove the client from the group and initiate a rebalance.
   *       Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.
   */
  sessionTimeout?: number | undefined;
  /**
   *       Maximum allowed time for each worker to join the group after a rebalance begins.
   *
   * @remarks
   *       If the timeout is exceeded, the coordinator broker will remove the worker from the group.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.
   */
  rebalanceTimeout?: number | undefined;
  /**
   *       Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.
   *
   * @remarks
   *       Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.
   */
  heartbeatInterval?: number | undefined;
  /**
   * How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.
   */
  autoCommitInterval?: number | undefined;
  /**
   * How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.
   */
  autoCommitThreshold?: number | undefined;
  /**
   * Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).
   */
  maxBytesPerPartition?: number | undefined;
  /**
   * Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).
   */
  maxBytes?: number | undefined;
  /**
   * Maximum number of network errors before the consumer re-creates a socket
   */
  maxSocketErrors?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputConfluentCloudSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputConfluentCloudType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * List of Confluent Cloud bootstrap servers to use, such as yourAccount.confluent.cloud:9092
   */
  brokers: Array<string>;
  tls?: models.TlsSettingsClientSideType1 | undefined;
  /**
   * Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only.
   */
  topics: Array<string>;
  /**
   * The consumer group to which this instance belongs. Defaults to 'Cribl'.
   */
  groupId?: string | undefined;
  /**
   * Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message
   */
  fromBeginning?: boolean | undefined;
  kafkaSchemaRegistry?:
    | models.KafkaSchemaRegistryAuthenticationType
    | undefined;
  /**
   * Maximum time to wait for a connection to complete successfully
   */
  connectionTimeout?: number | undefined;
  /**
   * Maximum time to wait for Kafka to respond to a request
   */
  requestTimeout?: number | undefined;
  /**
   * If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data
   */
  maxRetries?: number | undefined;
  /**
   * The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).
   */
  maxBackOff?: number | undefined;
  /**
   * Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).
   */
  initialBackoff?: number | undefined;
  /**
   * Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.
   */
  backoffRate?: number | undefined;
  /**
   * Maximum time to wait for Kafka to respond to an authentication request
   */
  authenticationTimeout?: number | undefined;
  /**
   * Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.
   */
  reauthenticationThreshold?: number | undefined;
  /**
   * Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.
   */
  sasl?: models.AuthenticationType | undefined;
  /**
   * @remarks
   *       Timeout used to detect client failures when using Kafka's group-management facilities.
   *       If the client sends no heartbeats to the broker before the timeout expires,
   *       the broker will remove the client from the group and initiate a rebalance.
   *       Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.
   */
  sessionTimeout?: number | undefined;
  /**
   *       Maximum allowed time for each worker to join the group after a rebalance begins.
   *
   * @remarks
   *       If the timeout is exceeded, the coordinator broker will remove the worker from the group.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.
   */
  rebalanceTimeout?: number | undefined;
  /**
   *       Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.
   *
   * @remarks
   *       Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.
   */
  heartbeatInterval?: number | undefined;
  /**
   * How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.
   */
  autoCommitInterval?: number | undefined;
  /**
   * How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.
   */
  autoCommitThreshold?: number | undefined;
  /**
   * Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).
   */
  maxBytesPerPartition?: number | undefined;
  /**
   * Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).
   */
  maxBytes?: number | undefined;
  /**
   * Maximum number of network errors before the consumer re-creates a socket
   */
  maxSocketErrors?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputConfluentCloudSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputConfluentCloudType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * List of Confluent Cloud bootstrap servers to use, such as yourAccount.confluent.cloud:9092
   */
  brokers: Array<string>;
  tls?: models.TlsSettingsClientSideType1 | undefined;
  /**
   * Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only.
   */
  topics: Array<string>;
  /**
   * The consumer group to which this instance belongs. Defaults to 'Cribl'.
   */
  groupId?: string | undefined;
  /**
   * Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message
   */
  fromBeginning?: boolean | undefined;
  kafkaSchemaRegistry?:
    | models.KafkaSchemaRegistryAuthenticationType
    | undefined;
  /**
   * Maximum time to wait for a connection to complete successfully
   */
  connectionTimeout?: number | undefined;
  /**
   * Maximum time to wait for Kafka to respond to a request
   */
  requestTimeout?: number | undefined;
  /**
   * If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data
   */
  maxRetries?: number | undefined;
  /**
   * The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).
   */
  maxBackOff?: number | undefined;
  /**
   * Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).
   */
  initialBackoff?: number | undefined;
  /**
   * Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.
   */
  backoffRate?: number | undefined;
  /**
   * Maximum time to wait for Kafka to respond to an authentication request
   */
  authenticationTimeout?: number | undefined;
  /**
   * Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.
   */
  reauthenticationThreshold?: number | undefined;
  /**
   * Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.
   */
  sasl?: models.AuthenticationType | undefined;
  /**
   * @remarks
   *       Timeout used to detect client failures when using Kafka's group-management facilities.
   *       If the client sends no heartbeats to the broker before the timeout expires,
   *       the broker will remove the client from the group and initiate a rebalance.
   *       Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.
   */
  sessionTimeout?: number | undefined;
  /**
   *       Maximum allowed time for each worker to join the group after a rebalance begins.
   *
   * @remarks
   *       If the timeout is exceeded, the coordinator broker will remove the worker from the group.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.
   */
  rebalanceTimeout?: number | undefined;
  /**
   *       Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.
   *
   * @remarks
   *       Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.
   */
  heartbeatInterval?: number | undefined;
  /**
   * How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.
   */
  autoCommitInterval?: number | undefined;
  /**
   * How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.
   */
  autoCommitThreshold?: number | undefined;
  /**
   * Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).
   */
  maxBytesPerPartition?: number | undefined;
  /**
   * Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).
   */
  maxBytes?: number | undefined;
  /**
   * Maximum number of network errors before the consumer re-creates a socket
   */
  maxSocketErrors?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputConfluentCloud =
  | InputConfluentCloudSendToRoutesTrueWithConnectionsConstraint
  | InputConfluentCloudSendToRoutesFalseWithConnectionsConstraint
  | InputConfluentCloudPqEnabledFalseWithPqConstraint
  | InputConfluentCloudPqEnabledTrueWithPqConstraint;

export const InputElasticType = {
  Elastic: "elastic",
} as const;
export type InputElasticType = ClosedEnum<typeof InputElasticType>;

export const InputElasticAuthenticationType = {
  /**
   * None
   */
  None: "none",
  /**
   * Basic
   */
  Basic: "basic",
  /**
   * Basic (credentials secret)
   */
  CredentialsSecret: "credentialsSecret",
  /**
   * Auth Tokens
   */
  AuthTokens: "authTokens",
} as const;
export type InputElasticAuthenticationType = OpenEnum<
  typeof InputElasticAuthenticationType
>;

/**
 * The API version to use for communicating with the server
 */
export const CreateInputAPIVersion = {
  /**
   * 6.8.4
   */
  SixDot8Dot4: "6.8.4",
  /**
   * 8.3.2
   */
  EightDot3Dot2: "8.3.2",
  /**
   * Custom
   */
  Custom: "custom",
} as const;
/**
 * The API version to use for communicating with the server
 */
export type CreateInputAPIVersion = OpenEnum<typeof CreateInputAPIVersion>;

/**
 * Enter credentials directly, or select a stored secret
 */
export const InputElasticAuthenticationMethod = {
  None: "none",
  Manual: "manual",
  Secret: "secret",
} as const;
/**
 * Enter credentials directly, or select a stored secret
 */
export type InputElasticAuthenticationMethod = OpenEnum<
  typeof InputElasticAuthenticationMethod
>;

export type InputElasticProxyMode = {
  /**
   * Enable proxying of non-bulk API requests to an external Elastic server. Enable this only if you understand the implications. See [Cribl Docs](https://docs.cribl.io/stream/sources-elastic/#proxy-mode) for more details.
   */
  enabled?: boolean | undefined;
  /**
   * Enter credentials directly, or select a stored secret
   */
  authType?: InputElasticAuthenticationMethod | undefined;
  username?: string | undefined;
  password?: string | undefined;
  /**
   * Select or create a secret that references your credentials
   */
  credentialsSecret?: string | undefined;
  /**
   * URL of the Elastic server to proxy non-bulk requests to, such as http://elastic:9200
   */
  url?: string | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA (such as self-signed certificates)
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * List of headers to remove from the request to proxy
   */
  removeHeaders?: Array<string> | undefined;
  /**
   * Amount of time, in seconds, to wait for a proxy request to complete before canceling it
   */
  timeoutSec?: number | undefined;
};

export type InputElasticPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputElasticType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Absolute path on which to listen for Elasticsearch API requests. Defaults to /. _bulk will be appended automatically. For example, /myPath becomes /myPath/_bulk. Requests can then be made to either /myPath/_bulk or /myPath/<myIndexName>/_bulk. Other entries are faked as success.
   */
  elasticAPI?: string | undefined;
  authType?: InputElasticAuthenticationType | undefined;
  /**
   * The API version to use for communicating with the server
   */
  apiVersion?: CreateInputAPIVersion | undefined;
  /**
   * Headers to add to all events
   */
  extraHttpHeaders?: Array<models.ItemsTypeExtraHttpHeaders> | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  proxyMode?: InputElasticProxyMode | undefined;
  description?: string | undefined;
  username?: string | undefined;
  password?: string | undefined;
  /**
   * Select or create a secret that references your credentials
   */
  credentialsSecret?: string | undefined;
  /**
   * Bearer tokens to include in the authorization header
   */
  authTokens?: Array<string> | undefined;
  /**
   * Custom version information to respond to requests
   */
  customAPIVersion?: string | undefined;
};

export type InputElasticPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputElasticType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Absolute path on which to listen for Elasticsearch API requests. Defaults to /. _bulk will be appended automatically. For example, /myPath becomes /myPath/_bulk. Requests can then be made to either /myPath/_bulk or /myPath/<myIndexName>/_bulk. Other entries are faked as success.
   */
  elasticAPI?: string | undefined;
  authType?: InputElasticAuthenticationType | undefined;
  /**
   * The API version to use for communicating with the server
   */
  apiVersion?: CreateInputAPIVersion | undefined;
  /**
   * Headers to add to all events
   */
  extraHttpHeaders?: Array<models.ItemsTypeExtraHttpHeaders> | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  proxyMode?: InputElasticProxyMode | undefined;
  description?: string | undefined;
  username?: string | undefined;
  password?: string | undefined;
  /**
   * Select or create a secret that references your credentials
   */
  credentialsSecret?: string | undefined;
  /**
   * Bearer tokens to include in the authorization header
   */
  authTokens?: Array<string> | undefined;
  /**
   * Custom version information to respond to requests
   */
  customAPIVersion?: string | undefined;
};

export type InputElasticSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputElasticType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Absolute path on which to listen for Elasticsearch API requests. Defaults to /. _bulk will be appended automatically. For example, /myPath becomes /myPath/_bulk. Requests can then be made to either /myPath/_bulk or /myPath/<myIndexName>/_bulk. Other entries are faked as success.
   */
  elasticAPI?: string | undefined;
  authType?: InputElasticAuthenticationType | undefined;
  /**
   * The API version to use for communicating with the server
   */
  apiVersion?: CreateInputAPIVersion | undefined;
  /**
   * Headers to add to all events
   */
  extraHttpHeaders?: Array<models.ItemsTypeExtraHttpHeaders> | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  proxyMode?: InputElasticProxyMode | undefined;
  description?: string | undefined;
  username?: string | undefined;
  password?: string | undefined;
  /**
   * Select or create a secret that references your credentials
   */
  credentialsSecret?: string | undefined;
  /**
   * Bearer tokens to include in the authorization header
   */
  authTokens?: Array<string> | undefined;
  /**
   * Custom version information to respond to requests
   */
  customAPIVersion?: string | undefined;
};

export type InputElasticSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputElasticType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Absolute path on which to listen for Elasticsearch API requests. Defaults to /. _bulk will be appended automatically. For example, /myPath becomes /myPath/_bulk. Requests can then be made to either /myPath/_bulk or /myPath/<myIndexName>/_bulk. Other entries are faked as success.
   */
  elasticAPI?: string | undefined;
  authType?: InputElasticAuthenticationType | undefined;
  /**
   * The API version to use for communicating with the server
   */
  apiVersion?: CreateInputAPIVersion | undefined;
  /**
   * Headers to add to all events
   */
  extraHttpHeaders?: Array<models.ItemsTypeExtraHttpHeaders> | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  proxyMode?: InputElasticProxyMode | undefined;
  description?: string | undefined;
  username?: string | undefined;
  password?: string | undefined;
  /**
   * Select or create a secret that references your credentials
   */
  credentialsSecret?: string | undefined;
  /**
   * Bearer tokens to include in the authorization header
   */
  authTokens?: Array<string> | undefined;
  /**
   * Custom version information to respond to requests
   */
  customAPIVersion?: string | undefined;
};

export type InputElastic =
  | InputElasticSendToRoutesTrueWithConnectionsConstraint
  | InputElasticSendToRoutesFalseWithConnectionsConstraint
  | InputElasticPqEnabledFalseWithPqConstraint
  | InputElasticPqEnabledTrueWithPqConstraint;

export const InputAzureBlobType = {
  AzureBlob: "azure_blob",
} as const;
export type InputAzureBlobType = ClosedEnum<typeof InputAzureBlobType>;

export type InputAzureBlobPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputAzureBlobType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * The storage account queue name blob notifications will be read from. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myQueue-${C.vars.myVar}`
   */
  queueName: string;
  /**
   * Regex matching file names to download and process. Defaults to: .*
   */
  fileFilter?: string | undefined;
  /**
   * The duration (in seconds) that the received messages are hidden from subsequent retrieve requests after being retrieved by a ReceiveMessage request.
   */
  visibilityTimeout?: number | undefined;
  /**
   * How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.
   */
  numReceivers?: number | undefined;
  /**
   * The maximum number of messages to return in a poll request. Azure storage queues never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 32.
   */
  maxMessages?: number | undefined;
  /**
   * The duration (in seconds) which pollers should be validated and restarted if exited
   */
  servicePeriodSecs?: number | undefined;
  /**
   * Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.
   */
  skipOnError?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * Maximum file size for each Parquet chunk
   */
  parquetChunkSizeMB?: number | undefined;
  /**
   * The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified.
   */
  parquetChunkDownloadTimeout?: number | undefined;
  authType?: models.AuthenticationMethodOptions | undefined;
  description?: string | undefined;
  /**
   * Enter your Azure Storage account connection string. If left blank, Stream will fall back to env.AZURE_STORAGE_CONNECTION_STRING.
   */
  connectionString?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
  /**
   * The name of your Azure storage account
   */
  storageAccountName?: string | undefined;
  /**
   * The service principal's tenant ID
   */
  tenantId?: string | undefined;
  /**
   * The service principal's client ID
   */
  clientId?: string | undefined;
  /**
   * The Azure cloud to use. Defaults to Azure Public Cloud.
   */
  azureCloud?: string | undefined;
  /**
   * Endpoint suffix for the service URL. Takes precedence over the Azure Cloud setting. Defaults to core.windows.net.
   */
  endpointSuffix?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  clientTextSecret?: string | undefined;
  certificate?: models.CertificateTypeAzureBlobAuthTypeClientCert | undefined;
};

export type InputAzureBlobPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputAzureBlobType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * The storage account queue name blob notifications will be read from. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myQueue-${C.vars.myVar}`
   */
  queueName: string;
  /**
   * Regex matching file names to download and process. Defaults to: .*
   */
  fileFilter?: string | undefined;
  /**
   * The duration (in seconds) that the received messages are hidden from subsequent retrieve requests after being retrieved by a ReceiveMessage request.
   */
  visibilityTimeout?: number | undefined;
  /**
   * How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.
   */
  numReceivers?: number | undefined;
  /**
   * The maximum number of messages to return in a poll request. Azure storage queues never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 32.
   */
  maxMessages?: number | undefined;
  /**
   * The duration (in seconds) which pollers should be validated and restarted if exited
   */
  servicePeriodSecs?: number | undefined;
  /**
   * Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.
   */
  skipOnError?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * Maximum file size for each Parquet chunk
   */
  parquetChunkSizeMB?: number | undefined;
  /**
   * The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified.
   */
  parquetChunkDownloadTimeout?: number | undefined;
  authType?: models.AuthenticationMethodOptions | undefined;
  description?: string | undefined;
  /**
   * Enter your Azure Storage account connection string. If left blank, Stream will fall back to env.AZURE_STORAGE_CONNECTION_STRING.
   */
  connectionString?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
  /**
   * The name of your Azure storage account
   */
  storageAccountName?: string | undefined;
  /**
   * The service principal's tenant ID
   */
  tenantId?: string | undefined;
  /**
   * The service principal's client ID
   */
  clientId?: string | undefined;
  /**
   * The Azure cloud to use. Defaults to Azure Public Cloud.
   */
  azureCloud?: string | undefined;
  /**
   * Endpoint suffix for the service URL. Takes precedence over the Azure Cloud setting. Defaults to core.windows.net.
   */
  endpointSuffix?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  clientTextSecret?: string | undefined;
  certificate?: models.CertificateTypeAzureBlobAuthTypeClientCert | undefined;
};

export type InputAzureBlobSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputAzureBlobType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * The storage account queue name blob notifications will be read from. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myQueue-${C.vars.myVar}`
   */
  queueName: string;
  /**
   * Regex matching file names to download and process. Defaults to: .*
   */
  fileFilter?: string | undefined;
  /**
   * The duration (in seconds) that the received messages are hidden from subsequent retrieve requests after being retrieved by a ReceiveMessage request.
   */
  visibilityTimeout?: number | undefined;
  /**
   * How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.
   */
  numReceivers?: number | undefined;
  /**
   * The maximum number of messages to return in a poll request. Azure storage queues never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 32.
   */
  maxMessages?: number | undefined;
  /**
   * The duration (in seconds) which pollers should be validated and restarted if exited
   */
  servicePeriodSecs?: number | undefined;
  /**
   * Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.
   */
  skipOnError?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * Maximum file size for each Parquet chunk
   */
  parquetChunkSizeMB?: number | undefined;
  /**
   * The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified.
   */
  parquetChunkDownloadTimeout?: number | undefined;
  authType?: models.AuthenticationMethodOptions | undefined;
  description?: string | undefined;
  /**
   * Enter your Azure Storage account connection string. If left blank, Stream will fall back to env.AZURE_STORAGE_CONNECTION_STRING.
   */
  connectionString?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
  /**
   * The name of your Azure storage account
   */
  storageAccountName?: string | undefined;
  /**
   * The service principal's tenant ID
   */
  tenantId?: string | undefined;
  /**
   * The service principal's client ID
   */
  clientId?: string | undefined;
  /**
   * The Azure cloud to use. Defaults to Azure Public Cloud.
   */
  azureCloud?: string | undefined;
  /**
   * Endpoint suffix for the service URL. Takes precedence over the Azure Cloud setting. Defaults to core.windows.net.
   */
  endpointSuffix?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  clientTextSecret?: string | undefined;
  certificate?: models.CertificateTypeAzureBlobAuthTypeClientCert | undefined;
};

export type InputAzureBlobSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputAzureBlobType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * The storage account queue name blob notifications will be read from. Value must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myQueue-${C.vars.myVar}`
   */
  queueName: string;
  /**
   * Regex matching file names to download and process. Defaults to: .*
   */
  fileFilter?: string | undefined;
  /**
   * The duration (in seconds) that the received messages are hidden from subsequent retrieve requests after being retrieved by a ReceiveMessage request.
   */
  visibilityTimeout?: number | undefined;
  /**
   * How many receiver processes to run. The higher the number, the better the throughput - at the expense of CPU overhead.
   */
  numReceivers?: number | undefined;
  /**
   * The maximum number of messages to return in a poll request. Azure storage queues never returns more messages than this value (however, fewer messages might be returned). Valid values: 1 to 32.
   */
  maxMessages?: number | undefined;
  /**
   * The duration (in seconds) which pollers should be validated and restarted if exited
   */
  servicePeriodSecs?: number | undefined;
  /**
   * Skip files that trigger a processing error. Disabled by default, which allows retries after processing errors.
   */
  skipOnError?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * Maximum file size for each Parquet chunk
   */
  parquetChunkSizeMB?: number | undefined;
  /**
   * The maximum time allowed for downloading a Parquet chunk. Processing will stop if a chunk cannot be downloaded within the time specified.
   */
  parquetChunkDownloadTimeout?: number | undefined;
  authType?: models.AuthenticationMethodOptions | undefined;
  description?: string | undefined;
  /**
   * Enter your Azure Storage account connection string. If left blank, Stream will fall back to env.AZURE_STORAGE_CONNECTION_STRING.
   */
  connectionString?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
  /**
   * The name of your Azure storage account
   */
  storageAccountName?: string | undefined;
  /**
   * The service principal's tenant ID
   */
  tenantId?: string | undefined;
  /**
   * The service principal's client ID
   */
  clientId?: string | undefined;
  /**
   * The Azure cloud to use. Defaults to Azure Public Cloud.
   */
  azureCloud?: string | undefined;
  /**
   * Endpoint suffix for the service URL. Takes precedence over the Azure Cloud setting. Defaults to core.windows.net.
   */
  endpointSuffix?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  clientTextSecret?: string | undefined;
  certificate?: models.CertificateTypeAzureBlobAuthTypeClientCert | undefined;
};

export type InputAzureBlob =
  | InputAzureBlobSendToRoutesTrueWithConnectionsConstraint
  | InputAzureBlobSendToRoutesFalseWithConnectionsConstraint
  | InputAzureBlobPqEnabledFalseWithPqConstraint
  | InputAzureBlobPqEnabledTrueWithPqConstraint;

export const InputSplunkHecType = {
  SplunkHec: "splunk_hec",
} as const;
export type InputSplunkHecType = ClosedEnum<typeof InputSplunkHecType>;

export type InputSplunkHecAuthToken = {
  /**
   * Select Manual to enter an auth token directly, or select Secret to use a text secret to authenticate
   */
  authType?: models.AuthenticationMethodOptionsAuthTokensItems | undefined;
  /**
   * Select or create a stored text secret
   */
  tokenSecret?: string | undefined;
  /**
   * Shared secret to be provided by any client (Authorization: <token>)
   */
  token: string;
  enabled?: boolean | undefined;
  /**
   * Optional token description
   */
  description?: string | undefined;
  /**
   * Enter the values you want to allow in the HEC event index field at the token level. Supports wildcards. To skip validation, leave blank.
   */
  allowedIndexesAtToken?: Array<string> | undefined;
  /**
   * Fields to add to events referencing this token
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
};

export type InputSplunkHecPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputSplunkHecType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokens?: Array<InputSplunkHecAuthToken> | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  enableHealthCheck?: any | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Absolute path on which to listen for the Splunk HTTP Event Collector API requests. This input supports the /event, /raw and /s2s endpoints.
   */
  splunkHecAPI?: string | undefined;
  /**
   * Fields to add to every event. Overrides fields added at the token or request level. See [the Source documentation](https://docs.cribl.io/stream/sources-splunk-hec/#fields) for more info.
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * List values allowed in HEC event index field. Leave blank to skip validation. Supports wildcards. The values here can expand index validation at the token level.
   */
  allowedIndexes?: Array<string> | undefined;
  /**
   * Enable Splunk HEC acknowledgements
   */
  splunkHecAcks?: boolean | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * Event Breakers will determine events' time zone from UF-provided metadata, when TZ can't be inferred from the raw event
   */
  useFwdTimezone?: boolean | undefined;
  /**
   * Drop Splunk control fields such as `crcSalt` and `_savedPort`. If disabled, control fields are stored in the internal field `__ctrlFields`.
   */
  dropControlFields?: boolean | undefined;
  /**
   * Extract and process Splunk-generated metrics as Cribl metrics
   */
  extractMetrics?: boolean | undefined;
  /**
   * Optionally, list HTTP origins to which @{product} should send CORS (cross-origin resource sharing) Access-Control-Allow-* headers. Supports wildcards.
   */
  accessControlAllowOrigin?: Array<string> | undefined;
  /**
   * Optionally, list HTTP headers that @{product} will send to allowed origins as "Access-Control-Allow-Headers" in a CORS preflight response. Use "*" to allow all headers.
   */
  accessControlAllowHeaders?: Array<string> | undefined;
  /**
   * Emit per-token (<prefix>.http.perToken) and summary (<prefix>.http.summary) request metrics
   */
  emitTokenMetrics?: boolean | undefined;
  description?: string | undefined;
};

export type InputSplunkHecPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputSplunkHecType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokens?: Array<InputSplunkHecAuthToken> | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  enableHealthCheck?: any | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Absolute path on which to listen for the Splunk HTTP Event Collector API requests. This input supports the /event, /raw and /s2s endpoints.
   */
  splunkHecAPI?: string | undefined;
  /**
   * Fields to add to every event. Overrides fields added at the token or request level. See [the Source documentation](https://docs.cribl.io/stream/sources-splunk-hec/#fields) for more info.
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * List values allowed in HEC event index field. Leave blank to skip validation. Supports wildcards. The values here can expand index validation at the token level.
   */
  allowedIndexes?: Array<string> | undefined;
  /**
   * Enable Splunk HEC acknowledgements
   */
  splunkHecAcks?: boolean | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * Event Breakers will determine events' time zone from UF-provided metadata, when TZ can't be inferred from the raw event
   */
  useFwdTimezone?: boolean | undefined;
  /**
   * Drop Splunk control fields such as `crcSalt` and `_savedPort`. If disabled, control fields are stored in the internal field `__ctrlFields`.
   */
  dropControlFields?: boolean | undefined;
  /**
   * Extract and process Splunk-generated metrics as Cribl metrics
   */
  extractMetrics?: boolean | undefined;
  /**
   * Optionally, list HTTP origins to which @{product} should send CORS (cross-origin resource sharing) Access-Control-Allow-* headers. Supports wildcards.
   */
  accessControlAllowOrigin?: Array<string> | undefined;
  /**
   * Optionally, list HTTP headers that @{product} will send to allowed origins as "Access-Control-Allow-Headers" in a CORS preflight response. Use "*" to allow all headers.
   */
  accessControlAllowHeaders?: Array<string> | undefined;
  /**
   * Emit per-token (<prefix>.http.perToken) and summary (<prefix>.http.summary) request metrics
   */
  emitTokenMetrics?: boolean | undefined;
  description?: string | undefined;
};

export type InputSplunkHecSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputSplunkHecType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokens?: Array<InputSplunkHecAuthToken> | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  enableHealthCheck?: any | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Absolute path on which to listen for the Splunk HTTP Event Collector API requests. This input supports the /event, /raw and /s2s endpoints.
   */
  splunkHecAPI?: string | undefined;
  /**
   * Fields to add to every event. Overrides fields added at the token or request level. See [the Source documentation](https://docs.cribl.io/stream/sources-splunk-hec/#fields) for more info.
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * List values allowed in HEC event index field. Leave blank to skip validation. Supports wildcards. The values here can expand index validation at the token level.
   */
  allowedIndexes?: Array<string> | undefined;
  /**
   * Enable Splunk HEC acknowledgements
   */
  splunkHecAcks?: boolean | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * Event Breakers will determine events' time zone from UF-provided metadata, when TZ can't be inferred from the raw event
   */
  useFwdTimezone?: boolean | undefined;
  /**
   * Drop Splunk control fields such as `crcSalt` and `_savedPort`. If disabled, control fields are stored in the internal field `__ctrlFields`.
   */
  dropControlFields?: boolean | undefined;
  /**
   * Extract and process Splunk-generated metrics as Cribl metrics
   */
  extractMetrics?: boolean | undefined;
  /**
   * Optionally, list HTTP origins to which @{product} should send CORS (cross-origin resource sharing) Access-Control-Allow-* headers. Supports wildcards.
   */
  accessControlAllowOrigin?: Array<string> | undefined;
  /**
   * Optionally, list HTTP headers that @{product} will send to allowed origins as "Access-Control-Allow-Headers" in a CORS preflight response. Use "*" to allow all headers.
   */
  accessControlAllowHeaders?: Array<string> | undefined;
  /**
   * Emit per-token (<prefix>.http.perToken) and summary (<prefix>.http.summary) request metrics
   */
  emitTokenMetrics?: boolean | undefined;
  description?: string | undefined;
};

export type InputSplunkHecSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputSplunkHecType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokens?: Array<InputSplunkHecAuthToken> | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  enableHealthCheck?: any | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Absolute path on which to listen for the Splunk HTTP Event Collector API requests. This input supports the /event, /raw and /s2s endpoints.
   */
  splunkHecAPI?: string | undefined;
  /**
   * Fields to add to every event. Overrides fields added at the token or request level. See [the Source documentation](https://docs.cribl.io/stream/sources-splunk-hec/#fields) for more info.
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * List values allowed in HEC event index field. Leave blank to skip validation. Supports wildcards. The values here can expand index validation at the token level.
   */
  allowedIndexes?: Array<string> | undefined;
  /**
   * Enable Splunk HEC acknowledgements
   */
  splunkHecAcks?: boolean | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * Event Breakers will determine events' time zone from UF-provided metadata, when TZ can't be inferred from the raw event
   */
  useFwdTimezone?: boolean | undefined;
  /**
   * Drop Splunk control fields such as `crcSalt` and `_savedPort`. If disabled, control fields are stored in the internal field `__ctrlFields`.
   */
  dropControlFields?: boolean | undefined;
  /**
   * Extract and process Splunk-generated metrics as Cribl metrics
   */
  extractMetrics?: boolean | undefined;
  /**
   * Optionally, list HTTP origins to which @{product} should send CORS (cross-origin resource sharing) Access-Control-Allow-* headers. Supports wildcards.
   */
  accessControlAllowOrigin?: Array<string> | undefined;
  /**
   * Optionally, list HTTP headers that @{product} will send to allowed origins as "Access-Control-Allow-Headers" in a CORS preflight response. Use "*" to allow all headers.
   */
  accessControlAllowHeaders?: Array<string> | undefined;
  /**
   * Emit per-token (<prefix>.http.perToken) and summary (<prefix>.http.summary) request metrics
   */
  emitTokenMetrics?: boolean | undefined;
  description?: string | undefined;
};

export type InputSplunkHec =
  | InputSplunkHecSendToRoutesTrueWithConnectionsConstraint
  | InputSplunkHecSendToRoutesFalseWithConnectionsConstraint
  | InputSplunkHecPqEnabledFalseWithPqConstraint
  | InputSplunkHecPqEnabledTrueWithPqConstraint;

export const InputSplunkSearchType = {
  SplunkSearch: "splunk_search",
} as const;
export type InputSplunkSearchType = ClosedEnum<typeof InputSplunkSearchType>;

export type EndpointParam = {
  name: string;
  /**
   * JavaScript expression to compute the parameter's value, normally enclosed in backticks (e.g., `${earliest}`). If a constant, use single quotes (e.g., 'earliest'). Values without delimiters (e.g., earliest) are evaluated as strings.
   */
  value: string;
};

export type EndpointHeader = {
  name: string;
  /**
   * JavaScript expression to compute the header's value, normally enclosed in backticks (e.g., `${earliest}`). If a constant, use single quotes (e.g., 'earliest'). Values without delimiters (e.g., earliest) are evaluated as strings.
   */
  value: string;
};

/**
 * Collector runtime log level (verbosity)
 */
export const InputSplunkSearchLogLevel = {
  Error: "error",
  Warn: "warn",
  Info: "info",
  Debug: "debug",
} as const;
/**
 * Collector runtime log level (verbosity)
 */
export type InputSplunkSearchLogLevel = OpenEnum<
  typeof InputSplunkSearchLogLevel
>;

/**
 * Splunk Search authentication type
 */
export const InputSplunkSearchAuthenticationType = {
  None: "none",
  Basic: "basic",
  CredentialsSecret: "credentialsSecret",
  Token: "token",
  TextSecret: "textSecret",
  Oauth: "oauth",
} as const;
/**
 * Splunk Search authentication type
 */
export type InputSplunkSearchAuthenticationType = OpenEnum<
  typeof InputSplunkSearchAuthenticationType
>;

export type InputSplunkSearchPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputSplunkSearchType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Search head base URL. Can be an expression. Default is https://localhost:8089.
   */
  searchHead?: string | undefined;
  /**
   * Enter Splunk search here. Examples: 'index=myAppLogs level=error channel=myApp' OR '| mstats avg(myStat) as myStat WHERE index=myStatsIndex.'
   */
  search: string;
  /**
   * The earliest time boundary for the search. Can be an exact or relative time. Examples: '2022-01-14T12:00:00Z' or '-16m@m'
   */
  earliest?: string | undefined;
  /**
   * The latest time boundary for the search. Can be an exact or relative time. Examples: '2022-01-14T12:00:00Z' or '-1m@m'
   */
  latest?: string | undefined;
  /**
   * A cron schedule on which to run this job
   */
  cronSchedule?: string | undefined;
  /**
   * REST API used to create a search
   */
  endpoint?: string | undefined;
  /**
   * Format of the returned output
   */
  outputMode?: models.OutputModeOptionsSplunkCollectorConf | undefined;
  /**
   * Optional request parameters to send to the endpoint
   */
  endpointParams?: Array<EndpointParam> | undefined;
  /**
   * Optional request headers to send to the endpoint
   */
  endpointHeaders?: Array<EndpointHeader> | undefined;
  /**
   * Collector runtime log level (verbosity)
   */
  logLevel?: InputSplunkSearchLogLevel | undefined;
  /**
   * HTTP request inactivity timeout. Use 0 for no timeout.
   */
  requestTimeout?: number | undefined;
  /**
   * When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned
   */
  useRoundRobinDns?: boolean | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA (such as self-signed certificates)
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters.
   */
  encoding?: string | undefined;
  /**
   * How often workers should check in with the scheduler to keep job subscription alive
   */
  keepAliveTime?: number | undefined;
  /**
   * Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.
   */
  jobTimeout?: string | undefined;
  /**
   * The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.
   */
  maxMissedKeepAlives?: number | undefined;
  /**
   * Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.
   */
  ttl?: string | undefined;
  /**
   * When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.
   */
  ignoreGroupJobsLimit?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  retryRules?: models.RetryRulesType | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * Splunk Search authentication type
   */
  authType?: InputSplunkSearchAuthenticationType | undefined;
  description?: string | undefined;
  username?: string | undefined;
  password?: string | undefined;
  /**
   * Bearer token to include in the authorization header
   */
  token?: string | undefined;
  /**
   * Select or create a secret that references your credentials
   */
  credentialsSecret?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
  /**
   * URL for OAuth
   */
  loginUrl?: string | undefined;
  /**
   * Secret parameter name to pass in request body
   */
  secretParamName?: string | undefined;
  /**
   * Secret parameter value to pass in request body
   */
  secret?: string | undefined;
  /**
   * Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
   */
  tokenAttributeName?: string | undefined;
  /**
   * JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.
   */
  authHeaderExpr?: string | undefined;
  /**
   * How often the OAuth token should be refreshed.
   */
  tokenTimeoutSecs?: number | undefined;
  /**
   * Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
   */
  oauthParams?: Array<models.ItemsTypeOauthParams> | undefined;
  /**
   * Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
   */
  oauthHeaders?: Array<models.ItemsTypeOauthHeaders> | undefined;
};

export type InputSplunkSearchPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputSplunkSearchType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Search head base URL. Can be an expression. Default is https://localhost:8089.
   */
  searchHead?: string | undefined;
  /**
   * Enter Splunk search here. Examples: 'index=myAppLogs level=error channel=myApp' OR '| mstats avg(myStat) as myStat WHERE index=myStatsIndex.'
   */
  search: string;
  /**
   * The earliest time boundary for the search. Can be an exact or relative time. Examples: '2022-01-14T12:00:00Z' or '-16m@m'
   */
  earliest?: string | undefined;
  /**
   * The latest time boundary for the search. Can be an exact or relative time. Examples: '2022-01-14T12:00:00Z' or '-1m@m'
   */
  latest?: string | undefined;
  /**
   * A cron schedule on which to run this job
   */
  cronSchedule?: string | undefined;
  /**
   * REST API used to create a search
   */
  endpoint?: string | undefined;
  /**
   * Format of the returned output
   */
  outputMode?: models.OutputModeOptionsSplunkCollectorConf | undefined;
  /**
   * Optional request parameters to send to the endpoint
   */
  endpointParams?: Array<EndpointParam> | undefined;
  /**
   * Optional request headers to send to the endpoint
   */
  endpointHeaders?: Array<EndpointHeader> | undefined;
  /**
   * Collector runtime log level (verbosity)
   */
  logLevel?: InputSplunkSearchLogLevel | undefined;
  /**
   * HTTP request inactivity timeout. Use 0 for no timeout.
   */
  requestTimeout?: number | undefined;
  /**
   * When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned
   */
  useRoundRobinDns?: boolean | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA (such as self-signed certificates)
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters.
   */
  encoding?: string | undefined;
  /**
   * How often workers should check in with the scheduler to keep job subscription alive
   */
  keepAliveTime?: number | undefined;
  /**
   * Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.
   */
  jobTimeout?: string | undefined;
  /**
   * The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.
   */
  maxMissedKeepAlives?: number | undefined;
  /**
   * Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.
   */
  ttl?: string | undefined;
  /**
   * When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.
   */
  ignoreGroupJobsLimit?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  retryRules?: models.RetryRulesType | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * Splunk Search authentication type
   */
  authType?: InputSplunkSearchAuthenticationType | undefined;
  description?: string | undefined;
  username?: string | undefined;
  password?: string | undefined;
  /**
   * Bearer token to include in the authorization header
   */
  token?: string | undefined;
  /**
   * Select or create a secret that references your credentials
   */
  credentialsSecret?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
  /**
   * URL for OAuth
   */
  loginUrl?: string | undefined;
  /**
   * Secret parameter name to pass in request body
   */
  secretParamName?: string | undefined;
  /**
   * Secret parameter value to pass in request body
   */
  secret?: string | undefined;
  /**
   * Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
   */
  tokenAttributeName?: string | undefined;
  /**
   * JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.
   */
  authHeaderExpr?: string | undefined;
  /**
   * How often the OAuth token should be refreshed.
   */
  tokenTimeoutSecs?: number | undefined;
  /**
   * Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
   */
  oauthParams?: Array<models.ItemsTypeOauthParams> | undefined;
  /**
   * Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
   */
  oauthHeaders?: Array<models.ItemsTypeOauthHeaders> | undefined;
};

export type InputSplunkSearchSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputSplunkSearchType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Search head base URL. Can be an expression. Default is https://localhost:8089.
   */
  searchHead?: string | undefined;
  /**
   * Enter Splunk search here. Examples: 'index=myAppLogs level=error channel=myApp' OR '| mstats avg(myStat) as myStat WHERE index=myStatsIndex.'
   */
  search: string;
  /**
   * The earliest time boundary for the search. Can be an exact or relative time. Examples: '2022-01-14T12:00:00Z' or '-16m@m'
   */
  earliest?: string | undefined;
  /**
   * The latest time boundary for the search. Can be an exact or relative time. Examples: '2022-01-14T12:00:00Z' or '-1m@m'
   */
  latest?: string | undefined;
  /**
   * A cron schedule on which to run this job
   */
  cronSchedule?: string | undefined;
  /**
   * REST API used to create a search
   */
  endpoint?: string | undefined;
  /**
   * Format of the returned output
   */
  outputMode?: models.OutputModeOptionsSplunkCollectorConf | undefined;
  /**
   * Optional request parameters to send to the endpoint
   */
  endpointParams?: Array<EndpointParam> | undefined;
  /**
   * Optional request headers to send to the endpoint
   */
  endpointHeaders?: Array<EndpointHeader> | undefined;
  /**
   * Collector runtime log level (verbosity)
   */
  logLevel?: InputSplunkSearchLogLevel | undefined;
  /**
   * HTTP request inactivity timeout. Use 0 for no timeout.
   */
  requestTimeout?: number | undefined;
  /**
   * When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned
   */
  useRoundRobinDns?: boolean | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA (such as self-signed certificates)
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters.
   */
  encoding?: string | undefined;
  /**
   * How often workers should check in with the scheduler to keep job subscription alive
   */
  keepAliveTime?: number | undefined;
  /**
   * Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.
   */
  jobTimeout?: string | undefined;
  /**
   * The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.
   */
  maxMissedKeepAlives?: number | undefined;
  /**
   * Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.
   */
  ttl?: string | undefined;
  /**
   * When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.
   */
  ignoreGroupJobsLimit?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  retryRules?: models.RetryRulesType | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * Splunk Search authentication type
   */
  authType?: InputSplunkSearchAuthenticationType | undefined;
  description?: string | undefined;
  username?: string | undefined;
  password?: string | undefined;
  /**
   * Bearer token to include in the authorization header
   */
  token?: string | undefined;
  /**
   * Select or create a secret that references your credentials
   */
  credentialsSecret?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
  /**
   * URL for OAuth
   */
  loginUrl?: string | undefined;
  /**
   * Secret parameter name to pass in request body
   */
  secretParamName?: string | undefined;
  /**
   * Secret parameter value to pass in request body
   */
  secret?: string | undefined;
  /**
   * Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
   */
  tokenAttributeName?: string | undefined;
  /**
   * JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.
   */
  authHeaderExpr?: string | undefined;
  /**
   * How often the OAuth token should be refreshed.
   */
  tokenTimeoutSecs?: number | undefined;
  /**
   * Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
   */
  oauthParams?: Array<models.ItemsTypeOauthParams> | undefined;
  /**
   * Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
   */
  oauthHeaders?: Array<models.ItemsTypeOauthHeaders> | undefined;
};

export type InputSplunkSearchSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputSplunkSearchType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Search head base URL. Can be an expression. Default is https://localhost:8089.
   */
  searchHead?: string | undefined;
  /**
   * Enter Splunk search here. Examples: 'index=myAppLogs level=error channel=myApp' OR '| mstats avg(myStat) as myStat WHERE index=myStatsIndex.'
   */
  search: string;
  /**
   * The earliest time boundary for the search. Can be an exact or relative time. Examples: '2022-01-14T12:00:00Z' or '-16m@m'
   */
  earliest?: string | undefined;
  /**
   * The latest time boundary for the search. Can be an exact or relative time. Examples: '2022-01-14T12:00:00Z' or '-1m@m'
   */
  latest?: string | undefined;
  /**
   * A cron schedule on which to run this job
   */
  cronSchedule?: string | undefined;
  /**
   * REST API used to create a search
   */
  endpoint?: string | undefined;
  /**
   * Format of the returned output
   */
  outputMode?: models.OutputModeOptionsSplunkCollectorConf | undefined;
  /**
   * Optional request parameters to send to the endpoint
   */
  endpointParams?: Array<EndpointParam> | undefined;
  /**
   * Optional request headers to send to the endpoint
   */
  endpointHeaders?: Array<EndpointHeader> | undefined;
  /**
   * Collector runtime log level (verbosity)
   */
  logLevel?: InputSplunkSearchLogLevel | undefined;
  /**
   * HTTP request inactivity timeout. Use 0 for no timeout.
   */
  requestTimeout?: number | undefined;
  /**
   * When a DNS server returns multiple addresses, @{product} will cycle through them in the order returned
   */
  useRoundRobinDns?: boolean | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA (such as self-signed certificates)
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * Character encoding to use when parsing ingested data. When not set, @{product} will default to UTF-8 but may incorrectly interpret multi-byte characters.
   */
  encoding?: string | undefined;
  /**
   * How often workers should check in with the scheduler to keep job subscription alive
   */
  keepAliveTime?: number | undefined;
  /**
   * Maximum time the job is allowed to run (e.g., 30, 45s or 15m). Units are seconds, if not specified. Enter 0 for unlimited time.
   */
  jobTimeout?: string | undefined;
  /**
   * The number of Keep Alive Time periods before an inactive worker will have its job subscription revoked.
   */
  maxMissedKeepAlives?: number | undefined;
  /**
   * Time to keep the job's artifacts on disk after job completion. This also affects how long a job is listed in the Job Inspector.
   */
  ttl?: string | undefined;
  /**
   * When enabled, this job's artifacts are not counted toward the Worker Group's finished job artifacts limit. Artifacts will be removed only after the Collector's configured time to live.
   */
  ignoreGroupJobsLimit?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  retryRules?: models.RetryRulesType | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * Splunk Search authentication type
   */
  authType?: InputSplunkSearchAuthenticationType | undefined;
  description?: string | undefined;
  username?: string | undefined;
  password?: string | undefined;
  /**
   * Bearer token to include in the authorization header
   */
  token?: string | undefined;
  /**
   * Select or create a secret that references your credentials
   */
  credentialsSecret?: string | undefined;
  /**
   * Select or create a stored text secret
   */
  textSecret?: string | undefined;
  /**
   * URL for OAuth
   */
  loginUrl?: string | undefined;
  /**
   * Secret parameter name to pass in request body
   */
  secretParamName?: string | undefined;
  /**
   * Secret parameter value to pass in request body
   */
  secret?: string | undefined;
  /**
   * Name of the auth token attribute in the OAuth response. Can be top-level (e.g., 'token'); or nested, using a period (e.g., 'data.token').
   */
  tokenAttributeName?: string | undefined;
  /**
   * JavaScript expression to compute the Authorization header value to pass in requests. The value `${token}` is used to reference the token obtained from authentication, e.g.: `Bearer ${token}`.
   */
  authHeaderExpr?: string | undefined;
  /**
   * How often the OAuth token should be refreshed.
   */
  tokenTimeoutSecs?: number | undefined;
  /**
   * Additional parameters to send in the OAuth login request. @{product} will combine the secret with these parameters, and will send the URL-encoded result in a POST request to the endpoint specified in the 'Login URL'. We'll automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
   */
  oauthParams?: Array<models.ItemsTypeOauthParams> | undefined;
  /**
   * Additional headers to send in the OAuth login request. @{product} will automatically add the content-type header 'application/x-www-form-urlencoded' when sending this request.
   */
  oauthHeaders?: Array<models.ItemsTypeOauthHeaders> | undefined;
};

export type InputSplunkSearch =
  | InputSplunkSearchSendToRoutesTrueWithConnectionsConstraint
  | InputSplunkSearchSendToRoutesFalseWithConnectionsConstraint
  | InputSplunkSearchPqEnabledFalseWithPqConstraint
  | InputSplunkSearchPqEnabledTrueWithPqConstraint;

export const InputSplunkType = {
  Splunk: "splunk",
} as const;
export type InputSplunkType = ClosedEnum<typeof InputSplunkType>;

export type InputSplunkAuthToken = {
  /**
   * Shared secrets to be provided by any Splunk forwarder. If empty, unauthorized access is permitted.
   */
  token: string;
  description?: string | undefined;
};

/**
 * The highest S2S protocol version to advertise during handshake
 */
export const MaxS2SVersion = {
  /**
   * v3
   */
  V3: "v3",
  /**
   * v4
   */
  V4: "v4",
} as const;
/**
 * The highest S2S protocol version to advertise during handshake
 */
export type MaxS2SVersion = OpenEnum<typeof MaxS2SVersion>;

/**
 * Controls whether to support reading compressed data from a forwarder. Select 'Automatic' to match the forwarder's configuration, or 'Disabled' to reject compressed connections.
 */
export const CreateInputCompression = {
  /**
   * Disabled
   */
  Disabled: "disabled",
  /**
   * Automatic
   */
  Auto: "auto",
  /**
   * Always
   */
  Always: "always",
} as const;
/**
 * Controls whether to support reading compressed data from a forwarder. Select 'Automatic' to match the forwarder's configuration, or 'Disabled' to reject compressed connections.
 */
export type CreateInputCompression = OpenEnum<typeof CreateInputCompression>;

export type InputSplunkPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputSplunkType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Regex matching IP addresses that are allowed to establish a connection
   */
  ipWhitelistRegex?: string | undefined;
  /**
   * Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.
   */
  maxActiveCxn?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.
   */
  socketIdleTimeout?: number | undefined;
  /**
   * How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.
   */
  socketEndingMaxWait?: number | undefined;
  /**
   * The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.
   */
  socketMaxLifespan?: number | undefined;
  /**
   * Enable if the connection is proxied by a device that supports proxy protocol v1 or v2
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * Shared secrets to be provided by any Splunk forwarder. If empty, unauthorized access is permitted.
   */
  authTokens?: Array<InputSplunkAuthToken> | undefined;
  /**
   * The highest S2S protocol version to advertise during handshake
   */
  maxS2Sversion?: MaxS2SVersion | undefined;
  description?: string | undefined;
  /**
   * Event Breakers will determine events' time zone from UF-provided metadata, when TZ can't be inferred from the raw event
   */
  useFwdTimezone?: boolean | undefined;
  /**
   * Drop Splunk control fields such as `crcSalt` and `_savedPort`. If disabled, control fields are stored in the internal field `__ctrlFields`.
   */
  dropControlFields?: boolean | undefined;
  /**
   * Extract and process Splunk-generated metrics as Cribl metrics
   */
  extractMetrics?: boolean | undefined;
  /**
   * Controls whether to support reading compressed data from a forwarder. Select 'Automatic' to match the forwarder's configuration, or 'Disabled' to reject compressed connections.
   */
  compress?: CreateInputCompression | undefined;
};

export type InputSplunkPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputSplunkType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Regex matching IP addresses that are allowed to establish a connection
   */
  ipWhitelistRegex?: string | undefined;
  /**
   * Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.
   */
  maxActiveCxn?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.
   */
  socketIdleTimeout?: number | undefined;
  /**
   * How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.
   */
  socketEndingMaxWait?: number | undefined;
  /**
   * The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.
   */
  socketMaxLifespan?: number | undefined;
  /**
   * Enable if the connection is proxied by a device that supports proxy protocol v1 or v2
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * Shared secrets to be provided by any Splunk forwarder. If empty, unauthorized access is permitted.
   */
  authTokens?: Array<InputSplunkAuthToken> | undefined;
  /**
   * The highest S2S protocol version to advertise during handshake
   */
  maxS2Sversion?: MaxS2SVersion | undefined;
  description?: string | undefined;
  /**
   * Event Breakers will determine events' time zone from UF-provided metadata, when TZ can't be inferred from the raw event
   */
  useFwdTimezone?: boolean | undefined;
  /**
   * Drop Splunk control fields such as `crcSalt` and `_savedPort`. If disabled, control fields are stored in the internal field `__ctrlFields`.
   */
  dropControlFields?: boolean | undefined;
  /**
   * Extract and process Splunk-generated metrics as Cribl metrics
   */
  extractMetrics?: boolean | undefined;
  /**
   * Controls whether to support reading compressed data from a forwarder. Select 'Automatic' to match the forwarder's configuration, or 'Disabled' to reject compressed connections.
   */
  compress?: CreateInputCompression | undefined;
};

export type InputSplunkSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputSplunkType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Regex matching IP addresses that are allowed to establish a connection
   */
  ipWhitelistRegex?: string | undefined;
  /**
   * Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.
   */
  maxActiveCxn?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.
   */
  socketIdleTimeout?: number | undefined;
  /**
   * How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.
   */
  socketEndingMaxWait?: number | undefined;
  /**
   * The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.
   */
  socketMaxLifespan?: number | undefined;
  /**
   * Enable if the connection is proxied by a device that supports proxy protocol v1 or v2
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * Shared secrets to be provided by any Splunk forwarder. If empty, unauthorized access is permitted.
   */
  authTokens?: Array<InputSplunkAuthToken> | undefined;
  /**
   * The highest S2S protocol version to advertise during handshake
   */
  maxS2Sversion?: MaxS2SVersion | undefined;
  description?: string | undefined;
  /**
   * Event Breakers will determine events' time zone from UF-provided metadata, when TZ can't be inferred from the raw event
   */
  useFwdTimezone?: boolean | undefined;
  /**
   * Drop Splunk control fields such as `crcSalt` and `_savedPort`. If disabled, control fields are stored in the internal field `__ctrlFields`.
   */
  dropControlFields?: boolean | undefined;
  /**
   * Extract and process Splunk-generated metrics as Cribl metrics
   */
  extractMetrics?: boolean | undefined;
  /**
   * Controls whether to support reading compressed data from a forwarder. Select 'Automatic' to match the forwarder's configuration, or 'Disabled' to reject compressed connections.
   */
  compress?: CreateInputCompression | undefined;
};

export type InputSplunkSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputSplunkType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Regex matching IP addresses that are allowed to establish a connection
   */
  ipWhitelistRegex?: string | undefined;
  /**
   * Maximum number of active connections allowed per Worker Process. Use 0 for unlimited.
   */
  maxActiveCxn?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. After this time, the connection will be closed. Leave at 0 for no inactive socket monitoring.
   */
  socketIdleTimeout?: number | undefined;
  /**
   * How long the server will wait after initiating a closure for a client to close its end of the connection. If the client doesn't close the connection within this time, the server will forcefully terminate the socket to prevent resource leaks and ensure efficient connection cleanup and system stability. Leave at 0 for no inactive socket monitoring.
   */
  socketEndingMaxWait?: number | undefined;
  /**
   * The maximum duration a socket can remain open, even if active. This helps manage resources and mitigate issues caused by TCP pinning. Set to 0 to disable.
   */
  socketMaxLifespan?: number | undefined;
  /**
   * Enable if the connection is proxied by a device that supports proxy protocol v1 or v2
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  /**
   * Shared secrets to be provided by any Splunk forwarder. If empty, unauthorized access is permitted.
   */
  authTokens?: Array<InputSplunkAuthToken> | undefined;
  /**
   * The highest S2S protocol version to advertise during handshake
   */
  maxS2Sversion?: MaxS2SVersion | undefined;
  description?: string | undefined;
  /**
   * Event Breakers will determine events' time zone from UF-provided metadata, when TZ can't be inferred from the raw event
   */
  useFwdTimezone?: boolean | undefined;
  /**
   * Drop Splunk control fields such as `crcSalt` and `_savedPort`. If disabled, control fields are stored in the internal field `__ctrlFields`.
   */
  dropControlFields?: boolean | undefined;
  /**
   * Extract and process Splunk-generated metrics as Cribl metrics
   */
  extractMetrics?: boolean | undefined;
  /**
   * Controls whether to support reading compressed data from a forwarder. Select 'Automatic' to match the forwarder's configuration, or 'Disabled' to reject compressed connections.
   */
  compress?: CreateInputCompression | undefined;
};

export type InputSplunk =
  | InputSplunkSendToRoutesTrueWithConnectionsConstraint
  | InputSplunkSendToRoutesFalseWithConnectionsConstraint
  | InputSplunkPqEnabledFalseWithPqConstraint
  | InputSplunkPqEnabledTrueWithPqConstraint;

export const InputHttpType = {
  Http: "http",
} as const;
export type InputHttpType = ClosedEnum<typeof InputHttpType>;

export type InputHttpPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputHttpType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokens?: Array<string> | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Absolute path on which to listen for the Cribl HTTP API requests. Only _bulk (default /cribl/_bulk) is available. Use empty string to disable.
   */
  criblAPI?: string | undefined;
  /**
   * Absolute path on which to listen for the Elasticsearch API requests. Only _bulk (default /elastic/_bulk) is available. Use empty string to disable.
   */
  elasticAPI?: string | undefined;
  /**
   * Absolute path on which listen for the Splunk HTTP Event Collector API requests. Use empty string to disable.
   */
  splunkHecAPI?: string | undefined;
  splunkHecAcks?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokensExt?: Array<models.ItemsTypeAuthTokensExt> | undefined;
  description?: string | undefined;
};

export type InputHttpPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputHttpType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokens?: Array<string> | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Absolute path on which to listen for the Cribl HTTP API requests. Only _bulk (default /cribl/_bulk) is available. Use empty string to disable.
   */
  criblAPI?: string | undefined;
  /**
   * Absolute path on which to listen for the Elasticsearch API requests. Only _bulk (default /elastic/_bulk) is available. Use empty string to disable.
   */
  elasticAPI?: string | undefined;
  /**
   * Absolute path on which listen for the Splunk HTTP Event Collector API requests. Use empty string to disable.
   */
  splunkHecAPI?: string | undefined;
  splunkHecAcks?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokensExt?: Array<models.ItemsTypeAuthTokensExt> | undefined;
  description?: string | undefined;
};

export type InputHttpSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputHttpType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokens?: Array<string> | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Absolute path on which to listen for the Cribl HTTP API requests. Only _bulk (default /cribl/_bulk) is available. Use empty string to disable.
   */
  criblAPI?: string | undefined;
  /**
   * Absolute path on which to listen for the Elasticsearch API requests. Only _bulk (default /elastic/_bulk) is available. Use empty string to disable.
   */
  elasticAPI?: string | undefined;
  /**
   * Absolute path on which listen for the Splunk HTTP Event Collector API requests. Use empty string to disable.
   */
  splunkHecAPI?: string | undefined;
  splunkHecAcks?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokensExt?: Array<models.ItemsTypeAuthTokensExt> | undefined;
  description?: string | undefined;
};

export type InputHttpSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputHttpType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Address to bind on. Defaults to 0.0.0.0 (all addresses).
   */
  host?: string | undefined;
  /**
   * Port to listen on
   */
  port: number;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokens?: Array<string> | undefined;
  tls?: models.TlsSettingsServerSideType | undefined;
  /**
   * Maximum number of active requests allowed per Worker Process. Set to 0 for unlimited. Caution: Increasing the limit above the default value, or setting it to unlimited, may degrade performance and reduce throughput.
   */
  maxActiveReq?: number | undefined;
  /**
   * Maximum number of requests per socket before @{product} instructs the client to close the connection. Default is 0 (unlimited).
   */
  maxRequestsPerSocket?: number | undefined;
  /**
   * Extract the client IP and port from PROXY protocol v1/v2. When enabled, the X-Forwarded-For header is ignored. Disable to use the X-Forwarded-For header for client IP extraction.
   */
  enableProxyHeader?: boolean | undefined;
  /**
   * Add request headers to events, in the __headers field
   */
  captureHeaders?: boolean | undefined;
  /**
   * How often request activity is logged at the `info` level. A value of 1 would log every request, 10 every 10th request, etc.
   */
  activityLogSampleRate?: number | undefined;
  /**
   * How long to wait for an incoming request to complete before aborting it. Use 0 to disable.
   */
  requestTimeout?: number | undefined;
  /**
   * How long @{product} should wait before assuming that an inactive socket has timed out. To wait forever, set to 0.
   */
  socketTimeout?: number | undefined;
  /**
   * After the last response is sent, @{product} will wait this long for additional data before closing the socket connection. Minimum 1 second, maximum 600 seconds (10 minutes).
   */
  keepAliveTimeout?: number | undefined;
  /**
   * Expose the /cribl_health endpoint, which returns 200 OK when this Source is healthy
   */
  enableHealthCheck?: boolean | undefined;
  /**
   * Messages from matched IP addresses will be processed, unless also matched by the denylist
   */
  ipAllowlistRegex?: string | undefined;
  /**
   * Messages from matched IP addresses will be ignored. This takes precedence over the allowlist.
   */
  ipDenylistRegex?: string | undefined;
  /**
   * Absolute path on which to listen for the Cribl HTTP API requests. Only _bulk (default /cribl/_bulk) is available. Use empty string to disable.
   */
  criblAPI?: string | undefined;
  /**
   * Absolute path on which to listen for the Elasticsearch API requests. Only _bulk (default /elastic/_bulk) is available. Use empty string to disable.
   */
  elasticAPI?: string | undefined;
  /**
   * Absolute path on which listen for the Splunk HTTP Event Collector API requests. Use empty string to disable.
   */
  splunkHecAPI?: string | undefined;
  splunkHecAcks?: boolean | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Shared secrets to be provided by any client (Authorization: <token>). If empty, unauthorized access is permitted.
   */
  authTokensExt?: Array<models.ItemsTypeAuthTokensExt> | undefined;
  description?: string | undefined;
};

export type InputHttp =
  | InputHttpSendToRoutesTrueWithConnectionsConstraint
  | InputHttpSendToRoutesFalseWithConnectionsConstraint
  | InputHttpPqEnabledFalseWithPqConstraint
  | InputHttpPqEnabledTrueWithPqConstraint;

export const InputMskType = {
  Msk: "msk",
} as const;
export type InputMskType = ClosedEnum<typeof InputMskType>;

export type InputMskPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputMskType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Enter each Kafka bootstrap server you want to use. Specify the hostname and port (such as mykafkabroker:9092) or just the hostname (in which case @{product} will assign port 9092).
   */
  brokers: Array<string>;
  /**
   * Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only.
   */
  topics: Array<string>;
  /**
   * The consumer group to which this instance belongs. Defaults to 'Cribl'.
   */
  groupId?: string | undefined;
  /**
   * Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message
   */
  fromBeginning?: boolean | undefined;
  /**
   * @remarks
   *       Timeout used to detect client failures when using Kafka's group-management facilities.
   *       If the client sends no heartbeats to the broker before the timeout expires,
   *       the broker will remove the client from the group and initiate a rebalance.
   *       Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.
   */
  sessionTimeout?: number | undefined;
  /**
   *       Maximum allowed time for each worker to join the group after a rebalance begins.
   *
   * @remarks
   *       If the timeout is exceeded, the coordinator broker will remove the worker from the group.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.
   */
  rebalanceTimeout?: number | undefined;
  /**
   *       Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.
   *
   * @remarks
   *       Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.
   */
  heartbeatInterval?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  kafkaSchemaRegistry?:
    | models.KafkaSchemaRegistryAuthenticationType
    | undefined;
  /**
   * Maximum time to wait for a connection to complete successfully
   */
  connectionTimeout?: number | undefined;
  /**
   * Maximum time to wait for Kafka to respond to a request
   */
  requestTimeout?: number | undefined;
  /**
   * If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data
   */
  maxRetries?: number | undefined;
  /**
   * The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).
   */
  maxBackOff?: number | undefined;
  /**
   * Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).
   */
  initialBackoff?: number | undefined;
  /**
   * Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.
   */
  backoffRate?: number | undefined;
  /**
   * Maximum time to wait for Kafka to respond to an authentication request
   */
  authenticationTimeout?: number | undefined;
  /**
   * Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.
   */
  reauthenticationThreshold?: number | undefined;
  /**
   * AWS authentication method. Choose Auto to use IAM roles.
   */
  awsAuthenticationMethod?: string | undefined;
  awsSecretKey?: string | undefined;
  /**
   * Region where the MSK cluster is located
   */
  region: string;
  /**
   * MSK cluster service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to MSK cluster-compatible endpoint.
   */
  endpoint?: string | undefined;
  /**
   * Signature version to use for signing MSK cluster requests
   */
  signatureVersion?: models.SignatureVersionOptions | undefined;
  /**
   * Reuse connections between requests, which can improve performance
   */
  reuseConnections?: boolean | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * Use Assume Role credentials to access MSK
   */
  enableAssumeRole?: boolean | undefined;
  /**
   * Amazon Resource Name (ARN) of the role to assume
   */
  assumeRoleArn?: string | undefined;
  /**
   * External ID to use when assuming role
   */
  assumeRoleExternalId?: string | undefined;
  /**
   * Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
   */
  durationSeconds?: number | undefined;
  tls?: models.TlsSettingsClientSideType1 | undefined;
  /**
   * How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.
   */
  autoCommitInterval?: number | undefined;
  /**
   * How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.
   */
  autoCommitThreshold?: number | undefined;
  /**
   * Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).
   */
  maxBytesPerPartition?: number | undefined;
  /**
   * Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).
   */
  maxBytes?: number | undefined;
  /**
   * Maximum number of network errors before the consumer re-creates a socket
   */
  maxSocketErrors?: number | undefined;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  /**
   * Select or create a stored secret that references your access key and secret key
   */
  awsSecret?: string | undefined;
};

export type InputMskPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputMskType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Enter each Kafka bootstrap server you want to use. Specify the hostname and port (such as mykafkabroker:9092) or just the hostname (in which case @{product} will assign port 9092).
   */
  brokers: Array<string>;
  /**
   * Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only.
   */
  topics: Array<string>;
  /**
   * The consumer group to which this instance belongs. Defaults to 'Cribl'.
   */
  groupId?: string | undefined;
  /**
   * Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message
   */
  fromBeginning?: boolean | undefined;
  /**
   * @remarks
   *       Timeout used to detect client failures when using Kafka's group-management facilities.
   *       If the client sends no heartbeats to the broker before the timeout expires,
   *       the broker will remove the client from the group and initiate a rebalance.
   *       Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.
   */
  sessionTimeout?: number | undefined;
  /**
   *       Maximum allowed time for each worker to join the group after a rebalance begins.
   *
   * @remarks
   *       If the timeout is exceeded, the coordinator broker will remove the worker from the group.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.
   */
  rebalanceTimeout?: number | undefined;
  /**
   *       Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.
   *
   * @remarks
   *       Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.
   */
  heartbeatInterval?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  kafkaSchemaRegistry?:
    | models.KafkaSchemaRegistryAuthenticationType
    | undefined;
  /**
   * Maximum time to wait for a connection to complete successfully
   */
  connectionTimeout?: number | undefined;
  /**
   * Maximum time to wait for Kafka to respond to a request
   */
  requestTimeout?: number | undefined;
  /**
   * If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data
   */
  maxRetries?: number | undefined;
  /**
   * The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).
   */
  maxBackOff?: number | undefined;
  /**
   * Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).
   */
  initialBackoff?: number | undefined;
  /**
   * Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.
   */
  backoffRate?: number | undefined;
  /**
   * Maximum time to wait for Kafka to respond to an authentication request
   */
  authenticationTimeout?: number | undefined;
  /**
   * Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.
   */
  reauthenticationThreshold?: number | undefined;
  /**
   * AWS authentication method. Choose Auto to use IAM roles.
   */
  awsAuthenticationMethod?: string | undefined;
  awsSecretKey?: string | undefined;
  /**
   * Region where the MSK cluster is located
   */
  region: string;
  /**
   * MSK cluster service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to MSK cluster-compatible endpoint.
   */
  endpoint?: string | undefined;
  /**
   * Signature version to use for signing MSK cluster requests
   */
  signatureVersion?: models.SignatureVersionOptions | undefined;
  /**
   * Reuse connections between requests, which can improve performance
   */
  reuseConnections?: boolean | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * Use Assume Role credentials to access MSK
   */
  enableAssumeRole?: boolean | undefined;
  /**
   * Amazon Resource Name (ARN) of the role to assume
   */
  assumeRoleArn?: string | undefined;
  /**
   * External ID to use when assuming role
   */
  assumeRoleExternalId?: string | undefined;
  /**
   * Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
   */
  durationSeconds?: number | undefined;
  tls?: models.TlsSettingsClientSideType1 | undefined;
  /**
   * How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.
   */
  autoCommitInterval?: number | undefined;
  /**
   * How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.
   */
  autoCommitThreshold?: number | undefined;
  /**
   * Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).
   */
  maxBytesPerPartition?: number | undefined;
  /**
   * Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).
   */
  maxBytes?: number | undefined;
  /**
   * Maximum number of network errors before the consumer re-creates a socket
   */
  maxSocketErrors?: number | undefined;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  /**
   * Select or create a stored secret that references your access key and secret key
   */
  awsSecret?: string | undefined;
};

export type InputMskSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputMskType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Enter each Kafka bootstrap server you want to use. Specify the hostname and port (such as mykafkabroker:9092) or just the hostname (in which case @{product} will assign port 9092).
   */
  brokers: Array<string>;
  /**
   * Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only.
   */
  topics: Array<string>;
  /**
   * The consumer group to which this instance belongs. Defaults to 'Cribl'.
   */
  groupId?: string | undefined;
  /**
   * Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message
   */
  fromBeginning?: boolean | undefined;
  /**
   * @remarks
   *       Timeout used to detect client failures when using Kafka's group-management facilities.
   *       If the client sends no heartbeats to the broker before the timeout expires,
   *       the broker will remove the client from the group and initiate a rebalance.
   *       Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.
   */
  sessionTimeout?: number | undefined;
  /**
   *       Maximum allowed time for each worker to join the group after a rebalance begins.
   *
   * @remarks
   *       If the timeout is exceeded, the coordinator broker will remove the worker from the group.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.
   */
  rebalanceTimeout?: number | undefined;
  /**
   *       Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.
   *
   * @remarks
   *       Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.
   */
  heartbeatInterval?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  kafkaSchemaRegistry?:
    | models.KafkaSchemaRegistryAuthenticationType
    | undefined;
  /**
   * Maximum time to wait for a connection to complete successfully
   */
  connectionTimeout?: number | undefined;
  /**
   * Maximum time to wait for Kafka to respond to a request
   */
  requestTimeout?: number | undefined;
  /**
   * If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data
   */
  maxRetries?: number | undefined;
  /**
   * The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).
   */
  maxBackOff?: number | undefined;
  /**
   * Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).
   */
  initialBackoff?: number | undefined;
  /**
   * Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.
   */
  backoffRate?: number | undefined;
  /**
   * Maximum time to wait for Kafka to respond to an authentication request
   */
  authenticationTimeout?: number | undefined;
  /**
   * Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.
   */
  reauthenticationThreshold?: number | undefined;
  /**
   * AWS authentication method. Choose Auto to use IAM roles.
   */
  awsAuthenticationMethod?: string | undefined;
  awsSecretKey?: string | undefined;
  /**
   * Region where the MSK cluster is located
   */
  region: string;
  /**
   * MSK cluster service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to MSK cluster-compatible endpoint.
   */
  endpoint?: string | undefined;
  /**
   * Signature version to use for signing MSK cluster requests
   */
  signatureVersion?: models.SignatureVersionOptions | undefined;
  /**
   * Reuse connections between requests, which can improve performance
   */
  reuseConnections?: boolean | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * Use Assume Role credentials to access MSK
   */
  enableAssumeRole?: boolean | undefined;
  /**
   * Amazon Resource Name (ARN) of the role to assume
   */
  assumeRoleArn?: string | undefined;
  /**
   * External ID to use when assuming role
   */
  assumeRoleExternalId?: string | undefined;
  /**
   * Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
   */
  durationSeconds?: number | undefined;
  tls?: models.TlsSettingsClientSideType1 | undefined;
  /**
   * How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.
   */
  autoCommitInterval?: number | undefined;
  /**
   * How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.
   */
  autoCommitThreshold?: number | undefined;
  /**
   * Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).
   */
  maxBytesPerPartition?: number | undefined;
  /**
   * Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).
   */
  maxBytes?: number | undefined;
  /**
   * Maximum number of network errors before the consumer re-creates a socket
   */
  maxSocketErrors?: number | undefined;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  /**
   * Select or create a stored secret that references your access key and secret key
   */
  awsSecret?: string | undefined;
};

export type InputMskSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputMskType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Enter each Kafka bootstrap server you want to use. Specify the hostname and port (such as mykafkabroker:9092) or just the hostname (in which case @{product} will assign port 9092).
   */
  brokers: Array<string>;
  /**
   * Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only.
   */
  topics: Array<string>;
  /**
   * The consumer group to which this instance belongs. Defaults to 'Cribl'.
   */
  groupId?: string | undefined;
  /**
   * Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message
   */
  fromBeginning?: boolean | undefined;
  /**
   * @remarks
   *       Timeout used to detect client failures when using Kafka's group-management facilities.
   *       If the client sends no heartbeats to the broker before the timeout expires,
   *       the broker will remove the client from the group and initiate a rebalance.
   *       Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.
   */
  sessionTimeout?: number | undefined;
  /**
   *       Maximum allowed time for each worker to join the group after a rebalance begins.
   *
   * @remarks
   *       If the timeout is exceeded, the coordinator broker will remove the worker from the group.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.
   */
  rebalanceTimeout?: number | undefined;
  /**
   *       Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.
   *
   * @remarks
   *       Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.
   */
  heartbeatInterval?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  kafkaSchemaRegistry?:
    | models.KafkaSchemaRegistryAuthenticationType
    | undefined;
  /**
   * Maximum time to wait for a connection to complete successfully
   */
  connectionTimeout?: number | undefined;
  /**
   * Maximum time to wait for Kafka to respond to a request
   */
  requestTimeout?: number | undefined;
  /**
   * If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data
   */
  maxRetries?: number | undefined;
  /**
   * The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).
   */
  maxBackOff?: number | undefined;
  /**
   * Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).
   */
  initialBackoff?: number | undefined;
  /**
   * Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.
   */
  backoffRate?: number | undefined;
  /**
   * Maximum time to wait for Kafka to respond to an authentication request
   */
  authenticationTimeout?: number | undefined;
  /**
   * Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.
   */
  reauthenticationThreshold?: number | undefined;
  /**
   * AWS authentication method. Choose Auto to use IAM roles.
   */
  awsAuthenticationMethod?: string | undefined;
  awsSecretKey?: string | undefined;
  /**
   * Region where the MSK cluster is located
   */
  region: string;
  /**
   * MSK cluster service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to MSK cluster-compatible endpoint.
   */
  endpoint?: string | undefined;
  /**
   * Signature version to use for signing MSK cluster requests
   */
  signatureVersion?: models.SignatureVersionOptions | undefined;
  /**
   * Reuse connections between requests, which can improve performance
   */
  reuseConnections?: boolean | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * Use Assume Role credentials to access MSK
   */
  enableAssumeRole?: boolean | undefined;
  /**
   * Amazon Resource Name (ARN) of the role to assume
   */
  assumeRoleArn?: string | undefined;
  /**
   * External ID to use when assuming role
   */
  assumeRoleExternalId?: string | undefined;
  /**
   * Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
   */
  durationSeconds?: number | undefined;
  tls?: models.TlsSettingsClientSideType1 | undefined;
  /**
   * How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.
   */
  autoCommitInterval?: number | undefined;
  /**
   * How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.
   */
  autoCommitThreshold?: number | undefined;
  /**
   * Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).
   */
  maxBytesPerPartition?: number | undefined;
  /**
   * Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).
   */
  maxBytes?: number | undefined;
  /**
   * Maximum number of network errors before the consumer re-creates a socket
   */
  maxSocketErrors?: number | undefined;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  /**
   * Select or create a stored secret that references your access key and secret key
   */
  awsSecret?: string | undefined;
};

export type InputMsk =
  | InputMskSendToRoutesTrueWithConnectionsConstraint
  | InputMskSendToRoutesFalseWithConnectionsConstraint
  | InputMskPqEnabledFalseWithPqConstraint
  | InputMskPqEnabledTrueWithPqConstraint;

export const InputKafkaType = {
  Kafka: "kafka",
} as const;
export type InputKafkaType = ClosedEnum<typeof InputKafkaType>;

export type InputKafkaPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputKafkaType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Enter each Kafka bootstrap server you want to use. Specify the hostname and port (such as mykafkabroker:9092) or just the hostname (in which case @{product} will assign port 9092).
   */
  brokers: Array<string>;
  /**
   * Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only.
   */
  topics: Array<string>;
  /**
   * The consumer group to which this instance belongs. Defaults to 'Cribl'.
   */
  groupId?: string | undefined;
  /**
   * Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message
   */
  fromBeginning?: boolean | undefined;
  kafkaSchemaRegistry?:
    | models.KafkaSchemaRegistryAuthenticationType
    | undefined;
  /**
   * Maximum time to wait for a connection to complete successfully
   */
  connectionTimeout?: number | undefined;
  /**
   * Maximum time to wait for Kafka to respond to a request
   */
  requestTimeout?: number | undefined;
  /**
   * If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data
   */
  maxRetries?: number | undefined;
  /**
   * The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).
   */
  maxBackOff?: number | undefined;
  /**
   * Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).
   */
  initialBackoff?: number | undefined;
  /**
   * Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.
   */
  backoffRate?: number | undefined;
  /**
   * Maximum time to wait for Kafka to respond to an authentication request
   */
  authenticationTimeout?: number | undefined;
  /**
   * Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.
   */
  reauthenticationThreshold?: number | undefined;
  /**
   * Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.
   */
  sasl?: models.AuthenticationType | undefined;
  tls?: models.TlsSettingsClientSideTypeKafkaSchemaRegistry | undefined;
  /**
   * @remarks
   *       Timeout used to detect client failures when using Kafka's group-management facilities.
   *       If the client sends no heartbeats to the broker before the timeout expires,
   *       the broker will remove the client from the group and initiate a rebalance.
   *       Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.
   */
  sessionTimeout?: number | undefined;
  /**
   *       Maximum allowed time for each worker to join the group after a rebalance begins.
   *
   * @remarks
   *       If the timeout is exceeded, the coordinator broker will remove the worker from the group.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.
   */
  rebalanceTimeout?: number | undefined;
  /**
   *       Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.
   *
   * @remarks
   *       Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.
   */
  heartbeatInterval?: number | undefined;
  /**
   * How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.
   */
  autoCommitInterval?: number | undefined;
  /**
   * How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.
   */
  autoCommitThreshold?: number | undefined;
  /**
   * Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).
   */
  maxBytesPerPartition?: number | undefined;
  /**
   * Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).
   */
  maxBytes?: number | undefined;
  /**
   * Maximum number of network errors before the consumer re-creates a socket
   */
  maxSocketErrors?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputKafkaPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputKafkaType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Enter each Kafka bootstrap server you want to use. Specify the hostname and port (such as mykafkabroker:9092) or just the hostname (in which case @{product} will assign port 9092).
   */
  brokers: Array<string>;
  /**
   * Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only.
   */
  topics: Array<string>;
  /**
   * The consumer group to which this instance belongs. Defaults to 'Cribl'.
   */
  groupId?: string | undefined;
  /**
   * Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message
   */
  fromBeginning?: boolean | undefined;
  kafkaSchemaRegistry?:
    | models.KafkaSchemaRegistryAuthenticationType
    | undefined;
  /**
   * Maximum time to wait for a connection to complete successfully
   */
  connectionTimeout?: number | undefined;
  /**
   * Maximum time to wait for Kafka to respond to a request
   */
  requestTimeout?: number | undefined;
  /**
   * If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data
   */
  maxRetries?: number | undefined;
  /**
   * The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).
   */
  maxBackOff?: number | undefined;
  /**
   * Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).
   */
  initialBackoff?: number | undefined;
  /**
   * Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.
   */
  backoffRate?: number | undefined;
  /**
   * Maximum time to wait for Kafka to respond to an authentication request
   */
  authenticationTimeout?: number | undefined;
  /**
   * Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.
   */
  reauthenticationThreshold?: number | undefined;
  /**
   * Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.
   */
  sasl?: models.AuthenticationType | undefined;
  tls?: models.TlsSettingsClientSideTypeKafkaSchemaRegistry | undefined;
  /**
   * @remarks
   *       Timeout used to detect client failures when using Kafka's group-management facilities.
   *       If the client sends no heartbeats to the broker before the timeout expires,
   *       the broker will remove the client from the group and initiate a rebalance.
   *       Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.
   */
  sessionTimeout?: number | undefined;
  /**
   *       Maximum allowed time for each worker to join the group after a rebalance begins.
   *
   * @remarks
   *       If the timeout is exceeded, the coordinator broker will remove the worker from the group.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.
   */
  rebalanceTimeout?: number | undefined;
  /**
   *       Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.
   *
   * @remarks
   *       Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.
   */
  heartbeatInterval?: number | undefined;
  /**
   * How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.
   */
  autoCommitInterval?: number | undefined;
  /**
   * How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.
   */
  autoCommitThreshold?: number | undefined;
  /**
   * Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).
   */
  maxBytesPerPartition?: number | undefined;
  /**
   * Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).
   */
  maxBytes?: number | undefined;
  /**
   * Maximum number of network errors before the consumer re-creates a socket
   */
  maxSocketErrors?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputKafkaSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputKafkaType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Enter each Kafka bootstrap server you want to use. Specify the hostname and port (such as mykafkabroker:9092) or just the hostname (in which case @{product} will assign port 9092).
   */
  brokers: Array<string>;
  /**
   * Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only.
   */
  topics: Array<string>;
  /**
   * The consumer group to which this instance belongs. Defaults to 'Cribl'.
   */
  groupId?: string | undefined;
  /**
   * Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message
   */
  fromBeginning?: boolean | undefined;
  kafkaSchemaRegistry?:
    | models.KafkaSchemaRegistryAuthenticationType
    | undefined;
  /**
   * Maximum time to wait for a connection to complete successfully
   */
  connectionTimeout?: number | undefined;
  /**
   * Maximum time to wait for Kafka to respond to a request
   */
  requestTimeout?: number | undefined;
  /**
   * If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data
   */
  maxRetries?: number | undefined;
  /**
   * The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).
   */
  maxBackOff?: number | undefined;
  /**
   * Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).
   */
  initialBackoff?: number | undefined;
  /**
   * Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.
   */
  backoffRate?: number | undefined;
  /**
   * Maximum time to wait for Kafka to respond to an authentication request
   */
  authenticationTimeout?: number | undefined;
  /**
   * Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.
   */
  reauthenticationThreshold?: number | undefined;
  /**
   * Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.
   */
  sasl?: models.AuthenticationType | undefined;
  tls?: models.TlsSettingsClientSideTypeKafkaSchemaRegistry | undefined;
  /**
   * @remarks
   *       Timeout used to detect client failures when using Kafka's group-management facilities.
   *       If the client sends no heartbeats to the broker before the timeout expires,
   *       the broker will remove the client from the group and initiate a rebalance.
   *       Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.
   */
  sessionTimeout?: number | undefined;
  /**
   *       Maximum allowed time for each worker to join the group after a rebalance begins.
   *
   * @remarks
   *       If the timeout is exceeded, the coordinator broker will remove the worker from the group.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.
   */
  rebalanceTimeout?: number | undefined;
  /**
   *       Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.
   *
   * @remarks
   *       Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.
   */
  heartbeatInterval?: number | undefined;
  /**
   * How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.
   */
  autoCommitInterval?: number | undefined;
  /**
   * How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.
   */
  autoCommitThreshold?: number | undefined;
  /**
   * Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).
   */
  maxBytesPerPartition?: number | undefined;
  /**
   * Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).
   */
  maxBytes?: number | undefined;
  /**
   * Maximum number of network errors before the consumer re-creates a socket
   */
  maxSocketErrors?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputKafkaSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Select whether to send data to Routes, or directly to Destinations.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type: InputKafkaType;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process data from this Source before sending it through the Routes
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * Enter each Kafka bootstrap server you want to use. Specify the hostname and port (such as mykafkabroker:9092) or just the hostname (in which case @{product} will assign port 9092).
   */
  brokers: Array<string>;
  /**
   * Topic to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Kafka Source to a single topic only.
   */
  topics: Array<string>;
  /**
   * The consumer group to which this instance belongs. Defaults to 'Cribl'.
   */
  groupId?: string | undefined;
  /**
   * Leave enabled if you want the Source, upon first subscribing to a topic, to read starting with the earliest available message
   */
  fromBeginning?: boolean | undefined;
  kafkaSchemaRegistry?:
    | models.KafkaSchemaRegistryAuthenticationType
    | undefined;
  /**
   * Maximum time to wait for a connection to complete successfully
   */
  connectionTimeout?: number | undefined;
  /**
   * Maximum time to wait for Kafka to respond to a request
   */
  requestTimeout?: number | undefined;
  /**
   * If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data
   */
  maxRetries?: number | undefined;
  /**
   * The maximum wait time for a retry, in milliseconds. Default (and minimum) is 30,000 ms (30 seconds); maximum is 180,000 ms (180 seconds).
   */
  maxBackOff?: number | undefined;
  /**
   * Initial value used to calculate the retry, in milliseconds. Maximum is 600,000 ms (10 minutes).
   */
  initialBackoff?: number | undefined;
  /**
   * Set the backoff multiplier (2-20) to control the retry frequency for failed messages. For faster retries, use a lower multiplier. For slower retries with more delay between attempts, use a higher multiplier. The multiplier is used in an exponential backoff formula; see the Kafka [documentation](https://kafka.js.org/docs/retry-detailed) for details.
   */
  backoffRate?: number | undefined;
  /**
   * Maximum time to wait for Kafka to respond to an authentication request
   */
  authenticationTimeout?: number | undefined;
  /**
   * Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backward from the moment when credentials are set to expire.
   */
  reauthenticationThreshold?: number | undefined;
  /**
   * Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.
   */
  sasl?: models.AuthenticationType | undefined;
  tls?: models.TlsSettingsClientSideTypeKafkaSchemaRegistry | undefined;
  /**
   * @remarks
   *       Timeout used to detect client failures when using Kafka's group-management facilities.
   *       If the client sends no heartbeats to the broker before the timeout expires,
   *       the broker will remove the client from the group and initiate a rebalance.
   *       Value must be between the broker's configured group.min.session.timeout.ms and group.max.session.timeout.ms.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_session.timeout.ms) for details.
   */
  sessionTimeout?: number | undefined;
  /**
   *       Maximum allowed time for each worker to join the group after a rebalance begins.
   *
   * @remarks
   *       If the timeout is exceeded, the coordinator broker will remove the worker from the group.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#connectconfigs_rebalance.timeout.ms) for details.
   */
  rebalanceTimeout?: number | undefined;
  /**
   *       Expected time between heartbeats to the consumer coordinator when using Kafka's group-management facilities.
   *
   * @remarks
   *       Value must be lower than sessionTimeout and typically should not exceed 1/3 of the sessionTimeout value.
   *       See [Kafka's documentation](https://kafka.apache.org/documentation/#consumerconfigs_heartbeat.interval.ms) for details.
   */
  heartbeatInterval?: number | undefined;
  /**
   * How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.
   */
  autoCommitInterval?: number | undefined;
  /**
   * How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.
   */
  autoCommitThreshold?: number | undefined;
  /**
   * Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).
   */
  maxBytesPerPartition?: number | undefined;
  /**
   * Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).
   */
  maxBytes?: number | undefined;
  /**
   * Maximum number of network errors before the consumer re-creates a socket
   */
  maxSocketErrors?: number | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  description?: string | undefined;
};

export type InputKafka =
  | InputKafkaSendToRoutesTrueWithConnectionsConstraint
  | InputKafkaSendToRoutesFalseWithConnectionsConstraint
  | InputKafkaPqEnabledFalseWithPqConstraint
  | InputKafkaPqEnabledTrueWithPqConstraint;

export const InputCollectionType = {
  Collection: "collection",
} as const;
export type InputCollectionType = ClosedEnum<typeof InputCollectionType>;

export type InputCollectionPqEnabledTrueWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type?: InputCollectionType | undefined;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process results
   */
  pipeline?: string | undefined;
  /**
   * Send events to normal routing and event processing. Disable to select a specific Pipeline/Destination combination.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  preprocess?: models.PreprocessTypeSavedJobCollectionInput | undefined;
  /**
   * Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.
   */
  throttleRatePerSec?: string | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Destination to send results to
   */
  output?: string | undefined;
};

export type InputCollectionPqEnabledFalseWithPqConstraint = {
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  pq?: models.PqType | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type?: InputCollectionType | undefined;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process results
   */
  pipeline?: string | undefined;
  /**
   * Send events to normal routing and event processing. Disable to select a specific Pipeline/Destination combination.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  preprocess?: models.PreprocessTypeSavedJobCollectionInput | undefined;
  /**
   * Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.
   */
  throttleRatePerSec?: string | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Destination to send results to
   */
  output?: string | undefined;
};

export type InputCollectionSendToRoutesFalseWithConnectionsConstraint = {
  /**
   * Send events to normal routing and event processing. Disable to select a specific Pipeline/Destination combination.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type?: InputCollectionType | undefined;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process results
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  preprocess?: models.PreprocessTypeSavedJobCollectionInput | undefined;
  /**
   * Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.
   */
  throttleRatePerSec?: string | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Destination to send results to
   */
  output?: string | undefined;
};

export type InputCollectionSendToRoutesTrueWithConnectionsConstraint = {
  /**
   * Send events to normal routing and event processing. Disable to select a specific Pipeline/Destination combination.
   */
  sendToRoutes?: boolean | undefined;
  /**
   * Direct connections to Destinations, and optionally via a Pipeline or a Pack
   */
  connections?: Array<models.ItemsTypeConnections> | undefined;
  /**
   * Unique ID for this input
   */
  id: string;
  type?: InputCollectionType | undefined;
  disabled?: boolean | undefined;
  /**
   * Pipeline to process results
   */
  pipeline?: string | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Use a disk queue to minimize data loss when connected services block. See [Cribl Docs](https://docs.cribl.io/stream/persistent-queues) for PQ defaults (Cribl-managed Cloud Workers) and configuration options (on-prem and hybrid Workers).
   */
  pqEnabled?: boolean | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  pq?: models.PqType | undefined;
  /**
   * A list of event-breaking rulesets that will be applied, in order, to the input data stream
   */
  breakerRulesets?: Array<string> | undefined;
  /**
   * How long (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel before flushing the data stream out, as is, to the Pipelines
   */
  staleChannelFlushMs?: number | undefined;
  preprocess?: models.PreprocessTypeSavedJobCollectionInput | undefined;
  /**
   * Rate (in bytes per second) to throttle while writing to an output. Accepts values with multiple-byte units, such as KB, MB, and GB. (Example: 42 MB) Default value of 0 specifies no throttling.
   */
  throttleRatePerSec?: string | undefined;
  /**
   * Fields to add to events from this input
   */
  metadata?: Array<models.ItemsTypeNotificationMetadata> | undefined;
  /**
   * Destination to send results to
   */
  output?: string | undefined;
};

export type InputCollection =
  | InputCollectionSendToRoutesTrueWithConnectionsConstraint
  | InputCollectionSendToRoutesFalseWithConnectionsConstraint
  | InputCollectionPqEnabledFalseWithPqConstraint
  | InputCollectionPqEnabledTrueWithPqConstraint;

/**
 * Input object
 */
export type CreateInputRequest =
  | (
    | InputCollectionSendToRoutesTrueWithConnectionsConstraint
    | InputCollectionSendToRoutesFalseWithConnectionsConstraint
    | InputCollectionPqEnabledFalseWithPqConstraint
    | InputCollectionPqEnabledTrueWithPqConstraint & { type: "collection" }
  )
  | (
    | InputKafkaSendToRoutesTrueWithConnectionsConstraint
    | InputKafkaSendToRoutesFalseWithConnectionsConstraint
    | InputKafkaPqEnabledFalseWithPqConstraint
    | InputKafkaPqEnabledTrueWithPqConstraint & { type: "kafka" }
  )
  | (
    | InputMskSendToRoutesTrueWithConnectionsConstraint
    | InputMskSendToRoutesFalseWithConnectionsConstraint
    | InputMskPqEnabledFalseWithPqConstraint
    | InputMskPqEnabledTrueWithPqConstraint & { type: "msk" }
  )
  | (
    | InputHttpSendToRoutesTrueWithConnectionsConstraint
    | InputHttpSendToRoutesFalseWithConnectionsConstraint
    | InputHttpPqEnabledFalseWithPqConstraint
    | InputHttpPqEnabledTrueWithPqConstraint & { type: "http" }
  )
  | (
    | InputSplunkSendToRoutesTrueWithConnectionsConstraint
    | InputSplunkSendToRoutesFalseWithConnectionsConstraint
    | InputSplunkPqEnabledFalseWithPqConstraint
    | InputSplunkPqEnabledTrueWithPqConstraint & { type: "splunk" }
  )
  | (
    | InputSplunkSearchSendToRoutesTrueWithConnectionsConstraint
    | InputSplunkSearchSendToRoutesFalseWithConnectionsConstraint
    | InputSplunkSearchPqEnabledFalseWithPqConstraint
    | InputSplunkSearchPqEnabledTrueWithPqConstraint & { type: "splunk_search" }
  )
  | (
    | InputSplunkHecSendToRoutesTrueWithConnectionsConstraint
    | InputSplunkHecSendToRoutesFalseWithConnectionsConstraint
    | InputSplunkHecPqEnabledFalseWithPqConstraint
    | InputSplunkHecPqEnabledTrueWithPqConstraint & { type: "splunk_hec" }
  )
  | (
    | InputAzureBlobSendToRoutesTrueWithConnectionsConstraint
    | InputAzureBlobSendToRoutesFalseWithConnectionsConstraint
    | InputAzureBlobPqEnabledFalseWithPqConstraint
    | InputAzureBlobPqEnabledTrueWithPqConstraint & { type: "azure_blob" }
  )
  | (
    | InputElasticSendToRoutesTrueWithConnectionsConstraint
    | InputElasticSendToRoutesFalseWithConnectionsConstraint
    | InputElasticPqEnabledFalseWithPqConstraint
    | InputElasticPqEnabledTrueWithPqConstraint & { type: "elastic" }
  )
  | (
    | InputConfluentCloudSendToRoutesTrueWithConnectionsConstraint
    | InputConfluentCloudSendToRoutesFalseWithConnectionsConstraint
    | InputConfluentCloudPqEnabledFalseWithPqConstraint
    | InputConfluentCloudPqEnabledTrueWithPqConstraint & {
      type: "confluent_cloud";
    }
  )
  | (InputGrafanaGrafana1 | InputGrafanaGrafana2 & { type: "grafana" })
  | (
    | InputLokiSendToRoutesTrueWithConnectionsConstraint
    | InputLokiSendToRoutesFalseWithConnectionsConstraint
    | InputLokiPqEnabledFalseWithPqConstraint
    | InputLokiPqEnabledTrueWithPqConstraint & { type: "loki" }
  )
  | (
    | InputPrometheusRwSendToRoutesTrueWithConnectionsConstraint
    | InputPrometheusRwSendToRoutesFalseWithConnectionsConstraint
    | InputPrometheusRwPqEnabledFalseWithPqConstraint
    | InputPrometheusRwPqEnabledTrueWithPqConstraint & { type: "prometheus_rw" }
  )
  | (
    | InputPrometheusSendToRoutesTrueWithConnectionsConstraint
    | InputPrometheusSendToRoutesFalseWithConnectionsConstraint
    | InputPrometheusPqEnabledFalseWithPqConstraint
    | InputPrometheusPqEnabledTrueWithPqConstraint & { type: "prometheus" }
  )
  | (
    | InputEdgePrometheusSendToRoutesTrueWithConnectionsConstraint
    | InputEdgePrometheusSendToRoutesFalseWithConnectionsConstraint
    | InputEdgePrometheusPqEnabledFalseWithPqConstraint
    | InputEdgePrometheusPqEnabledTrueWithPqConstraint & {
      type: "edge_prometheus";
    }
  )
  | (
    | InputOffice365MgmtSendToRoutesTrueWithConnectionsConstraint
    | InputOffice365MgmtSendToRoutesFalseWithConnectionsConstraint
    | InputOffice365MgmtPqEnabledFalseWithPqConstraint
    | InputOffice365MgmtPqEnabledTrueWithPqConstraint & {
      type: "office365_mgmt";
    }
  )
  | (
    | InputOffice365ServiceSendToRoutesTrueWithConnectionsConstraint
    | InputOffice365ServiceSendToRoutesFalseWithConnectionsConstraint
    | InputOffice365ServicePqEnabledFalseWithPqConstraint
    | InputOffice365ServicePqEnabledTrueWithPqConstraint & {
      type: "office365_service";
    }
  )
  | (
    | InputOffice365MsgTraceSendToRoutesTrueWithConnectionsConstraint
    | InputOffice365MsgTraceSendToRoutesFalseWithConnectionsConstraint
    | InputOffice365MsgTracePqEnabledFalseWithPqConstraint
    | InputOffice365MsgTracePqEnabledTrueWithPqConstraint & {
      type: "office365_msg_trace";
    }
  )
  | (
    | InputEventhubSendToRoutesTrueWithConnectionsConstraint
    | InputEventhubSendToRoutesFalseWithConnectionsConstraint
    | InputEventhubPqEnabledFalseWithPqConstraint
    | InputEventhubPqEnabledTrueWithPqConstraint & { type: "eventhub" }
  )
  | (
    | InputExecSendToRoutesTrueWithConnectionsConstraint
    | InputExecSendToRoutesFalseWithConnectionsConstraint
    | InputExecPqEnabledFalseWithPqConstraint
    | InputExecPqEnabledTrueWithPqConstraint & { type: "exec" }
  )
  | (
    | InputFirehoseSendToRoutesTrueWithConnectionsConstraint
    | InputFirehoseSendToRoutesFalseWithConnectionsConstraint
    | InputFirehosePqEnabledFalseWithPqConstraint
    | InputFirehosePqEnabledTrueWithPqConstraint & { type: "firehose" }
  )
  | (
    | InputGooglePubsubSendToRoutesTrueWithConnectionsConstraint
    | InputGooglePubsubSendToRoutesFalseWithConnectionsConstraint
    | InputGooglePubsubPqEnabledFalseWithPqConstraint
    | InputGooglePubsubPqEnabledTrueWithPqConstraint & { type: "google_pubsub" }
  )
  | (
    | InputCriblSendToRoutesTrueWithConnectionsConstraint
    | InputCriblSendToRoutesFalseWithConnectionsConstraint
    | InputCriblPqEnabledFalseWithPqConstraint
    | InputCriblPqEnabledTrueWithPqConstraint & { type: "cribl" }
  )
  | (
    | InputCriblTcpSendToRoutesTrueWithConnectionsConstraint
    | InputCriblTcpSendToRoutesFalseWithConnectionsConstraint
    | InputCriblTcpPqEnabledFalseWithPqConstraint
    | InputCriblTcpPqEnabledTrueWithPqConstraint & { type: "cribl_tcp" }
  )
  | (
    | InputCriblHttpSendToRoutesTrueWithConnectionsConstraint
    | InputCriblHttpSendToRoutesFalseWithConnectionsConstraint
    | InputCriblHttpPqEnabledFalseWithPqConstraint
    | InputCriblHttpPqEnabledTrueWithPqConstraint & { type: "cribl_http" }
  )
  | (
    | InputCriblLakeHttpSendToRoutesTrueWithConnectionsConstraint
    | InputCriblLakeHttpSendToRoutesFalseWithConnectionsConstraint
    | InputCriblLakeHttpPqEnabledFalseWithPqConstraint
    | InputCriblLakeHttpPqEnabledTrueWithPqConstraint & {
      type: "cribl_lake_http";
    }
  )
  | (
    | InputTcpjsonSendToRoutesTrueWithConnectionsConstraint
    | InputTcpjsonSendToRoutesFalseWithConnectionsConstraint
    | InputTcpjsonPqEnabledFalseWithPqConstraint
    | InputTcpjsonPqEnabledTrueWithPqConstraint & { type: "tcpjson" }
  )
  | (
    | InputSystemMetricsSendToRoutesTrueWithConnectionsConstraint
    | InputSystemMetricsSendToRoutesFalseWithConnectionsConstraint
    | InputSystemMetricsPqEnabledFalseWithPqConstraint
    | InputSystemMetricsPqEnabledTrueWithPqConstraint & {
      type: "system_metrics";
    }
  )
  | (
    | InputSystemStateSendToRoutesTrueWithConnectionsConstraint
    | InputSystemStateSendToRoutesFalseWithConnectionsConstraint
    | InputSystemStatePqEnabledFalseWithPqConstraint
    | InputSystemStatePqEnabledTrueWithPqConstraint & { type: "system_state" }
  )
  | (
    | InputKubeMetricsSendToRoutesTrueWithConnectionsConstraint
    | InputKubeMetricsSendToRoutesFalseWithConnectionsConstraint
    | InputKubeMetricsPqEnabledFalseWithPqConstraint
    | InputKubeMetricsPqEnabledTrueWithPqConstraint & { type: "kube_metrics" }
  )
  | (
    | InputKubeLogsSendToRoutesTrueWithConnectionsConstraint
    | InputKubeLogsSendToRoutesFalseWithConnectionsConstraint
    | InputKubeLogsPqEnabledFalseWithPqConstraint
    | InputKubeLogsPqEnabledTrueWithPqConstraint & { type: "kube_logs" }
  )
  | (
    | InputKubeEventsSendToRoutesTrueWithConnectionsConstraint
    | InputKubeEventsSendToRoutesFalseWithConnectionsConstraint
    | InputKubeEventsPqEnabledFalseWithPqConstraint
    | InputKubeEventsPqEnabledTrueWithPqConstraint & { type: "kube_events" }
  )
  | (
    | InputWindowsMetricsSendToRoutesTrueWithConnectionsConstraint
    | InputWindowsMetricsSendToRoutesFalseWithConnectionsConstraint
    | InputWindowsMetricsPqEnabledFalseWithPqConstraint
    | InputWindowsMetricsPqEnabledTrueWithPqConstraint & {
      type: "windows_metrics";
    }
  )
  | (
    | InputCrowdstrikeSendToRoutesTrueWithConnectionsConstraint
    | InputCrowdstrikeSendToRoutesFalseWithConnectionsConstraint
    | InputCrowdstrikePqEnabledFalseWithPqConstraint
    | InputCrowdstrikePqEnabledTrueWithPqConstraint & { type: "crowdstrike" }
  )
  | (
    | InputDatadogAgentSendToRoutesTrueWithConnectionsConstraint
    | InputDatadogAgentSendToRoutesFalseWithConnectionsConstraint
    | InputDatadogAgentPqEnabledFalseWithPqConstraint
    | InputDatadogAgentPqEnabledTrueWithPqConstraint & { type: "datadog_agent" }
  )
  | (
    | InputDatagenSendToRoutesTrueWithConnectionsConstraint
    | InputDatagenSendToRoutesFalseWithConnectionsConstraint
    | InputDatagenPqEnabledFalseWithPqConstraint
    | InputDatagenPqEnabledTrueWithPqConstraint & { type: "datagen" }
  )
  | (
    | InputHttpRawSendToRoutesTrueWithConnectionsConstraint
    | InputHttpRawSendToRoutesFalseWithConnectionsConstraint
    | InputHttpRawPqEnabledFalseWithPqConstraint
    | InputHttpRawPqEnabledTrueWithPqConstraint & { type: "http_raw" }
  )
  | (
    | InputKinesisSendToRoutesTrueWithConnectionsConstraint
    | InputKinesisSendToRoutesFalseWithConnectionsConstraint
    | InputKinesisPqEnabledFalseWithPqConstraint
    | InputKinesisPqEnabledTrueWithPqConstraint & { type: "kinesis" }
  )
  | (
    | InputCriblmetricsSendToRoutesTrueWithConnectionsConstraint
    | InputCriblmetricsSendToRoutesFalseWithConnectionsConstraint
    | InputCriblmetricsPqEnabledFalseWithPqConstraint
    | InputCriblmetricsPqEnabledTrueWithPqConstraint & { type: "criblmetrics" }
  )
  | (
    | InputMetricsSendToRoutesTrueWithConnectionsConstraint
    | InputMetricsSendToRoutesFalseWithConnectionsConstraint
    | InputMetricsPqEnabledFalseWithPqConstraint
    | InputMetricsPqEnabledTrueWithPqConstraint & { type: "metrics" }
  )
  | (
    | InputS3SendToRoutesTrueWithConnectionsConstraint
    | InputS3SendToRoutesFalseWithConnectionsConstraint
    | InputS3PqEnabledFalseWithPqConstraint
    | InputS3PqEnabledTrueWithPqConstraint & { type: "s3" }
  )
  | (
    | InputS3InventorySendToRoutesTrueWithConnectionsConstraint
    | InputS3InventorySendToRoutesFalseWithConnectionsConstraint
    | InputS3InventoryPqEnabledFalseWithPqConstraint
    | InputS3InventoryPqEnabledTrueWithPqConstraint & { type: "s3_inventory" }
  )
  | (
    | InputSnmpSendToRoutesTrueWithConnectionsConstraint
    | InputSnmpSendToRoutesFalseWithConnectionsConstraint
    | InputSnmpPqEnabledFalseWithPqConstraint
    | InputSnmpPqEnabledTrueWithPqConstraint & { type: "snmp" }
  )
  | (
    | InputOpenTelemetrySendToRoutesTrueWithConnectionsConstraint
    | InputOpenTelemetrySendToRoutesFalseWithConnectionsConstraint
    | InputOpenTelemetryPqEnabledFalseWithPqConstraint
    | InputOpenTelemetryPqEnabledTrueWithPqConstraint & {
      type: "open_telemetry";
    }
  )
  | (
    | InputModelDrivenTelemetrySendToRoutesTrueWithConnectionsConstraint
    | InputModelDrivenTelemetrySendToRoutesFalseWithConnectionsConstraint
    | InputModelDrivenTelemetryPqEnabledFalseWithPqConstraint
    | InputModelDrivenTelemetryPqEnabledTrueWithPqConstraint & {
      type: "model_driven_telemetry";
    }
  )
  | (
    | InputSqsSendToRoutesTrueWithConnectionsConstraint
    | InputSqsSendToRoutesFalseWithConnectionsConstraint
    | InputSqsPqEnabledFalseWithPqConstraint
    | InputSqsPqEnabledTrueWithPqConstraint & { type: "sqs" }
  )
  | (InputSyslogSyslog1 | InputSyslogSyslog2 & { type: "syslog" })
  | (
    | InputFileSendToRoutesTrueWithConnectionsConstraint
    | InputFileSendToRoutesFalseWithConnectionsConstraint
    | InputFilePqEnabledFalseWithPqConstraint
    | InputFilePqEnabledTrueWithPqConstraint & { type: "file" }
  )
  | (
    | InputTcpSendToRoutesTrueWithConnectionsConstraint
    | InputTcpSendToRoutesFalseWithConnectionsConstraint
    | InputTcpPqEnabledFalseWithPqConstraint
    | InputTcpPqEnabledTrueWithPqConstraint & { type: "tcp" }
  )
  | (
    | InputAppscopeSendToRoutesTrueWithConnectionsConstraint
    | InputAppscopeSendToRoutesFalseWithConnectionsConstraint
    | InputAppscopePqEnabledFalseWithPqConstraint
    | InputAppscopePqEnabledTrueWithPqConstraint & { type: "appscope" }
  )
  | (
    | InputWefSendToRoutesTrueWithConnectionsConstraint
    | InputWefSendToRoutesFalseWithConnectionsConstraint
    | InputWefPqEnabledFalseWithPqConstraint
    | InputWefPqEnabledTrueWithPqConstraint & { type: "wef" }
  )
  | (
    | InputWinEventLogsSendToRoutesTrueWithConnectionsConstraint
    | InputWinEventLogsSendToRoutesFalseWithConnectionsConstraint
    | InputWinEventLogsPqEnabledFalseWithPqConstraint
    | InputWinEventLogsPqEnabledTrueWithPqConstraint & {
      type: "win_event_logs";
    }
  )
  | (
    | InputRawUdpSendToRoutesTrueWithConnectionsConstraint
    | InputRawUdpSendToRoutesFalseWithConnectionsConstraint
    | InputRawUdpPqEnabledFalseWithPqConstraint
    | InputRawUdpPqEnabledTrueWithPqConstraint & { type: "raw_udp" }
  )
  | (
    | InputJournalFilesSendToRoutesTrueWithConnectionsConstraint
    | InputJournalFilesSendToRoutesFalseWithConnectionsConstraint
    | InputJournalFilesPqEnabledFalseWithPqConstraint
    | InputJournalFilesPqEnabledTrueWithPqConstraint & { type: "journal_files" }
  )
  | (
    | InputWizSendToRoutesTrueWithConnectionsConstraint
    | InputWizSendToRoutesFalseWithConnectionsConstraint
    | InputWizPqEnabledFalseWithPqConstraint
    | InputWizPqEnabledTrueWithPqConstraint & { type: "wiz" }
  )
  | (
    | InputWizWebhookSendToRoutesTrueWithConnectionsConstraint
    | InputWizWebhookSendToRoutesFalseWithConnectionsConstraint
    | InputWizWebhookPqEnabledFalseWithPqConstraint
    | InputWizWebhookPqEnabledTrueWithPqConstraint & { type: "wiz_webhook" }
  )
  | (
    | InputNetflowSendToRoutesTrueWithConnectionsConstraint
    | InputNetflowSendToRoutesFalseWithConnectionsConstraint
    | InputNetflowPqEnabledFalseWithPqConstraint
    | InputNetflowPqEnabledTrueWithPqConstraint & { type: "netflow" }
  )
  | (
    | InputSecurityLakeSendToRoutesTrueWithConnectionsConstraint
    | InputSecurityLakeSendToRoutesFalseWithConnectionsConstraint
    | InputSecurityLakePqEnabledFalseWithPqConstraint
    | InputSecurityLakePqEnabledTrueWithPqConstraint & { type: "security_lake" }
  )
  | (
    | InputZscalerHecSendToRoutesTrueWithConnectionsConstraint
    | InputZscalerHecSendToRoutesFalseWithConnectionsConstraint
    | InputZscalerHecPqEnabledFalseWithPqConstraint
    | InputZscalerHecPqEnabledTrueWithPqConstraint & { type: "zscaler_hec" }
  )
  | (
    | InputCloudflareHecSendToRoutesTrueWithConnectionsConstraint
    | InputCloudflareHecSendToRoutesFalseWithConnectionsConstraint
    | InputCloudflareHecPqEnabledFalseWithPqConstraint
    | InputCloudflareHecPqEnabledTrueWithPqConstraint & {
      type: "cloudflare_hec";
    }
  );

/** @internal */
export const InputCloudflareHecType$outboundSchema: z.ZodNativeEnum<
  typeof InputCloudflareHecType
> = z.nativeEnum(InputCloudflareHecType);

/** @internal */
export const InputCloudflareHecAuthenticationMethod$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  InputCloudflareHecAuthenticationMethod
> = openEnums.outboundSchema(InputCloudflareHecAuthenticationMethod);

/** @internal */
export type InputCloudflareHecAuthToken$Outbound = {
  authType: string;
  tokenSecret?: string | undefined;
  token?: string | undefined;
  enabled: boolean;
  description?: string | undefined;
  allowedIndexesAtToken?: Array<string> | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
};

/** @internal */
export const InputCloudflareHecAuthToken$outboundSchema: z.ZodType<
  InputCloudflareHecAuthToken$Outbound,
  z.ZodTypeDef,
  InputCloudflareHecAuthToken
> = z.object({
  authType: InputCloudflareHecAuthenticationMethod$outboundSchema.default(
    "secret",
  ),
  tokenSecret: z.string().optional(),
  token: z.string().optional(),
  enabled: z.boolean().default(true),
  description: z.string().optional(),
  allowedIndexesAtToken: z.array(z.string()).optional(),
  metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
    .optional(),
});

export function inputCloudflareHecAuthTokenToJSON(
  inputCloudflareHecAuthToken: InputCloudflareHecAuthToken,
): string {
  return JSON.stringify(
    InputCloudflareHecAuthToken$outboundSchema.parse(
      inputCloudflareHecAuthToken,
    ),
  );
}

/** @internal */
export type InputCloudflareHecPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  authTokens?: Array<InputCloudflareHecAuthToken$Outbound> | undefined;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  activityLogSampleRate: number;
  requestTimeout: number;
  socketTimeout: number;
  keepAliveTimeout: number;
  enableHealthCheck?: any | undefined;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  hecAPI: string;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  allowedIndexes?: Array<string> | undefined;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  accessControlAllowOrigin?: Array<string> | undefined;
  accessControlAllowHeaders?: Array<string> | undefined;
  emitTokenMetrics: boolean;
  description?: string | undefined;
};

/** @internal */
export const InputCloudflareHecPqEnabledTrueWithPqConstraint$outboundSchema:
  z.ZodType<
    InputCloudflareHecPqEnabledTrueWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputCloudflareHecPqEnabledTrueWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputCloudflareHecType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    authTokens: z.array(
      z.lazy(() => InputCloudflareHecAuthToken$outboundSchema),
    ).optional(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.any().optional(),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    hecAPI: z.string(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    allowedIndexes: z.array(z.string()).optional(),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    accessControlAllowOrigin: z.array(z.string()).optional(),
    accessControlAllowHeaders: z.array(z.string()).optional(),
    emitTokenMetrics: z.boolean().default(false),
    description: z.string().optional(),
  });

export function inputCloudflareHecPqEnabledTrueWithPqConstraintToJSON(
  inputCloudflareHecPqEnabledTrueWithPqConstraint:
    InputCloudflareHecPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputCloudflareHecPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputCloudflareHecPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputCloudflareHecPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  authTokens?: Array<InputCloudflareHecAuthToken$Outbound> | undefined;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  activityLogSampleRate: number;
  requestTimeout: number;
  socketTimeout: number;
  keepAliveTimeout: number;
  enableHealthCheck?: any | undefined;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  hecAPI: string;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  allowedIndexes?: Array<string> | undefined;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  accessControlAllowOrigin?: Array<string> | undefined;
  accessControlAllowHeaders?: Array<string> | undefined;
  emitTokenMetrics: boolean;
  description?: string | undefined;
};

/** @internal */
export const InputCloudflareHecPqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputCloudflareHecPqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputCloudflareHecPqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputCloudflareHecType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    authTokens: z.array(
      z.lazy(() => InputCloudflareHecAuthToken$outboundSchema),
    ).optional(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.any().optional(),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    hecAPI: z.string(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    allowedIndexes: z.array(z.string()).optional(),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    accessControlAllowOrigin: z.array(z.string()).optional(),
    accessControlAllowHeaders: z.array(z.string()).optional(),
    emitTokenMetrics: z.boolean().default(false),
    description: z.string().optional(),
  });

export function inputCloudflareHecPqEnabledFalseWithPqConstraintToJSON(
  inputCloudflareHecPqEnabledFalseWithPqConstraint:
    InputCloudflareHecPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputCloudflareHecPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputCloudflareHecPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputCloudflareHecSendToRoutesFalseWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    host: string;
    port: number;
    authTokens?: Array<InputCloudflareHecAuthToken$Outbound> | undefined;
    tls?: models.TlsSettingsServerSideType$Outbound | undefined;
    maxActiveReq: number;
    maxRequestsPerSocket: number;
    enableProxyHeader: boolean;
    captureHeaders: boolean;
    activityLogSampleRate: number;
    requestTimeout: number;
    socketTimeout: number;
    keepAliveTimeout: number;
    enableHealthCheck?: any | undefined;
    ipAllowlistRegex: string;
    ipDenylistRegex: string;
    hecAPI: string;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    allowedIndexes?: Array<string> | undefined;
    breakerRulesets?: Array<string> | undefined;
    staleChannelFlushMs: number;
    accessControlAllowOrigin?: Array<string> | undefined;
    accessControlAllowHeaders?: Array<string> | undefined;
    emitTokenMetrics: boolean;
    description?: string | undefined;
  };

/** @internal */
export const InputCloudflareHecSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputCloudflareHecSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputCloudflareHecSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputCloudflareHecType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    authTokens: z.array(
      z.lazy(() => InputCloudflareHecAuthToken$outboundSchema),
    ).optional(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.any().optional(),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    hecAPI: z.string(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    allowedIndexes: z.array(z.string()).optional(),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    accessControlAllowOrigin: z.array(z.string()).optional(),
    accessControlAllowHeaders: z.array(z.string()).optional(),
    emitTokenMetrics: z.boolean().default(false),
    description: z.string().optional(),
  });

export function inputCloudflareHecSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputCloudflareHecSendToRoutesFalseWithConnectionsConstraint:
    InputCloudflareHecSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputCloudflareHecSendToRoutesFalseWithConnectionsConstraint$outboundSchema
      .parse(inputCloudflareHecSendToRoutesFalseWithConnectionsConstraint),
  );
}

/** @internal */
export type InputCloudflareHecSendToRoutesTrueWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    host: string;
    port: number;
    authTokens?: Array<InputCloudflareHecAuthToken$Outbound> | undefined;
    tls?: models.TlsSettingsServerSideType$Outbound | undefined;
    maxActiveReq: number;
    maxRequestsPerSocket: number;
    enableProxyHeader: boolean;
    captureHeaders: boolean;
    activityLogSampleRate: number;
    requestTimeout: number;
    socketTimeout: number;
    keepAliveTimeout: number;
    enableHealthCheck?: any | undefined;
    ipAllowlistRegex: string;
    ipDenylistRegex: string;
    hecAPI: string;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    allowedIndexes?: Array<string> | undefined;
    breakerRulesets?: Array<string> | undefined;
    staleChannelFlushMs: number;
    accessControlAllowOrigin?: Array<string> | undefined;
    accessControlAllowHeaders?: Array<string> | undefined;
    emitTokenMetrics: boolean;
    description?: string | undefined;
  };

/** @internal */
export const InputCloudflareHecSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputCloudflareHecSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputCloudflareHecSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputCloudflareHecType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    authTokens: z.array(
      z.lazy(() => InputCloudflareHecAuthToken$outboundSchema),
    ).optional(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.any().optional(),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    hecAPI: z.string(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    allowedIndexes: z.array(z.string()).optional(),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    accessControlAllowOrigin: z.array(z.string()).optional(),
    accessControlAllowHeaders: z.array(z.string()).optional(),
    emitTokenMetrics: z.boolean().default(false),
    description: z.string().optional(),
  });

export function inputCloudflareHecSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputCloudflareHecSendToRoutesTrueWithConnectionsConstraint:
    InputCloudflareHecSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputCloudflareHecSendToRoutesTrueWithConnectionsConstraint$outboundSchema
      .parse(inputCloudflareHecSendToRoutesTrueWithConnectionsConstraint),
  );
}

/** @internal */
export type InputCloudflareHec$Outbound =
  | InputCloudflareHecSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputCloudflareHecSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputCloudflareHecPqEnabledFalseWithPqConstraint$Outbound
  | InputCloudflareHecPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputCloudflareHec$outboundSchema: z.ZodType<
  InputCloudflareHec$Outbound,
  z.ZodTypeDef,
  InputCloudflareHec
> = z.union([
  z.lazy(() =>
    InputCloudflareHecSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputCloudflareHecSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputCloudflareHecPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputCloudflareHecPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputCloudflareHecToJSON(
  inputCloudflareHec: InputCloudflareHec,
): string {
  return JSON.stringify(
    InputCloudflareHec$outboundSchema.parse(inputCloudflareHec),
  );
}

/** @internal */
export const InputZscalerHecType$outboundSchema: z.ZodNativeEnum<
  typeof InputZscalerHecType
> = z.nativeEnum(InputZscalerHecType);

/** @internal */
export type InputZscalerHecAuthToken$Outbound = {
  authType: string;
  tokenSecret?: string | undefined;
  token: string;
  enabled: boolean;
  description?: string | undefined;
  allowedIndexesAtToken?: Array<string> | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
};

/** @internal */
export const InputZscalerHecAuthToken$outboundSchema: z.ZodType<
  InputZscalerHecAuthToken$Outbound,
  z.ZodTypeDef,
  InputZscalerHecAuthToken
> = z.object({
  authType: models.AuthenticationMethodOptionsAuthTokensItems$outboundSchema
    .default("manual"),
  tokenSecret: z.string().optional(),
  token: z.string(),
  enabled: z.boolean().default(true),
  description: z.string().optional(),
  allowedIndexesAtToken: z.array(z.string()).optional(),
  metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
    .optional(),
});

export function inputZscalerHecAuthTokenToJSON(
  inputZscalerHecAuthToken: InputZscalerHecAuthToken,
): string {
  return JSON.stringify(
    InputZscalerHecAuthToken$outboundSchema.parse(inputZscalerHecAuthToken),
  );
}

/** @internal */
export type InputZscalerHecPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  authTokens?: Array<InputZscalerHecAuthToken$Outbound> | undefined;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  activityLogSampleRate: number;
  requestTimeout: number;
  socketTimeout: number;
  keepAliveTimeout: number;
  enableHealthCheck?: any | undefined;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  hecAPI: string;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  allowedIndexes?: Array<string> | undefined;
  hecAcks: boolean;
  accessControlAllowOrigin?: Array<string> | undefined;
  accessControlAllowHeaders?: Array<string> | undefined;
  emitTokenMetrics: boolean;
  description?: string | undefined;
};

/** @internal */
export const InputZscalerHecPqEnabledTrueWithPqConstraint$outboundSchema:
  z.ZodType<
    InputZscalerHecPqEnabledTrueWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputZscalerHecPqEnabledTrueWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputZscalerHecType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    authTokens: z.array(z.lazy(() => InputZscalerHecAuthToken$outboundSchema))
      .optional(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.any().optional(),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    hecAPI: z.string().default("/services/collector"),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    allowedIndexes: z.array(z.string()).optional(),
    hecAcks: z.boolean().default(false),
    accessControlAllowOrigin: z.array(z.string()).optional(),
    accessControlAllowHeaders: z.array(z.string()).optional(),
    emitTokenMetrics: z.boolean().default(false),
    description: z.string().optional(),
  });

export function inputZscalerHecPqEnabledTrueWithPqConstraintToJSON(
  inputZscalerHecPqEnabledTrueWithPqConstraint:
    InputZscalerHecPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputZscalerHecPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputZscalerHecPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputZscalerHecPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  authTokens?: Array<InputZscalerHecAuthToken$Outbound> | undefined;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  activityLogSampleRate: number;
  requestTimeout: number;
  socketTimeout: number;
  keepAliveTimeout: number;
  enableHealthCheck?: any | undefined;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  hecAPI: string;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  allowedIndexes?: Array<string> | undefined;
  hecAcks: boolean;
  accessControlAllowOrigin?: Array<string> | undefined;
  accessControlAllowHeaders?: Array<string> | undefined;
  emitTokenMetrics: boolean;
  description?: string | undefined;
};

/** @internal */
export const InputZscalerHecPqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputZscalerHecPqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputZscalerHecPqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputZscalerHecType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    authTokens: z.array(z.lazy(() => InputZscalerHecAuthToken$outboundSchema))
      .optional(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.any().optional(),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    hecAPI: z.string().default("/services/collector"),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    allowedIndexes: z.array(z.string()).optional(),
    hecAcks: z.boolean().default(false),
    accessControlAllowOrigin: z.array(z.string()).optional(),
    accessControlAllowHeaders: z.array(z.string()).optional(),
    emitTokenMetrics: z.boolean().default(false),
    description: z.string().optional(),
  });

export function inputZscalerHecPqEnabledFalseWithPqConstraintToJSON(
  inputZscalerHecPqEnabledFalseWithPqConstraint:
    InputZscalerHecPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputZscalerHecPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputZscalerHecPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputZscalerHecSendToRoutesFalseWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    host: string;
    port: number;
    authTokens?: Array<InputZscalerHecAuthToken$Outbound> | undefined;
    tls?: models.TlsSettingsServerSideType$Outbound | undefined;
    maxActiveReq: number;
    maxRequestsPerSocket: number;
    enableProxyHeader: boolean;
    captureHeaders: boolean;
    activityLogSampleRate: number;
    requestTimeout: number;
    socketTimeout: number;
    keepAliveTimeout: number;
    enableHealthCheck?: any | undefined;
    ipAllowlistRegex: string;
    ipDenylistRegex: string;
    hecAPI: string;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    allowedIndexes?: Array<string> | undefined;
    hecAcks: boolean;
    accessControlAllowOrigin?: Array<string> | undefined;
    accessControlAllowHeaders?: Array<string> | undefined;
    emitTokenMetrics: boolean;
    description?: string | undefined;
  };

/** @internal */
export const InputZscalerHecSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputZscalerHecSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputZscalerHecSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputZscalerHecType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    authTokens: z.array(z.lazy(() => InputZscalerHecAuthToken$outboundSchema))
      .optional(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.any().optional(),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    hecAPI: z.string().default("/services/collector"),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    allowedIndexes: z.array(z.string()).optional(),
    hecAcks: z.boolean().default(false),
    accessControlAllowOrigin: z.array(z.string()).optional(),
    accessControlAllowHeaders: z.array(z.string()).optional(),
    emitTokenMetrics: z.boolean().default(false),
    description: z.string().optional(),
  });

export function inputZscalerHecSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputZscalerHecSendToRoutesFalseWithConnectionsConstraint:
    InputZscalerHecSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputZscalerHecSendToRoutesFalseWithConnectionsConstraint$outboundSchema
      .parse(inputZscalerHecSendToRoutesFalseWithConnectionsConstraint),
  );
}

/** @internal */
export type InputZscalerHecSendToRoutesTrueWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    host: string;
    port: number;
    authTokens?: Array<InputZscalerHecAuthToken$Outbound> | undefined;
    tls?: models.TlsSettingsServerSideType$Outbound | undefined;
    maxActiveReq: number;
    maxRequestsPerSocket: number;
    enableProxyHeader: boolean;
    captureHeaders: boolean;
    activityLogSampleRate: number;
    requestTimeout: number;
    socketTimeout: number;
    keepAliveTimeout: number;
    enableHealthCheck?: any | undefined;
    ipAllowlistRegex: string;
    ipDenylistRegex: string;
    hecAPI: string;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    allowedIndexes?: Array<string> | undefined;
    hecAcks: boolean;
    accessControlAllowOrigin?: Array<string> | undefined;
    accessControlAllowHeaders?: Array<string> | undefined;
    emitTokenMetrics: boolean;
    description?: string | undefined;
  };

/** @internal */
export const InputZscalerHecSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputZscalerHecSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputZscalerHecSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputZscalerHecType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    authTokens: z.array(z.lazy(() => InputZscalerHecAuthToken$outboundSchema))
      .optional(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.any().optional(),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    hecAPI: z.string().default("/services/collector"),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    allowedIndexes: z.array(z.string()).optional(),
    hecAcks: z.boolean().default(false),
    accessControlAllowOrigin: z.array(z.string()).optional(),
    accessControlAllowHeaders: z.array(z.string()).optional(),
    emitTokenMetrics: z.boolean().default(false),
    description: z.string().optional(),
  });

export function inputZscalerHecSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputZscalerHecSendToRoutesTrueWithConnectionsConstraint:
    InputZscalerHecSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputZscalerHecSendToRoutesTrueWithConnectionsConstraint$outboundSchema
      .parse(inputZscalerHecSendToRoutesTrueWithConnectionsConstraint),
  );
}

/** @internal */
export type InputZscalerHec$Outbound =
  | InputZscalerHecSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputZscalerHecSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputZscalerHecPqEnabledFalseWithPqConstraint$Outbound
  | InputZscalerHecPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputZscalerHec$outboundSchema: z.ZodType<
  InputZscalerHec$Outbound,
  z.ZodTypeDef,
  InputZscalerHec
> = z.union([
  z.lazy(() =>
    InputZscalerHecSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputZscalerHecSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputZscalerHecPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputZscalerHecPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputZscalerHecToJSON(
  inputZscalerHec: InputZscalerHec,
): string {
  return JSON.stringify(InputZscalerHec$outboundSchema.parse(inputZscalerHec));
}

/** @internal */
export const InputSecurityLakeType$outboundSchema: z.ZodNativeEnum<
  typeof InputSecurityLakeType
> = z.nativeEnum(InputSecurityLakeType);

/** @internal */
export type InputSecurityLakePqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  queueName: string;
  fileFilter: string;
  awsAccountId?: string | undefined;
  awsAuthenticationMethod: string;
  awsSecretKey?: string | undefined;
  region?: string | undefined;
  endpoint?: string | undefined;
  signatureVersion: string;
  reuseConnections: boolean;
  rejectUnauthorized: boolean;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  maxMessages: number;
  visibilityTimeout: number;
  numReceivers: number;
  socketTimeout: number;
  skipOnError: boolean;
  includeSqsMetadata: boolean;
  enableAssumeRole: boolean;
  assumeRoleArn?: string | undefined;
  assumeRoleExternalId?: string | undefined;
  durationSeconds: number;
  enableSQSAssumeRole: boolean;
  preprocess?:
    | models.PreprocessTypeSavedJobCollectionInput$Outbound
    | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  parquetChunkSizeMB: number;
  parquetChunkDownloadTimeout: number;
  checkpointing?: models.CheckpointingType$Outbound | undefined;
  pollTimeout: number;
  encoding?: string | undefined;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  awsSecret?: string | undefined;
  tagAfterProcessing?: string | undefined;
  processedTagKey?: string | undefined;
  processedTagValue?: string | undefined;
};

/** @internal */
export const InputSecurityLakePqEnabledTrueWithPqConstraint$outboundSchema:
  z.ZodType<
    InputSecurityLakePqEnabledTrueWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputSecurityLakePqEnabledTrueWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputSecurityLakeType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    queueName: z.string(),
    fileFilter: z.string().default("/.*/"),
    awsAccountId: z.string().optional(),
    awsAuthenticationMethod: z.string().default("auto"),
    awsSecretKey: z.string().optional(),
    region: z.string().optional(),
    endpoint: z.string().optional(),
    signatureVersion: models
      .SignatureVersionOptionsS3CollectorConf$outboundSchema.default("v4"),
    reuseConnections: z.boolean().default(true),
    rejectUnauthorized: z.boolean().default(true),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    maxMessages: z.number().default(1),
    visibilityTimeout: z.number().default(600),
    numReceivers: z.number().default(1),
    socketTimeout: z.number().default(300),
    skipOnError: z.boolean().default(false),
    includeSqsMetadata: z.boolean().default(false),
    enableAssumeRole: z.boolean().default(true),
    assumeRoleArn: z.string().optional(),
    assumeRoleExternalId: z.string().optional(),
    durationSeconds: z.number().default(3600),
    enableSQSAssumeRole: z.boolean().default(false),
    preprocess: models.PreprocessTypeSavedJobCollectionInput$outboundSchema
      .optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    parquetChunkSizeMB: z.number().default(5),
    parquetChunkDownloadTimeout: z.number().default(600),
    checkpointing: models.CheckpointingType$outboundSchema.optional(),
    pollTimeout: z.number().default(10),
    encoding: z.string().optional(),
    description: z.string().optional(),
    awsApiKey: z.string().optional(),
    awsSecret: z.string().optional(),
    tagAfterProcessing: models.TagAfterProcessingOptions$outboundSchema
      .optional(),
    processedTagKey: z.string().optional(),
    processedTagValue: z.string().optional(),
  });

export function inputSecurityLakePqEnabledTrueWithPqConstraintToJSON(
  inputSecurityLakePqEnabledTrueWithPqConstraint:
    InputSecurityLakePqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputSecurityLakePqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputSecurityLakePqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputSecurityLakePqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  queueName: string;
  fileFilter: string;
  awsAccountId?: string | undefined;
  awsAuthenticationMethod: string;
  awsSecretKey?: string | undefined;
  region?: string | undefined;
  endpoint?: string | undefined;
  signatureVersion: string;
  reuseConnections: boolean;
  rejectUnauthorized: boolean;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  maxMessages: number;
  visibilityTimeout: number;
  numReceivers: number;
  socketTimeout: number;
  skipOnError: boolean;
  includeSqsMetadata: boolean;
  enableAssumeRole: boolean;
  assumeRoleArn?: string | undefined;
  assumeRoleExternalId?: string | undefined;
  durationSeconds: number;
  enableSQSAssumeRole: boolean;
  preprocess?:
    | models.PreprocessTypeSavedJobCollectionInput$Outbound
    | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  parquetChunkSizeMB: number;
  parquetChunkDownloadTimeout: number;
  checkpointing?: models.CheckpointingType$Outbound | undefined;
  pollTimeout: number;
  encoding?: string | undefined;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  awsSecret?: string | undefined;
  tagAfterProcessing?: string | undefined;
  processedTagKey?: string | undefined;
  processedTagValue?: string | undefined;
};

/** @internal */
export const InputSecurityLakePqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputSecurityLakePqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputSecurityLakePqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputSecurityLakeType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    queueName: z.string(),
    fileFilter: z.string().default("/.*/"),
    awsAccountId: z.string().optional(),
    awsAuthenticationMethod: z.string().default("auto"),
    awsSecretKey: z.string().optional(),
    region: z.string().optional(),
    endpoint: z.string().optional(),
    signatureVersion: models
      .SignatureVersionOptionsS3CollectorConf$outboundSchema.default("v4"),
    reuseConnections: z.boolean().default(true),
    rejectUnauthorized: z.boolean().default(true),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    maxMessages: z.number().default(1),
    visibilityTimeout: z.number().default(600),
    numReceivers: z.number().default(1),
    socketTimeout: z.number().default(300),
    skipOnError: z.boolean().default(false),
    includeSqsMetadata: z.boolean().default(false),
    enableAssumeRole: z.boolean().default(true),
    assumeRoleArn: z.string().optional(),
    assumeRoleExternalId: z.string().optional(),
    durationSeconds: z.number().default(3600),
    enableSQSAssumeRole: z.boolean().default(false),
    preprocess: models.PreprocessTypeSavedJobCollectionInput$outboundSchema
      .optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    parquetChunkSizeMB: z.number().default(5),
    parquetChunkDownloadTimeout: z.number().default(600),
    checkpointing: models.CheckpointingType$outboundSchema.optional(),
    pollTimeout: z.number().default(10),
    encoding: z.string().optional(),
    description: z.string().optional(),
    awsApiKey: z.string().optional(),
    awsSecret: z.string().optional(),
    tagAfterProcessing: models.TagAfterProcessingOptions$outboundSchema
      .optional(),
    processedTagKey: z.string().optional(),
    processedTagValue: z.string().optional(),
  });

export function inputSecurityLakePqEnabledFalseWithPqConstraintToJSON(
  inputSecurityLakePqEnabledFalseWithPqConstraint:
    InputSecurityLakePqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputSecurityLakePqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputSecurityLakePqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputSecurityLakeSendToRoutesFalseWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    queueName: string;
    fileFilter: string;
    awsAccountId?: string | undefined;
    awsAuthenticationMethod: string;
    awsSecretKey?: string | undefined;
    region?: string | undefined;
    endpoint?: string | undefined;
    signatureVersion: string;
    reuseConnections: boolean;
    rejectUnauthorized: boolean;
    breakerRulesets?: Array<string> | undefined;
    staleChannelFlushMs: number;
    maxMessages: number;
    visibilityTimeout: number;
    numReceivers: number;
    socketTimeout: number;
    skipOnError: boolean;
    includeSqsMetadata: boolean;
    enableAssumeRole: boolean;
    assumeRoleArn?: string | undefined;
    assumeRoleExternalId?: string | undefined;
    durationSeconds: number;
    enableSQSAssumeRole: boolean;
    preprocess?:
      | models.PreprocessTypeSavedJobCollectionInput$Outbound
      | undefined;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    parquetChunkSizeMB: number;
    parquetChunkDownloadTimeout: number;
    checkpointing?: models.CheckpointingType$Outbound | undefined;
    pollTimeout: number;
    encoding?: string | undefined;
    description?: string | undefined;
    awsApiKey?: string | undefined;
    awsSecret?: string | undefined;
    tagAfterProcessing?: string | undefined;
    processedTagKey?: string | undefined;
    processedTagValue?: string | undefined;
  };

/** @internal */
export const InputSecurityLakeSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputSecurityLakeSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputSecurityLakeSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputSecurityLakeType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    queueName: z.string(),
    fileFilter: z.string().default("/.*/"),
    awsAccountId: z.string().optional(),
    awsAuthenticationMethod: z.string().default("auto"),
    awsSecretKey: z.string().optional(),
    region: z.string().optional(),
    endpoint: z.string().optional(),
    signatureVersion: models
      .SignatureVersionOptionsS3CollectorConf$outboundSchema.default("v4"),
    reuseConnections: z.boolean().default(true),
    rejectUnauthorized: z.boolean().default(true),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    maxMessages: z.number().default(1),
    visibilityTimeout: z.number().default(600),
    numReceivers: z.number().default(1),
    socketTimeout: z.number().default(300),
    skipOnError: z.boolean().default(false),
    includeSqsMetadata: z.boolean().default(false),
    enableAssumeRole: z.boolean().default(true),
    assumeRoleArn: z.string().optional(),
    assumeRoleExternalId: z.string().optional(),
    durationSeconds: z.number().default(3600),
    enableSQSAssumeRole: z.boolean().default(false),
    preprocess: models.PreprocessTypeSavedJobCollectionInput$outboundSchema
      .optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    parquetChunkSizeMB: z.number().default(5),
    parquetChunkDownloadTimeout: z.number().default(600),
    checkpointing: models.CheckpointingType$outboundSchema.optional(),
    pollTimeout: z.number().default(10),
    encoding: z.string().optional(),
    description: z.string().optional(),
    awsApiKey: z.string().optional(),
    awsSecret: z.string().optional(),
    tagAfterProcessing: models.TagAfterProcessingOptions$outboundSchema
      .optional(),
    processedTagKey: z.string().optional(),
    processedTagValue: z.string().optional(),
  });

export function inputSecurityLakeSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputSecurityLakeSendToRoutesFalseWithConnectionsConstraint:
    InputSecurityLakeSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputSecurityLakeSendToRoutesFalseWithConnectionsConstraint$outboundSchema
      .parse(inputSecurityLakeSendToRoutesFalseWithConnectionsConstraint),
  );
}

/** @internal */
export type InputSecurityLakeSendToRoutesTrueWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    queueName: string;
    fileFilter: string;
    awsAccountId?: string | undefined;
    awsAuthenticationMethod: string;
    awsSecretKey?: string | undefined;
    region?: string | undefined;
    endpoint?: string | undefined;
    signatureVersion: string;
    reuseConnections: boolean;
    rejectUnauthorized: boolean;
    breakerRulesets?: Array<string> | undefined;
    staleChannelFlushMs: number;
    maxMessages: number;
    visibilityTimeout: number;
    numReceivers: number;
    socketTimeout: number;
    skipOnError: boolean;
    includeSqsMetadata: boolean;
    enableAssumeRole: boolean;
    assumeRoleArn?: string | undefined;
    assumeRoleExternalId?: string | undefined;
    durationSeconds: number;
    enableSQSAssumeRole: boolean;
    preprocess?:
      | models.PreprocessTypeSavedJobCollectionInput$Outbound
      | undefined;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    parquetChunkSizeMB: number;
    parquetChunkDownloadTimeout: number;
    checkpointing?: models.CheckpointingType$Outbound | undefined;
    pollTimeout: number;
    encoding?: string | undefined;
    description?: string | undefined;
    awsApiKey?: string | undefined;
    awsSecret?: string | undefined;
    tagAfterProcessing?: string | undefined;
    processedTagKey?: string | undefined;
    processedTagValue?: string | undefined;
  };

/** @internal */
export const InputSecurityLakeSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputSecurityLakeSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputSecurityLakeSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputSecurityLakeType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    queueName: z.string(),
    fileFilter: z.string().default("/.*/"),
    awsAccountId: z.string().optional(),
    awsAuthenticationMethod: z.string().default("auto"),
    awsSecretKey: z.string().optional(),
    region: z.string().optional(),
    endpoint: z.string().optional(),
    signatureVersion: models
      .SignatureVersionOptionsS3CollectorConf$outboundSchema.default("v4"),
    reuseConnections: z.boolean().default(true),
    rejectUnauthorized: z.boolean().default(true),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    maxMessages: z.number().default(1),
    visibilityTimeout: z.number().default(600),
    numReceivers: z.number().default(1),
    socketTimeout: z.number().default(300),
    skipOnError: z.boolean().default(false),
    includeSqsMetadata: z.boolean().default(false),
    enableAssumeRole: z.boolean().default(true),
    assumeRoleArn: z.string().optional(),
    assumeRoleExternalId: z.string().optional(),
    durationSeconds: z.number().default(3600),
    enableSQSAssumeRole: z.boolean().default(false),
    preprocess: models.PreprocessTypeSavedJobCollectionInput$outboundSchema
      .optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    parquetChunkSizeMB: z.number().default(5),
    parquetChunkDownloadTimeout: z.number().default(600),
    checkpointing: models.CheckpointingType$outboundSchema.optional(),
    pollTimeout: z.number().default(10),
    encoding: z.string().optional(),
    description: z.string().optional(),
    awsApiKey: z.string().optional(),
    awsSecret: z.string().optional(),
    tagAfterProcessing: models.TagAfterProcessingOptions$outboundSchema
      .optional(),
    processedTagKey: z.string().optional(),
    processedTagValue: z.string().optional(),
  });

export function inputSecurityLakeSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputSecurityLakeSendToRoutesTrueWithConnectionsConstraint:
    InputSecurityLakeSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputSecurityLakeSendToRoutesTrueWithConnectionsConstraint$outboundSchema
      .parse(inputSecurityLakeSendToRoutesTrueWithConnectionsConstraint),
  );
}

/** @internal */
export type InputSecurityLake$Outbound =
  | InputSecurityLakeSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputSecurityLakeSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputSecurityLakePqEnabledFalseWithPqConstraint$Outbound
  | InputSecurityLakePqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputSecurityLake$outboundSchema: z.ZodType<
  InputSecurityLake$Outbound,
  z.ZodTypeDef,
  InputSecurityLake
> = z.union([
  z.lazy(() =>
    InputSecurityLakeSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputSecurityLakeSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputSecurityLakePqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputSecurityLakePqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputSecurityLakeToJSON(
  inputSecurityLake: InputSecurityLake,
): string {
  return JSON.stringify(
    InputSecurityLake$outboundSchema.parse(inputSecurityLake),
  );
}

/** @internal */
export const InputNetflowType$outboundSchema: z.ZodNativeEnum<
  typeof InputNetflowType
> = z.nativeEnum(InputNetflowType);

/** @internal */
export type InputNetflowPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  enablePassThrough: boolean;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  udpSocketRxBufSize?: number | undefined;
  templateCacheMinutes: number;
  v5Enabled: boolean;
  v9Enabled: boolean;
  ipfixEnabled: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputNetflowPqEnabledTrueWithPqConstraint$outboundSchema:
  z.ZodType<
    InputNetflowPqEnabledTrueWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputNetflowPqEnabledTrueWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputNetflowType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number().default(2055),
    enablePassThrough: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    udpSocketRxBufSize: z.number().optional(),
    templateCacheMinutes: z.number().default(30),
    v5Enabled: z.boolean().default(true),
    v9Enabled: z.boolean().default(true),
    ipfixEnabled: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputNetflowPqEnabledTrueWithPqConstraintToJSON(
  inputNetflowPqEnabledTrueWithPqConstraint:
    InputNetflowPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputNetflowPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputNetflowPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputNetflowPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  enablePassThrough: boolean;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  udpSocketRxBufSize?: number | undefined;
  templateCacheMinutes: number;
  v5Enabled: boolean;
  v9Enabled: boolean;
  ipfixEnabled: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputNetflowPqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputNetflowPqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputNetflowPqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputNetflowType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number().default(2055),
    enablePassThrough: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    udpSocketRxBufSize: z.number().optional(),
    templateCacheMinutes: z.number().default(30),
    v5Enabled: z.boolean().default(true),
    v9Enabled: z.boolean().default(true),
    ipfixEnabled: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputNetflowPqEnabledFalseWithPqConstraintToJSON(
  inputNetflowPqEnabledFalseWithPqConstraint:
    InputNetflowPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputNetflowPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputNetflowPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputNetflowSendToRoutesFalseWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  host: string;
  port: number;
  enablePassThrough: boolean;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  udpSocketRxBufSize?: number | undefined;
  templateCacheMinutes: number;
  v5Enabled: boolean;
  v9Enabled: boolean;
  ipfixEnabled: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputNetflowSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputNetflowSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputNetflowSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputNetflowType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number().default(2055),
    enablePassThrough: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    udpSocketRxBufSize: z.number().optional(),
    templateCacheMinutes: z.number().default(30),
    v5Enabled: z.boolean().default(true),
    v9Enabled: z.boolean().default(true),
    ipfixEnabled: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputNetflowSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputNetflowSendToRoutesFalseWithConnectionsConstraint:
    InputNetflowSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputNetflowSendToRoutesFalseWithConnectionsConstraint$outboundSchema.parse(
      inputNetflowSendToRoutesFalseWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputNetflowSendToRoutesTrueWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  host: string;
  port: number;
  enablePassThrough: boolean;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  udpSocketRxBufSize?: number | undefined;
  templateCacheMinutes: number;
  v5Enabled: boolean;
  v9Enabled: boolean;
  ipfixEnabled: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputNetflowSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputNetflowSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputNetflowSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputNetflowType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number().default(2055),
    enablePassThrough: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    udpSocketRxBufSize: z.number().optional(),
    templateCacheMinutes: z.number().default(30),
    v5Enabled: z.boolean().default(true),
    v9Enabled: z.boolean().default(true),
    ipfixEnabled: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputNetflowSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputNetflowSendToRoutesTrueWithConnectionsConstraint:
    InputNetflowSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputNetflowSendToRoutesTrueWithConnectionsConstraint$outboundSchema.parse(
      inputNetflowSendToRoutesTrueWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputNetflow$Outbound =
  | InputNetflowSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputNetflowSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputNetflowPqEnabledFalseWithPqConstraint$Outbound
  | InputNetflowPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputNetflow$outboundSchema: z.ZodType<
  InputNetflow$Outbound,
  z.ZodTypeDef,
  InputNetflow
> = z.union([
  z.lazy(() =>
    InputNetflowSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputNetflowSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputNetflowPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputNetflowPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputNetflowToJSON(inputNetflow: InputNetflow): string {
  return JSON.stringify(InputNetflow$outboundSchema.parse(inputNetflow));
}

/** @internal */
export const InputWizWebhookType$outboundSchema: z.ZodNativeEnum<
  typeof InputWizWebhookType
> = z.nativeEnum(InputWizWebhookType);

/** @internal */
export type InputWizWebhookPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  authTokens?: Array<string> | undefined;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  activityLogSampleRate: number;
  requestTimeout: number;
  socketTimeout: number;
  keepAliveTimeout: number;
  enableHealthCheck: boolean;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  allowedPaths?: Array<string> | undefined;
  allowedMethods?: Array<string> | undefined;
  authTokensExt?: Array<models.ItemsTypeAuthTokensExt$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputWizWebhookPqEnabledTrueWithPqConstraint$outboundSchema:
  z.ZodType<
    InputWizWebhookPqEnabledTrueWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputWizWebhookPqEnabledTrueWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputWizWebhookType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    authTokens: z.array(z.string()).optional(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    allowedPaths: z.array(z.string()).optional(),
    allowedMethods: z.array(z.string()).optional(),
    authTokensExt: z.array(models.ItemsTypeAuthTokensExt$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputWizWebhookPqEnabledTrueWithPqConstraintToJSON(
  inputWizWebhookPqEnabledTrueWithPqConstraint:
    InputWizWebhookPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputWizWebhookPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputWizWebhookPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputWizWebhookPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  authTokens?: Array<string> | undefined;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  activityLogSampleRate: number;
  requestTimeout: number;
  socketTimeout: number;
  keepAliveTimeout: number;
  enableHealthCheck: boolean;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  allowedPaths?: Array<string> | undefined;
  allowedMethods?: Array<string> | undefined;
  authTokensExt?: Array<models.ItemsTypeAuthTokensExt$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputWizWebhookPqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputWizWebhookPqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputWizWebhookPqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputWizWebhookType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    authTokens: z.array(z.string()).optional(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    allowedPaths: z.array(z.string()).optional(),
    allowedMethods: z.array(z.string()).optional(),
    authTokensExt: z.array(models.ItemsTypeAuthTokensExt$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputWizWebhookPqEnabledFalseWithPqConstraintToJSON(
  inputWizWebhookPqEnabledFalseWithPqConstraint:
    InputWizWebhookPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputWizWebhookPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputWizWebhookPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputWizWebhookSendToRoutesFalseWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    host: string;
    port: number;
    authTokens?: Array<string> | undefined;
    tls?: models.TlsSettingsServerSideType$Outbound | undefined;
    maxActiveReq: number;
    maxRequestsPerSocket: number;
    enableProxyHeader: boolean;
    captureHeaders: boolean;
    activityLogSampleRate: number;
    requestTimeout: number;
    socketTimeout: number;
    keepAliveTimeout: number;
    enableHealthCheck: boolean;
    ipAllowlistRegex: string;
    ipDenylistRegex: string;
    breakerRulesets?: Array<string> | undefined;
    staleChannelFlushMs: number;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    allowedPaths?: Array<string> | undefined;
    allowedMethods?: Array<string> | undefined;
    authTokensExt?: Array<models.ItemsTypeAuthTokensExt$Outbound> | undefined;
    description?: string | undefined;
  };

/** @internal */
export const InputWizWebhookSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputWizWebhookSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputWizWebhookSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputWizWebhookType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    authTokens: z.array(z.string()).optional(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    allowedPaths: z.array(z.string()).optional(),
    allowedMethods: z.array(z.string()).optional(),
    authTokensExt: z.array(models.ItemsTypeAuthTokensExt$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputWizWebhookSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputWizWebhookSendToRoutesFalseWithConnectionsConstraint:
    InputWizWebhookSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputWizWebhookSendToRoutesFalseWithConnectionsConstraint$outboundSchema
      .parse(inputWizWebhookSendToRoutesFalseWithConnectionsConstraint),
  );
}

/** @internal */
export type InputWizWebhookSendToRoutesTrueWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    host: string;
    port: number;
    authTokens?: Array<string> | undefined;
    tls?: models.TlsSettingsServerSideType$Outbound | undefined;
    maxActiveReq: number;
    maxRequestsPerSocket: number;
    enableProxyHeader: boolean;
    captureHeaders: boolean;
    activityLogSampleRate: number;
    requestTimeout: number;
    socketTimeout: number;
    keepAliveTimeout: number;
    enableHealthCheck: boolean;
    ipAllowlistRegex: string;
    ipDenylistRegex: string;
    breakerRulesets?: Array<string> | undefined;
    staleChannelFlushMs: number;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    allowedPaths?: Array<string> | undefined;
    allowedMethods?: Array<string> | undefined;
    authTokensExt?: Array<models.ItemsTypeAuthTokensExt$Outbound> | undefined;
    description?: string | undefined;
  };

/** @internal */
export const InputWizWebhookSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputWizWebhookSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputWizWebhookSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputWizWebhookType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    authTokens: z.array(z.string()).optional(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    allowedPaths: z.array(z.string()).optional(),
    allowedMethods: z.array(z.string()).optional(),
    authTokensExt: z.array(models.ItemsTypeAuthTokensExt$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputWizWebhookSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputWizWebhookSendToRoutesTrueWithConnectionsConstraint:
    InputWizWebhookSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputWizWebhookSendToRoutesTrueWithConnectionsConstraint$outboundSchema
      .parse(inputWizWebhookSendToRoutesTrueWithConnectionsConstraint),
  );
}

/** @internal */
export type InputWizWebhook$Outbound =
  | InputWizWebhookSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputWizWebhookSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputWizWebhookPqEnabledFalseWithPqConstraint$Outbound
  | InputWizWebhookPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputWizWebhook$outboundSchema: z.ZodType<
  InputWizWebhook$Outbound,
  z.ZodTypeDef,
  InputWizWebhook
> = z.union([
  z.lazy(() =>
    InputWizWebhookSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputWizWebhookSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputWizWebhookPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputWizWebhookPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputWizWebhookToJSON(
  inputWizWebhook: InputWizWebhook,
): string {
  return JSON.stringify(InputWizWebhook$outboundSchema.parse(inputWizWebhook));
}

/** @internal */
export const InputWizType$outboundSchema: z.ZodNativeEnum<typeof InputWizType> =
  z.nativeEnum(InputWizType);

/** @internal */
export type ManageState$Outbound = {};

/** @internal */
export const ManageState$outboundSchema: z.ZodType<
  ManageState$Outbound,
  z.ZodTypeDef,
  ManageState
> = z.object({});

export function manageStateToJSON(manageState: ManageState): string {
  return JSON.stringify(ManageState$outboundSchema.parse(manageState));
}

/** @internal */
export const InputWizLogLevel$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  InputWizLogLevel
> = openEnums.outboundSchema(InputWizLogLevel);

/** @internal */
export type InputWizContentConfig$Outbound = {
  contentType: string;
  contentDescription?: string | undefined;
  enabled: boolean;
  stateTracking: boolean;
  stateUpdateExpression: string;
  stateMergeExpression: string;
  manageState?: ManageState$Outbound | undefined;
  contentQuery: string;
  cronSchedule: string;
  earliest: string;
  latest: string;
  jobTimeout: string;
  logLevel: string;
  maxPages: number;
};

/** @internal */
export const InputWizContentConfig$outboundSchema: z.ZodType<
  InputWizContentConfig$Outbound,
  z.ZodTypeDef,
  InputWizContentConfig
> = z.object({
  contentType: z.string(),
  contentDescription: z.string().optional(),
  enabled: z.boolean().default(false),
  stateTracking: z.boolean().default(false),
  stateUpdateExpression: z.string().default(
    "__timestampExtracted !== false && {latestTime: (state.latestTime || 0) > _time ? state.latestTime : _time}",
  ),
  stateMergeExpression: z.string().default(
    "prevState.latestTime > newState.latestTime ? prevState : newState",
  ),
  manageState: z.lazy(() => ManageState$outboundSchema).optional(),
  contentQuery: z.string(),
  cronSchedule: z.string().default("0 */12 * * *"),
  earliest: z.string().default("-12h@h"),
  latest: z.string().default("now"),
  jobTimeout: z.string().default("0"),
  logLevel: InputWizLogLevel$outboundSchema.default("info"),
  maxPages: z.number().default(0),
});

export function inputWizContentConfigToJSON(
  inputWizContentConfig: InputWizContentConfig,
): string {
  return JSON.stringify(
    InputWizContentConfig$outboundSchema.parse(inputWizContentConfig),
  );
}

/** @internal */
export type InputWizPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  endpoint: string;
  authUrl: string;
  authAudienceOverride?: string | undefined;
  clientId: string;
  contentConfig: Array<InputWizContentConfig$Outbound>;
  requestTimeout: number;
  keepAliveTime: number;
  maxMissedKeepAlives: number;
  ttl: string;
  ignoreGroupJobsLimit: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  retryRules?: models.RetryRulesType$Outbound | undefined;
  authType: string;
  description?: string | undefined;
  clientSecret?: string | undefined;
  textSecret?: string | undefined;
};

/** @internal */
export const InputWizPqEnabledTrueWithPqConstraint$outboundSchema: z.ZodType<
  InputWizPqEnabledTrueWithPqConstraint$Outbound,
  z.ZodTypeDef,
  InputWizPqEnabledTrueWithPqConstraint
> = z.object({
  pqEnabled: z.boolean().default(false),
  pq: models.PqType$outboundSchema.optional(),
  id: z.string(),
  type: InputWizType$outboundSchema,
  disabled: z.boolean().default(false),
  pipeline: z.string().optional(),
  sendToRoutes: z.boolean().default(true),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
  endpoint: z.string().default("https://api.<region>.app.wiz.io/graphql"),
  authUrl: z.string(),
  authAudienceOverride: z.string().optional(),
  clientId: z.string(),
  contentConfig: z.array(z.lazy(() => InputWizContentConfig$outboundSchema)),
  requestTimeout: z.number().default(300),
  keepAliveTime: z.number().default(30),
  maxMissedKeepAlives: z.number().default(3),
  ttl: z.string().default("4h"),
  ignoreGroupJobsLimit: z.boolean().default(false),
  metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
    .optional(),
  retryRules: models.RetryRulesType$outboundSchema.optional(),
  authType: models.AuthenticationMethodOptions1$outboundSchema.default(
    "manual",
  ),
  description: z.string().optional(),
  clientSecret: z.string().optional(),
  textSecret: z.string().optional(),
});

export function inputWizPqEnabledTrueWithPqConstraintToJSON(
  inputWizPqEnabledTrueWithPqConstraint: InputWizPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputWizPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputWizPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputWizPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  endpoint: string;
  authUrl: string;
  authAudienceOverride?: string | undefined;
  clientId: string;
  contentConfig: Array<InputWizContentConfig$Outbound>;
  requestTimeout: number;
  keepAliveTime: number;
  maxMissedKeepAlives: number;
  ttl: string;
  ignoreGroupJobsLimit: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  retryRules?: models.RetryRulesType$Outbound | undefined;
  authType: string;
  description?: string | undefined;
  clientSecret?: string | undefined;
  textSecret?: string | undefined;
};

/** @internal */
export const InputWizPqEnabledFalseWithPqConstraint$outboundSchema: z.ZodType<
  InputWizPqEnabledFalseWithPqConstraint$Outbound,
  z.ZodTypeDef,
  InputWizPqEnabledFalseWithPqConstraint
> = z.object({
  pqEnabled: z.boolean().default(false),
  pq: models.PqType$outboundSchema.optional(),
  id: z.string(),
  type: InputWizType$outboundSchema,
  disabled: z.boolean().default(false),
  pipeline: z.string().optional(),
  sendToRoutes: z.boolean().default(true),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
  endpoint: z.string().default("https://api.<region>.app.wiz.io/graphql"),
  authUrl: z.string(),
  authAudienceOverride: z.string().optional(),
  clientId: z.string(),
  contentConfig: z.array(z.lazy(() => InputWizContentConfig$outboundSchema)),
  requestTimeout: z.number().default(300),
  keepAliveTime: z.number().default(30),
  maxMissedKeepAlives: z.number().default(3),
  ttl: z.string().default("4h"),
  ignoreGroupJobsLimit: z.boolean().default(false),
  metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
    .optional(),
  retryRules: models.RetryRulesType$outboundSchema.optional(),
  authType: models.AuthenticationMethodOptions1$outboundSchema.default(
    "manual",
  ),
  description: z.string().optional(),
  clientSecret: z.string().optional(),
  textSecret: z.string().optional(),
});

export function inputWizPqEnabledFalseWithPqConstraintToJSON(
  inputWizPqEnabledFalseWithPqConstraint:
    InputWizPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputWizPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputWizPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputWizSendToRoutesFalseWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  endpoint: string;
  authUrl: string;
  authAudienceOverride?: string | undefined;
  clientId: string;
  contentConfig: Array<InputWizContentConfig$Outbound>;
  requestTimeout: number;
  keepAliveTime: number;
  maxMissedKeepAlives: number;
  ttl: string;
  ignoreGroupJobsLimit: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  retryRules?: models.RetryRulesType$Outbound | undefined;
  authType: string;
  description?: string | undefined;
  clientSecret?: string | undefined;
  textSecret?: string | undefined;
};

/** @internal */
export const InputWizSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputWizSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputWizSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputWizType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    endpoint: z.string().default("https://api.<region>.app.wiz.io/graphql"),
    authUrl: z.string(),
    authAudienceOverride: z.string().optional(),
    clientId: z.string(),
    contentConfig: z.array(z.lazy(() => InputWizContentConfig$outboundSchema)),
    requestTimeout: z.number().default(300),
    keepAliveTime: z.number().default(30),
    maxMissedKeepAlives: z.number().default(3),
    ttl: z.string().default("4h"),
    ignoreGroupJobsLimit: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    retryRules: models.RetryRulesType$outboundSchema.optional(),
    authType: models.AuthenticationMethodOptions1$outboundSchema.default(
      "manual",
    ),
    description: z.string().optional(),
    clientSecret: z.string().optional(),
    textSecret: z.string().optional(),
  });

export function inputWizSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputWizSendToRoutesFalseWithConnectionsConstraint:
    InputWizSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputWizSendToRoutesFalseWithConnectionsConstraint$outboundSchema.parse(
      inputWizSendToRoutesFalseWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputWizSendToRoutesTrueWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  endpoint: string;
  authUrl: string;
  authAudienceOverride?: string | undefined;
  clientId: string;
  contentConfig: Array<InputWizContentConfig$Outbound>;
  requestTimeout: number;
  keepAliveTime: number;
  maxMissedKeepAlives: number;
  ttl: string;
  ignoreGroupJobsLimit: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  retryRules?: models.RetryRulesType$Outbound | undefined;
  authType: string;
  description?: string | undefined;
  clientSecret?: string | undefined;
  textSecret?: string | undefined;
};

/** @internal */
export const InputWizSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputWizSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputWizSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputWizType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    endpoint: z.string().default("https://api.<region>.app.wiz.io/graphql"),
    authUrl: z.string(),
    authAudienceOverride: z.string().optional(),
    clientId: z.string(),
    contentConfig: z.array(z.lazy(() => InputWizContentConfig$outboundSchema)),
    requestTimeout: z.number().default(300),
    keepAliveTime: z.number().default(30),
    maxMissedKeepAlives: z.number().default(3),
    ttl: z.string().default("4h"),
    ignoreGroupJobsLimit: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    retryRules: models.RetryRulesType$outboundSchema.optional(),
    authType: models.AuthenticationMethodOptions1$outboundSchema.default(
      "manual",
    ),
    description: z.string().optional(),
    clientSecret: z.string().optional(),
    textSecret: z.string().optional(),
  });

export function inputWizSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputWizSendToRoutesTrueWithConnectionsConstraint:
    InputWizSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputWizSendToRoutesTrueWithConnectionsConstraint$outboundSchema.parse(
      inputWizSendToRoutesTrueWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputWiz$Outbound =
  | InputWizSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputWizSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputWizPqEnabledFalseWithPqConstraint$Outbound
  | InputWizPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputWiz$outboundSchema: z.ZodType<
  InputWiz$Outbound,
  z.ZodTypeDef,
  InputWiz
> = z.union([
  z.lazy(() =>
    InputWizSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputWizSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputWizPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputWizPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputWizToJSON(inputWiz: InputWiz): string {
  return JSON.stringify(InputWiz$outboundSchema.parse(inputWiz));
}

/** @internal */
export const PqEnabledTrueWithPqConstraintInputJournalFilesType$outboundSchema:
  z.ZodNativeEnum<typeof PqEnabledTrueWithPqConstraintInputJournalFilesType> = z
    .nativeEnum(PqEnabledTrueWithPqConstraintInputJournalFilesType);

/** @internal */
export type PqEnabledTrueWithPqConstraintRule$Outbound = {
  filter: string;
  description?: string | undefined;
};

/** @internal */
export const PqEnabledTrueWithPqConstraintRule$outboundSchema: z.ZodType<
  PqEnabledTrueWithPqConstraintRule$Outbound,
  z.ZodTypeDef,
  PqEnabledTrueWithPqConstraintRule
> = z.object({
  filter: z.string(),
  description: z.string().optional(),
});

export function pqEnabledTrueWithPqConstraintRuleToJSON(
  pqEnabledTrueWithPqConstraintRule: PqEnabledTrueWithPqConstraintRule,
): string {
  return JSON.stringify(
    PqEnabledTrueWithPqConstraintRule$outboundSchema.parse(
      pqEnabledTrueWithPqConstraintRule,
    ),
  );
}

/** @internal */
export type InputJournalFilesPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  path: string;
  interval: number;
  journals: Array<string>;
  rules?: Array<PqEnabledTrueWithPqConstraintRule$Outbound> | undefined;
  currentBoot: boolean;
  maxAgeDur?: string | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputJournalFilesPqEnabledTrueWithPqConstraint$outboundSchema:
  z.ZodType<
    InputJournalFilesPqEnabledTrueWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputJournalFilesPqEnabledTrueWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: PqEnabledTrueWithPqConstraintInputJournalFilesType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    path: z.string(),
    interval: z.number().default(10),
    journals: z.array(z.string()),
    rules: z.array(
      z.lazy(() => PqEnabledTrueWithPqConstraintRule$outboundSchema),
    ).optional(),
    currentBoot: z.boolean().default(false),
    maxAgeDur: z.string().optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputJournalFilesPqEnabledTrueWithPqConstraintToJSON(
  inputJournalFilesPqEnabledTrueWithPqConstraint:
    InputJournalFilesPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputJournalFilesPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputJournalFilesPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export const PqEnabledFalseWithPqConstraintInputJournalFilesType$outboundSchema:
  z.ZodNativeEnum<typeof PqEnabledFalseWithPqConstraintInputJournalFilesType> =
    z.nativeEnum(PqEnabledFalseWithPqConstraintInputJournalFilesType);

/** @internal */
export type PqEnabledFalseWithPqConstraintRule$Outbound = {
  filter: string;
  description?: string | undefined;
};

/** @internal */
export const PqEnabledFalseWithPqConstraintRule$outboundSchema: z.ZodType<
  PqEnabledFalseWithPqConstraintRule$Outbound,
  z.ZodTypeDef,
  PqEnabledFalseWithPqConstraintRule
> = z.object({
  filter: z.string(),
  description: z.string().optional(),
});

export function pqEnabledFalseWithPqConstraintRuleToJSON(
  pqEnabledFalseWithPqConstraintRule: PqEnabledFalseWithPqConstraintRule,
): string {
  return JSON.stringify(
    PqEnabledFalseWithPqConstraintRule$outboundSchema.parse(
      pqEnabledFalseWithPqConstraintRule,
    ),
  );
}

/** @internal */
export type InputJournalFilesPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  path: string;
  interval: number;
  journals: Array<string>;
  rules?: Array<PqEnabledFalseWithPqConstraintRule$Outbound> | undefined;
  currentBoot: boolean;
  maxAgeDur?: string | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputJournalFilesPqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputJournalFilesPqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputJournalFilesPqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: PqEnabledFalseWithPqConstraintInputJournalFilesType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    path: z.string(),
    interval: z.number().default(10),
    journals: z.array(z.string()),
    rules: z.array(
      z.lazy(() => PqEnabledFalseWithPqConstraintRule$outboundSchema),
    ).optional(),
    currentBoot: z.boolean().default(false),
    maxAgeDur: z.string().optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputJournalFilesPqEnabledFalseWithPqConstraintToJSON(
  inputJournalFilesPqEnabledFalseWithPqConstraint:
    InputJournalFilesPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputJournalFilesPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputJournalFilesPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export const SendToRoutesFalseWithConnectionsConstraintInputJournalFilesType$outboundSchema:
  z.ZodNativeEnum<
    typeof SendToRoutesFalseWithConnectionsConstraintInputJournalFilesType
  > = z.nativeEnum(
    SendToRoutesFalseWithConnectionsConstraintInputJournalFilesType,
  );

/** @internal */
export type SendToRoutesFalseWithConnectionsConstraintRule$Outbound = {
  filter: string;
  description?: string | undefined;
};

/** @internal */
export const SendToRoutesFalseWithConnectionsConstraintRule$outboundSchema:
  z.ZodType<
    SendToRoutesFalseWithConnectionsConstraintRule$Outbound,
    z.ZodTypeDef,
    SendToRoutesFalseWithConnectionsConstraintRule
  > = z.object({
    filter: z.string(),
    description: z.string().optional(),
  });

export function sendToRoutesFalseWithConnectionsConstraintRuleToJSON(
  sendToRoutesFalseWithConnectionsConstraintRule:
    SendToRoutesFalseWithConnectionsConstraintRule,
): string {
  return JSON.stringify(
    SendToRoutesFalseWithConnectionsConstraintRule$outboundSchema.parse(
      sendToRoutesFalseWithConnectionsConstraintRule,
    ),
  );
}

/** @internal */
export type InputJournalFilesSendToRoutesFalseWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    path: string;
    interval: number;
    journals: Array<string>;
    rules?:
      | Array<SendToRoutesFalseWithConnectionsConstraintRule$Outbound>
      | undefined;
    currentBoot: boolean;
    maxAgeDur?: string | undefined;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    description?: string | undefined;
  };

/** @internal */
export const InputJournalFilesSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputJournalFilesSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputJournalFilesSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type:
      SendToRoutesFalseWithConnectionsConstraintInputJournalFilesType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    path: z.string(),
    interval: z.number().default(10),
    journals: z.array(z.string()),
    rules: z.array(
      z.lazy(() =>
        SendToRoutesFalseWithConnectionsConstraintRule$outboundSchema
      ),
    ).optional(),
    currentBoot: z.boolean().default(false),
    maxAgeDur: z.string().optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputJournalFilesSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputJournalFilesSendToRoutesFalseWithConnectionsConstraint:
    InputJournalFilesSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputJournalFilesSendToRoutesFalseWithConnectionsConstraint$outboundSchema
      .parse(inputJournalFilesSendToRoutesFalseWithConnectionsConstraint),
  );
}

/** @internal */
export const SendToRoutesTrueWithConnectionsConstraintInputJournalFilesType$outboundSchema:
  z.ZodNativeEnum<
    typeof SendToRoutesTrueWithConnectionsConstraintInputJournalFilesType
  > = z.nativeEnum(
    SendToRoutesTrueWithConnectionsConstraintInputJournalFilesType,
  );

/** @internal */
export type SendToRoutesTrueWithConnectionsConstraintRule$Outbound = {
  filter: string;
  description?: string | undefined;
};

/** @internal */
export const SendToRoutesTrueWithConnectionsConstraintRule$outboundSchema:
  z.ZodType<
    SendToRoutesTrueWithConnectionsConstraintRule$Outbound,
    z.ZodTypeDef,
    SendToRoutesTrueWithConnectionsConstraintRule
  > = z.object({
    filter: z.string(),
    description: z.string().optional(),
  });

export function sendToRoutesTrueWithConnectionsConstraintRuleToJSON(
  sendToRoutesTrueWithConnectionsConstraintRule:
    SendToRoutesTrueWithConnectionsConstraintRule,
): string {
  return JSON.stringify(
    SendToRoutesTrueWithConnectionsConstraintRule$outboundSchema.parse(
      sendToRoutesTrueWithConnectionsConstraintRule,
    ),
  );
}

/** @internal */
export type InputJournalFilesSendToRoutesTrueWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    path: string;
    interval: number;
    journals: Array<string>;
    rules?:
      | Array<SendToRoutesTrueWithConnectionsConstraintRule$Outbound>
      | undefined;
    currentBoot: boolean;
    maxAgeDur?: string | undefined;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    description?: string | undefined;
  };

/** @internal */
export const InputJournalFilesSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputJournalFilesSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputJournalFilesSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type:
      SendToRoutesTrueWithConnectionsConstraintInputJournalFilesType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    path: z.string(),
    interval: z.number().default(10),
    journals: z.array(z.string()),
    rules: z.array(
      z.lazy(() =>
        SendToRoutesTrueWithConnectionsConstraintRule$outboundSchema
      ),
    ).optional(),
    currentBoot: z.boolean().default(false),
    maxAgeDur: z.string().optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputJournalFilesSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputJournalFilesSendToRoutesTrueWithConnectionsConstraint:
    InputJournalFilesSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputJournalFilesSendToRoutesTrueWithConnectionsConstraint$outboundSchema
      .parse(inputJournalFilesSendToRoutesTrueWithConnectionsConstraint),
  );
}

/** @internal */
export type InputJournalFiles$Outbound =
  | InputJournalFilesSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputJournalFilesSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputJournalFilesPqEnabledFalseWithPqConstraint$Outbound
  | InputJournalFilesPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputJournalFiles$outboundSchema: z.ZodType<
  InputJournalFiles$Outbound,
  z.ZodTypeDef,
  InputJournalFiles
> = z.union([
  z.lazy(() =>
    InputJournalFilesSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputJournalFilesSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputJournalFilesPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputJournalFilesPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputJournalFilesToJSON(
  inputJournalFiles: InputJournalFiles,
): string {
  return JSON.stringify(
    InputJournalFiles$outboundSchema.parse(inputJournalFiles),
  );
}

/** @internal */
export const InputRawUdpType$outboundSchema: z.ZodNativeEnum<
  typeof InputRawUdpType
> = z.nativeEnum(InputRawUdpType);

/** @internal */
export type InputRawUdpPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  maxBufferSize: number;
  ipWhitelistRegex: string;
  singleMsgUdpPackets: boolean;
  ingestRawBytes: boolean;
  udpSocketRxBufSize?: number | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputRawUdpPqEnabledTrueWithPqConstraint$outboundSchema: z.ZodType<
  InputRawUdpPqEnabledTrueWithPqConstraint$Outbound,
  z.ZodTypeDef,
  InputRawUdpPqEnabledTrueWithPqConstraint
> = z.object({
  pqEnabled: z.boolean().default(false),
  pq: models.PqType$outboundSchema.optional(),
  id: z.string(),
  type: InputRawUdpType$outboundSchema,
  disabled: z.boolean().default(false),
  pipeline: z.string().optional(),
  sendToRoutes: z.boolean().default(true),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
  host: z.string().default("0.0.0.0"),
  port: z.number(),
  maxBufferSize: z.number().default(1000),
  ipWhitelistRegex: z.string().default("/.*/"),
  singleMsgUdpPackets: z.boolean().default(false),
  ingestRawBytes: z.boolean().default(false),
  udpSocketRxBufSize: z.number().optional(),
  metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
    .optional(),
  description: z.string().optional(),
});

export function inputRawUdpPqEnabledTrueWithPqConstraintToJSON(
  inputRawUdpPqEnabledTrueWithPqConstraint:
    InputRawUdpPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputRawUdpPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputRawUdpPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputRawUdpPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  maxBufferSize: number;
  ipWhitelistRegex: string;
  singleMsgUdpPackets: boolean;
  ingestRawBytes: boolean;
  udpSocketRxBufSize?: number | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputRawUdpPqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputRawUdpPqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputRawUdpPqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputRawUdpType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    maxBufferSize: z.number().default(1000),
    ipWhitelistRegex: z.string().default("/.*/"),
    singleMsgUdpPackets: z.boolean().default(false),
    ingestRawBytes: z.boolean().default(false),
    udpSocketRxBufSize: z.number().optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputRawUdpPqEnabledFalseWithPqConstraintToJSON(
  inputRawUdpPqEnabledFalseWithPqConstraint:
    InputRawUdpPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputRawUdpPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputRawUdpPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputRawUdpSendToRoutesFalseWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  host: string;
  port: number;
  maxBufferSize: number;
  ipWhitelistRegex: string;
  singleMsgUdpPackets: boolean;
  ingestRawBytes: boolean;
  udpSocketRxBufSize?: number | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputRawUdpSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputRawUdpSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputRawUdpSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputRawUdpType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    maxBufferSize: z.number().default(1000),
    ipWhitelistRegex: z.string().default("/.*/"),
    singleMsgUdpPackets: z.boolean().default(false),
    ingestRawBytes: z.boolean().default(false),
    udpSocketRxBufSize: z.number().optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputRawUdpSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputRawUdpSendToRoutesFalseWithConnectionsConstraint:
    InputRawUdpSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputRawUdpSendToRoutesFalseWithConnectionsConstraint$outboundSchema.parse(
      inputRawUdpSendToRoutesFalseWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputRawUdpSendToRoutesTrueWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  host: string;
  port: number;
  maxBufferSize: number;
  ipWhitelistRegex: string;
  singleMsgUdpPackets: boolean;
  ingestRawBytes: boolean;
  udpSocketRxBufSize?: number | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputRawUdpSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputRawUdpSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputRawUdpSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputRawUdpType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    maxBufferSize: z.number().default(1000),
    ipWhitelistRegex: z.string().default("/.*/"),
    singleMsgUdpPackets: z.boolean().default(false),
    ingestRawBytes: z.boolean().default(false),
    udpSocketRxBufSize: z.number().optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputRawUdpSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputRawUdpSendToRoutesTrueWithConnectionsConstraint:
    InputRawUdpSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputRawUdpSendToRoutesTrueWithConnectionsConstraint$outboundSchema.parse(
      inputRawUdpSendToRoutesTrueWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputRawUdp$Outbound =
  | InputRawUdpSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputRawUdpSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputRawUdpPqEnabledFalseWithPqConstraint$Outbound
  | InputRawUdpPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputRawUdp$outboundSchema: z.ZodType<
  InputRawUdp$Outbound,
  z.ZodTypeDef,
  InputRawUdp
> = z.union([
  z.lazy(() =>
    InputRawUdpSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputRawUdpSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputRawUdpPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputRawUdpPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputRawUdpToJSON(inputRawUdp: InputRawUdp): string {
  return JSON.stringify(InputRawUdp$outboundSchema.parse(inputRawUdp));
}

/** @internal */
export const InputWinEventLogsType$outboundSchema: z.ZodNativeEnum<
  typeof InputWinEventLogsType
> = z.nativeEnum(InputWinEventLogsType);

/** @internal */
export const ReadMode$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  ReadMode
> = openEnums.outboundSchema(ReadMode);

/** @internal */
export const EventFormat$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  EventFormat
> = openEnums.outboundSchema(EventFormat);

/** @internal */
export type InputWinEventLogsPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  logNames: Array<string>;
  readMode: string;
  eventFormat: string;
  disableNativeModule: boolean;
  interval: number;
  batchSize: number;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  maxEventBytes: number;
  description?: string | undefined;
  disableJsonRendering: boolean;
  disableXmlRendering: boolean;
};

/** @internal */
export const InputWinEventLogsPqEnabledTrueWithPqConstraint$outboundSchema:
  z.ZodType<
    InputWinEventLogsPqEnabledTrueWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputWinEventLogsPqEnabledTrueWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputWinEventLogsType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    logNames: z.array(z.string()),
    readMode: ReadMode$outboundSchema.default("newest"),
    eventFormat: EventFormat$outboundSchema.default("json"),
    disableNativeModule: z.boolean().default(false),
    interval: z.number().default(10),
    batchSize: z.number().default(500),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    maxEventBytes: z.number().default(51200),
    description: z.string().optional(),
    disableJsonRendering: z.boolean().default(false),
    disableXmlRendering: z.boolean().default(true),
  });

export function inputWinEventLogsPqEnabledTrueWithPqConstraintToJSON(
  inputWinEventLogsPqEnabledTrueWithPqConstraint:
    InputWinEventLogsPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputWinEventLogsPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputWinEventLogsPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputWinEventLogsPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  logNames: Array<string>;
  readMode: string;
  eventFormat: string;
  disableNativeModule: boolean;
  interval: number;
  batchSize: number;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  maxEventBytes: number;
  description?: string | undefined;
  disableJsonRendering: boolean;
  disableXmlRendering: boolean;
};

/** @internal */
export const InputWinEventLogsPqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputWinEventLogsPqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputWinEventLogsPqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputWinEventLogsType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    logNames: z.array(z.string()),
    readMode: ReadMode$outboundSchema.default("newest"),
    eventFormat: EventFormat$outboundSchema.default("json"),
    disableNativeModule: z.boolean().default(false),
    interval: z.number().default(10),
    batchSize: z.number().default(500),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    maxEventBytes: z.number().default(51200),
    description: z.string().optional(),
    disableJsonRendering: z.boolean().default(false),
    disableXmlRendering: z.boolean().default(true),
  });

export function inputWinEventLogsPqEnabledFalseWithPqConstraintToJSON(
  inputWinEventLogsPqEnabledFalseWithPqConstraint:
    InputWinEventLogsPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputWinEventLogsPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputWinEventLogsPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputWinEventLogsSendToRoutesFalseWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    logNames: Array<string>;
    readMode: string;
    eventFormat: string;
    disableNativeModule: boolean;
    interval: number;
    batchSize: number;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    maxEventBytes: number;
    description?: string | undefined;
    disableJsonRendering: boolean;
    disableXmlRendering: boolean;
  };

/** @internal */
export const InputWinEventLogsSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputWinEventLogsSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputWinEventLogsSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputWinEventLogsType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    logNames: z.array(z.string()),
    readMode: ReadMode$outboundSchema.default("newest"),
    eventFormat: EventFormat$outboundSchema.default("json"),
    disableNativeModule: z.boolean().default(false),
    interval: z.number().default(10),
    batchSize: z.number().default(500),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    maxEventBytes: z.number().default(51200),
    description: z.string().optional(),
    disableJsonRendering: z.boolean().default(false),
    disableXmlRendering: z.boolean().default(true),
  });

export function inputWinEventLogsSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputWinEventLogsSendToRoutesFalseWithConnectionsConstraint:
    InputWinEventLogsSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputWinEventLogsSendToRoutesFalseWithConnectionsConstraint$outboundSchema
      .parse(inputWinEventLogsSendToRoutesFalseWithConnectionsConstraint),
  );
}

/** @internal */
export type InputWinEventLogsSendToRoutesTrueWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    logNames: Array<string>;
    readMode: string;
    eventFormat: string;
    disableNativeModule: boolean;
    interval: number;
    batchSize: number;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    maxEventBytes: number;
    description?: string | undefined;
    disableJsonRendering: boolean;
    disableXmlRendering: boolean;
  };

/** @internal */
export const InputWinEventLogsSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputWinEventLogsSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputWinEventLogsSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputWinEventLogsType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    logNames: z.array(z.string()),
    readMode: ReadMode$outboundSchema.default("newest"),
    eventFormat: EventFormat$outboundSchema.default("json"),
    disableNativeModule: z.boolean().default(false),
    interval: z.number().default(10),
    batchSize: z.number().default(500),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    maxEventBytes: z.number().default(51200),
    description: z.string().optional(),
    disableJsonRendering: z.boolean().default(false),
    disableXmlRendering: z.boolean().default(true),
  });

export function inputWinEventLogsSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputWinEventLogsSendToRoutesTrueWithConnectionsConstraint:
    InputWinEventLogsSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputWinEventLogsSendToRoutesTrueWithConnectionsConstraint$outboundSchema
      .parse(inputWinEventLogsSendToRoutesTrueWithConnectionsConstraint),
  );
}

/** @internal */
export type InputWinEventLogs$Outbound =
  | InputWinEventLogsSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputWinEventLogsSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputWinEventLogsPqEnabledFalseWithPqConstraint$Outbound
  | InputWinEventLogsPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputWinEventLogs$outboundSchema: z.ZodType<
  InputWinEventLogs$Outbound,
  z.ZodTypeDef,
  InputWinEventLogs
> = z.union([
  z.lazy(() =>
    InputWinEventLogsSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputWinEventLogsSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputWinEventLogsPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputWinEventLogsPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputWinEventLogsToJSON(
  inputWinEventLogs: InputWinEventLogs,
): string {
  return JSON.stringify(
    InputWinEventLogs$outboundSchema.parse(inputWinEventLogs),
  );
}

/** @internal */
export const InputWefType$outboundSchema: z.ZodNativeEnum<typeof InputWefType> =
  z.nativeEnum(InputWefType);

/** @internal */
export const InputWefAuthenticationMethod$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  InputWefAuthenticationMethod
> = openEnums.outboundSchema(InputWefAuthenticationMethod);

/** @internal */
export type MTLSSettings$Outbound = {
  disabled: boolean;
  rejectUnauthorized: boolean;
  requestCert: boolean;
  certificateName?: string | undefined;
  privKeyPath: string;
  passphrase?: string | undefined;
  certPath: string;
  caPath: string;
  commonNameRegex: string;
  minVersion?: string | undefined;
  maxVersion?: string | undefined;
  ocspCheck: boolean;
  keytab?: any | undefined;
  principal?: any | undefined;
  ocspCheckFailClose: boolean;
};

/** @internal */
export const MTLSSettings$outboundSchema: z.ZodType<
  MTLSSettings$Outbound,
  z.ZodTypeDef,
  MTLSSettings
> = z.object({
  disabled: z.boolean().default(false),
  rejectUnauthorized: z.boolean().default(true),
  requestCert: z.boolean().default(true),
  certificateName: z.string().optional(),
  privKeyPath: z.string(),
  passphrase: z.string().optional(),
  certPath: z.string(),
  caPath: z.string(),
  commonNameRegex: z.string().default("/.*/"),
  minVersion: models
    .MinimumTlsVersionOptionsKafkaSchemaRegistryTls$outboundSchema.optional(),
  maxVersion: models
    .MaximumTlsVersionOptionsKafkaSchemaRegistryTls$outboundSchema.optional(),
  ocspCheck: z.boolean().default(false),
  keytab: z.any().optional(),
  principal: z.any().optional(),
  ocspCheckFailClose: z.boolean().default(false),
});

export function mTLSSettingsToJSON(mtlsSettings: MTLSSettings): string {
  return JSON.stringify(MTLSSettings$outboundSchema.parse(mtlsSettings));
}

/** @internal */
export const CreateInputFormat$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  CreateInputFormat
> = openEnums.outboundSchema(CreateInputFormat);

/** @internal */
export const QueryBuilderMode$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  QueryBuilderMode
> = openEnums.outboundSchema(QueryBuilderMode);

/** @internal */
export type Query$Outbound = {
  path: string;
  queryExpression: string;
};

/** @internal */
export const Query$outboundSchema: z.ZodType<
  Query$Outbound,
  z.ZodTypeDef,
  Query
> = z.object({
  path: z.string(),
  queryExpression: z.string(),
});

export function queryToJSON(query: Query): string {
  return JSON.stringify(Query$outboundSchema.parse(query));
}

/** @internal */
export type Subscription$Outbound = {
  subscriptionName: string;
  version?: string | undefined;
  contentFormat: string;
  heartbeatInterval: number;
  batchTimeout: number;
  readExistingEvents: boolean;
  sendBookmarks: boolean;
  compress: boolean;
  targets: Array<string>;
  locale: string;
  querySelector: string;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  queries?: Array<Query$Outbound> | undefined;
  xmlQuery?: string | undefined;
};

/** @internal */
export const Subscription$outboundSchema: z.ZodType<
  Subscription$Outbound,
  z.ZodTypeDef,
  Subscription
> = z.object({
  subscriptionName: z.string(),
  version: z.string().optional(),
  contentFormat: CreateInputFormat$outboundSchema.default("Raw"),
  heartbeatInterval: z.number().default(60),
  batchTimeout: z.number().default(60),
  readExistingEvents: z.boolean().default(false),
  sendBookmarks: z.boolean().default(true),
  compress: z.boolean().default(true),
  targets: z.array(z.string()),
  locale: z.string().default("en-US"),
  querySelector: QueryBuilderMode$outboundSchema.default("simple"),
  metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
    .optional(),
  queries: z.array(z.lazy(() => Query$outboundSchema)).optional(),
  xmlQuery: z.string().optional(),
});

export function subscriptionToJSON(subscription: Subscription): string {
  return JSON.stringify(Subscription$outboundSchema.parse(subscription));
}

/** @internal */
export type InputWefPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  authMethod: string;
  tls?: MTLSSettings$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  keepAliveTimeout: number;
  enableHealthCheck: boolean;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  socketTimeout: number;
  caFingerprint?: string | undefined;
  keytab?: string | undefined;
  principal?: string | undefined;
  allowMachineIdMismatch: boolean;
  subscriptions: Array<Subscription$Outbound>;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
  logFingerprintMismatch: boolean;
};

/** @internal */
export const InputWefPqEnabledTrueWithPqConstraint$outboundSchema: z.ZodType<
  InputWefPqEnabledTrueWithPqConstraint$Outbound,
  z.ZodTypeDef,
  InputWefPqEnabledTrueWithPqConstraint
> = z.object({
  pqEnabled: z.boolean().default(false),
  pq: models.PqType$outboundSchema.optional(),
  id: z.string(),
  type: InputWefType$outboundSchema,
  disabled: z.boolean().default(false),
  pipeline: z.string().optional(),
  sendToRoutes: z.boolean().default(true),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
  host: z.string().default("0.0.0.0"),
  port: z.number().default(5986),
  authMethod: InputWefAuthenticationMethod$outboundSchema.default("clientCert"),
  tls: z.lazy(() => MTLSSettings$outboundSchema).optional(),
  maxActiveReq: z.number().default(256),
  maxRequestsPerSocket: z.number().int().default(0),
  enableProxyHeader: z.boolean().default(false),
  captureHeaders: z.boolean().default(false),
  keepAliveTimeout: z.number().default(90),
  enableHealthCheck: z.boolean().default(false),
  ipAllowlistRegex: z.string().default("/.*/"),
  ipDenylistRegex: z.string().default("/^$/"),
  socketTimeout: z.number().default(0),
  caFingerprint: z.string().optional(),
  keytab: z.string().optional(),
  principal: z.string().optional(),
  allowMachineIdMismatch: z.boolean().default(false),
  subscriptions: z.array(z.lazy(() => Subscription$outboundSchema)),
  metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
    .optional(),
  description: z.string().optional(),
  logFingerprintMismatch: z.boolean().default(false),
});

export function inputWefPqEnabledTrueWithPqConstraintToJSON(
  inputWefPqEnabledTrueWithPqConstraint: InputWefPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputWefPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputWefPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputWefPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  authMethod: string;
  tls?: MTLSSettings$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  keepAliveTimeout: number;
  enableHealthCheck: boolean;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  socketTimeout: number;
  caFingerprint?: string | undefined;
  keytab?: string | undefined;
  principal?: string | undefined;
  allowMachineIdMismatch: boolean;
  subscriptions: Array<Subscription$Outbound>;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
  logFingerprintMismatch: boolean;
};

/** @internal */
export const InputWefPqEnabledFalseWithPqConstraint$outboundSchema: z.ZodType<
  InputWefPqEnabledFalseWithPqConstraint$Outbound,
  z.ZodTypeDef,
  InputWefPqEnabledFalseWithPqConstraint
> = z.object({
  pqEnabled: z.boolean().default(false),
  pq: models.PqType$outboundSchema.optional(),
  id: z.string(),
  type: InputWefType$outboundSchema,
  disabled: z.boolean().default(false),
  pipeline: z.string().optional(),
  sendToRoutes: z.boolean().default(true),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
  host: z.string().default("0.0.0.0"),
  port: z.number().default(5986),
  authMethod: InputWefAuthenticationMethod$outboundSchema.default("clientCert"),
  tls: z.lazy(() => MTLSSettings$outboundSchema).optional(),
  maxActiveReq: z.number().default(256),
  maxRequestsPerSocket: z.number().int().default(0),
  enableProxyHeader: z.boolean().default(false),
  captureHeaders: z.boolean().default(false),
  keepAliveTimeout: z.number().default(90),
  enableHealthCheck: z.boolean().default(false),
  ipAllowlistRegex: z.string().default("/.*/"),
  ipDenylistRegex: z.string().default("/^$/"),
  socketTimeout: z.number().default(0),
  caFingerprint: z.string().optional(),
  keytab: z.string().optional(),
  principal: z.string().optional(),
  allowMachineIdMismatch: z.boolean().default(false),
  subscriptions: z.array(z.lazy(() => Subscription$outboundSchema)),
  metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
    .optional(),
  description: z.string().optional(),
  logFingerprintMismatch: z.boolean().default(false),
});

export function inputWefPqEnabledFalseWithPqConstraintToJSON(
  inputWefPqEnabledFalseWithPqConstraint:
    InputWefPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputWefPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputWefPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputWefSendToRoutesFalseWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  host: string;
  port: number;
  authMethod: string;
  tls?: MTLSSettings$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  keepAliveTimeout: number;
  enableHealthCheck: boolean;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  socketTimeout: number;
  caFingerprint?: string | undefined;
  keytab?: string | undefined;
  principal?: string | undefined;
  allowMachineIdMismatch: boolean;
  subscriptions: Array<Subscription$Outbound>;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
  logFingerprintMismatch: boolean;
};

/** @internal */
export const InputWefSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputWefSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputWefSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputWefType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number().default(5986),
    authMethod: InputWefAuthenticationMethod$outboundSchema.default(
      "clientCert",
    ),
    tls: z.lazy(() => MTLSSettings$outboundSchema).optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    keepAliveTimeout: z.number().default(90),
    enableHealthCheck: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    socketTimeout: z.number().default(0),
    caFingerprint: z.string().optional(),
    keytab: z.string().optional(),
    principal: z.string().optional(),
    allowMachineIdMismatch: z.boolean().default(false),
    subscriptions: z.array(z.lazy(() => Subscription$outboundSchema)),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
    logFingerprintMismatch: z.boolean().default(false),
  });

export function inputWefSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputWefSendToRoutesFalseWithConnectionsConstraint:
    InputWefSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputWefSendToRoutesFalseWithConnectionsConstraint$outboundSchema.parse(
      inputWefSendToRoutesFalseWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputWefSendToRoutesTrueWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  host: string;
  port: number;
  authMethod: string;
  tls?: MTLSSettings$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  keepAliveTimeout: number;
  enableHealthCheck: boolean;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  socketTimeout: number;
  caFingerprint?: string | undefined;
  keytab?: string | undefined;
  principal?: string | undefined;
  allowMachineIdMismatch: boolean;
  subscriptions: Array<Subscription$Outbound>;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
  logFingerprintMismatch: boolean;
};

/** @internal */
export const InputWefSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputWefSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputWefSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputWefType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number().default(5986),
    authMethod: InputWefAuthenticationMethod$outboundSchema.default(
      "clientCert",
    ),
    tls: z.lazy(() => MTLSSettings$outboundSchema).optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    keepAliveTimeout: z.number().default(90),
    enableHealthCheck: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    socketTimeout: z.number().default(0),
    caFingerprint: z.string().optional(),
    keytab: z.string().optional(),
    principal: z.string().optional(),
    allowMachineIdMismatch: z.boolean().default(false),
    subscriptions: z.array(z.lazy(() => Subscription$outboundSchema)),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
    logFingerprintMismatch: z.boolean().default(false),
  });

export function inputWefSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputWefSendToRoutesTrueWithConnectionsConstraint:
    InputWefSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputWefSendToRoutesTrueWithConnectionsConstraint$outboundSchema.parse(
      inputWefSendToRoutesTrueWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputWef$Outbound =
  | InputWefSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputWefSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputWefPqEnabledFalseWithPqConstraint$Outbound
  | InputWefPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputWef$outboundSchema: z.ZodType<
  InputWef$Outbound,
  z.ZodTypeDef,
  InputWef
> = z.union([
  z.lazy(() =>
    InputWefSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputWefSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputWefPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputWefPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputWefToJSON(inputWef: InputWef): string {
  return JSON.stringify(InputWef$outboundSchema.parse(inputWef));
}

/** @internal */
export const InputAppscopeType$outboundSchema: z.ZodNativeEnum<
  typeof InputAppscopeType
> = z.nativeEnum(InputAppscopeType);

/** @internal */
export type Allow$Outbound = {
  procname: string;
  arg?: string | undefined;
  config: string;
};

/** @internal */
export const Allow$outboundSchema: z.ZodType<
  Allow$Outbound,
  z.ZodTypeDef,
  Allow
> = z.object({
  procname: z.string(),
  arg: z.string().optional(),
  config: z.string(),
});

export function allowToJSON(allow: Allow): string {
  return JSON.stringify(Allow$outboundSchema.parse(allow));
}

/** @internal */
export type InputAppscopeFilter$Outbound = {
  allow?: Array<Allow$Outbound> | undefined;
  transportURL?: string | undefined;
};

/** @internal */
export const InputAppscopeFilter$outboundSchema: z.ZodType<
  InputAppscopeFilter$Outbound,
  z.ZodTypeDef,
  InputAppscopeFilter
> = z.object({
  allow: z.array(z.lazy(() => Allow$outboundSchema)).optional(),
  transportURL: z.string().optional(),
});

export function inputAppscopeFilterToJSON(
  inputAppscopeFilter: InputAppscopeFilter,
): string {
  return JSON.stringify(
    InputAppscopeFilter$outboundSchema.parse(inputAppscopeFilter),
  );
}

/** @internal */
export type InputAppscopePersistence$Outbound = {
  enable: boolean;
  timeWindow: string;
  maxDataSize: string;
  maxDataTime: string;
  compress: string;
  destPath: string;
};

/** @internal */
export const InputAppscopePersistence$outboundSchema: z.ZodType<
  InputAppscopePersistence$Outbound,
  z.ZodTypeDef,
  InputAppscopePersistence
> = z.object({
  enable: z.boolean().default(false),
  timeWindow: z.string().default("10m"),
  maxDataSize: z.string().default("1GB"),
  maxDataTime: z.string().default("24h"),
  compress: models.DataCompressionFormatOptionsPersistence$outboundSchema
    .default("gzip"),
  destPath: z.string().default("$CRIBL_HOME/state/appscope"),
});

export function inputAppscopePersistenceToJSON(
  inputAppscopePersistence: InputAppscopePersistence,
): string {
  return JSON.stringify(
    InputAppscopePersistence$outboundSchema.parse(inputAppscopePersistence),
  );
}

/** @internal */
export type InputAppscopePqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  ipWhitelistRegex: string;
  maxActiveCxn: number;
  socketIdleTimeout: number;
  socketEndingMaxWait: number;
  socketMaxLifespan: number;
  enableProxyHeader: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  enableUnixPath: boolean;
  filter?: InputAppscopeFilter$Outbound | undefined;
  persistence?: InputAppscopePersistence$Outbound | undefined;
  authType: string;
  description?: string | undefined;
  host?: string | undefined;
  port?: number | undefined;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  unixSocketPath: string;
  unixSocketPerms?: string | undefined;
  authToken: string;
  textSecret?: string | undefined;
};

/** @internal */
export const InputAppscopePqEnabledTrueWithPqConstraint$outboundSchema:
  z.ZodType<
    InputAppscopePqEnabledTrueWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputAppscopePqEnabledTrueWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputAppscopeType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    ipWhitelistRegex: z.string().default("/.*/"),
    maxActiveCxn: z.number().default(1000),
    socketIdleTimeout: z.number().default(0),
    socketEndingMaxWait: z.number().default(30),
    socketMaxLifespan: z.number().default(0),
    enableProxyHeader: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    enableUnixPath: z.boolean().default(false),
    filter: z.lazy(() => InputAppscopeFilter$outboundSchema).optional(),
    persistence: z.lazy(() => InputAppscopePersistence$outboundSchema)
      .optional(),
    authType: models.AuthenticationMethodOptionsAuthTokensItems$outboundSchema
      .default("manual"),
    description: z.string().optional(),
    host: z.string().optional(),
    port: z.number().optional(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    unixSocketPath: z.string().default("$CRIBL_HOME/state/appscope.sock"),
    unixSocketPerms: z.string().optional(),
    authToken: z.string().default(""),
    textSecret: z.string().optional(),
  });

export function inputAppscopePqEnabledTrueWithPqConstraintToJSON(
  inputAppscopePqEnabledTrueWithPqConstraint:
    InputAppscopePqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputAppscopePqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputAppscopePqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputAppscopePqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  ipWhitelistRegex: string;
  maxActiveCxn: number;
  socketIdleTimeout: number;
  socketEndingMaxWait: number;
  socketMaxLifespan: number;
  enableProxyHeader: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  enableUnixPath: boolean;
  filter?: InputAppscopeFilter$Outbound | undefined;
  persistence?: InputAppscopePersistence$Outbound | undefined;
  authType: string;
  description?: string | undefined;
  host?: string | undefined;
  port?: number | undefined;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  unixSocketPath: string;
  unixSocketPerms?: string | undefined;
  authToken: string;
  textSecret?: string | undefined;
};

/** @internal */
export const InputAppscopePqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputAppscopePqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputAppscopePqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputAppscopeType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    ipWhitelistRegex: z.string().default("/.*/"),
    maxActiveCxn: z.number().default(1000),
    socketIdleTimeout: z.number().default(0),
    socketEndingMaxWait: z.number().default(30),
    socketMaxLifespan: z.number().default(0),
    enableProxyHeader: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    enableUnixPath: z.boolean().default(false),
    filter: z.lazy(() => InputAppscopeFilter$outboundSchema).optional(),
    persistence: z.lazy(() => InputAppscopePersistence$outboundSchema)
      .optional(),
    authType: models.AuthenticationMethodOptionsAuthTokensItems$outboundSchema
      .default("manual"),
    description: z.string().optional(),
    host: z.string().optional(),
    port: z.number().optional(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    unixSocketPath: z.string().default("$CRIBL_HOME/state/appscope.sock"),
    unixSocketPerms: z.string().optional(),
    authToken: z.string().default(""),
    textSecret: z.string().optional(),
  });

export function inputAppscopePqEnabledFalseWithPqConstraintToJSON(
  inputAppscopePqEnabledFalseWithPqConstraint:
    InputAppscopePqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputAppscopePqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputAppscopePqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputAppscopeSendToRoutesFalseWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  ipWhitelistRegex: string;
  maxActiveCxn: number;
  socketIdleTimeout: number;
  socketEndingMaxWait: number;
  socketMaxLifespan: number;
  enableProxyHeader: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  enableUnixPath: boolean;
  filter?: InputAppscopeFilter$Outbound | undefined;
  persistence?: InputAppscopePersistence$Outbound | undefined;
  authType: string;
  description?: string | undefined;
  host?: string | undefined;
  port?: number | undefined;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  unixSocketPath: string;
  unixSocketPerms?: string | undefined;
  authToken: string;
  textSecret?: string | undefined;
};

/** @internal */
export const InputAppscopeSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputAppscopeSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputAppscopeSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputAppscopeType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    ipWhitelistRegex: z.string().default("/.*/"),
    maxActiveCxn: z.number().default(1000),
    socketIdleTimeout: z.number().default(0),
    socketEndingMaxWait: z.number().default(30),
    socketMaxLifespan: z.number().default(0),
    enableProxyHeader: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    enableUnixPath: z.boolean().default(false),
    filter: z.lazy(() => InputAppscopeFilter$outboundSchema).optional(),
    persistence: z.lazy(() => InputAppscopePersistence$outboundSchema)
      .optional(),
    authType: models.AuthenticationMethodOptionsAuthTokensItems$outboundSchema
      .default("manual"),
    description: z.string().optional(),
    host: z.string().optional(),
    port: z.number().optional(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    unixSocketPath: z.string().default("$CRIBL_HOME/state/appscope.sock"),
    unixSocketPerms: z.string().optional(),
    authToken: z.string().default(""),
    textSecret: z.string().optional(),
  });

export function inputAppscopeSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputAppscopeSendToRoutesFalseWithConnectionsConstraint:
    InputAppscopeSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputAppscopeSendToRoutesFalseWithConnectionsConstraint$outboundSchema
      .parse(inputAppscopeSendToRoutesFalseWithConnectionsConstraint),
  );
}

/** @internal */
export type InputAppscopeSendToRoutesTrueWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  ipWhitelistRegex: string;
  maxActiveCxn: number;
  socketIdleTimeout: number;
  socketEndingMaxWait: number;
  socketMaxLifespan: number;
  enableProxyHeader: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  enableUnixPath: boolean;
  filter?: InputAppscopeFilter$Outbound | undefined;
  persistence?: InputAppscopePersistence$Outbound | undefined;
  authType: string;
  description?: string | undefined;
  host?: string | undefined;
  port?: number | undefined;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  unixSocketPath: string;
  unixSocketPerms?: string | undefined;
  authToken: string;
  textSecret?: string | undefined;
};

/** @internal */
export const InputAppscopeSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputAppscopeSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputAppscopeSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputAppscopeType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    ipWhitelistRegex: z.string().default("/.*/"),
    maxActiveCxn: z.number().default(1000),
    socketIdleTimeout: z.number().default(0),
    socketEndingMaxWait: z.number().default(30),
    socketMaxLifespan: z.number().default(0),
    enableProxyHeader: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    enableUnixPath: z.boolean().default(false),
    filter: z.lazy(() => InputAppscopeFilter$outboundSchema).optional(),
    persistence: z.lazy(() => InputAppscopePersistence$outboundSchema)
      .optional(),
    authType: models.AuthenticationMethodOptionsAuthTokensItems$outboundSchema
      .default("manual"),
    description: z.string().optional(),
    host: z.string().optional(),
    port: z.number().optional(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    unixSocketPath: z.string().default("$CRIBL_HOME/state/appscope.sock"),
    unixSocketPerms: z.string().optional(),
    authToken: z.string().default(""),
    textSecret: z.string().optional(),
  });

export function inputAppscopeSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputAppscopeSendToRoutesTrueWithConnectionsConstraint:
    InputAppscopeSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputAppscopeSendToRoutesTrueWithConnectionsConstraint$outboundSchema.parse(
      inputAppscopeSendToRoutesTrueWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputAppscope$Outbound =
  | InputAppscopeSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputAppscopeSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputAppscopePqEnabledFalseWithPqConstraint$Outbound
  | InputAppscopePqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputAppscope$outboundSchema: z.ZodType<
  InputAppscope$Outbound,
  z.ZodTypeDef,
  InputAppscope
> = z.union([
  z.lazy(() =>
    InputAppscopeSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputAppscopeSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputAppscopePqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputAppscopePqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputAppscopeToJSON(inputAppscope: InputAppscope): string {
  return JSON.stringify(InputAppscope$outboundSchema.parse(inputAppscope));
}

/** @internal */
export const InputTcpType$outboundSchema: z.ZodNativeEnum<typeof InputTcpType> =
  z.nativeEnum(InputTcpType);

/** @internal */
export type InputTcpPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  ipWhitelistRegex: string;
  maxActiveCxn: number;
  socketIdleTimeout: number;
  socketEndingMaxWait: number;
  socketMaxLifespan: number;
  enableProxyHeader: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  enableHeader: boolean;
  preprocess?:
    | models.PreprocessTypeSavedJobCollectionInput$Outbound
    | undefined;
  description?: string | undefined;
  authToken: string;
  authType: string;
  textSecret?: string | undefined;
};

/** @internal */
export const InputTcpPqEnabledTrueWithPqConstraint$outboundSchema: z.ZodType<
  InputTcpPqEnabledTrueWithPqConstraint$Outbound,
  z.ZodTypeDef,
  InputTcpPqEnabledTrueWithPqConstraint
> = z.object({
  pqEnabled: z.boolean().default(false),
  pq: models.PqType$outboundSchema.optional(),
  id: z.string(),
  type: InputTcpType$outboundSchema,
  disabled: z.boolean().default(false),
  pipeline: z.string().optional(),
  sendToRoutes: z.boolean().default(true),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
  host: z.string().default("0.0.0.0"),
  port: z.number(),
  tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
  ipWhitelistRegex: z.string().default("/.*/"),
  maxActiveCxn: z.number().default(1000),
  socketIdleTimeout: z.number().default(0),
  socketEndingMaxWait: z.number().default(30),
  socketMaxLifespan: z.number().default(0),
  enableProxyHeader: z.boolean().default(false),
  metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
    .optional(),
  breakerRulesets: z.array(z.string()).optional(),
  staleChannelFlushMs: z.number().default(10000),
  enableHeader: z.boolean().default(false),
  preprocess: models.PreprocessTypeSavedJobCollectionInput$outboundSchema
    .optional(),
  description: z.string().optional(),
  authToken: z.string().default(""),
  authType: models.AuthenticationMethodOptionsAuthTokensItems$outboundSchema
    .default("manual"),
  textSecret: z.string().optional(),
});

export function inputTcpPqEnabledTrueWithPqConstraintToJSON(
  inputTcpPqEnabledTrueWithPqConstraint: InputTcpPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputTcpPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputTcpPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputTcpPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  ipWhitelistRegex: string;
  maxActiveCxn: number;
  socketIdleTimeout: number;
  socketEndingMaxWait: number;
  socketMaxLifespan: number;
  enableProxyHeader: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  enableHeader: boolean;
  preprocess?:
    | models.PreprocessTypeSavedJobCollectionInput$Outbound
    | undefined;
  description?: string | undefined;
  authToken: string;
  authType: string;
  textSecret?: string | undefined;
};

/** @internal */
export const InputTcpPqEnabledFalseWithPqConstraint$outboundSchema: z.ZodType<
  InputTcpPqEnabledFalseWithPqConstraint$Outbound,
  z.ZodTypeDef,
  InputTcpPqEnabledFalseWithPqConstraint
> = z.object({
  pqEnabled: z.boolean().default(false),
  pq: models.PqType$outboundSchema.optional(),
  id: z.string(),
  type: InputTcpType$outboundSchema,
  disabled: z.boolean().default(false),
  pipeline: z.string().optional(),
  sendToRoutes: z.boolean().default(true),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
  host: z.string().default("0.0.0.0"),
  port: z.number(),
  tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
  ipWhitelistRegex: z.string().default("/.*/"),
  maxActiveCxn: z.number().default(1000),
  socketIdleTimeout: z.number().default(0),
  socketEndingMaxWait: z.number().default(30),
  socketMaxLifespan: z.number().default(0),
  enableProxyHeader: z.boolean().default(false),
  metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
    .optional(),
  breakerRulesets: z.array(z.string()).optional(),
  staleChannelFlushMs: z.number().default(10000),
  enableHeader: z.boolean().default(false),
  preprocess: models.PreprocessTypeSavedJobCollectionInput$outboundSchema
    .optional(),
  description: z.string().optional(),
  authToken: z.string().default(""),
  authType: models.AuthenticationMethodOptionsAuthTokensItems$outboundSchema
    .default("manual"),
  textSecret: z.string().optional(),
});

export function inputTcpPqEnabledFalseWithPqConstraintToJSON(
  inputTcpPqEnabledFalseWithPqConstraint:
    InputTcpPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputTcpPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputTcpPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputTcpSendToRoutesFalseWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  host: string;
  port: number;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  ipWhitelistRegex: string;
  maxActiveCxn: number;
  socketIdleTimeout: number;
  socketEndingMaxWait: number;
  socketMaxLifespan: number;
  enableProxyHeader: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  enableHeader: boolean;
  preprocess?:
    | models.PreprocessTypeSavedJobCollectionInput$Outbound
    | undefined;
  description?: string | undefined;
  authToken: string;
  authType: string;
  textSecret?: string | undefined;
};

/** @internal */
export const InputTcpSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputTcpSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputTcpSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputTcpType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    ipWhitelistRegex: z.string().default("/.*/"),
    maxActiveCxn: z.number().default(1000),
    socketIdleTimeout: z.number().default(0),
    socketEndingMaxWait: z.number().default(30),
    socketMaxLifespan: z.number().default(0),
    enableProxyHeader: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    enableHeader: z.boolean().default(false),
    preprocess: models.PreprocessTypeSavedJobCollectionInput$outboundSchema
      .optional(),
    description: z.string().optional(),
    authToken: z.string().default(""),
    authType: models.AuthenticationMethodOptionsAuthTokensItems$outboundSchema
      .default("manual"),
    textSecret: z.string().optional(),
  });

export function inputTcpSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputTcpSendToRoutesFalseWithConnectionsConstraint:
    InputTcpSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputTcpSendToRoutesFalseWithConnectionsConstraint$outboundSchema.parse(
      inputTcpSendToRoutesFalseWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputTcpSendToRoutesTrueWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  host: string;
  port: number;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  ipWhitelistRegex: string;
  maxActiveCxn: number;
  socketIdleTimeout: number;
  socketEndingMaxWait: number;
  socketMaxLifespan: number;
  enableProxyHeader: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  enableHeader: boolean;
  preprocess?:
    | models.PreprocessTypeSavedJobCollectionInput$Outbound
    | undefined;
  description?: string | undefined;
  authToken: string;
  authType: string;
  textSecret?: string | undefined;
};

/** @internal */
export const InputTcpSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputTcpSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputTcpSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputTcpType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    ipWhitelistRegex: z.string().default("/.*/"),
    maxActiveCxn: z.number().default(1000),
    socketIdleTimeout: z.number().default(0),
    socketEndingMaxWait: z.number().default(30),
    socketMaxLifespan: z.number().default(0),
    enableProxyHeader: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    enableHeader: z.boolean().default(false),
    preprocess: models.PreprocessTypeSavedJobCollectionInput$outboundSchema
      .optional(),
    description: z.string().optional(),
    authToken: z.string().default(""),
    authType: models.AuthenticationMethodOptionsAuthTokensItems$outboundSchema
      .default("manual"),
    textSecret: z.string().optional(),
  });

export function inputTcpSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputTcpSendToRoutesTrueWithConnectionsConstraint:
    InputTcpSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputTcpSendToRoutesTrueWithConnectionsConstraint$outboundSchema.parse(
      inputTcpSendToRoutesTrueWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputTcp$Outbound =
  | InputTcpSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputTcpSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputTcpPqEnabledFalseWithPqConstraint$Outbound
  | InputTcpPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputTcp$outboundSchema: z.ZodType<
  InputTcp$Outbound,
  z.ZodTypeDef,
  InputTcp
> = z.union([
  z.lazy(() =>
    InputTcpSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputTcpSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputTcpPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputTcpPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputTcpToJSON(inputTcp: InputTcp): string {
  return JSON.stringify(InputTcp$outboundSchema.parse(inputTcp));
}

/** @internal */
export const PqEnabledTrueWithPqConstraintInputFileType$outboundSchema:
  z.ZodNativeEnum<typeof PqEnabledTrueWithPqConstraintInputFileType> = z
    .nativeEnum(PqEnabledTrueWithPqConstraintInputFileType);

/** @internal */
export const PqEnabledTrueWithPqConstraintMode$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  PqEnabledTrueWithPqConstraintMode
> = openEnums.outboundSchema(PqEnabledTrueWithPqConstraintMode);

/** @internal */
export type InputFilePqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  mode: string;
  interval: number;
  filenames?: Array<string> | undefined;
  filterArchivedFiles: boolean;
  tailOnly: boolean;
  idleTimeout: number;
  minAgeDur?: string | undefined;
  maxAgeDur?: string | undefined;
  checkFileModTime: boolean;
  forceText: boolean;
  hashLen: number;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  description?: string | undefined;
  path?: string | undefined;
  depth?: number | undefined;
  suppressMissingPathErrors: boolean;
  deleteFiles: boolean;
  includeUnidentifiableBinary: boolean;
};

/** @internal */
export const InputFilePqEnabledTrueWithPqConstraint$outboundSchema: z.ZodType<
  InputFilePqEnabledTrueWithPqConstraint$Outbound,
  z.ZodTypeDef,
  InputFilePqEnabledTrueWithPqConstraint
> = z.object({
  pqEnabled: z.boolean().default(false),
  pq: models.PqType$outboundSchema.optional(),
  id: z.string(),
  type: PqEnabledTrueWithPqConstraintInputFileType$outboundSchema,
  disabled: z.boolean().default(false),
  pipeline: z.string().optional(),
  sendToRoutes: z.boolean().default(true),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
  mode: PqEnabledTrueWithPqConstraintMode$outboundSchema.default("manual"),
  interval: z.number().default(10),
  filenames: z.array(z.string()).optional(),
  filterArchivedFiles: z.boolean().default(false),
  tailOnly: z.boolean().default(true),
  idleTimeout: z.number().default(300),
  minAgeDur: z.string().optional(),
  maxAgeDur: z.string().optional(),
  checkFileModTime: z.boolean().default(false),
  forceText: z.boolean().default(false),
  hashLen: z.number().default(256),
  metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
    .optional(),
  breakerRulesets: z.array(z.string()).optional(),
  staleChannelFlushMs: z.number().default(10000),
  description: z.string().optional(),
  path: z.string().optional(),
  depth: z.number().optional(),
  suppressMissingPathErrors: z.boolean().default(false),
  deleteFiles: z.boolean().default(false),
  includeUnidentifiableBinary: z.boolean().default(false),
});

export function inputFilePqEnabledTrueWithPqConstraintToJSON(
  inputFilePqEnabledTrueWithPqConstraint:
    InputFilePqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputFilePqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputFilePqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export const PqEnabledFalseWithPqConstraintInputFileType$outboundSchema:
  z.ZodNativeEnum<typeof PqEnabledFalseWithPqConstraintInputFileType> = z
    .nativeEnum(PqEnabledFalseWithPqConstraintInputFileType);

/** @internal */
export const PqEnabledFalseWithPqConstraintMode$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  PqEnabledFalseWithPqConstraintMode
> = openEnums.outboundSchema(PqEnabledFalseWithPqConstraintMode);

/** @internal */
export type InputFilePqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  mode: string;
  interval: number;
  filenames?: Array<string> | undefined;
  filterArchivedFiles: boolean;
  tailOnly: boolean;
  idleTimeout: number;
  minAgeDur?: string | undefined;
  maxAgeDur?: string | undefined;
  checkFileModTime: boolean;
  forceText: boolean;
  hashLen: number;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  description?: string | undefined;
  path?: string | undefined;
  depth?: number | undefined;
  suppressMissingPathErrors: boolean;
  deleteFiles: boolean;
  includeUnidentifiableBinary: boolean;
};

/** @internal */
export const InputFilePqEnabledFalseWithPqConstraint$outboundSchema: z.ZodType<
  InputFilePqEnabledFalseWithPqConstraint$Outbound,
  z.ZodTypeDef,
  InputFilePqEnabledFalseWithPqConstraint
> = z.object({
  pqEnabled: z.boolean().default(false),
  pq: models.PqType$outboundSchema.optional(),
  id: z.string(),
  type: PqEnabledFalseWithPqConstraintInputFileType$outboundSchema,
  disabled: z.boolean().default(false),
  pipeline: z.string().optional(),
  sendToRoutes: z.boolean().default(true),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
  mode: PqEnabledFalseWithPqConstraintMode$outboundSchema.default("manual"),
  interval: z.number().default(10),
  filenames: z.array(z.string()).optional(),
  filterArchivedFiles: z.boolean().default(false),
  tailOnly: z.boolean().default(true),
  idleTimeout: z.number().default(300),
  minAgeDur: z.string().optional(),
  maxAgeDur: z.string().optional(),
  checkFileModTime: z.boolean().default(false),
  forceText: z.boolean().default(false),
  hashLen: z.number().default(256),
  metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
    .optional(),
  breakerRulesets: z.array(z.string()).optional(),
  staleChannelFlushMs: z.number().default(10000),
  description: z.string().optional(),
  path: z.string().optional(),
  depth: z.number().optional(),
  suppressMissingPathErrors: z.boolean().default(false),
  deleteFiles: z.boolean().default(false),
  includeUnidentifiableBinary: z.boolean().default(false),
});

export function inputFilePqEnabledFalseWithPqConstraintToJSON(
  inputFilePqEnabledFalseWithPqConstraint:
    InputFilePqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputFilePqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputFilePqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export const SendToRoutesFalseWithConnectionsConstraintInputFileType$outboundSchema:
  z.ZodNativeEnum<
    typeof SendToRoutesFalseWithConnectionsConstraintInputFileType
  > = z.nativeEnum(SendToRoutesFalseWithConnectionsConstraintInputFileType);

/** @internal */
export const SendToRoutesFalseWithConnectionsConstraintMode$outboundSchema:
  z.ZodType<
    string,
    z.ZodTypeDef,
    SendToRoutesFalseWithConnectionsConstraintMode
  > = openEnums.outboundSchema(SendToRoutesFalseWithConnectionsConstraintMode);

/** @internal */
export type InputFileSendToRoutesFalseWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  mode: string;
  interval: number;
  filenames?: Array<string> | undefined;
  filterArchivedFiles: boolean;
  tailOnly: boolean;
  idleTimeout: number;
  minAgeDur?: string | undefined;
  maxAgeDur?: string | undefined;
  checkFileModTime: boolean;
  forceText: boolean;
  hashLen: number;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  description?: string | undefined;
  path?: string | undefined;
  depth?: number | undefined;
  suppressMissingPathErrors: boolean;
  deleteFiles: boolean;
  includeUnidentifiableBinary: boolean;
};

/** @internal */
export const InputFileSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputFileSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputFileSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type:
      SendToRoutesFalseWithConnectionsConstraintInputFileType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    mode: SendToRoutesFalseWithConnectionsConstraintMode$outboundSchema.default(
      "manual",
    ),
    interval: z.number().default(10),
    filenames: z.array(z.string()).optional(),
    filterArchivedFiles: z.boolean().default(false),
    tailOnly: z.boolean().default(true),
    idleTimeout: z.number().default(300),
    minAgeDur: z.string().optional(),
    maxAgeDur: z.string().optional(),
    checkFileModTime: z.boolean().default(false),
    forceText: z.boolean().default(false),
    hashLen: z.number().default(256),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    description: z.string().optional(),
    path: z.string().optional(),
    depth: z.number().optional(),
    suppressMissingPathErrors: z.boolean().default(false),
    deleteFiles: z.boolean().default(false),
    includeUnidentifiableBinary: z.boolean().default(false),
  });

export function inputFileSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputFileSendToRoutesFalseWithConnectionsConstraint:
    InputFileSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputFileSendToRoutesFalseWithConnectionsConstraint$outboundSchema.parse(
      inputFileSendToRoutesFalseWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export const SendToRoutesTrueWithConnectionsConstraintInputFileType$outboundSchema:
  z.ZodNativeEnum<
    typeof SendToRoutesTrueWithConnectionsConstraintInputFileType
  > = z.nativeEnum(SendToRoutesTrueWithConnectionsConstraintInputFileType);

/** @internal */
export const SendToRoutesTrueWithConnectionsConstraintMode$outboundSchema:
  z.ZodType<
    string,
    z.ZodTypeDef,
    SendToRoutesTrueWithConnectionsConstraintMode
  > = openEnums.outboundSchema(SendToRoutesTrueWithConnectionsConstraintMode);

/** @internal */
export type InputFileSendToRoutesTrueWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  mode: string;
  interval: number;
  filenames?: Array<string> | undefined;
  filterArchivedFiles: boolean;
  tailOnly: boolean;
  idleTimeout: number;
  minAgeDur?: string | undefined;
  maxAgeDur?: string | undefined;
  checkFileModTime: boolean;
  forceText: boolean;
  hashLen: number;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  description?: string | undefined;
  path?: string | undefined;
  depth?: number | undefined;
  suppressMissingPathErrors: boolean;
  deleteFiles: boolean;
  includeUnidentifiableBinary: boolean;
};

/** @internal */
export const InputFileSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputFileSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputFileSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: SendToRoutesTrueWithConnectionsConstraintInputFileType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    mode: SendToRoutesTrueWithConnectionsConstraintMode$outboundSchema.default(
      "manual",
    ),
    interval: z.number().default(10),
    filenames: z.array(z.string()).optional(),
    filterArchivedFiles: z.boolean().default(false),
    tailOnly: z.boolean().default(true),
    idleTimeout: z.number().default(300),
    minAgeDur: z.string().optional(),
    maxAgeDur: z.string().optional(),
    checkFileModTime: z.boolean().default(false),
    forceText: z.boolean().default(false),
    hashLen: z.number().default(256),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    description: z.string().optional(),
    path: z.string().optional(),
    depth: z.number().optional(),
    suppressMissingPathErrors: z.boolean().default(false),
    deleteFiles: z.boolean().default(false),
    includeUnidentifiableBinary: z.boolean().default(false),
  });

export function inputFileSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputFileSendToRoutesTrueWithConnectionsConstraint:
    InputFileSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputFileSendToRoutesTrueWithConnectionsConstraint$outboundSchema.parse(
      inputFileSendToRoutesTrueWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputFile$Outbound =
  | InputFileSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputFileSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputFilePqEnabledFalseWithPqConstraint$Outbound
  | InputFilePqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputFile$outboundSchema: z.ZodType<
  InputFile$Outbound,
  z.ZodTypeDef,
  InputFile
> = z.union([
  z.lazy(() =>
    InputFileSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputFileSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputFilePqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputFilePqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputFileToJSON(inputFile: InputFile): string {
  return JSON.stringify(InputFile$outboundSchema.parse(inputFile));
}

/** @internal */
export const InputSyslogType2$outboundSchema: z.ZodNativeEnum<
  typeof InputSyslogType2
> = z.nativeEnum(InputSyslogType2);

/** @internal */
export type InputSyslogSyslog2$Outbound = {
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  pq?: models.PqType$Outbound | undefined;
  host: string;
  udpPort?: number | undefined;
  tcpPort: number;
  maxBufferSize: number;
  ipWhitelistRegex: string;
  timestampTimezone: string;
  singleMsgUdpPackets: boolean;
  enableProxyHeader: boolean;
  keepFieldsList?: Array<string> | undefined;
  octetCounting: boolean;
  inferFraming: boolean;
  strictlyInferOctetCounting: boolean;
  allowNonStandardAppName: boolean;
  maxActiveCxn: number;
  socketIdleTimeout: number;
  socketEndingMaxWait: number;
  socketMaxLifespan: number;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  udpSocketRxBufSize?: number | undefined;
  enableLoadBalancing: boolean;
  description?: string | undefined;
  enableEnhancedProxyHeaderParsing?: boolean | undefined;
};

/** @internal */
export const InputSyslogSyslog2$outboundSchema: z.ZodType<
  InputSyslogSyslog2$Outbound,
  z.ZodTypeDef,
  InputSyslogSyslog2
> = z.object({
  id: z.string(),
  type: InputSyslogType2$outboundSchema,
  disabled: z.boolean().default(false),
  pipeline: z.string().optional(),
  sendToRoutes: z.boolean().default(true),
  environment: z.string().optional(),
  pqEnabled: z.boolean().default(false),
  streamtags: z.array(z.string()).optional(),
  connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
  pq: models.PqType$outboundSchema.optional(),
  host: z.string().default("0.0.0.0"),
  udpPort: z.number().optional(),
  tcpPort: z.number(),
  maxBufferSize: z.number().default(1000),
  ipWhitelistRegex: z.string().default("/.*/"),
  timestampTimezone: z.string().default("local"),
  singleMsgUdpPackets: z.boolean().default(false),
  enableProxyHeader: z.boolean().default(false),
  keepFieldsList: z.array(z.string()).optional(),
  octetCounting: z.boolean().default(false),
  inferFraming: z.boolean().default(true),
  strictlyInferOctetCounting: z.boolean().default(true),
  allowNonStandardAppName: z.boolean().default(false),
  maxActiveCxn: z.number().default(1000),
  socketIdleTimeout: z.number().default(0),
  socketEndingMaxWait: z.number().default(30),
  socketMaxLifespan: z.number().default(0),
  tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
  metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
    .optional(),
  udpSocketRxBufSize: z.number().optional(),
  enableLoadBalancing: z.boolean().default(false),
  description: z.string().optional(),
  enableEnhancedProxyHeaderParsing: z.boolean().optional(),
});

export function inputSyslogSyslog2ToJSON(
  inputSyslogSyslog2: InputSyslogSyslog2,
): string {
  return JSON.stringify(
    InputSyslogSyslog2$outboundSchema.parse(inputSyslogSyslog2),
  );
}

/** @internal */
export const InputSyslogType1$outboundSchema: z.ZodNativeEnum<
  typeof InputSyslogType1
> = z.nativeEnum(InputSyslogType1);

/** @internal */
export type InputSyslogSyslog1$Outbound = {
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  pq?: models.PqType$Outbound | undefined;
  host: string;
  udpPort: number;
  tcpPort?: number | undefined;
  maxBufferSize: number;
  ipWhitelistRegex: string;
  timestampTimezone: string;
  singleMsgUdpPackets: boolean;
  enableProxyHeader: boolean;
  keepFieldsList?: Array<string> | undefined;
  octetCounting: boolean;
  inferFraming: boolean;
  strictlyInferOctetCounting: boolean;
  allowNonStandardAppName: boolean;
  maxActiveCxn: number;
  socketIdleTimeout: number;
  socketEndingMaxWait: number;
  socketMaxLifespan: number;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  udpSocketRxBufSize?: number | undefined;
  enableLoadBalancing: boolean;
  description?: string | undefined;
  enableEnhancedProxyHeaderParsing?: boolean | undefined;
};

/** @internal */
export const InputSyslogSyslog1$outboundSchema: z.ZodType<
  InputSyslogSyslog1$Outbound,
  z.ZodTypeDef,
  InputSyslogSyslog1
> = z.object({
  id: z.string(),
  type: InputSyslogType1$outboundSchema,
  disabled: z.boolean().default(false),
  pipeline: z.string().optional(),
  sendToRoutes: z.boolean().default(true),
  environment: z.string().optional(),
  pqEnabled: z.boolean().default(false),
  streamtags: z.array(z.string()).optional(),
  connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
  pq: models.PqType$outboundSchema.optional(),
  host: z.string().default("0.0.0.0"),
  udpPort: z.number(),
  tcpPort: z.number().optional(),
  maxBufferSize: z.number().default(1000),
  ipWhitelistRegex: z.string().default("/.*/"),
  timestampTimezone: z.string().default("local"),
  singleMsgUdpPackets: z.boolean().default(false),
  enableProxyHeader: z.boolean().default(false),
  keepFieldsList: z.array(z.string()).optional(),
  octetCounting: z.boolean().default(false),
  inferFraming: z.boolean().default(true),
  strictlyInferOctetCounting: z.boolean().default(true),
  allowNonStandardAppName: z.boolean().default(false),
  maxActiveCxn: z.number().default(1000),
  socketIdleTimeout: z.number().default(0),
  socketEndingMaxWait: z.number().default(30),
  socketMaxLifespan: z.number().default(0),
  tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
  metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
    .optional(),
  udpSocketRxBufSize: z.number().optional(),
  enableLoadBalancing: z.boolean().default(false),
  description: z.string().optional(),
  enableEnhancedProxyHeaderParsing: z.boolean().optional(),
});

export function inputSyslogSyslog1ToJSON(
  inputSyslogSyslog1: InputSyslogSyslog1,
): string {
  return JSON.stringify(
    InputSyslogSyslog1$outboundSchema.parse(inputSyslogSyslog1),
  );
}

/** @internal */
export type InputSyslog$Outbound =
  | InputSyslogSyslog1$Outbound
  | InputSyslogSyslog2$Outbound;

/** @internal */
export const InputSyslog$outboundSchema: z.ZodType<
  InputSyslog$Outbound,
  z.ZodTypeDef,
  InputSyslog
> = z.union([
  z.lazy(() => InputSyslogSyslog1$outboundSchema),
  z.lazy(() => InputSyslogSyslog2$outboundSchema),
]);

export function inputSyslogToJSON(inputSyslog: InputSyslog): string {
  return JSON.stringify(InputSyslog$outboundSchema.parse(inputSyslog));
}

/** @internal */
export const InputSqsType$outboundSchema: z.ZodNativeEnum<typeof InputSqsType> =
  z.nativeEnum(InputSqsType);

/** @internal */
export const CreateInputQueueType$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  CreateInputQueueType
> = openEnums.outboundSchema(CreateInputQueueType);

/** @internal */
export type InputSqsPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  queueName: string;
  queueType: string;
  awsAccountId?: string | undefined;
  createQueue: boolean;
  awsAuthenticationMethod: string;
  awsSecretKey?: string | undefined;
  region?: string | undefined;
  endpoint?: string | undefined;
  signatureVersion: string;
  reuseConnections: boolean;
  rejectUnauthorized: boolean;
  enableAssumeRole: boolean;
  assumeRoleArn?: string | undefined;
  assumeRoleExternalId?: string | undefined;
  durationSeconds: number;
  maxMessages: number;
  visibilityTimeout: number;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  pollTimeout: number;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  awsSecret?: string | undefined;
  numReceivers: number;
};

/** @internal */
export const InputSqsPqEnabledTrueWithPqConstraint$outboundSchema: z.ZodType<
  InputSqsPqEnabledTrueWithPqConstraint$Outbound,
  z.ZodTypeDef,
  InputSqsPqEnabledTrueWithPqConstraint
> = z.object({
  pqEnabled: z.boolean().default(false),
  pq: models.PqType$outboundSchema.optional(),
  id: z.string(),
  type: InputSqsType$outboundSchema,
  disabled: z.boolean().default(false),
  pipeline: z.string().optional(),
  sendToRoutes: z.boolean().default(true),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
  queueName: z.string(),
  queueType: CreateInputQueueType$outboundSchema,
  awsAccountId: z.string().optional(),
  createQueue: z.boolean().default(false),
  awsAuthenticationMethod: z.string().default("auto"),
  awsSecretKey: z.string().optional(),
  region: z.string().optional(),
  endpoint: z.string().optional(),
  signatureVersion: models.SignatureVersionOptions3$outboundSchema.default(
    "v4",
  ),
  reuseConnections: z.boolean().default(true),
  rejectUnauthorized: z.boolean().default(true),
  enableAssumeRole: z.boolean().default(false),
  assumeRoleArn: z.string().optional(),
  assumeRoleExternalId: z.string().optional(),
  durationSeconds: z.number().default(3600),
  maxMessages: z.number().default(10),
  visibilityTimeout: z.number().default(600),
  metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
    .optional(),
  pollTimeout: z.number().default(10),
  description: z.string().optional(),
  awsApiKey: z.string().optional(),
  awsSecret: z.string().optional(),
  numReceivers: z.number().default(3),
});

export function inputSqsPqEnabledTrueWithPqConstraintToJSON(
  inputSqsPqEnabledTrueWithPqConstraint: InputSqsPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputSqsPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputSqsPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputSqsPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  queueName: string;
  queueType: string;
  awsAccountId?: string | undefined;
  createQueue: boolean;
  awsAuthenticationMethod: string;
  awsSecretKey?: string | undefined;
  region?: string | undefined;
  endpoint?: string | undefined;
  signatureVersion: string;
  reuseConnections: boolean;
  rejectUnauthorized: boolean;
  enableAssumeRole: boolean;
  assumeRoleArn?: string | undefined;
  assumeRoleExternalId?: string | undefined;
  durationSeconds: number;
  maxMessages: number;
  visibilityTimeout: number;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  pollTimeout: number;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  awsSecret?: string | undefined;
  numReceivers: number;
};

/** @internal */
export const InputSqsPqEnabledFalseWithPqConstraint$outboundSchema: z.ZodType<
  InputSqsPqEnabledFalseWithPqConstraint$Outbound,
  z.ZodTypeDef,
  InputSqsPqEnabledFalseWithPqConstraint
> = z.object({
  pqEnabled: z.boolean().default(false),
  pq: models.PqType$outboundSchema.optional(),
  id: z.string(),
  type: InputSqsType$outboundSchema,
  disabled: z.boolean().default(false),
  pipeline: z.string().optional(),
  sendToRoutes: z.boolean().default(true),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
  queueName: z.string(),
  queueType: CreateInputQueueType$outboundSchema,
  awsAccountId: z.string().optional(),
  createQueue: z.boolean().default(false),
  awsAuthenticationMethod: z.string().default("auto"),
  awsSecretKey: z.string().optional(),
  region: z.string().optional(),
  endpoint: z.string().optional(),
  signatureVersion: models.SignatureVersionOptions3$outboundSchema.default(
    "v4",
  ),
  reuseConnections: z.boolean().default(true),
  rejectUnauthorized: z.boolean().default(true),
  enableAssumeRole: z.boolean().default(false),
  assumeRoleArn: z.string().optional(),
  assumeRoleExternalId: z.string().optional(),
  durationSeconds: z.number().default(3600),
  maxMessages: z.number().default(10),
  visibilityTimeout: z.number().default(600),
  metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
    .optional(),
  pollTimeout: z.number().default(10),
  description: z.string().optional(),
  awsApiKey: z.string().optional(),
  awsSecret: z.string().optional(),
  numReceivers: z.number().default(3),
});

export function inputSqsPqEnabledFalseWithPqConstraintToJSON(
  inputSqsPqEnabledFalseWithPqConstraint:
    InputSqsPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputSqsPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputSqsPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputSqsSendToRoutesFalseWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  queueName: string;
  queueType: string;
  awsAccountId?: string | undefined;
  createQueue: boolean;
  awsAuthenticationMethod: string;
  awsSecretKey?: string | undefined;
  region?: string | undefined;
  endpoint?: string | undefined;
  signatureVersion: string;
  reuseConnections: boolean;
  rejectUnauthorized: boolean;
  enableAssumeRole: boolean;
  assumeRoleArn?: string | undefined;
  assumeRoleExternalId?: string | undefined;
  durationSeconds: number;
  maxMessages: number;
  visibilityTimeout: number;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  pollTimeout: number;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  awsSecret?: string | undefined;
  numReceivers: number;
};

/** @internal */
export const InputSqsSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputSqsSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputSqsSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputSqsType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    queueName: z.string(),
    queueType: CreateInputQueueType$outboundSchema,
    awsAccountId: z.string().optional(),
    createQueue: z.boolean().default(false),
    awsAuthenticationMethod: z.string().default("auto"),
    awsSecretKey: z.string().optional(),
    region: z.string().optional(),
    endpoint: z.string().optional(),
    signatureVersion: models.SignatureVersionOptions3$outboundSchema.default(
      "v4",
    ),
    reuseConnections: z.boolean().default(true),
    rejectUnauthorized: z.boolean().default(true),
    enableAssumeRole: z.boolean().default(false),
    assumeRoleArn: z.string().optional(),
    assumeRoleExternalId: z.string().optional(),
    durationSeconds: z.number().default(3600),
    maxMessages: z.number().default(10),
    visibilityTimeout: z.number().default(600),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    pollTimeout: z.number().default(10),
    description: z.string().optional(),
    awsApiKey: z.string().optional(),
    awsSecret: z.string().optional(),
    numReceivers: z.number().default(3),
  });

export function inputSqsSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputSqsSendToRoutesFalseWithConnectionsConstraint:
    InputSqsSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputSqsSendToRoutesFalseWithConnectionsConstraint$outboundSchema.parse(
      inputSqsSendToRoutesFalseWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputSqsSendToRoutesTrueWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  queueName: string;
  queueType: string;
  awsAccountId?: string | undefined;
  createQueue: boolean;
  awsAuthenticationMethod: string;
  awsSecretKey?: string | undefined;
  region?: string | undefined;
  endpoint?: string | undefined;
  signatureVersion: string;
  reuseConnections: boolean;
  rejectUnauthorized: boolean;
  enableAssumeRole: boolean;
  assumeRoleArn?: string | undefined;
  assumeRoleExternalId?: string | undefined;
  durationSeconds: number;
  maxMessages: number;
  visibilityTimeout: number;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  pollTimeout: number;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  awsSecret?: string | undefined;
  numReceivers: number;
};

/** @internal */
export const InputSqsSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputSqsSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputSqsSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputSqsType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    queueName: z.string(),
    queueType: CreateInputQueueType$outboundSchema,
    awsAccountId: z.string().optional(),
    createQueue: z.boolean().default(false),
    awsAuthenticationMethod: z.string().default("auto"),
    awsSecretKey: z.string().optional(),
    region: z.string().optional(),
    endpoint: z.string().optional(),
    signatureVersion: models.SignatureVersionOptions3$outboundSchema.default(
      "v4",
    ),
    reuseConnections: z.boolean().default(true),
    rejectUnauthorized: z.boolean().default(true),
    enableAssumeRole: z.boolean().default(false),
    assumeRoleArn: z.string().optional(),
    assumeRoleExternalId: z.string().optional(),
    durationSeconds: z.number().default(3600),
    maxMessages: z.number().default(10),
    visibilityTimeout: z.number().default(600),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    pollTimeout: z.number().default(10),
    description: z.string().optional(),
    awsApiKey: z.string().optional(),
    awsSecret: z.string().optional(),
    numReceivers: z.number().default(3),
  });

export function inputSqsSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputSqsSendToRoutesTrueWithConnectionsConstraint:
    InputSqsSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputSqsSendToRoutesTrueWithConnectionsConstraint$outboundSchema.parse(
      inputSqsSendToRoutesTrueWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputSqs$Outbound =
  | InputSqsSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputSqsSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputSqsPqEnabledFalseWithPqConstraint$Outbound
  | InputSqsPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputSqs$outboundSchema: z.ZodType<
  InputSqs$Outbound,
  z.ZodTypeDef,
  InputSqs
> = z.union([
  z.lazy(() =>
    InputSqsSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputSqsSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputSqsPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputSqsPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputSqsToJSON(inputSqs: InputSqs): string {
  return JSON.stringify(InputSqs$outboundSchema.parse(inputSqs));
}

/** @internal */
export const InputModelDrivenTelemetryType$outboundSchema: z.ZodNativeEnum<
  typeof InputModelDrivenTelemetryType
> = z.nativeEnum(InputModelDrivenTelemetryType);

/** @internal */
export type InputModelDrivenTelemetryPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  maxActiveCxn: number;
  shutdownTimeoutMs: number;
  description?: string | undefined;
};

/** @internal */
export const InputModelDrivenTelemetryPqEnabledTrueWithPqConstraint$outboundSchema:
  z.ZodType<
    InputModelDrivenTelemetryPqEnabledTrueWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputModelDrivenTelemetryPqEnabledTrueWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputModelDrivenTelemetryType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number().default(57000),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    maxActiveCxn: z.number().default(1000),
    shutdownTimeoutMs: z.number().default(5000),
    description: z.string().optional(),
  });

export function inputModelDrivenTelemetryPqEnabledTrueWithPqConstraintToJSON(
  inputModelDrivenTelemetryPqEnabledTrueWithPqConstraint:
    InputModelDrivenTelemetryPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputModelDrivenTelemetryPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputModelDrivenTelemetryPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputModelDrivenTelemetryPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  maxActiveCxn: number;
  shutdownTimeoutMs: number;
  description?: string | undefined;
};

/** @internal */
export const InputModelDrivenTelemetryPqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputModelDrivenTelemetryPqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputModelDrivenTelemetryPqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputModelDrivenTelemetryType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number().default(57000),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    maxActiveCxn: z.number().default(1000),
    shutdownTimeoutMs: z.number().default(5000),
    description: z.string().optional(),
  });

export function inputModelDrivenTelemetryPqEnabledFalseWithPqConstraintToJSON(
  inputModelDrivenTelemetryPqEnabledFalseWithPqConstraint:
    InputModelDrivenTelemetryPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputModelDrivenTelemetryPqEnabledFalseWithPqConstraint$outboundSchema
      .parse(inputModelDrivenTelemetryPqEnabledFalseWithPqConstraint),
  );
}

/** @internal */
export type InputModelDrivenTelemetrySendToRoutesFalseWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    host: string;
    port: number;
    tls?: models.TlsSettingsServerSideType$Outbound | undefined;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    maxActiveCxn: number;
    shutdownTimeoutMs: number;
    description?: string | undefined;
  };

/** @internal */
export const InputModelDrivenTelemetrySendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputModelDrivenTelemetrySendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputModelDrivenTelemetrySendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputModelDrivenTelemetryType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number().default(57000),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    maxActiveCxn: z.number().default(1000),
    shutdownTimeoutMs: z.number().default(5000),
    description: z.string().optional(),
  });

export function inputModelDrivenTelemetrySendToRoutesFalseWithConnectionsConstraintToJSON(
  inputModelDrivenTelemetrySendToRoutesFalseWithConnectionsConstraint:
    InputModelDrivenTelemetrySendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputModelDrivenTelemetrySendToRoutesFalseWithConnectionsConstraint$outboundSchema
      .parse(
        inputModelDrivenTelemetrySendToRoutesFalseWithConnectionsConstraint,
      ),
  );
}

/** @internal */
export type InputModelDrivenTelemetrySendToRoutesTrueWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    host: string;
    port: number;
    tls?: models.TlsSettingsServerSideType$Outbound | undefined;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    maxActiveCxn: number;
    shutdownTimeoutMs: number;
    description?: string | undefined;
  };

/** @internal */
export const InputModelDrivenTelemetrySendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputModelDrivenTelemetrySendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputModelDrivenTelemetrySendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputModelDrivenTelemetryType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number().default(57000),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    maxActiveCxn: z.number().default(1000),
    shutdownTimeoutMs: z.number().default(5000),
    description: z.string().optional(),
  });

export function inputModelDrivenTelemetrySendToRoutesTrueWithConnectionsConstraintToJSON(
  inputModelDrivenTelemetrySendToRoutesTrueWithConnectionsConstraint:
    InputModelDrivenTelemetrySendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputModelDrivenTelemetrySendToRoutesTrueWithConnectionsConstraint$outboundSchema
      .parse(
        inputModelDrivenTelemetrySendToRoutesTrueWithConnectionsConstraint,
      ),
  );
}

/** @internal */
export type InputModelDrivenTelemetry$Outbound =
  | InputModelDrivenTelemetrySendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputModelDrivenTelemetrySendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputModelDrivenTelemetryPqEnabledFalseWithPqConstraint$Outbound
  | InputModelDrivenTelemetryPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputModelDrivenTelemetry$outboundSchema: z.ZodType<
  InputModelDrivenTelemetry$Outbound,
  z.ZodTypeDef,
  InputModelDrivenTelemetry
> = z.union([
  z.lazy(() =>
    InputModelDrivenTelemetrySendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputModelDrivenTelemetrySendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputModelDrivenTelemetryPqEnabledFalseWithPqConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputModelDrivenTelemetryPqEnabledTrueWithPqConstraint$outboundSchema
  ),
]);

export function inputModelDrivenTelemetryToJSON(
  inputModelDrivenTelemetry: InputModelDrivenTelemetry,
): string {
  return JSON.stringify(
    InputModelDrivenTelemetry$outboundSchema.parse(inputModelDrivenTelemetry),
  );
}

/** @internal */
export const InputOpenTelemetryType$outboundSchema: z.ZodNativeEnum<
  typeof InputOpenTelemetryType
> = z.nativeEnum(InputOpenTelemetryType);

/** @internal */
export const CreateInputProtocol$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  CreateInputProtocol
> = openEnums.outboundSchema(CreateInputProtocol);

/** @internal */
export const CreateInputOTLPVersion$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  CreateInputOTLPVersion
> = openEnums.outboundSchema(CreateInputOTLPVersion);

/** @internal */
export type InputOpenTelemetryPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader?: any | undefined;
  captureHeaders?: any | undefined;
  activityLogSampleRate?: any | undefined;
  requestTimeout: number;
  socketTimeout: number;
  keepAliveTimeout: number;
  enableHealthCheck: boolean;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  protocol: string;
  extractSpans: boolean;
  extractMetrics: boolean;
  otlpVersion: string;
  authType: string;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  maxActiveCxn: number;
  description?: string | undefined;
  username?: string | undefined;
  password?: string | undefined;
  token?: string | undefined;
  credentialsSecret?: string | undefined;
  textSecret?: string | undefined;
  loginUrl?: string | undefined;
  secretParamName?: string | undefined;
  secret?: string | undefined;
  tokenAttributeName?: string | undefined;
  authHeaderExpr: string;
  tokenTimeoutSecs: number;
  oauthParams?: Array<models.ItemsTypeOauthParams$Outbound> | undefined;
  oauthHeaders?: Array<models.ItemsTypeOauthHeaders$Outbound> | undefined;
  extractLogs: boolean;
};

/** @internal */
export const InputOpenTelemetryPqEnabledTrueWithPqConstraint$outboundSchema:
  z.ZodType<
    InputOpenTelemetryPqEnabledTrueWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputOpenTelemetryPqEnabledTrueWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputOpenTelemetryType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number().default(4317),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.any().optional(),
    captureHeaders: z.any().optional(),
    activityLogSampleRate: z.any().optional(),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(15),
    enableHealthCheck: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    protocol: CreateInputProtocol$outboundSchema.default("grpc"),
    extractSpans: z.boolean().default(false),
    extractMetrics: z.boolean().default(false),
    otlpVersion: CreateInputOTLPVersion$outboundSchema.default("0.10.0"),
    authType: models.AuthenticationTypeOptions$outboundSchema.default("none"),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    maxActiveCxn: z.number().default(1000),
    description: z.string().optional(),
    username: z.string().optional(),
    password: z.string().optional(),
    token: z.string().optional(),
    credentialsSecret: z.string().optional(),
    textSecret: z.string().optional(),
    loginUrl: z.string().optional(),
    secretParamName: z.string().optional(),
    secret: z.string().optional(),
    tokenAttributeName: z.string().optional(),
    authHeaderExpr: z.string().default("`Bearer ${token}`"),
    tokenTimeoutSecs: z.number().default(3600),
    oauthParams: z.array(models.ItemsTypeOauthParams$outboundSchema).optional(),
    oauthHeaders: z.array(models.ItemsTypeOauthHeaders$outboundSchema)
      .optional(),
    extractLogs: z.boolean().default(false),
  });

export function inputOpenTelemetryPqEnabledTrueWithPqConstraintToJSON(
  inputOpenTelemetryPqEnabledTrueWithPqConstraint:
    InputOpenTelemetryPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputOpenTelemetryPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputOpenTelemetryPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputOpenTelemetryPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader?: any | undefined;
  captureHeaders?: any | undefined;
  activityLogSampleRate?: any | undefined;
  requestTimeout: number;
  socketTimeout: number;
  keepAliveTimeout: number;
  enableHealthCheck: boolean;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  protocol: string;
  extractSpans: boolean;
  extractMetrics: boolean;
  otlpVersion: string;
  authType: string;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  maxActiveCxn: number;
  description?: string | undefined;
  username?: string | undefined;
  password?: string | undefined;
  token?: string | undefined;
  credentialsSecret?: string | undefined;
  textSecret?: string | undefined;
  loginUrl?: string | undefined;
  secretParamName?: string | undefined;
  secret?: string | undefined;
  tokenAttributeName?: string | undefined;
  authHeaderExpr: string;
  tokenTimeoutSecs: number;
  oauthParams?: Array<models.ItemsTypeOauthParams$Outbound> | undefined;
  oauthHeaders?: Array<models.ItemsTypeOauthHeaders$Outbound> | undefined;
  extractLogs: boolean;
};

/** @internal */
export const InputOpenTelemetryPqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputOpenTelemetryPqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputOpenTelemetryPqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputOpenTelemetryType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number().default(4317),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.any().optional(),
    captureHeaders: z.any().optional(),
    activityLogSampleRate: z.any().optional(),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(15),
    enableHealthCheck: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    protocol: CreateInputProtocol$outboundSchema.default("grpc"),
    extractSpans: z.boolean().default(false),
    extractMetrics: z.boolean().default(false),
    otlpVersion: CreateInputOTLPVersion$outboundSchema.default("0.10.0"),
    authType: models.AuthenticationTypeOptions$outboundSchema.default("none"),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    maxActiveCxn: z.number().default(1000),
    description: z.string().optional(),
    username: z.string().optional(),
    password: z.string().optional(),
    token: z.string().optional(),
    credentialsSecret: z.string().optional(),
    textSecret: z.string().optional(),
    loginUrl: z.string().optional(),
    secretParamName: z.string().optional(),
    secret: z.string().optional(),
    tokenAttributeName: z.string().optional(),
    authHeaderExpr: z.string().default("`Bearer ${token}`"),
    tokenTimeoutSecs: z.number().default(3600),
    oauthParams: z.array(models.ItemsTypeOauthParams$outboundSchema).optional(),
    oauthHeaders: z.array(models.ItemsTypeOauthHeaders$outboundSchema)
      .optional(),
    extractLogs: z.boolean().default(false),
  });

export function inputOpenTelemetryPqEnabledFalseWithPqConstraintToJSON(
  inputOpenTelemetryPqEnabledFalseWithPqConstraint:
    InputOpenTelemetryPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputOpenTelemetryPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputOpenTelemetryPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputOpenTelemetrySendToRoutesFalseWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    host: string;
    port: number;
    tls?: models.TlsSettingsServerSideType$Outbound | undefined;
    maxActiveReq: number;
    maxRequestsPerSocket: number;
    enableProxyHeader?: any | undefined;
    captureHeaders?: any | undefined;
    activityLogSampleRate?: any | undefined;
    requestTimeout: number;
    socketTimeout: number;
    keepAliveTimeout: number;
    enableHealthCheck: boolean;
    ipAllowlistRegex: string;
    ipDenylistRegex: string;
    protocol: string;
    extractSpans: boolean;
    extractMetrics: boolean;
    otlpVersion: string;
    authType: string;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    maxActiveCxn: number;
    description?: string | undefined;
    username?: string | undefined;
    password?: string | undefined;
    token?: string | undefined;
    credentialsSecret?: string | undefined;
    textSecret?: string | undefined;
    loginUrl?: string | undefined;
    secretParamName?: string | undefined;
    secret?: string | undefined;
    tokenAttributeName?: string | undefined;
    authHeaderExpr: string;
    tokenTimeoutSecs: number;
    oauthParams?: Array<models.ItemsTypeOauthParams$Outbound> | undefined;
    oauthHeaders?: Array<models.ItemsTypeOauthHeaders$Outbound> | undefined;
    extractLogs: boolean;
  };

/** @internal */
export const InputOpenTelemetrySendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputOpenTelemetrySendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputOpenTelemetrySendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputOpenTelemetryType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number().default(4317),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.any().optional(),
    captureHeaders: z.any().optional(),
    activityLogSampleRate: z.any().optional(),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(15),
    enableHealthCheck: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    protocol: CreateInputProtocol$outboundSchema.default("grpc"),
    extractSpans: z.boolean().default(false),
    extractMetrics: z.boolean().default(false),
    otlpVersion: CreateInputOTLPVersion$outboundSchema.default("0.10.0"),
    authType: models.AuthenticationTypeOptions$outboundSchema.default("none"),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    maxActiveCxn: z.number().default(1000),
    description: z.string().optional(),
    username: z.string().optional(),
    password: z.string().optional(),
    token: z.string().optional(),
    credentialsSecret: z.string().optional(),
    textSecret: z.string().optional(),
    loginUrl: z.string().optional(),
    secretParamName: z.string().optional(),
    secret: z.string().optional(),
    tokenAttributeName: z.string().optional(),
    authHeaderExpr: z.string().default("`Bearer ${token}`"),
    tokenTimeoutSecs: z.number().default(3600),
    oauthParams: z.array(models.ItemsTypeOauthParams$outboundSchema).optional(),
    oauthHeaders: z.array(models.ItemsTypeOauthHeaders$outboundSchema)
      .optional(),
    extractLogs: z.boolean().default(false),
  });

export function inputOpenTelemetrySendToRoutesFalseWithConnectionsConstraintToJSON(
  inputOpenTelemetrySendToRoutesFalseWithConnectionsConstraint:
    InputOpenTelemetrySendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputOpenTelemetrySendToRoutesFalseWithConnectionsConstraint$outboundSchema
      .parse(inputOpenTelemetrySendToRoutesFalseWithConnectionsConstraint),
  );
}

/** @internal */
export type InputOpenTelemetrySendToRoutesTrueWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    host: string;
    port: number;
    tls?: models.TlsSettingsServerSideType$Outbound | undefined;
    maxActiveReq: number;
    maxRequestsPerSocket: number;
    enableProxyHeader?: any | undefined;
    captureHeaders?: any | undefined;
    activityLogSampleRate?: any | undefined;
    requestTimeout: number;
    socketTimeout: number;
    keepAliveTimeout: number;
    enableHealthCheck: boolean;
    ipAllowlistRegex: string;
    ipDenylistRegex: string;
    protocol: string;
    extractSpans: boolean;
    extractMetrics: boolean;
    otlpVersion: string;
    authType: string;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    maxActiveCxn: number;
    description?: string | undefined;
    username?: string | undefined;
    password?: string | undefined;
    token?: string | undefined;
    credentialsSecret?: string | undefined;
    textSecret?: string | undefined;
    loginUrl?: string | undefined;
    secretParamName?: string | undefined;
    secret?: string | undefined;
    tokenAttributeName?: string | undefined;
    authHeaderExpr: string;
    tokenTimeoutSecs: number;
    oauthParams?: Array<models.ItemsTypeOauthParams$Outbound> | undefined;
    oauthHeaders?: Array<models.ItemsTypeOauthHeaders$Outbound> | undefined;
    extractLogs: boolean;
  };

/** @internal */
export const InputOpenTelemetrySendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputOpenTelemetrySendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputOpenTelemetrySendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputOpenTelemetryType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number().default(4317),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.any().optional(),
    captureHeaders: z.any().optional(),
    activityLogSampleRate: z.any().optional(),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(15),
    enableHealthCheck: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    protocol: CreateInputProtocol$outboundSchema.default("grpc"),
    extractSpans: z.boolean().default(false),
    extractMetrics: z.boolean().default(false),
    otlpVersion: CreateInputOTLPVersion$outboundSchema.default("0.10.0"),
    authType: models.AuthenticationTypeOptions$outboundSchema.default("none"),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    maxActiveCxn: z.number().default(1000),
    description: z.string().optional(),
    username: z.string().optional(),
    password: z.string().optional(),
    token: z.string().optional(),
    credentialsSecret: z.string().optional(),
    textSecret: z.string().optional(),
    loginUrl: z.string().optional(),
    secretParamName: z.string().optional(),
    secret: z.string().optional(),
    tokenAttributeName: z.string().optional(),
    authHeaderExpr: z.string().default("`Bearer ${token}`"),
    tokenTimeoutSecs: z.number().default(3600),
    oauthParams: z.array(models.ItemsTypeOauthParams$outboundSchema).optional(),
    oauthHeaders: z.array(models.ItemsTypeOauthHeaders$outboundSchema)
      .optional(),
    extractLogs: z.boolean().default(false),
  });

export function inputOpenTelemetrySendToRoutesTrueWithConnectionsConstraintToJSON(
  inputOpenTelemetrySendToRoutesTrueWithConnectionsConstraint:
    InputOpenTelemetrySendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputOpenTelemetrySendToRoutesTrueWithConnectionsConstraint$outboundSchema
      .parse(inputOpenTelemetrySendToRoutesTrueWithConnectionsConstraint),
  );
}

/** @internal */
export type InputOpenTelemetry$Outbound =
  | InputOpenTelemetrySendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputOpenTelemetrySendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputOpenTelemetryPqEnabledFalseWithPqConstraint$Outbound
  | InputOpenTelemetryPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputOpenTelemetry$outboundSchema: z.ZodType<
  InputOpenTelemetry$Outbound,
  z.ZodTypeDef,
  InputOpenTelemetry
> = z.union([
  z.lazy(() =>
    InputOpenTelemetrySendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputOpenTelemetrySendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputOpenTelemetryPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputOpenTelemetryPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputOpenTelemetryToJSON(
  inputOpenTelemetry: InputOpenTelemetry,
): string {
  return JSON.stringify(
    InputOpenTelemetry$outboundSchema.parse(inputOpenTelemetry),
  );
}

/** @internal */
export const InputSnmpType$outboundSchema: z.ZodNativeEnum<
  typeof InputSnmpType
> = z.nativeEnum(InputSnmpType);

/** @internal */
export const PrivacyProtocol$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  PrivacyProtocol
> = openEnums.outboundSchema(PrivacyProtocol);

/** @internal */
export type V3User$Outbound = {
  name: string;
  authProtocol: string;
  authKey?: string | undefined;
  privProtocol: string;
  privKey?: string | undefined;
};

/** @internal */
export const V3User$outboundSchema: z.ZodType<
  V3User$Outbound,
  z.ZodTypeDef,
  V3User
> = z.object({
  name: z.string(),
  authProtocol: models.AuthenticationProtocolOptionsV3User$outboundSchema
    .default("none"),
  authKey: z.string().optional(),
  privProtocol: PrivacyProtocol$outboundSchema.default("none"),
  privKey: z.string().optional(),
});

export function v3UserToJSON(v3User: V3User): string {
  return JSON.stringify(V3User$outboundSchema.parse(v3User));
}

/** @internal */
export type SNMPv3Authentication$Outbound = {
  v3AuthEnabled: boolean;
  allowUnmatchedTrap: boolean;
  v3Users?: Array<V3User$Outbound> | undefined;
};

/** @internal */
export const SNMPv3Authentication$outboundSchema: z.ZodType<
  SNMPv3Authentication$Outbound,
  z.ZodTypeDef,
  SNMPv3Authentication
> = z.object({
  v3AuthEnabled: z.boolean().default(false),
  allowUnmatchedTrap: z.boolean().default(false),
  v3Users: z.array(z.lazy(() => V3User$outboundSchema)).optional(),
});

export function snmPv3AuthenticationToJSON(
  snmPv3Authentication: SNMPv3Authentication,
): string {
  return JSON.stringify(
    SNMPv3Authentication$outboundSchema.parse(snmPv3Authentication),
  );
}

/** @internal */
export type InputSnmpPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  snmpV3Auth?: SNMPv3Authentication$Outbound | undefined;
  maxBufferSize: number;
  ipWhitelistRegex: string;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  udpSocketRxBufSize?: number | undefined;
  varbindsWithTypes: boolean;
  bestEffortParsing: boolean;
  description?: string | undefined;
};

/** @internal */
export const InputSnmpPqEnabledTrueWithPqConstraint$outboundSchema: z.ZodType<
  InputSnmpPqEnabledTrueWithPqConstraint$Outbound,
  z.ZodTypeDef,
  InputSnmpPqEnabledTrueWithPqConstraint
> = z.object({
  pqEnabled: z.boolean().default(false),
  pq: models.PqType$outboundSchema.optional(),
  id: z.string(),
  type: InputSnmpType$outboundSchema,
  disabled: z.boolean().default(false),
  pipeline: z.string().optional(),
  sendToRoutes: z.boolean().default(true),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
  host: z.string().default("0.0.0.0"),
  port: z.number().default(162),
  snmpV3Auth: z.lazy(() => SNMPv3Authentication$outboundSchema).optional(),
  maxBufferSize: z.number().default(1000),
  ipWhitelistRegex: z.string().default("/.*/"),
  metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
    .optional(),
  udpSocketRxBufSize: z.number().optional(),
  varbindsWithTypes: z.boolean().default(false),
  bestEffortParsing: z.boolean().default(false),
  description: z.string().optional(),
});

export function inputSnmpPqEnabledTrueWithPqConstraintToJSON(
  inputSnmpPqEnabledTrueWithPqConstraint:
    InputSnmpPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputSnmpPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputSnmpPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputSnmpPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  snmpV3Auth?: SNMPv3Authentication$Outbound | undefined;
  maxBufferSize: number;
  ipWhitelistRegex: string;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  udpSocketRxBufSize?: number | undefined;
  varbindsWithTypes: boolean;
  bestEffortParsing: boolean;
  description?: string | undefined;
};

/** @internal */
export const InputSnmpPqEnabledFalseWithPqConstraint$outboundSchema: z.ZodType<
  InputSnmpPqEnabledFalseWithPqConstraint$Outbound,
  z.ZodTypeDef,
  InputSnmpPqEnabledFalseWithPqConstraint
> = z.object({
  pqEnabled: z.boolean().default(false),
  pq: models.PqType$outboundSchema.optional(),
  id: z.string(),
  type: InputSnmpType$outboundSchema,
  disabled: z.boolean().default(false),
  pipeline: z.string().optional(),
  sendToRoutes: z.boolean().default(true),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
  host: z.string().default("0.0.0.0"),
  port: z.number().default(162),
  snmpV3Auth: z.lazy(() => SNMPv3Authentication$outboundSchema).optional(),
  maxBufferSize: z.number().default(1000),
  ipWhitelistRegex: z.string().default("/.*/"),
  metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
    .optional(),
  udpSocketRxBufSize: z.number().optional(),
  varbindsWithTypes: z.boolean().default(false),
  bestEffortParsing: z.boolean().default(false),
  description: z.string().optional(),
});

export function inputSnmpPqEnabledFalseWithPqConstraintToJSON(
  inputSnmpPqEnabledFalseWithPqConstraint:
    InputSnmpPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputSnmpPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputSnmpPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputSnmpSendToRoutesFalseWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  host: string;
  port: number;
  snmpV3Auth?: SNMPv3Authentication$Outbound | undefined;
  maxBufferSize: number;
  ipWhitelistRegex: string;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  udpSocketRxBufSize?: number | undefined;
  varbindsWithTypes: boolean;
  bestEffortParsing: boolean;
  description?: string | undefined;
};

/** @internal */
export const InputSnmpSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputSnmpSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputSnmpSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputSnmpType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number().default(162),
    snmpV3Auth: z.lazy(() => SNMPv3Authentication$outboundSchema).optional(),
    maxBufferSize: z.number().default(1000),
    ipWhitelistRegex: z.string().default("/.*/"),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    udpSocketRxBufSize: z.number().optional(),
    varbindsWithTypes: z.boolean().default(false),
    bestEffortParsing: z.boolean().default(false),
    description: z.string().optional(),
  });

export function inputSnmpSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputSnmpSendToRoutesFalseWithConnectionsConstraint:
    InputSnmpSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputSnmpSendToRoutesFalseWithConnectionsConstraint$outboundSchema.parse(
      inputSnmpSendToRoutesFalseWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputSnmpSendToRoutesTrueWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  host: string;
  port: number;
  snmpV3Auth?: SNMPv3Authentication$Outbound | undefined;
  maxBufferSize: number;
  ipWhitelistRegex: string;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  udpSocketRxBufSize?: number | undefined;
  varbindsWithTypes: boolean;
  bestEffortParsing: boolean;
  description?: string | undefined;
};

/** @internal */
export const InputSnmpSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputSnmpSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputSnmpSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputSnmpType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number().default(162),
    snmpV3Auth: z.lazy(() => SNMPv3Authentication$outboundSchema).optional(),
    maxBufferSize: z.number().default(1000),
    ipWhitelistRegex: z.string().default("/.*/"),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    udpSocketRxBufSize: z.number().optional(),
    varbindsWithTypes: z.boolean().default(false),
    bestEffortParsing: z.boolean().default(false),
    description: z.string().optional(),
  });

export function inputSnmpSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputSnmpSendToRoutesTrueWithConnectionsConstraint:
    InputSnmpSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputSnmpSendToRoutesTrueWithConnectionsConstraint$outboundSchema.parse(
      inputSnmpSendToRoutesTrueWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputSnmp$Outbound =
  | InputSnmpSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputSnmpSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputSnmpPqEnabledFalseWithPqConstraint$Outbound
  | InputSnmpPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputSnmp$outboundSchema: z.ZodType<
  InputSnmp$Outbound,
  z.ZodTypeDef,
  InputSnmp
> = z.union([
  z.lazy(() =>
    InputSnmpSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputSnmpSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputSnmpPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputSnmpPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputSnmpToJSON(inputSnmp: InputSnmp): string {
  return JSON.stringify(InputSnmp$outboundSchema.parse(inputSnmp));
}

/** @internal */
export const InputS3InventoryType$outboundSchema: z.ZodNativeEnum<
  typeof InputS3InventoryType
> = z.nativeEnum(InputS3InventoryType);

/** @internal */
export type InputS3InventoryPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  queueName: string;
  fileFilter: string;
  awsAccountId?: string | undefined;
  awsAuthenticationMethod: string;
  awsSecretKey?: string | undefined;
  region?: string | undefined;
  endpoint?: string | undefined;
  signatureVersion: string;
  reuseConnections: boolean;
  rejectUnauthorized: boolean;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  maxMessages: number;
  visibilityTimeout: number;
  numReceivers: number;
  socketTimeout: number;
  skipOnError: boolean;
  includeSqsMetadata: boolean;
  enableAssumeRole: boolean;
  assumeRoleArn?: string | undefined;
  assumeRoleExternalId?: string | undefined;
  durationSeconds: number;
  enableSQSAssumeRole: boolean;
  preprocess?:
    | models.PreprocessTypeSavedJobCollectionInput$Outbound
    | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  parquetChunkSizeMB: number;
  parquetChunkDownloadTimeout: number;
  checkpointing?: models.CheckpointingType$Outbound | undefined;
  pollTimeout: number;
  checksumSuffix: string;
  maxManifestSizeKB: number;
  validateInventoryFiles: boolean;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  awsSecret?: string | undefined;
  tagAfterProcessing?: string | undefined;
  processedTagKey?: string | undefined;
  processedTagValue?: string | undefined;
};

/** @internal */
export const InputS3InventoryPqEnabledTrueWithPqConstraint$outboundSchema:
  z.ZodType<
    InputS3InventoryPqEnabledTrueWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputS3InventoryPqEnabledTrueWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputS3InventoryType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    queueName: z.string(),
    fileFilter: z.string().default("/.*/"),
    awsAccountId: z.string().optional(),
    awsAuthenticationMethod: z.string().default("auto"),
    awsSecretKey: z.string().optional(),
    region: z.string().optional(),
    endpoint: z.string().optional(),
    signatureVersion: models
      .SignatureVersionOptionsS3CollectorConf$outboundSchema.default("v4"),
    reuseConnections: z.boolean().default(true),
    rejectUnauthorized: z.boolean().default(true),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    maxMessages: z.number().default(1),
    visibilityTimeout: z.number().default(600),
    numReceivers: z.number().default(1),
    socketTimeout: z.number().default(300),
    skipOnError: z.boolean().default(false),
    includeSqsMetadata: z.boolean().default(false),
    enableAssumeRole: z.boolean().default(true),
    assumeRoleArn: z.string().optional(),
    assumeRoleExternalId: z.string().optional(),
    durationSeconds: z.number().default(3600),
    enableSQSAssumeRole: z.boolean().default(false),
    preprocess: models.PreprocessTypeSavedJobCollectionInput$outboundSchema
      .optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    parquetChunkSizeMB: z.number().default(5),
    parquetChunkDownloadTimeout: z.number().default(600),
    checkpointing: models.CheckpointingType$outboundSchema.optional(),
    pollTimeout: z.number().default(10),
    checksumSuffix: z.string().default("checksum"),
    maxManifestSizeKB: z.number().int().default(4096),
    validateInventoryFiles: z.boolean().default(false),
    description: z.string().optional(),
    awsApiKey: z.string().optional(),
    awsSecret: z.string().optional(),
    tagAfterProcessing: models.TagAfterProcessingOptions$outboundSchema
      .optional(),
    processedTagKey: z.string().optional(),
    processedTagValue: z.string().optional(),
  });

export function inputS3InventoryPqEnabledTrueWithPqConstraintToJSON(
  inputS3InventoryPqEnabledTrueWithPqConstraint:
    InputS3InventoryPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputS3InventoryPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputS3InventoryPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputS3InventoryPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  queueName: string;
  fileFilter: string;
  awsAccountId?: string | undefined;
  awsAuthenticationMethod: string;
  awsSecretKey?: string | undefined;
  region?: string | undefined;
  endpoint?: string | undefined;
  signatureVersion: string;
  reuseConnections: boolean;
  rejectUnauthorized: boolean;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  maxMessages: number;
  visibilityTimeout: number;
  numReceivers: number;
  socketTimeout: number;
  skipOnError: boolean;
  includeSqsMetadata: boolean;
  enableAssumeRole: boolean;
  assumeRoleArn?: string | undefined;
  assumeRoleExternalId?: string | undefined;
  durationSeconds: number;
  enableSQSAssumeRole: boolean;
  preprocess?:
    | models.PreprocessTypeSavedJobCollectionInput$Outbound
    | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  parquetChunkSizeMB: number;
  parquetChunkDownloadTimeout: number;
  checkpointing?: models.CheckpointingType$Outbound | undefined;
  pollTimeout: number;
  checksumSuffix: string;
  maxManifestSizeKB: number;
  validateInventoryFiles: boolean;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  awsSecret?: string | undefined;
  tagAfterProcessing?: string | undefined;
  processedTagKey?: string | undefined;
  processedTagValue?: string | undefined;
};

/** @internal */
export const InputS3InventoryPqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputS3InventoryPqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputS3InventoryPqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputS3InventoryType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    queueName: z.string(),
    fileFilter: z.string().default("/.*/"),
    awsAccountId: z.string().optional(),
    awsAuthenticationMethod: z.string().default("auto"),
    awsSecretKey: z.string().optional(),
    region: z.string().optional(),
    endpoint: z.string().optional(),
    signatureVersion: models
      .SignatureVersionOptionsS3CollectorConf$outboundSchema.default("v4"),
    reuseConnections: z.boolean().default(true),
    rejectUnauthorized: z.boolean().default(true),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    maxMessages: z.number().default(1),
    visibilityTimeout: z.number().default(600),
    numReceivers: z.number().default(1),
    socketTimeout: z.number().default(300),
    skipOnError: z.boolean().default(false),
    includeSqsMetadata: z.boolean().default(false),
    enableAssumeRole: z.boolean().default(true),
    assumeRoleArn: z.string().optional(),
    assumeRoleExternalId: z.string().optional(),
    durationSeconds: z.number().default(3600),
    enableSQSAssumeRole: z.boolean().default(false),
    preprocess: models.PreprocessTypeSavedJobCollectionInput$outboundSchema
      .optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    parquetChunkSizeMB: z.number().default(5),
    parquetChunkDownloadTimeout: z.number().default(600),
    checkpointing: models.CheckpointingType$outboundSchema.optional(),
    pollTimeout: z.number().default(10),
    checksumSuffix: z.string().default("checksum"),
    maxManifestSizeKB: z.number().int().default(4096),
    validateInventoryFiles: z.boolean().default(false),
    description: z.string().optional(),
    awsApiKey: z.string().optional(),
    awsSecret: z.string().optional(),
    tagAfterProcessing: models.TagAfterProcessingOptions$outboundSchema
      .optional(),
    processedTagKey: z.string().optional(),
    processedTagValue: z.string().optional(),
  });

export function inputS3InventoryPqEnabledFalseWithPqConstraintToJSON(
  inputS3InventoryPqEnabledFalseWithPqConstraint:
    InputS3InventoryPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputS3InventoryPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputS3InventoryPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputS3InventorySendToRoutesFalseWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    queueName: string;
    fileFilter: string;
    awsAccountId?: string | undefined;
    awsAuthenticationMethod: string;
    awsSecretKey?: string | undefined;
    region?: string | undefined;
    endpoint?: string | undefined;
    signatureVersion: string;
    reuseConnections: boolean;
    rejectUnauthorized: boolean;
    breakerRulesets?: Array<string> | undefined;
    staleChannelFlushMs: number;
    maxMessages: number;
    visibilityTimeout: number;
    numReceivers: number;
    socketTimeout: number;
    skipOnError: boolean;
    includeSqsMetadata: boolean;
    enableAssumeRole: boolean;
    assumeRoleArn?: string | undefined;
    assumeRoleExternalId?: string | undefined;
    durationSeconds: number;
    enableSQSAssumeRole: boolean;
    preprocess?:
      | models.PreprocessTypeSavedJobCollectionInput$Outbound
      | undefined;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    parquetChunkSizeMB: number;
    parquetChunkDownloadTimeout: number;
    checkpointing?: models.CheckpointingType$Outbound | undefined;
    pollTimeout: number;
    checksumSuffix: string;
    maxManifestSizeKB: number;
    validateInventoryFiles: boolean;
    description?: string | undefined;
    awsApiKey?: string | undefined;
    awsSecret?: string | undefined;
    tagAfterProcessing?: string | undefined;
    processedTagKey?: string | undefined;
    processedTagValue?: string | undefined;
  };

/** @internal */
export const InputS3InventorySendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputS3InventorySendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputS3InventorySendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputS3InventoryType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    queueName: z.string(),
    fileFilter: z.string().default("/.*/"),
    awsAccountId: z.string().optional(),
    awsAuthenticationMethod: z.string().default("auto"),
    awsSecretKey: z.string().optional(),
    region: z.string().optional(),
    endpoint: z.string().optional(),
    signatureVersion: models
      .SignatureVersionOptionsS3CollectorConf$outboundSchema.default("v4"),
    reuseConnections: z.boolean().default(true),
    rejectUnauthorized: z.boolean().default(true),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    maxMessages: z.number().default(1),
    visibilityTimeout: z.number().default(600),
    numReceivers: z.number().default(1),
    socketTimeout: z.number().default(300),
    skipOnError: z.boolean().default(false),
    includeSqsMetadata: z.boolean().default(false),
    enableAssumeRole: z.boolean().default(true),
    assumeRoleArn: z.string().optional(),
    assumeRoleExternalId: z.string().optional(),
    durationSeconds: z.number().default(3600),
    enableSQSAssumeRole: z.boolean().default(false),
    preprocess: models.PreprocessTypeSavedJobCollectionInput$outboundSchema
      .optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    parquetChunkSizeMB: z.number().default(5),
    parquetChunkDownloadTimeout: z.number().default(600),
    checkpointing: models.CheckpointingType$outboundSchema.optional(),
    pollTimeout: z.number().default(10),
    checksumSuffix: z.string().default("checksum"),
    maxManifestSizeKB: z.number().int().default(4096),
    validateInventoryFiles: z.boolean().default(false),
    description: z.string().optional(),
    awsApiKey: z.string().optional(),
    awsSecret: z.string().optional(),
    tagAfterProcessing: models.TagAfterProcessingOptions$outboundSchema
      .optional(),
    processedTagKey: z.string().optional(),
    processedTagValue: z.string().optional(),
  });

export function inputS3InventorySendToRoutesFalseWithConnectionsConstraintToJSON(
  inputS3InventorySendToRoutesFalseWithConnectionsConstraint:
    InputS3InventorySendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputS3InventorySendToRoutesFalseWithConnectionsConstraint$outboundSchema
      .parse(inputS3InventorySendToRoutesFalseWithConnectionsConstraint),
  );
}

/** @internal */
export type InputS3InventorySendToRoutesTrueWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    queueName: string;
    fileFilter: string;
    awsAccountId?: string | undefined;
    awsAuthenticationMethod: string;
    awsSecretKey?: string | undefined;
    region?: string | undefined;
    endpoint?: string | undefined;
    signatureVersion: string;
    reuseConnections: boolean;
    rejectUnauthorized: boolean;
    breakerRulesets?: Array<string> | undefined;
    staleChannelFlushMs: number;
    maxMessages: number;
    visibilityTimeout: number;
    numReceivers: number;
    socketTimeout: number;
    skipOnError: boolean;
    includeSqsMetadata: boolean;
    enableAssumeRole: boolean;
    assumeRoleArn?: string | undefined;
    assumeRoleExternalId?: string | undefined;
    durationSeconds: number;
    enableSQSAssumeRole: boolean;
    preprocess?:
      | models.PreprocessTypeSavedJobCollectionInput$Outbound
      | undefined;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    parquetChunkSizeMB: number;
    parquetChunkDownloadTimeout: number;
    checkpointing?: models.CheckpointingType$Outbound | undefined;
    pollTimeout: number;
    checksumSuffix: string;
    maxManifestSizeKB: number;
    validateInventoryFiles: boolean;
    description?: string | undefined;
    awsApiKey?: string | undefined;
    awsSecret?: string | undefined;
    tagAfterProcessing?: string | undefined;
    processedTagKey?: string | undefined;
    processedTagValue?: string | undefined;
  };

/** @internal */
export const InputS3InventorySendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputS3InventorySendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputS3InventorySendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputS3InventoryType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    queueName: z.string(),
    fileFilter: z.string().default("/.*/"),
    awsAccountId: z.string().optional(),
    awsAuthenticationMethod: z.string().default("auto"),
    awsSecretKey: z.string().optional(),
    region: z.string().optional(),
    endpoint: z.string().optional(),
    signatureVersion: models
      .SignatureVersionOptionsS3CollectorConf$outboundSchema.default("v4"),
    reuseConnections: z.boolean().default(true),
    rejectUnauthorized: z.boolean().default(true),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    maxMessages: z.number().default(1),
    visibilityTimeout: z.number().default(600),
    numReceivers: z.number().default(1),
    socketTimeout: z.number().default(300),
    skipOnError: z.boolean().default(false),
    includeSqsMetadata: z.boolean().default(false),
    enableAssumeRole: z.boolean().default(true),
    assumeRoleArn: z.string().optional(),
    assumeRoleExternalId: z.string().optional(),
    durationSeconds: z.number().default(3600),
    enableSQSAssumeRole: z.boolean().default(false),
    preprocess: models.PreprocessTypeSavedJobCollectionInput$outboundSchema
      .optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    parquetChunkSizeMB: z.number().default(5),
    parquetChunkDownloadTimeout: z.number().default(600),
    checkpointing: models.CheckpointingType$outboundSchema.optional(),
    pollTimeout: z.number().default(10),
    checksumSuffix: z.string().default("checksum"),
    maxManifestSizeKB: z.number().int().default(4096),
    validateInventoryFiles: z.boolean().default(false),
    description: z.string().optional(),
    awsApiKey: z.string().optional(),
    awsSecret: z.string().optional(),
    tagAfterProcessing: models.TagAfterProcessingOptions$outboundSchema
      .optional(),
    processedTagKey: z.string().optional(),
    processedTagValue: z.string().optional(),
  });

export function inputS3InventorySendToRoutesTrueWithConnectionsConstraintToJSON(
  inputS3InventorySendToRoutesTrueWithConnectionsConstraint:
    InputS3InventorySendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputS3InventorySendToRoutesTrueWithConnectionsConstraint$outboundSchema
      .parse(inputS3InventorySendToRoutesTrueWithConnectionsConstraint),
  );
}

/** @internal */
export type InputS3Inventory$Outbound =
  | InputS3InventorySendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputS3InventorySendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputS3InventoryPqEnabledFalseWithPqConstraint$Outbound
  | InputS3InventoryPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputS3Inventory$outboundSchema: z.ZodType<
  InputS3Inventory$Outbound,
  z.ZodTypeDef,
  InputS3Inventory
> = z.union([
  z.lazy(() =>
    InputS3InventorySendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputS3InventorySendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputS3InventoryPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputS3InventoryPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputS3InventoryToJSON(
  inputS3Inventory: InputS3Inventory,
): string {
  return JSON.stringify(
    InputS3Inventory$outboundSchema.parse(inputS3Inventory),
  );
}

/** @internal */
export const InputS3Type$outboundSchema: z.ZodNativeEnum<typeof InputS3Type> = z
  .nativeEnum(InputS3Type);

/** @internal */
export type InputS3PqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  queueName: string;
  fileFilter: string;
  awsAccountId?: string | undefined;
  awsAuthenticationMethod: string;
  awsSecretKey?: string | undefined;
  region?: string | undefined;
  endpoint?: string | undefined;
  signatureVersion: string;
  reuseConnections: boolean;
  rejectUnauthorized: boolean;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  maxMessages: number;
  visibilityTimeout: number;
  numReceivers: number;
  socketTimeout: number;
  skipOnError: boolean;
  includeSqsMetadata: boolean;
  enableAssumeRole: boolean;
  assumeRoleArn?: string | undefined;
  assumeRoleExternalId?: string | undefined;
  durationSeconds: number;
  enableSQSAssumeRole: boolean;
  preprocess?:
    | models.PreprocessTypeSavedJobCollectionInput$Outbound
    | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  parquetChunkSizeMB: number;
  parquetChunkDownloadTimeout: number;
  checkpointing?: models.CheckpointingType$Outbound | undefined;
  pollTimeout: number;
  encoding?: string | undefined;
  tagAfterProcessing: boolean;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  awsSecret?: string | undefined;
  processedTagKey?: string | undefined;
  processedTagValue?: string | undefined;
};

/** @internal */
export const InputS3PqEnabledTrueWithPqConstraint$outboundSchema: z.ZodType<
  InputS3PqEnabledTrueWithPqConstraint$Outbound,
  z.ZodTypeDef,
  InputS3PqEnabledTrueWithPqConstraint
> = z.object({
  pqEnabled: z.boolean().default(false),
  pq: models.PqType$outboundSchema.optional(),
  id: z.string(),
  type: InputS3Type$outboundSchema,
  disabled: z.boolean().default(false),
  pipeline: z.string().optional(),
  sendToRoutes: z.boolean().default(true),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
  queueName: z.string(),
  fileFilter: z.string().default("/.*/"),
  awsAccountId: z.string().optional(),
  awsAuthenticationMethod: z.string().default("auto"),
  awsSecretKey: z.string().optional(),
  region: z.string().optional(),
  endpoint: z.string().optional(),
  signatureVersion: models.SignatureVersionOptionsS3CollectorConf$outboundSchema
    .default("v4"),
  reuseConnections: z.boolean().default(true),
  rejectUnauthorized: z.boolean().default(true),
  breakerRulesets: z.array(z.string()).optional(),
  staleChannelFlushMs: z.number().default(10000),
  maxMessages: z.number().default(1),
  visibilityTimeout: z.number().default(600),
  numReceivers: z.number().default(1),
  socketTimeout: z.number().default(300),
  skipOnError: z.boolean().default(false),
  includeSqsMetadata: z.boolean().default(false),
  enableAssumeRole: z.boolean().default(true),
  assumeRoleArn: z.string().optional(),
  assumeRoleExternalId: z.string().optional(),
  durationSeconds: z.number().default(3600),
  enableSQSAssumeRole: z.boolean().default(false),
  preprocess: models.PreprocessTypeSavedJobCollectionInput$outboundSchema
    .optional(),
  metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
    .optional(),
  parquetChunkSizeMB: z.number().default(5),
  parquetChunkDownloadTimeout: z.number().default(600),
  checkpointing: models.CheckpointingType$outboundSchema.optional(),
  pollTimeout: z.number().default(10),
  encoding: z.string().optional(),
  tagAfterProcessing: z.boolean().default(false),
  description: z.string().optional(),
  awsApiKey: z.string().optional(),
  awsSecret: z.string().optional(),
  processedTagKey: z.string().optional(),
  processedTagValue: z.string().optional(),
});

export function inputS3PqEnabledTrueWithPqConstraintToJSON(
  inputS3PqEnabledTrueWithPqConstraint: InputS3PqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputS3PqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputS3PqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputS3PqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  queueName: string;
  fileFilter: string;
  awsAccountId?: string | undefined;
  awsAuthenticationMethod: string;
  awsSecretKey?: string | undefined;
  region?: string | undefined;
  endpoint?: string | undefined;
  signatureVersion: string;
  reuseConnections: boolean;
  rejectUnauthorized: boolean;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  maxMessages: number;
  visibilityTimeout: number;
  numReceivers: number;
  socketTimeout: number;
  skipOnError: boolean;
  includeSqsMetadata: boolean;
  enableAssumeRole: boolean;
  assumeRoleArn?: string | undefined;
  assumeRoleExternalId?: string | undefined;
  durationSeconds: number;
  enableSQSAssumeRole: boolean;
  preprocess?:
    | models.PreprocessTypeSavedJobCollectionInput$Outbound
    | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  parquetChunkSizeMB: number;
  parquetChunkDownloadTimeout: number;
  checkpointing?: models.CheckpointingType$Outbound | undefined;
  pollTimeout: number;
  encoding?: string | undefined;
  tagAfterProcessing: boolean;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  awsSecret?: string | undefined;
  processedTagKey?: string | undefined;
  processedTagValue?: string | undefined;
};

/** @internal */
export const InputS3PqEnabledFalseWithPqConstraint$outboundSchema: z.ZodType<
  InputS3PqEnabledFalseWithPqConstraint$Outbound,
  z.ZodTypeDef,
  InputS3PqEnabledFalseWithPqConstraint
> = z.object({
  pqEnabled: z.boolean().default(false),
  pq: models.PqType$outboundSchema.optional(),
  id: z.string(),
  type: InputS3Type$outboundSchema,
  disabled: z.boolean().default(false),
  pipeline: z.string().optional(),
  sendToRoutes: z.boolean().default(true),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
  queueName: z.string(),
  fileFilter: z.string().default("/.*/"),
  awsAccountId: z.string().optional(),
  awsAuthenticationMethod: z.string().default("auto"),
  awsSecretKey: z.string().optional(),
  region: z.string().optional(),
  endpoint: z.string().optional(),
  signatureVersion: models.SignatureVersionOptionsS3CollectorConf$outboundSchema
    .default("v4"),
  reuseConnections: z.boolean().default(true),
  rejectUnauthorized: z.boolean().default(true),
  breakerRulesets: z.array(z.string()).optional(),
  staleChannelFlushMs: z.number().default(10000),
  maxMessages: z.number().default(1),
  visibilityTimeout: z.number().default(600),
  numReceivers: z.number().default(1),
  socketTimeout: z.number().default(300),
  skipOnError: z.boolean().default(false),
  includeSqsMetadata: z.boolean().default(false),
  enableAssumeRole: z.boolean().default(true),
  assumeRoleArn: z.string().optional(),
  assumeRoleExternalId: z.string().optional(),
  durationSeconds: z.number().default(3600),
  enableSQSAssumeRole: z.boolean().default(false),
  preprocess: models.PreprocessTypeSavedJobCollectionInput$outboundSchema
    .optional(),
  metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
    .optional(),
  parquetChunkSizeMB: z.number().default(5),
  parquetChunkDownloadTimeout: z.number().default(600),
  checkpointing: models.CheckpointingType$outboundSchema.optional(),
  pollTimeout: z.number().default(10),
  encoding: z.string().optional(),
  tagAfterProcessing: z.boolean().default(false),
  description: z.string().optional(),
  awsApiKey: z.string().optional(),
  awsSecret: z.string().optional(),
  processedTagKey: z.string().optional(),
  processedTagValue: z.string().optional(),
});

export function inputS3PqEnabledFalseWithPqConstraintToJSON(
  inputS3PqEnabledFalseWithPqConstraint: InputS3PqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputS3PqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputS3PqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputS3SendToRoutesFalseWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  queueName: string;
  fileFilter: string;
  awsAccountId?: string | undefined;
  awsAuthenticationMethod: string;
  awsSecretKey?: string | undefined;
  region?: string | undefined;
  endpoint?: string | undefined;
  signatureVersion: string;
  reuseConnections: boolean;
  rejectUnauthorized: boolean;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  maxMessages: number;
  visibilityTimeout: number;
  numReceivers: number;
  socketTimeout: number;
  skipOnError: boolean;
  includeSqsMetadata: boolean;
  enableAssumeRole: boolean;
  assumeRoleArn?: string | undefined;
  assumeRoleExternalId?: string | undefined;
  durationSeconds: number;
  enableSQSAssumeRole: boolean;
  preprocess?:
    | models.PreprocessTypeSavedJobCollectionInput$Outbound
    | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  parquetChunkSizeMB: number;
  parquetChunkDownloadTimeout: number;
  checkpointing?: models.CheckpointingType$Outbound | undefined;
  pollTimeout: number;
  encoding?: string | undefined;
  tagAfterProcessing: boolean;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  awsSecret?: string | undefined;
  processedTagKey?: string | undefined;
  processedTagValue?: string | undefined;
};

/** @internal */
export const InputS3SendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputS3SendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputS3SendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputS3Type$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    queueName: z.string(),
    fileFilter: z.string().default("/.*/"),
    awsAccountId: z.string().optional(),
    awsAuthenticationMethod: z.string().default("auto"),
    awsSecretKey: z.string().optional(),
    region: z.string().optional(),
    endpoint: z.string().optional(),
    signatureVersion: models
      .SignatureVersionOptionsS3CollectorConf$outboundSchema.default("v4"),
    reuseConnections: z.boolean().default(true),
    rejectUnauthorized: z.boolean().default(true),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    maxMessages: z.number().default(1),
    visibilityTimeout: z.number().default(600),
    numReceivers: z.number().default(1),
    socketTimeout: z.number().default(300),
    skipOnError: z.boolean().default(false),
    includeSqsMetadata: z.boolean().default(false),
    enableAssumeRole: z.boolean().default(true),
    assumeRoleArn: z.string().optional(),
    assumeRoleExternalId: z.string().optional(),
    durationSeconds: z.number().default(3600),
    enableSQSAssumeRole: z.boolean().default(false),
    preprocess: models.PreprocessTypeSavedJobCollectionInput$outboundSchema
      .optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    parquetChunkSizeMB: z.number().default(5),
    parquetChunkDownloadTimeout: z.number().default(600),
    checkpointing: models.CheckpointingType$outboundSchema.optional(),
    pollTimeout: z.number().default(10),
    encoding: z.string().optional(),
    tagAfterProcessing: z.boolean().default(false),
    description: z.string().optional(),
    awsApiKey: z.string().optional(),
    awsSecret: z.string().optional(),
    processedTagKey: z.string().optional(),
    processedTagValue: z.string().optional(),
  });

export function inputS3SendToRoutesFalseWithConnectionsConstraintToJSON(
  inputS3SendToRoutesFalseWithConnectionsConstraint:
    InputS3SendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputS3SendToRoutesFalseWithConnectionsConstraint$outboundSchema.parse(
      inputS3SendToRoutesFalseWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputS3SendToRoutesTrueWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  queueName: string;
  fileFilter: string;
  awsAccountId?: string | undefined;
  awsAuthenticationMethod: string;
  awsSecretKey?: string | undefined;
  region?: string | undefined;
  endpoint?: string | undefined;
  signatureVersion: string;
  reuseConnections: boolean;
  rejectUnauthorized: boolean;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  maxMessages: number;
  visibilityTimeout: number;
  numReceivers: number;
  socketTimeout: number;
  skipOnError: boolean;
  includeSqsMetadata: boolean;
  enableAssumeRole: boolean;
  assumeRoleArn?: string | undefined;
  assumeRoleExternalId?: string | undefined;
  durationSeconds: number;
  enableSQSAssumeRole: boolean;
  preprocess?:
    | models.PreprocessTypeSavedJobCollectionInput$Outbound
    | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  parquetChunkSizeMB: number;
  parquetChunkDownloadTimeout: number;
  checkpointing?: models.CheckpointingType$Outbound | undefined;
  pollTimeout: number;
  encoding?: string | undefined;
  tagAfterProcessing: boolean;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  awsSecret?: string | undefined;
  processedTagKey?: string | undefined;
  processedTagValue?: string | undefined;
};

/** @internal */
export const InputS3SendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputS3SendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputS3SendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputS3Type$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    queueName: z.string(),
    fileFilter: z.string().default("/.*/"),
    awsAccountId: z.string().optional(),
    awsAuthenticationMethod: z.string().default("auto"),
    awsSecretKey: z.string().optional(),
    region: z.string().optional(),
    endpoint: z.string().optional(),
    signatureVersion: models
      .SignatureVersionOptionsS3CollectorConf$outboundSchema.default("v4"),
    reuseConnections: z.boolean().default(true),
    rejectUnauthorized: z.boolean().default(true),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    maxMessages: z.number().default(1),
    visibilityTimeout: z.number().default(600),
    numReceivers: z.number().default(1),
    socketTimeout: z.number().default(300),
    skipOnError: z.boolean().default(false),
    includeSqsMetadata: z.boolean().default(false),
    enableAssumeRole: z.boolean().default(true),
    assumeRoleArn: z.string().optional(),
    assumeRoleExternalId: z.string().optional(),
    durationSeconds: z.number().default(3600),
    enableSQSAssumeRole: z.boolean().default(false),
    preprocess: models.PreprocessTypeSavedJobCollectionInput$outboundSchema
      .optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    parquetChunkSizeMB: z.number().default(5),
    parquetChunkDownloadTimeout: z.number().default(600),
    checkpointing: models.CheckpointingType$outboundSchema.optional(),
    pollTimeout: z.number().default(10),
    encoding: z.string().optional(),
    tagAfterProcessing: z.boolean().default(false),
    description: z.string().optional(),
    awsApiKey: z.string().optional(),
    awsSecret: z.string().optional(),
    processedTagKey: z.string().optional(),
    processedTagValue: z.string().optional(),
  });

export function inputS3SendToRoutesTrueWithConnectionsConstraintToJSON(
  inputS3SendToRoutesTrueWithConnectionsConstraint:
    InputS3SendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputS3SendToRoutesTrueWithConnectionsConstraint$outboundSchema.parse(
      inputS3SendToRoutesTrueWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputS3$Outbound =
  | InputS3SendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputS3SendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputS3PqEnabledFalseWithPqConstraint$Outbound
  | InputS3PqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputS3$outboundSchema: z.ZodType<
  InputS3$Outbound,
  z.ZodTypeDef,
  InputS3
> = z.union([
  z.lazy(() => InputS3SendToRoutesTrueWithConnectionsConstraint$outboundSchema),
  z.lazy(() =>
    InputS3SendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputS3PqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputS3PqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputS3ToJSON(inputS3: InputS3): string {
  return JSON.stringify(InputS3$outboundSchema.parse(inputS3));
}

/** @internal */
export const InputMetricsType$outboundSchema: z.ZodNativeEnum<
  typeof InputMetricsType
> = z.nativeEnum(InputMetricsType);

/** @internal */
export type InputMetricsPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  udpPort?: number | undefined;
  tcpPort?: number | undefined;
  maxBufferSize: number;
  ipWhitelistRegex: string;
  enableProxyHeader: boolean;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  udpSocketRxBufSize?: number | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputMetricsPqEnabledTrueWithPqConstraint$outboundSchema:
  z.ZodType<
    InputMetricsPqEnabledTrueWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputMetricsPqEnabledTrueWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputMetricsType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    host: z.string().default("0.0.0.0"),
    udpPort: z.number().optional(),
    tcpPort: z.number().optional(),
    maxBufferSize: z.number().default(1000),
    ipWhitelistRegex: z.string().default("/.*/"),
    enableProxyHeader: z.boolean().default(false),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    udpSocketRxBufSize: z.number().optional(),
    description: z.string().optional(),
  });

export function inputMetricsPqEnabledTrueWithPqConstraintToJSON(
  inputMetricsPqEnabledTrueWithPqConstraint:
    InputMetricsPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputMetricsPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputMetricsPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputMetricsPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  udpPort?: number | undefined;
  tcpPort?: number | undefined;
  maxBufferSize: number;
  ipWhitelistRegex: string;
  enableProxyHeader: boolean;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  udpSocketRxBufSize?: number | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputMetricsPqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputMetricsPqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputMetricsPqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputMetricsType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    host: z.string().default("0.0.0.0"),
    udpPort: z.number().optional(),
    tcpPort: z.number().optional(),
    maxBufferSize: z.number().default(1000),
    ipWhitelistRegex: z.string().default("/.*/"),
    enableProxyHeader: z.boolean().default(false),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    udpSocketRxBufSize: z.number().optional(),
    description: z.string().optional(),
  });

export function inputMetricsPqEnabledFalseWithPqConstraintToJSON(
  inputMetricsPqEnabledFalseWithPqConstraint:
    InputMetricsPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputMetricsPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputMetricsPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputMetricsSendToRoutesFalseWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  host: string;
  udpPort?: number | undefined;
  tcpPort?: number | undefined;
  maxBufferSize: number;
  ipWhitelistRegex: string;
  enableProxyHeader: boolean;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  udpSocketRxBufSize?: number | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputMetricsSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputMetricsSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputMetricsSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputMetricsType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    udpPort: z.number().optional(),
    tcpPort: z.number().optional(),
    maxBufferSize: z.number().default(1000),
    ipWhitelistRegex: z.string().default("/.*/"),
    enableProxyHeader: z.boolean().default(false),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    udpSocketRxBufSize: z.number().optional(),
    description: z.string().optional(),
  });

export function inputMetricsSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputMetricsSendToRoutesFalseWithConnectionsConstraint:
    InputMetricsSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputMetricsSendToRoutesFalseWithConnectionsConstraint$outboundSchema.parse(
      inputMetricsSendToRoutesFalseWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputMetricsSendToRoutesTrueWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  host: string;
  udpPort?: number | undefined;
  tcpPort?: number | undefined;
  maxBufferSize: number;
  ipWhitelistRegex: string;
  enableProxyHeader: boolean;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  udpSocketRxBufSize?: number | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputMetricsSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputMetricsSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputMetricsSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputMetricsType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    udpPort: z.number().optional(),
    tcpPort: z.number().optional(),
    maxBufferSize: z.number().default(1000),
    ipWhitelistRegex: z.string().default("/.*/"),
    enableProxyHeader: z.boolean().default(false),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    udpSocketRxBufSize: z.number().optional(),
    description: z.string().optional(),
  });

export function inputMetricsSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputMetricsSendToRoutesTrueWithConnectionsConstraint:
    InputMetricsSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputMetricsSendToRoutesTrueWithConnectionsConstraint$outboundSchema.parse(
      inputMetricsSendToRoutesTrueWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputMetrics$Outbound =
  | InputMetricsSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputMetricsSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputMetricsPqEnabledFalseWithPqConstraint$Outbound
  | InputMetricsPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputMetrics$outboundSchema: z.ZodType<
  InputMetrics$Outbound,
  z.ZodTypeDef,
  InputMetrics
> = z.union([
  z.lazy(() =>
    InputMetricsSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputMetricsSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputMetricsPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputMetricsPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputMetricsToJSON(inputMetrics: InputMetrics): string {
  return JSON.stringify(InputMetrics$outboundSchema.parse(inputMetrics));
}

/** @internal */
export const InputCriblmetricsType$outboundSchema: z.ZodNativeEnum<
  typeof InputCriblmetricsType
> = z.nativeEnum(InputCriblmetricsType);

/** @internal */
export type InputCriblmetricsPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  prefix: string;
  fullFidelity: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputCriblmetricsPqEnabledTrueWithPqConstraint$outboundSchema:
  z.ZodType<
    InputCriblmetricsPqEnabledTrueWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputCriblmetricsPqEnabledTrueWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputCriblmetricsType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    prefix: z.string().default("cribl.logstream."),
    fullFidelity: z.boolean().default(true),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputCriblmetricsPqEnabledTrueWithPqConstraintToJSON(
  inputCriblmetricsPqEnabledTrueWithPqConstraint:
    InputCriblmetricsPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputCriblmetricsPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputCriblmetricsPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputCriblmetricsPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  prefix: string;
  fullFidelity: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputCriblmetricsPqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputCriblmetricsPqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputCriblmetricsPqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputCriblmetricsType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    prefix: z.string().default("cribl.logstream."),
    fullFidelity: z.boolean().default(true),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputCriblmetricsPqEnabledFalseWithPqConstraintToJSON(
  inputCriblmetricsPqEnabledFalseWithPqConstraint:
    InputCriblmetricsPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputCriblmetricsPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputCriblmetricsPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputCriblmetricsSendToRoutesFalseWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    prefix: string;
    fullFidelity: boolean;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    description?: string | undefined;
  };

/** @internal */
export const InputCriblmetricsSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputCriblmetricsSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputCriblmetricsSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputCriblmetricsType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    prefix: z.string().default("cribl.logstream."),
    fullFidelity: z.boolean().default(true),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputCriblmetricsSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputCriblmetricsSendToRoutesFalseWithConnectionsConstraint:
    InputCriblmetricsSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputCriblmetricsSendToRoutesFalseWithConnectionsConstraint$outboundSchema
      .parse(inputCriblmetricsSendToRoutesFalseWithConnectionsConstraint),
  );
}

/** @internal */
export type InputCriblmetricsSendToRoutesTrueWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    prefix: string;
    fullFidelity: boolean;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    description?: string | undefined;
  };

/** @internal */
export const InputCriblmetricsSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputCriblmetricsSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputCriblmetricsSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputCriblmetricsType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    prefix: z.string().default("cribl.logstream."),
    fullFidelity: z.boolean().default(true),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputCriblmetricsSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputCriblmetricsSendToRoutesTrueWithConnectionsConstraint:
    InputCriblmetricsSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputCriblmetricsSendToRoutesTrueWithConnectionsConstraint$outboundSchema
      .parse(inputCriblmetricsSendToRoutesTrueWithConnectionsConstraint),
  );
}

/** @internal */
export type InputCriblmetrics$Outbound =
  | InputCriblmetricsSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputCriblmetricsSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputCriblmetricsPqEnabledFalseWithPqConstraint$Outbound
  | InputCriblmetricsPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputCriblmetrics$outboundSchema: z.ZodType<
  InputCriblmetrics$Outbound,
  z.ZodTypeDef,
  InputCriblmetrics
> = z.union([
  z.lazy(() =>
    InputCriblmetricsSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputCriblmetricsSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputCriblmetricsPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputCriblmetricsPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputCriblmetricsToJSON(
  inputCriblmetrics: InputCriblmetrics,
): string {
  return JSON.stringify(
    InputCriblmetrics$outboundSchema.parse(inputCriblmetrics),
  );
}

/** @internal */
export const InputKinesisType$outboundSchema: z.ZodNativeEnum<
  typeof InputKinesisType
> = z.nativeEnum(InputKinesisType);

/** @internal */
export const ShardIteratorStart$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  ShardIteratorStart
> = openEnums.outboundSchema(ShardIteratorStart);

/** @internal */
export const RecordDataFormat$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  RecordDataFormat
> = openEnums.outboundSchema(RecordDataFormat);

/** @internal */
export const ShardLoadBalancing$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  ShardLoadBalancing
> = openEnums.outboundSchema(ShardLoadBalancing);

/** @internal */
export type InputKinesisPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  streamName: string;
  serviceInterval: number;
  shardExpr: string;
  shardIteratorType: string;
  payloadFormat: string;
  getRecordsLimit: number;
  getRecordsLimitTotal: number;
  loadBalancingAlgorithm: string;
  awsAuthenticationMethod: string;
  awsSecretKey?: string | undefined;
  region: string;
  endpoint?: string | undefined;
  signatureVersion: string;
  reuseConnections: boolean;
  rejectUnauthorized: boolean;
  enableAssumeRole: boolean;
  assumeRoleArn?: string | undefined;
  assumeRoleExternalId?: string | undefined;
  durationSeconds: number;
  verifyKPLCheckSums: boolean;
  avoidDuplicates: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  awsSecret?: string | undefined;
};

/** @internal */
export const InputKinesisPqEnabledTrueWithPqConstraint$outboundSchema:
  z.ZodType<
    InputKinesisPqEnabledTrueWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputKinesisPqEnabledTrueWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputKinesisType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    streamName: z.string(),
    serviceInterval: z.number().default(1),
    shardExpr: z.string().default("true"),
    shardIteratorType: ShardIteratorStart$outboundSchema.default(
      "TRIM_HORIZON",
    ),
    payloadFormat: RecordDataFormat$outboundSchema.default("cribl"),
    getRecordsLimit: z.number().default(5000),
    getRecordsLimitTotal: z.number().default(20000),
    loadBalancingAlgorithm: ShardLoadBalancing$outboundSchema.default(
      "ConsistentHashing",
    ),
    awsAuthenticationMethod: z.string().default("auto"),
    awsSecretKey: z.string().optional(),
    region: z.string(),
    endpoint: z.string().optional(),
    signatureVersion: models.SignatureVersionOptions2$outboundSchema.default(
      "v4",
    ),
    reuseConnections: z.boolean().default(true),
    rejectUnauthorized: z.boolean().default(true),
    enableAssumeRole: z.boolean().default(false),
    assumeRoleArn: z.string().optional(),
    assumeRoleExternalId: z.string().optional(),
    durationSeconds: z.number().default(3600),
    verifyKPLCheckSums: z.boolean().default(false),
    avoidDuplicates: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
    awsApiKey: z.string().optional(),
    awsSecret: z.string().optional(),
  });

export function inputKinesisPqEnabledTrueWithPqConstraintToJSON(
  inputKinesisPqEnabledTrueWithPqConstraint:
    InputKinesisPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputKinesisPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputKinesisPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputKinesisPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  streamName: string;
  serviceInterval: number;
  shardExpr: string;
  shardIteratorType: string;
  payloadFormat: string;
  getRecordsLimit: number;
  getRecordsLimitTotal: number;
  loadBalancingAlgorithm: string;
  awsAuthenticationMethod: string;
  awsSecretKey?: string | undefined;
  region: string;
  endpoint?: string | undefined;
  signatureVersion: string;
  reuseConnections: boolean;
  rejectUnauthorized: boolean;
  enableAssumeRole: boolean;
  assumeRoleArn?: string | undefined;
  assumeRoleExternalId?: string | undefined;
  durationSeconds: number;
  verifyKPLCheckSums: boolean;
  avoidDuplicates: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  awsSecret?: string | undefined;
};

/** @internal */
export const InputKinesisPqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputKinesisPqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputKinesisPqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputKinesisType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    streamName: z.string(),
    serviceInterval: z.number().default(1),
    shardExpr: z.string().default("true"),
    shardIteratorType: ShardIteratorStart$outboundSchema.default(
      "TRIM_HORIZON",
    ),
    payloadFormat: RecordDataFormat$outboundSchema.default("cribl"),
    getRecordsLimit: z.number().default(5000),
    getRecordsLimitTotal: z.number().default(20000),
    loadBalancingAlgorithm: ShardLoadBalancing$outboundSchema.default(
      "ConsistentHashing",
    ),
    awsAuthenticationMethod: z.string().default("auto"),
    awsSecretKey: z.string().optional(),
    region: z.string(),
    endpoint: z.string().optional(),
    signatureVersion: models.SignatureVersionOptions2$outboundSchema.default(
      "v4",
    ),
    reuseConnections: z.boolean().default(true),
    rejectUnauthorized: z.boolean().default(true),
    enableAssumeRole: z.boolean().default(false),
    assumeRoleArn: z.string().optional(),
    assumeRoleExternalId: z.string().optional(),
    durationSeconds: z.number().default(3600),
    verifyKPLCheckSums: z.boolean().default(false),
    avoidDuplicates: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
    awsApiKey: z.string().optional(),
    awsSecret: z.string().optional(),
  });

export function inputKinesisPqEnabledFalseWithPqConstraintToJSON(
  inputKinesisPqEnabledFalseWithPqConstraint:
    InputKinesisPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputKinesisPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputKinesisPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputKinesisSendToRoutesFalseWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  streamName: string;
  serviceInterval: number;
  shardExpr: string;
  shardIteratorType: string;
  payloadFormat: string;
  getRecordsLimit: number;
  getRecordsLimitTotal: number;
  loadBalancingAlgorithm: string;
  awsAuthenticationMethod: string;
  awsSecretKey?: string | undefined;
  region: string;
  endpoint?: string | undefined;
  signatureVersion: string;
  reuseConnections: boolean;
  rejectUnauthorized: boolean;
  enableAssumeRole: boolean;
  assumeRoleArn?: string | undefined;
  assumeRoleExternalId?: string | undefined;
  durationSeconds: number;
  verifyKPLCheckSums: boolean;
  avoidDuplicates: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  awsSecret?: string | undefined;
};

/** @internal */
export const InputKinesisSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputKinesisSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputKinesisSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputKinesisType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    streamName: z.string(),
    serviceInterval: z.number().default(1),
    shardExpr: z.string().default("true"),
    shardIteratorType: ShardIteratorStart$outboundSchema.default(
      "TRIM_HORIZON",
    ),
    payloadFormat: RecordDataFormat$outboundSchema.default("cribl"),
    getRecordsLimit: z.number().default(5000),
    getRecordsLimitTotal: z.number().default(20000),
    loadBalancingAlgorithm: ShardLoadBalancing$outboundSchema.default(
      "ConsistentHashing",
    ),
    awsAuthenticationMethod: z.string().default("auto"),
    awsSecretKey: z.string().optional(),
    region: z.string(),
    endpoint: z.string().optional(),
    signatureVersion: models.SignatureVersionOptions2$outboundSchema.default(
      "v4",
    ),
    reuseConnections: z.boolean().default(true),
    rejectUnauthorized: z.boolean().default(true),
    enableAssumeRole: z.boolean().default(false),
    assumeRoleArn: z.string().optional(),
    assumeRoleExternalId: z.string().optional(),
    durationSeconds: z.number().default(3600),
    verifyKPLCheckSums: z.boolean().default(false),
    avoidDuplicates: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
    awsApiKey: z.string().optional(),
    awsSecret: z.string().optional(),
  });

export function inputKinesisSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputKinesisSendToRoutesFalseWithConnectionsConstraint:
    InputKinesisSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputKinesisSendToRoutesFalseWithConnectionsConstraint$outboundSchema.parse(
      inputKinesisSendToRoutesFalseWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputKinesisSendToRoutesTrueWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  streamName: string;
  serviceInterval: number;
  shardExpr: string;
  shardIteratorType: string;
  payloadFormat: string;
  getRecordsLimit: number;
  getRecordsLimitTotal: number;
  loadBalancingAlgorithm: string;
  awsAuthenticationMethod: string;
  awsSecretKey?: string | undefined;
  region: string;
  endpoint?: string | undefined;
  signatureVersion: string;
  reuseConnections: boolean;
  rejectUnauthorized: boolean;
  enableAssumeRole: boolean;
  assumeRoleArn?: string | undefined;
  assumeRoleExternalId?: string | undefined;
  durationSeconds: number;
  verifyKPLCheckSums: boolean;
  avoidDuplicates: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  awsSecret?: string | undefined;
};

/** @internal */
export const InputKinesisSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputKinesisSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputKinesisSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputKinesisType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    streamName: z.string(),
    serviceInterval: z.number().default(1),
    shardExpr: z.string().default("true"),
    shardIteratorType: ShardIteratorStart$outboundSchema.default(
      "TRIM_HORIZON",
    ),
    payloadFormat: RecordDataFormat$outboundSchema.default("cribl"),
    getRecordsLimit: z.number().default(5000),
    getRecordsLimitTotal: z.number().default(20000),
    loadBalancingAlgorithm: ShardLoadBalancing$outboundSchema.default(
      "ConsistentHashing",
    ),
    awsAuthenticationMethod: z.string().default("auto"),
    awsSecretKey: z.string().optional(),
    region: z.string(),
    endpoint: z.string().optional(),
    signatureVersion: models.SignatureVersionOptions2$outboundSchema.default(
      "v4",
    ),
    reuseConnections: z.boolean().default(true),
    rejectUnauthorized: z.boolean().default(true),
    enableAssumeRole: z.boolean().default(false),
    assumeRoleArn: z.string().optional(),
    assumeRoleExternalId: z.string().optional(),
    durationSeconds: z.number().default(3600),
    verifyKPLCheckSums: z.boolean().default(false),
    avoidDuplicates: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
    awsApiKey: z.string().optional(),
    awsSecret: z.string().optional(),
  });

export function inputKinesisSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputKinesisSendToRoutesTrueWithConnectionsConstraint:
    InputKinesisSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputKinesisSendToRoutesTrueWithConnectionsConstraint$outboundSchema.parse(
      inputKinesisSendToRoutesTrueWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputKinesis$Outbound =
  | InputKinesisSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputKinesisSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputKinesisPqEnabledFalseWithPqConstraint$Outbound
  | InputKinesisPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputKinesis$outboundSchema: z.ZodType<
  InputKinesis$Outbound,
  z.ZodTypeDef,
  InputKinesis
> = z.union([
  z.lazy(() =>
    InputKinesisSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputKinesisSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputKinesisPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputKinesisPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputKinesisToJSON(inputKinesis: InputKinesis): string {
  return JSON.stringify(InputKinesis$outboundSchema.parse(inputKinesis));
}

/** @internal */
export const InputHttpRawType$outboundSchema: z.ZodNativeEnum<
  typeof InputHttpRawType
> = z.nativeEnum(InputHttpRawType);

/** @internal */
export type InputHttpRawPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  authTokens?: Array<string> | undefined;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  activityLogSampleRate: number;
  requestTimeout: number;
  socketTimeout: number;
  keepAliveTimeout: number;
  enableHealthCheck: boolean;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  allowedPaths?: Array<string> | undefined;
  allowedMethods?: Array<string> | undefined;
  authTokensExt?: Array<models.ItemsTypeAuthTokensExt$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputHttpRawPqEnabledTrueWithPqConstraint$outboundSchema:
  z.ZodType<
    InputHttpRawPqEnabledTrueWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputHttpRawPqEnabledTrueWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputHttpRawType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    authTokens: z.array(z.string()).optional(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    allowedPaths: z.array(z.string()).optional(),
    allowedMethods: z.array(z.string()).optional(),
    authTokensExt: z.array(models.ItemsTypeAuthTokensExt$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputHttpRawPqEnabledTrueWithPqConstraintToJSON(
  inputHttpRawPqEnabledTrueWithPqConstraint:
    InputHttpRawPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputHttpRawPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputHttpRawPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputHttpRawPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  authTokens?: Array<string> | undefined;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  activityLogSampleRate: number;
  requestTimeout: number;
  socketTimeout: number;
  keepAliveTimeout: number;
  enableHealthCheck: boolean;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  allowedPaths?: Array<string> | undefined;
  allowedMethods?: Array<string> | undefined;
  authTokensExt?: Array<models.ItemsTypeAuthTokensExt$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputHttpRawPqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputHttpRawPqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputHttpRawPqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputHttpRawType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    authTokens: z.array(z.string()).optional(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    allowedPaths: z.array(z.string()).optional(),
    allowedMethods: z.array(z.string()).optional(),
    authTokensExt: z.array(models.ItemsTypeAuthTokensExt$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputHttpRawPqEnabledFalseWithPqConstraintToJSON(
  inputHttpRawPqEnabledFalseWithPqConstraint:
    InputHttpRawPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputHttpRawPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputHttpRawPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputHttpRawSendToRoutesFalseWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  host: string;
  port: number;
  authTokens?: Array<string> | undefined;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  activityLogSampleRate: number;
  requestTimeout: number;
  socketTimeout: number;
  keepAliveTimeout: number;
  enableHealthCheck: boolean;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  allowedPaths?: Array<string> | undefined;
  allowedMethods?: Array<string> | undefined;
  authTokensExt?: Array<models.ItemsTypeAuthTokensExt$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputHttpRawSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputHttpRawSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputHttpRawSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputHttpRawType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    authTokens: z.array(z.string()).optional(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    allowedPaths: z.array(z.string()).optional(),
    allowedMethods: z.array(z.string()).optional(),
    authTokensExt: z.array(models.ItemsTypeAuthTokensExt$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputHttpRawSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputHttpRawSendToRoutesFalseWithConnectionsConstraint:
    InputHttpRawSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputHttpRawSendToRoutesFalseWithConnectionsConstraint$outboundSchema.parse(
      inputHttpRawSendToRoutesFalseWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputHttpRawSendToRoutesTrueWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  host: string;
  port: number;
  authTokens?: Array<string> | undefined;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  activityLogSampleRate: number;
  requestTimeout: number;
  socketTimeout: number;
  keepAliveTimeout: number;
  enableHealthCheck: boolean;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  allowedPaths?: Array<string> | undefined;
  allowedMethods?: Array<string> | undefined;
  authTokensExt?: Array<models.ItemsTypeAuthTokensExt$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputHttpRawSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputHttpRawSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputHttpRawSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputHttpRawType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    authTokens: z.array(z.string()).optional(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    allowedPaths: z.array(z.string()).optional(),
    allowedMethods: z.array(z.string()).optional(),
    authTokensExt: z.array(models.ItemsTypeAuthTokensExt$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputHttpRawSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputHttpRawSendToRoutesTrueWithConnectionsConstraint:
    InputHttpRawSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputHttpRawSendToRoutesTrueWithConnectionsConstraint$outboundSchema.parse(
      inputHttpRawSendToRoutesTrueWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputHttpRaw$Outbound =
  | InputHttpRawSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputHttpRawSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputHttpRawPqEnabledFalseWithPqConstraint$Outbound
  | InputHttpRawPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputHttpRaw$outboundSchema: z.ZodType<
  InputHttpRaw$Outbound,
  z.ZodTypeDef,
  InputHttpRaw
> = z.union([
  z.lazy(() =>
    InputHttpRawSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputHttpRawSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputHttpRawPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputHttpRawPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputHttpRawToJSON(inputHttpRaw: InputHttpRaw): string {
  return JSON.stringify(InputHttpRaw$outboundSchema.parse(inputHttpRaw));
}

/** @internal */
export const InputDatagenType$outboundSchema: z.ZodNativeEnum<
  typeof InputDatagenType
> = z.nativeEnum(InputDatagenType);

/** @internal */
export type Sample$Outbound = {
  sample: string;
  eventsPerSec: number;
};

/** @internal */
export const Sample$outboundSchema: z.ZodType<
  Sample$Outbound,
  z.ZodTypeDef,
  Sample
> = z.object({
  sample: z.string(),
  eventsPerSec: z.number().default(10),
});

export function sampleToJSON(sample: Sample): string {
  return JSON.stringify(Sample$outboundSchema.parse(sample));
}

/** @internal */
export type InputDatagenPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  samples: Array<Sample$Outbound>;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputDatagenPqEnabledTrueWithPqConstraint$outboundSchema:
  z.ZodType<
    InputDatagenPqEnabledTrueWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputDatagenPqEnabledTrueWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputDatagenType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    samples: z.array(z.lazy(() => Sample$outboundSchema)),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputDatagenPqEnabledTrueWithPqConstraintToJSON(
  inputDatagenPqEnabledTrueWithPqConstraint:
    InputDatagenPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputDatagenPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputDatagenPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputDatagenPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  samples: Array<Sample$Outbound>;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputDatagenPqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputDatagenPqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputDatagenPqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputDatagenType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    samples: z.array(z.lazy(() => Sample$outboundSchema)),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputDatagenPqEnabledFalseWithPqConstraintToJSON(
  inputDatagenPqEnabledFalseWithPqConstraint:
    InputDatagenPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputDatagenPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputDatagenPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputDatagenSendToRoutesFalseWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  samples: Array<Sample$Outbound>;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputDatagenSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputDatagenSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputDatagenSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputDatagenType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    samples: z.array(z.lazy(() => Sample$outboundSchema)),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputDatagenSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputDatagenSendToRoutesFalseWithConnectionsConstraint:
    InputDatagenSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputDatagenSendToRoutesFalseWithConnectionsConstraint$outboundSchema.parse(
      inputDatagenSendToRoutesFalseWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputDatagenSendToRoutesTrueWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  samples: Array<Sample$Outbound>;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputDatagenSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputDatagenSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputDatagenSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputDatagenType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    samples: z.array(z.lazy(() => Sample$outboundSchema)),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputDatagenSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputDatagenSendToRoutesTrueWithConnectionsConstraint:
    InputDatagenSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputDatagenSendToRoutesTrueWithConnectionsConstraint$outboundSchema.parse(
      inputDatagenSendToRoutesTrueWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputDatagen$Outbound =
  | InputDatagenSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputDatagenSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputDatagenPqEnabledFalseWithPqConstraint$Outbound
  | InputDatagenPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputDatagen$outboundSchema: z.ZodType<
  InputDatagen$Outbound,
  z.ZodTypeDef,
  InputDatagen
> = z.union([
  z.lazy(() =>
    InputDatagenSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputDatagenSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputDatagenPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputDatagenPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputDatagenToJSON(inputDatagen: InputDatagen): string {
  return JSON.stringify(InputDatagen$outboundSchema.parse(inputDatagen));
}

/** @internal */
export const InputDatadogAgentType$outboundSchema: z.ZodNativeEnum<
  typeof InputDatadogAgentType
> = z.nativeEnum(InputDatadogAgentType);

/** @internal */
export type InputDatadogAgentProxyMode$Outbound = {
  enabled: boolean;
  rejectUnauthorized: boolean;
};

/** @internal */
export const InputDatadogAgentProxyMode$outboundSchema: z.ZodType<
  InputDatadogAgentProxyMode$Outbound,
  z.ZodTypeDef,
  InputDatadogAgentProxyMode
> = z.object({
  enabled: z.boolean().default(false),
  rejectUnauthorized: z.boolean().default(true),
});

export function inputDatadogAgentProxyModeToJSON(
  inputDatadogAgentProxyMode: InputDatadogAgentProxyMode,
): string {
  return JSON.stringify(
    InputDatadogAgentProxyMode$outboundSchema.parse(inputDatadogAgentProxyMode),
  );
}

/** @internal */
export type InputDatadogAgentPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  activityLogSampleRate: number;
  requestTimeout: number;
  socketTimeout: number;
  keepAliveTimeout: number;
  enableHealthCheck: boolean;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  extractMetrics: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  proxyMode?: InputDatadogAgentProxyMode$Outbound | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputDatadogAgentPqEnabledTrueWithPqConstraint$outboundSchema:
  z.ZodType<
    InputDatadogAgentPqEnabledTrueWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputDatadogAgentPqEnabledTrueWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputDatadogAgentType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    extractMetrics: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    proxyMode: z.lazy(() => InputDatadogAgentProxyMode$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputDatadogAgentPqEnabledTrueWithPqConstraintToJSON(
  inputDatadogAgentPqEnabledTrueWithPqConstraint:
    InputDatadogAgentPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputDatadogAgentPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputDatadogAgentPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputDatadogAgentPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  activityLogSampleRate: number;
  requestTimeout: number;
  socketTimeout: number;
  keepAliveTimeout: number;
  enableHealthCheck: boolean;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  extractMetrics: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  proxyMode?: InputDatadogAgentProxyMode$Outbound | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputDatadogAgentPqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputDatadogAgentPqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputDatadogAgentPqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputDatadogAgentType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    extractMetrics: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    proxyMode: z.lazy(() => InputDatadogAgentProxyMode$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputDatadogAgentPqEnabledFalseWithPqConstraintToJSON(
  inputDatadogAgentPqEnabledFalseWithPqConstraint:
    InputDatadogAgentPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputDatadogAgentPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputDatadogAgentPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputDatadogAgentSendToRoutesFalseWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    host: string;
    port: number;
    tls?: models.TlsSettingsServerSideType$Outbound | undefined;
    maxActiveReq: number;
    maxRequestsPerSocket: number;
    enableProxyHeader: boolean;
    captureHeaders: boolean;
    activityLogSampleRate: number;
    requestTimeout: number;
    socketTimeout: number;
    keepAliveTimeout: number;
    enableHealthCheck: boolean;
    ipAllowlistRegex: string;
    ipDenylistRegex: string;
    extractMetrics: boolean;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    proxyMode?: InputDatadogAgentProxyMode$Outbound | undefined;
    description?: string | undefined;
  };

/** @internal */
export const InputDatadogAgentSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputDatadogAgentSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputDatadogAgentSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputDatadogAgentType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    extractMetrics: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    proxyMode: z.lazy(() => InputDatadogAgentProxyMode$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputDatadogAgentSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputDatadogAgentSendToRoutesFalseWithConnectionsConstraint:
    InputDatadogAgentSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputDatadogAgentSendToRoutesFalseWithConnectionsConstraint$outboundSchema
      .parse(inputDatadogAgentSendToRoutesFalseWithConnectionsConstraint),
  );
}

/** @internal */
export type InputDatadogAgentSendToRoutesTrueWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    host: string;
    port: number;
    tls?: models.TlsSettingsServerSideType$Outbound | undefined;
    maxActiveReq: number;
    maxRequestsPerSocket: number;
    enableProxyHeader: boolean;
    captureHeaders: boolean;
    activityLogSampleRate: number;
    requestTimeout: number;
    socketTimeout: number;
    keepAliveTimeout: number;
    enableHealthCheck: boolean;
    ipAllowlistRegex: string;
    ipDenylistRegex: string;
    extractMetrics: boolean;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    proxyMode?: InputDatadogAgentProxyMode$Outbound | undefined;
    description?: string | undefined;
  };

/** @internal */
export const InputDatadogAgentSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputDatadogAgentSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputDatadogAgentSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputDatadogAgentType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    extractMetrics: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    proxyMode: z.lazy(() => InputDatadogAgentProxyMode$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputDatadogAgentSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputDatadogAgentSendToRoutesTrueWithConnectionsConstraint:
    InputDatadogAgentSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputDatadogAgentSendToRoutesTrueWithConnectionsConstraint$outboundSchema
      .parse(inputDatadogAgentSendToRoutesTrueWithConnectionsConstraint),
  );
}

/** @internal */
export type InputDatadogAgent$Outbound =
  | InputDatadogAgentSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputDatadogAgentSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputDatadogAgentPqEnabledFalseWithPqConstraint$Outbound
  | InputDatadogAgentPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputDatadogAgent$outboundSchema: z.ZodType<
  InputDatadogAgent$Outbound,
  z.ZodTypeDef,
  InputDatadogAgent
> = z.union([
  z.lazy(() =>
    InputDatadogAgentSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputDatadogAgentSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputDatadogAgentPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputDatadogAgentPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputDatadogAgentToJSON(
  inputDatadogAgent: InputDatadogAgent,
): string {
  return JSON.stringify(
    InputDatadogAgent$outboundSchema.parse(inputDatadogAgent),
  );
}

/** @internal */
export const InputCrowdstrikeType$outboundSchema: z.ZodNativeEnum<
  typeof InputCrowdstrikeType
> = z.nativeEnum(InputCrowdstrikeType);

/** @internal */
export type InputCrowdstrikePqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  queueName: string;
  fileFilter: string;
  awsAccountId?: string | undefined;
  awsAuthenticationMethod: string;
  awsSecretKey?: string | undefined;
  region?: string | undefined;
  endpoint?: string | undefined;
  signatureVersion: string;
  reuseConnections: boolean;
  rejectUnauthorized: boolean;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  maxMessages: number;
  visibilityTimeout: number;
  numReceivers: number;
  socketTimeout: number;
  skipOnError: boolean;
  includeSqsMetadata: boolean;
  enableAssumeRole: boolean;
  assumeRoleArn?: string | undefined;
  assumeRoleExternalId?: string | undefined;
  durationSeconds: number;
  enableSQSAssumeRole: boolean;
  preprocess?:
    | models.PreprocessTypeSavedJobCollectionInput$Outbound
    | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  checkpointing?: models.CheckpointingType$Outbound | undefined;
  pollTimeout: number;
  encoding?: string | undefined;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  awsSecret?: string | undefined;
  tagAfterProcessing?: string | undefined;
  processedTagKey?: string | undefined;
  processedTagValue?: string | undefined;
};

/** @internal */
export const InputCrowdstrikePqEnabledTrueWithPqConstraint$outboundSchema:
  z.ZodType<
    InputCrowdstrikePqEnabledTrueWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputCrowdstrikePqEnabledTrueWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputCrowdstrikeType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    queueName: z.string(),
    fileFilter: z.string().default("/.*/"),
    awsAccountId: z.string().optional(),
    awsAuthenticationMethod: z.string().default("auto"),
    awsSecretKey: z.string().optional(),
    region: z.string().optional(),
    endpoint: z.string().optional(),
    signatureVersion: models
      .SignatureVersionOptionsS3CollectorConf$outboundSchema.default("v4"),
    reuseConnections: z.boolean().default(true),
    rejectUnauthorized: z.boolean().default(true),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    maxMessages: z.number().default(1),
    visibilityTimeout: z.number().default(21600),
    numReceivers: z.number().default(1),
    socketTimeout: z.number().default(300),
    skipOnError: z.boolean().default(false),
    includeSqsMetadata: z.boolean().default(false),
    enableAssumeRole: z.boolean().default(true),
    assumeRoleArn: z.string().optional(),
    assumeRoleExternalId: z.string().optional(),
    durationSeconds: z.number().default(3600),
    enableSQSAssumeRole: z.boolean().default(false),
    preprocess: models.PreprocessTypeSavedJobCollectionInput$outboundSchema
      .optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    checkpointing: models.CheckpointingType$outboundSchema.optional(),
    pollTimeout: z.number().default(10),
    encoding: z.string().optional(),
    description: z.string().optional(),
    awsApiKey: z.string().optional(),
    awsSecret: z.string().optional(),
    tagAfterProcessing: models.TagAfterProcessingOptions$outboundSchema
      .optional(),
    processedTagKey: z.string().optional(),
    processedTagValue: z.string().optional(),
  });

export function inputCrowdstrikePqEnabledTrueWithPqConstraintToJSON(
  inputCrowdstrikePqEnabledTrueWithPqConstraint:
    InputCrowdstrikePqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputCrowdstrikePqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputCrowdstrikePqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputCrowdstrikePqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  queueName: string;
  fileFilter: string;
  awsAccountId?: string | undefined;
  awsAuthenticationMethod: string;
  awsSecretKey?: string | undefined;
  region?: string | undefined;
  endpoint?: string | undefined;
  signatureVersion: string;
  reuseConnections: boolean;
  rejectUnauthorized: boolean;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  maxMessages: number;
  visibilityTimeout: number;
  numReceivers: number;
  socketTimeout: number;
  skipOnError: boolean;
  includeSqsMetadata: boolean;
  enableAssumeRole: boolean;
  assumeRoleArn?: string | undefined;
  assumeRoleExternalId?: string | undefined;
  durationSeconds: number;
  enableSQSAssumeRole: boolean;
  preprocess?:
    | models.PreprocessTypeSavedJobCollectionInput$Outbound
    | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  checkpointing?: models.CheckpointingType$Outbound | undefined;
  pollTimeout: number;
  encoding?: string | undefined;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  awsSecret?: string | undefined;
  tagAfterProcessing?: string | undefined;
  processedTagKey?: string | undefined;
  processedTagValue?: string | undefined;
};

/** @internal */
export const InputCrowdstrikePqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputCrowdstrikePqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputCrowdstrikePqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputCrowdstrikeType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    queueName: z.string(),
    fileFilter: z.string().default("/.*/"),
    awsAccountId: z.string().optional(),
    awsAuthenticationMethod: z.string().default("auto"),
    awsSecretKey: z.string().optional(),
    region: z.string().optional(),
    endpoint: z.string().optional(),
    signatureVersion: models
      .SignatureVersionOptionsS3CollectorConf$outboundSchema.default("v4"),
    reuseConnections: z.boolean().default(true),
    rejectUnauthorized: z.boolean().default(true),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    maxMessages: z.number().default(1),
    visibilityTimeout: z.number().default(21600),
    numReceivers: z.number().default(1),
    socketTimeout: z.number().default(300),
    skipOnError: z.boolean().default(false),
    includeSqsMetadata: z.boolean().default(false),
    enableAssumeRole: z.boolean().default(true),
    assumeRoleArn: z.string().optional(),
    assumeRoleExternalId: z.string().optional(),
    durationSeconds: z.number().default(3600),
    enableSQSAssumeRole: z.boolean().default(false),
    preprocess: models.PreprocessTypeSavedJobCollectionInput$outboundSchema
      .optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    checkpointing: models.CheckpointingType$outboundSchema.optional(),
    pollTimeout: z.number().default(10),
    encoding: z.string().optional(),
    description: z.string().optional(),
    awsApiKey: z.string().optional(),
    awsSecret: z.string().optional(),
    tagAfterProcessing: models.TagAfterProcessingOptions$outboundSchema
      .optional(),
    processedTagKey: z.string().optional(),
    processedTagValue: z.string().optional(),
  });

export function inputCrowdstrikePqEnabledFalseWithPqConstraintToJSON(
  inputCrowdstrikePqEnabledFalseWithPqConstraint:
    InputCrowdstrikePqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputCrowdstrikePqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputCrowdstrikePqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputCrowdstrikeSendToRoutesFalseWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    queueName: string;
    fileFilter: string;
    awsAccountId?: string | undefined;
    awsAuthenticationMethod: string;
    awsSecretKey?: string | undefined;
    region?: string | undefined;
    endpoint?: string | undefined;
    signatureVersion: string;
    reuseConnections: boolean;
    rejectUnauthorized: boolean;
    breakerRulesets?: Array<string> | undefined;
    staleChannelFlushMs: number;
    maxMessages: number;
    visibilityTimeout: number;
    numReceivers: number;
    socketTimeout: number;
    skipOnError: boolean;
    includeSqsMetadata: boolean;
    enableAssumeRole: boolean;
    assumeRoleArn?: string | undefined;
    assumeRoleExternalId?: string | undefined;
    durationSeconds: number;
    enableSQSAssumeRole: boolean;
    preprocess?:
      | models.PreprocessTypeSavedJobCollectionInput$Outbound
      | undefined;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    checkpointing?: models.CheckpointingType$Outbound | undefined;
    pollTimeout: number;
    encoding?: string | undefined;
    description?: string | undefined;
    awsApiKey?: string | undefined;
    awsSecret?: string | undefined;
    tagAfterProcessing?: string | undefined;
    processedTagKey?: string | undefined;
    processedTagValue?: string | undefined;
  };

/** @internal */
export const InputCrowdstrikeSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputCrowdstrikeSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputCrowdstrikeSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputCrowdstrikeType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    queueName: z.string(),
    fileFilter: z.string().default("/.*/"),
    awsAccountId: z.string().optional(),
    awsAuthenticationMethod: z.string().default("auto"),
    awsSecretKey: z.string().optional(),
    region: z.string().optional(),
    endpoint: z.string().optional(),
    signatureVersion: models
      .SignatureVersionOptionsS3CollectorConf$outboundSchema.default("v4"),
    reuseConnections: z.boolean().default(true),
    rejectUnauthorized: z.boolean().default(true),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    maxMessages: z.number().default(1),
    visibilityTimeout: z.number().default(21600),
    numReceivers: z.number().default(1),
    socketTimeout: z.number().default(300),
    skipOnError: z.boolean().default(false),
    includeSqsMetadata: z.boolean().default(false),
    enableAssumeRole: z.boolean().default(true),
    assumeRoleArn: z.string().optional(),
    assumeRoleExternalId: z.string().optional(),
    durationSeconds: z.number().default(3600),
    enableSQSAssumeRole: z.boolean().default(false),
    preprocess: models.PreprocessTypeSavedJobCollectionInput$outboundSchema
      .optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    checkpointing: models.CheckpointingType$outboundSchema.optional(),
    pollTimeout: z.number().default(10),
    encoding: z.string().optional(),
    description: z.string().optional(),
    awsApiKey: z.string().optional(),
    awsSecret: z.string().optional(),
    tagAfterProcessing: models.TagAfterProcessingOptions$outboundSchema
      .optional(),
    processedTagKey: z.string().optional(),
    processedTagValue: z.string().optional(),
  });

export function inputCrowdstrikeSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputCrowdstrikeSendToRoutesFalseWithConnectionsConstraint:
    InputCrowdstrikeSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputCrowdstrikeSendToRoutesFalseWithConnectionsConstraint$outboundSchema
      .parse(inputCrowdstrikeSendToRoutesFalseWithConnectionsConstraint),
  );
}

/** @internal */
export type InputCrowdstrikeSendToRoutesTrueWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    queueName: string;
    fileFilter: string;
    awsAccountId?: string | undefined;
    awsAuthenticationMethod: string;
    awsSecretKey?: string | undefined;
    region?: string | undefined;
    endpoint?: string | undefined;
    signatureVersion: string;
    reuseConnections: boolean;
    rejectUnauthorized: boolean;
    breakerRulesets?: Array<string> | undefined;
    staleChannelFlushMs: number;
    maxMessages: number;
    visibilityTimeout: number;
    numReceivers: number;
    socketTimeout: number;
    skipOnError: boolean;
    includeSqsMetadata: boolean;
    enableAssumeRole: boolean;
    assumeRoleArn?: string | undefined;
    assumeRoleExternalId?: string | undefined;
    durationSeconds: number;
    enableSQSAssumeRole: boolean;
    preprocess?:
      | models.PreprocessTypeSavedJobCollectionInput$Outbound
      | undefined;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    checkpointing?: models.CheckpointingType$Outbound | undefined;
    pollTimeout: number;
    encoding?: string | undefined;
    description?: string | undefined;
    awsApiKey?: string | undefined;
    awsSecret?: string | undefined;
    tagAfterProcessing?: string | undefined;
    processedTagKey?: string | undefined;
    processedTagValue?: string | undefined;
  };

/** @internal */
export const InputCrowdstrikeSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputCrowdstrikeSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputCrowdstrikeSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputCrowdstrikeType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    queueName: z.string(),
    fileFilter: z.string().default("/.*/"),
    awsAccountId: z.string().optional(),
    awsAuthenticationMethod: z.string().default("auto"),
    awsSecretKey: z.string().optional(),
    region: z.string().optional(),
    endpoint: z.string().optional(),
    signatureVersion: models
      .SignatureVersionOptionsS3CollectorConf$outboundSchema.default("v4"),
    reuseConnections: z.boolean().default(true),
    rejectUnauthorized: z.boolean().default(true),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    maxMessages: z.number().default(1),
    visibilityTimeout: z.number().default(21600),
    numReceivers: z.number().default(1),
    socketTimeout: z.number().default(300),
    skipOnError: z.boolean().default(false),
    includeSqsMetadata: z.boolean().default(false),
    enableAssumeRole: z.boolean().default(true),
    assumeRoleArn: z.string().optional(),
    assumeRoleExternalId: z.string().optional(),
    durationSeconds: z.number().default(3600),
    enableSQSAssumeRole: z.boolean().default(false),
    preprocess: models.PreprocessTypeSavedJobCollectionInput$outboundSchema
      .optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    checkpointing: models.CheckpointingType$outboundSchema.optional(),
    pollTimeout: z.number().default(10),
    encoding: z.string().optional(),
    description: z.string().optional(),
    awsApiKey: z.string().optional(),
    awsSecret: z.string().optional(),
    tagAfterProcessing: models.TagAfterProcessingOptions$outboundSchema
      .optional(),
    processedTagKey: z.string().optional(),
    processedTagValue: z.string().optional(),
  });

export function inputCrowdstrikeSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputCrowdstrikeSendToRoutesTrueWithConnectionsConstraint:
    InputCrowdstrikeSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputCrowdstrikeSendToRoutesTrueWithConnectionsConstraint$outboundSchema
      .parse(inputCrowdstrikeSendToRoutesTrueWithConnectionsConstraint),
  );
}

/** @internal */
export type InputCrowdstrike$Outbound =
  | InputCrowdstrikeSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputCrowdstrikeSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputCrowdstrikePqEnabledFalseWithPqConstraint$Outbound
  | InputCrowdstrikePqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputCrowdstrike$outboundSchema: z.ZodType<
  InputCrowdstrike$Outbound,
  z.ZodTypeDef,
  InputCrowdstrike
> = z.union([
  z.lazy(() =>
    InputCrowdstrikeSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputCrowdstrikeSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputCrowdstrikePqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputCrowdstrikePqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputCrowdstrikeToJSON(
  inputCrowdstrike: InputCrowdstrike,
): string {
  return JSON.stringify(
    InputCrowdstrike$outboundSchema.parse(inputCrowdstrike),
  );
}

/** @internal */
export const InputWindowsMetricsType$outboundSchema: z.ZodNativeEnum<
  typeof InputWindowsMetricsType
> = z.nativeEnum(InputWindowsMetricsType);

/** @internal */
export const InputWindowsMetricsSystemMode$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  InputWindowsMetricsSystemMode
> = openEnums.outboundSchema(InputWindowsMetricsSystemMode);

/** @internal */
export type InputWindowsMetricsSystem$Outbound = {
  mode: string;
  detail: boolean;
};

/** @internal */
export const InputWindowsMetricsSystem$outboundSchema: z.ZodType<
  InputWindowsMetricsSystem$Outbound,
  z.ZodTypeDef,
  InputWindowsMetricsSystem
> = z.object({
  mode: InputWindowsMetricsSystemMode$outboundSchema.default("basic"),
  detail: z.boolean().default(false),
});

export function inputWindowsMetricsSystemToJSON(
  inputWindowsMetricsSystem: InputWindowsMetricsSystem,
): string {
  return JSON.stringify(
    InputWindowsMetricsSystem$outboundSchema.parse(inputWindowsMetricsSystem),
  );
}

/** @internal */
export const InputWindowsMetricsCpuMode$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  InputWindowsMetricsCpuMode
> = openEnums.outboundSchema(InputWindowsMetricsCpuMode);

/** @internal */
export type InputWindowsMetricsCpu$Outbound = {
  mode: string;
  perCpu: boolean;
  detail: boolean;
  time: boolean;
};

/** @internal */
export const InputWindowsMetricsCpu$outboundSchema: z.ZodType<
  InputWindowsMetricsCpu$Outbound,
  z.ZodTypeDef,
  InputWindowsMetricsCpu
> = z.object({
  mode: InputWindowsMetricsCpuMode$outboundSchema.default("basic"),
  perCpu: z.boolean().default(false),
  detail: z.boolean().default(false),
  time: z.boolean().default(false),
});

export function inputWindowsMetricsCpuToJSON(
  inputWindowsMetricsCpu: InputWindowsMetricsCpu,
): string {
  return JSON.stringify(
    InputWindowsMetricsCpu$outboundSchema.parse(inputWindowsMetricsCpu),
  );
}

/** @internal */
export const InputWindowsMetricsMemoryMode$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  InputWindowsMetricsMemoryMode
> = openEnums.outboundSchema(InputWindowsMetricsMemoryMode);

/** @internal */
export type InputWindowsMetricsMemory$Outbound = {
  mode: string;
  detail: boolean;
};

/** @internal */
export const InputWindowsMetricsMemory$outboundSchema: z.ZodType<
  InputWindowsMetricsMemory$Outbound,
  z.ZodTypeDef,
  InputWindowsMetricsMemory
> = z.object({
  mode: InputWindowsMetricsMemoryMode$outboundSchema.default("basic"),
  detail: z.boolean().default(false),
});

export function inputWindowsMetricsMemoryToJSON(
  inputWindowsMetricsMemory: InputWindowsMetricsMemory,
): string {
  return JSON.stringify(
    InputWindowsMetricsMemory$outboundSchema.parse(inputWindowsMetricsMemory),
  );
}

/** @internal */
export const InputWindowsMetricsNetworkMode$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  InputWindowsMetricsNetworkMode
> = openEnums.outboundSchema(InputWindowsMetricsNetworkMode);

/** @internal */
export type InputWindowsMetricsNetwork$Outbound = {
  mode: string;
  detail: boolean;
  protocols: boolean;
  devices?: Array<string> | undefined;
  perInterface: boolean;
};

/** @internal */
export const InputWindowsMetricsNetwork$outboundSchema: z.ZodType<
  InputWindowsMetricsNetwork$Outbound,
  z.ZodTypeDef,
  InputWindowsMetricsNetwork
> = z.object({
  mode: InputWindowsMetricsNetworkMode$outboundSchema.default("basic"),
  detail: z.boolean().default(false),
  protocols: z.boolean().default(false),
  devices: z.array(z.string()).optional(),
  perInterface: z.boolean().default(false),
});

export function inputWindowsMetricsNetworkToJSON(
  inputWindowsMetricsNetwork: InputWindowsMetricsNetwork,
): string {
  return JSON.stringify(
    InputWindowsMetricsNetwork$outboundSchema.parse(inputWindowsMetricsNetwork),
  );
}

/** @internal */
export const InputWindowsMetricsDiskMode$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  InputWindowsMetricsDiskMode
> = openEnums.outboundSchema(InputWindowsMetricsDiskMode);

/** @internal */
export type InputWindowsMetricsDisk$Outbound = {
  mode: string;
  perVolume: boolean;
  detail: boolean;
  volumes?: Array<string> | undefined;
};

/** @internal */
export const InputWindowsMetricsDisk$outboundSchema: z.ZodType<
  InputWindowsMetricsDisk$Outbound,
  z.ZodTypeDef,
  InputWindowsMetricsDisk
> = z.object({
  mode: InputWindowsMetricsDiskMode$outboundSchema.default("basic"),
  perVolume: z.boolean().default(false),
  detail: z.boolean().default(false),
  volumes: z.array(z.string()).optional(),
});

export function inputWindowsMetricsDiskToJSON(
  inputWindowsMetricsDisk: InputWindowsMetricsDisk,
): string {
  return JSON.stringify(
    InputWindowsMetricsDisk$outboundSchema.parse(inputWindowsMetricsDisk),
  );
}

/** @internal */
export type InputWindowsMetricsCustom$Outbound = {
  system?: InputWindowsMetricsSystem$Outbound | undefined;
  cpu?: InputWindowsMetricsCpu$Outbound | undefined;
  memory?: InputWindowsMetricsMemory$Outbound | undefined;
  network?: InputWindowsMetricsNetwork$Outbound | undefined;
  disk?: InputWindowsMetricsDisk$Outbound | undefined;
};

/** @internal */
export const InputWindowsMetricsCustom$outboundSchema: z.ZodType<
  InputWindowsMetricsCustom$Outbound,
  z.ZodTypeDef,
  InputWindowsMetricsCustom
> = z.object({
  system: z.lazy(() => InputWindowsMetricsSystem$outboundSchema).optional(),
  cpu: z.lazy(() => InputWindowsMetricsCpu$outboundSchema).optional(),
  memory: z.lazy(() => InputWindowsMetricsMemory$outboundSchema).optional(),
  network: z.lazy(() => InputWindowsMetricsNetwork$outboundSchema).optional(),
  disk: z.lazy(() => InputWindowsMetricsDisk$outboundSchema).optional(),
});

export function inputWindowsMetricsCustomToJSON(
  inputWindowsMetricsCustom: InputWindowsMetricsCustom,
): string {
  return JSON.stringify(
    InputWindowsMetricsCustom$outboundSchema.parse(inputWindowsMetricsCustom),
  );
}

/** @internal */
export type InputWindowsMetricsHost$Outbound = {
  mode: string;
  custom?: InputWindowsMetricsCustom$Outbound | undefined;
};

/** @internal */
export const InputWindowsMetricsHost$outboundSchema: z.ZodType<
  InputWindowsMetricsHost$Outbound,
  z.ZodTypeDef,
  InputWindowsMetricsHost
> = z.object({
  mode: models.ModeOptionsHost$outboundSchema.default("basic"),
  custom: z.lazy(() => InputWindowsMetricsCustom$outboundSchema).optional(),
});

export function inputWindowsMetricsHostToJSON(
  inputWindowsMetricsHost: InputWindowsMetricsHost,
): string {
  return JSON.stringify(
    InputWindowsMetricsHost$outboundSchema.parse(inputWindowsMetricsHost),
  );
}

/** @internal */
export type InputWindowsMetricsPersistence$Outbound = {
  enable: boolean;
  timeWindow: string;
  maxDataSize: string;
  maxDataTime: string;
  compress: string;
  destPath: string;
};

/** @internal */
export const InputWindowsMetricsPersistence$outboundSchema: z.ZodType<
  InputWindowsMetricsPersistence$Outbound,
  z.ZodTypeDef,
  InputWindowsMetricsPersistence
> = z.object({
  enable: z.boolean().default(false),
  timeWindow: z.string().default("10m"),
  maxDataSize: z.string().default("1GB"),
  maxDataTime: z.string().default("24h"),
  compress: models.DataCompressionFormatOptionsPersistence$outboundSchema
    .default("gzip"),
  destPath: z.string().default("$CRIBL_HOME/state/windows_metrics"),
});

export function inputWindowsMetricsPersistenceToJSON(
  inputWindowsMetricsPersistence: InputWindowsMetricsPersistence,
): string {
  return JSON.stringify(
    InputWindowsMetricsPersistence$outboundSchema.parse(
      inputWindowsMetricsPersistence,
    ),
  );
}

/** @internal */
export type InputWindowsMetricsPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  interval: number;
  host?: InputWindowsMetricsHost$Outbound | undefined;
  process?: models.ProcessType$Outbound | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  persistence?: InputWindowsMetricsPersistence$Outbound | undefined;
  disableNativeModule: boolean;
  description?: string | undefined;
};

/** @internal */
export const InputWindowsMetricsPqEnabledTrueWithPqConstraint$outboundSchema:
  z.ZodType<
    InputWindowsMetricsPqEnabledTrueWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputWindowsMetricsPqEnabledTrueWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputWindowsMetricsType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    interval: z.number().default(10),
    host: z.lazy(() => InputWindowsMetricsHost$outboundSchema).optional(),
    process: models.ProcessType$outboundSchema.optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    persistence: z.lazy(() => InputWindowsMetricsPersistence$outboundSchema)
      .optional(),
    disableNativeModule: z.boolean().default(false),
    description: z.string().optional(),
  });

export function inputWindowsMetricsPqEnabledTrueWithPqConstraintToJSON(
  inputWindowsMetricsPqEnabledTrueWithPqConstraint:
    InputWindowsMetricsPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputWindowsMetricsPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputWindowsMetricsPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputWindowsMetricsPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  interval: number;
  host?: InputWindowsMetricsHost$Outbound | undefined;
  process?: models.ProcessType$Outbound | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  persistence?: InputWindowsMetricsPersistence$Outbound | undefined;
  disableNativeModule: boolean;
  description?: string | undefined;
};

/** @internal */
export const InputWindowsMetricsPqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputWindowsMetricsPqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputWindowsMetricsPqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputWindowsMetricsType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    interval: z.number().default(10),
    host: z.lazy(() => InputWindowsMetricsHost$outboundSchema).optional(),
    process: models.ProcessType$outboundSchema.optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    persistence: z.lazy(() => InputWindowsMetricsPersistence$outboundSchema)
      .optional(),
    disableNativeModule: z.boolean().default(false),
    description: z.string().optional(),
  });

export function inputWindowsMetricsPqEnabledFalseWithPqConstraintToJSON(
  inputWindowsMetricsPqEnabledFalseWithPqConstraint:
    InputWindowsMetricsPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputWindowsMetricsPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputWindowsMetricsPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputWindowsMetricsSendToRoutesFalseWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    interval: number;
    host?: InputWindowsMetricsHost$Outbound | undefined;
    process?: models.ProcessType$Outbound | undefined;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    persistence?: InputWindowsMetricsPersistence$Outbound | undefined;
    disableNativeModule: boolean;
    description?: string | undefined;
  };

/** @internal */
export const InputWindowsMetricsSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputWindowsMetricsSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputWindowsMetricsSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputWindowsMetricsType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    interval: z.number().default(10),
    host: z.lazy(() => InputWindowsMetricsHost$outboundSchema).optional(),
    process: models.ProcessType$outboundSchema.optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    persistence: z.lazy(() => InputWindowsMetricsPersistence$outboundSchema)
      .optional(),
    disableNativeModule: z.boolean().default(false),
    description: z.string().optional(),
  });

export function inputWindowsMetricsSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputWindowsMetricsSendToRoutesFalseWithConnectionsConstraint:
    InputWindowsMetricsSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputWindowsMetricsSendToRoutesFalseWithConnectionsConstraint$outboundSchema
      .parse(inputWindowsMetricsSendToRoutesFalseWithConnectionsConstraint),
  );
}

/** @internal */
export type InputWindowsMetricsSendToRoutesTrueWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    interval: number;
    host?: InputWindowsMetricsHost$Outbound | undefined;
    process?: models.ProcessType$Outbound | undefined;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    persistence?: InputWindowsMetricsPersistence$Outbound | undefined;
    disableNativeModule: boolean;
    description?: string | undefined;
  };

/** @internal */
export const InputWindowsMetricsSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputWindowsMetricsSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputWindowsMetricsSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputWindowsMetricsType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    interval: z.number().default(10),
    host: z.lazy(() => InputWindowsMetricsHost$outboundSchema).optional(),
    process: models.ProcessType$outboundSchema.optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    persistence: z.lazy(() => InputWindowsMetricsPersistence$outboundSchema)
      .optional(),
    disableNativeModule: z.boolean().default(false),
    description: z.string().optional(),
  });

export function inputWindowsMetricsSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputWindowsMetricsSendToRoutesTrueWithConnectionsConstraint:
    InputWindowsMetricsSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputWindowsMetricsSendToRoutesTrueWithConnectionsConstraint$outboundSchema
      .parse(inputWindowsMetricsSendToRoutesTrueWithConnectionsConstraint),
  );
}

/** @internal */
export type InputWindowsMetrics$Outbound =
  | InputWindowsMetricsSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputWindowsMetricsSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputWindowsMetricsPqEnabledFalseWithPqConstraint$Outbound
  | InputWindowsMetricsPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputWindowsMetrics$outboundSchema: z.ZodType<
  InputWindowsMetrics$Outbound,
  z.ZodTypeDef,
  InputWindowsMetrics
> = z.union([
  z.lazy(() =>
    InputWindowsMetricsSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputWindowsMetricsSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputWindowsMetricsPqEnabledFalseWithPqConstraint$outboundSchema
  ),
  z.lazy(() => InputWindowsMetricsPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputWindowsMetricsToJSON(
  inputWindowsMetrics: InputWindowsMetrics,
): string {
  return JSON.stringify(
    InputWindowsMetrics$outboundSchema.parse(inputWindowsMetrics),
  );
}

/** @internal */
export const InputKubeEventsType$outboundSchema: z.ZodNativeEnum<
  typeof InputKubeEventsType
> = z.nativeEnum(InputKubeEventsType);

/** @internal */
export type InputKubeEventsPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  rules?: Array<models.ItemsTypeRules$Outbound> | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputKubeEventsPqEnabledTrueWithPqConstraint$outboundSchema:
  z.ZodType<
    InputKubeEventsPqEnabledTrueWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputKubeEventsPqEnabledTrueWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputKubeEventsType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    rules: z.array(models.ItemsTypeRules$outboundSchema).optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputKubeEventsPqEnabledTrueWithPqConstraintToJSON(
  inputKubeEventsPqEnabledTrueWithPqConstraint:
    InputKubeEventsPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputKubeEventsPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputKubeEventsPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputKubeEventsPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  rules?: Array<models.ItemsTypeRules$Outbound> | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputKubeEventsPqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputKubeEventsPqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputKubeEventsPqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputKubeEventsType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    rules: z.array(models.ItemsTypeRules$outboundSchema).optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputKubeEventsPqEnabledFalseWithPqConstraintToJSON(
  inputKubeEventsPqEnabledFalseWithPqConstraint:
    InputKubeEventsPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputKubeEventsPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputKubeEventsPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputKubeEventsSendToRoutesFalseWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    rules?: Array<models.ItemsTypeRules$Outbound> | undefined;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    description?: string | undefined;
  };

/** @internal */
export const InputKubeEventsSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputKubeEventsSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputKubeEventsSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputKubeEventsType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    rules: z.array(models.ItemsTypeRules$outboundSchema).optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputKubeEventsSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputKubeEventsSendToRoutesFalseWithConnectionsConstraint:
    InputKubeEventsSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputKubeEventsSendToRoutesFalseWithConnectionsConstraint$outboundSchema
      .parse(inputKubeEventsSendToRoutesFalseWithConnectionsConstraint),
  );
}

/** @internal */
export type InputKubeEventsSendToRoutesTrueWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    rules?: Array<models.ItemsTypeRules$Outbound> | undefined;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    description?: string | undefined;
  };

/** @internal */
export const InputKubeEventsSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputKubeEventsSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputKubeEventsSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputKubeEventsType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    rules: z.array(models.ItemsTypeRules$outboundSchema).optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputKubeEventsSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputKubeEventsSendToRoutesTrueWithConnectionsConstraint:
    InputKubeEventsSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputKubeEventsSendToRoutesTrueWithConnectionsConstraint$outboundSchema
      .parse(inputKubeEventsSendToRoutesTrueWithConnectionsConstraint),
  );
}

/** @internal */
export type InputKubeEvents$Outbound =
  | InputKubeEventsSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputKubeEventsSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputKubeEventsPqEnabledFalseWithPqConstraint$Outbound
  | InputKubeEventsPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputKubeEvents$outboundSchema: z.ZodType<
  InputKubeEvents$Outbound,
  z.ZodTypeDef,
  InputKubeEvents
> = z.union([
  z.lazy(() =>
    InputKubeEventsSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputKubeEventsSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputKubeEventsPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputKubeEventsPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputKubeEventsToJSON(
  inputKubeEvents: InputKubeEvents,
): string {
  return JSON.stringify(InputKubeEvents$outboundSchema.parse(inputKubeEvents));
}

/** @internal */
export const InputKubeLogsType$outboundSchema: z.ZodNativeEnum<
  typeof InputKubeLogsType
> = z.nativeEnum(InputKubeLogsType);

/** @internal */
export type InputKubeLogsRule$Outbound = {
  filter: string;
  description?: string | undefined;
};

/** @internal */
export const InputKubeLogsRule$outboundSchema: z.ZodType<
  InputKubeLogsRule$Outbound,
  z.ZodTypeDef,
  InputKubeLogsRule
> = z.object({
  filter: z.string(),
  description: z.string().optional(),
});

export function inputKubeLogsRuleToJSON(
  inputKubeLogsRule: InputKubeLogsRule,
): string {
  return JSON.stringify(
    InputKubeLogsRule$outboundSchema.parse(inputKubeLogsRule),
  );
}

/** @internal */
export type InputKubeLogsPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  interval: number;
  rules?: Array<InputKubeLogsRule$Outbound> | undefined;
  timestamps: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  persistence?: models.DiskSpoolingType$Outbound | undefined;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  enableLoadBalancing: boolean;
  description?: string | undefined;
};

/** @internal */
export const InputKubeLogsPqEnabledTrueWithPqConstraint$outboundSchema:
  z.ZodType<
    InputKubeLogsPqEnabledTrueWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputKubeLogsPqEnabledTrueWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputKubeLogsType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    interval: z.number().default(15),
    rules: z.array(z.lazy(() => InputKubeLogsRule$outboundSchema)).optional(),
    timestamps: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    persistence: models.DiskSpoolingType$outboundSchema.optional(),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    enableLoadBalancing: z.boolean().default(false),
    description: z.string().optional(),
  });

export function inputKubeLogsPqEnabledTrueWithPqConstraintToJSON(
  inputKubeLogsPqEnabledTrueWithPqConstraint:
    InputKubeLogsPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputKubeLogsPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputKubeLogsPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputKubeLogsPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  interval: number;
  rules?: Array<InputKubeLogsRule$Outbound> | undefined;
  timestamps: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  persistence?: models.DiskSpoolingType$Outbound | undefined;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  enableLoadBalancing: boolean;
  description?: string | undefined;
};

/** @internal */
export const InputKubeLogsPqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputKubeLogsPqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputKubeLogsPqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputKubeLogsType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    interval: z.number().default(15),
    rules: z.array(z.lazy(() => InputKubeLogsRule$outboundSchema)).optional(),
    timestamps: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    persistence: models.DiskSpoolingType$outboundSchema.optional(),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    enableLoadBalancing: z.boolean().default(false),
    description: z.string().optional(),
  });

export function inputKubeLogsPqEnabledFalseWithPqConstraintToJSON(
  inputKubeLogsPqEnabledFalseWithPqConstraint:
    InputKubeLogsPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputKubeLogsPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputKubeLogsPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputKubeLogsSendToRoutesFalseWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  interval: number;
  rules?: Array<InputKubeLogsRule$Outbound> | undefined;
  timestamps: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  persistence?: models.DiskSpoolingType$Outbound | undefined;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  enableLoadBalancing: boolean;
  description?: string | undefined;
};

/** @internal */
export const InputKubeLogsSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputKubeLogsSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputKubeLogsSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputKubeLogsType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    interval: z.number().default(15),
    rules: z.array(z.lazy(() => InputKubeLogsRule$outboundSchema)).optional(),
    timestamps: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    persistence: models.DiskSpoolingType$outboundSchema.optional(),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    enableLoadBalancing: z.boolean().default(false),
    description: z.string().optional(),
  });

export function inputKubeLogsSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputKubeLogsSendToRoutesFalseWithConnectionsConstraint:
    InputKubeLogsSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputKubeLogsSendToRoutesFalseWithConnectionsConstraint$outboundSchema
      .parse(inputKubeLogsSendToRoutesFalseWithConnectionsConstraint),
  );
}

/** @internal */
export type InputKubeLogsSendToRoutesTrueWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  interval: number;
  rules?: Array<InputKubeLogsRule$Outbound> | undefined;
  timestamps: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  persistence?: models.DiskSpoolingType$Outbound | undefined;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  enableLoadBalancing: boolean;
  description?: string | undefined;
};

/** @internal */
export const InputKubeLogsSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputKubeLogsSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputKubeLogsSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputKubeLogsType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    interval: z.number().default(15),
    rules: z.array(z.lazy(() => InputKubeLogsRule$outboundSchema)).optional(),
    timestamps: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    persistence: models.DiskSpoolingType$outboundSchema.optional(),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    enableLoadBalancing: z.boolean().default(false),
    description: z.string().optional(),
  });

export function inputKubeLogsSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputKubeLogsSendToRoutesTrueWithConnectionsConstraint:
    InputKubeLogsSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputKubeLogsSendToRoutesTrueWithConnectionsConstraint$outboundSchema.parse(
      inputKubeLogsSendToRoutesTrueWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputKubeLogs$Outbound =
  | InputKubeLogsSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputKubeLogsSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputKubeLogsPqEnabledFalseWithPqConstraint$Outbound
  | InputKubeLogsPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputKubeLogs$outboundSchema: z.ZodType<
  InputKubeLogs$Outbound,
  z.ZodTypeDef,
  InputKubeLogs
> = z.union([
  z.lazy(() =>
    InputKubeLogsSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputKubeLogsSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputKubeLogsPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputKubeLogsPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputKubeLogsToJSON(inputKubeLogs: InputKubeLogs): string {
  return JSON.stringify(InputKubeLogs$outboundSchema.parse(inputKubeLogs));
}

/** @internal */
export const InputKubeMetricsType$outboundSchema: z.ZodNativeEnum<
  typeof InputKubeMetricsType
> = z.nativeEnum(InputKubeMetricsType);

/** @internal */
export type InputKubeMetricsPersistence$Outbound = {
  enable: boolean;
  timeWindow: string;
  maxDataSize: string;
  maxDataTime: string;
  compress: string;
  destPath: string;
};

/** @internal */
export const InputKubeMetricsPersistence$outboundSchema: z.ZodType<
  InputKubeMetricsPersistence$Outbound,
  z.ZodTypeDef,
  InputKubeMetricsPersistence
> = z.object({
  enable: z.boolean().default(false),
  timeWindow: z.string().default("10m"),
  maxDataSize: z.string().default("1GB"),
  maxDataTime: z.string().default("24h"),
  compress: models.DataCompressionFormatOptionsPersistence$outboundSchema
    .default("gzip"),
  destPath: z.string().default("$CRIBL_HOME/state/kube_metrics"),
});

export function inputKubeMetricsPersistenceToJSON(
  inputKubeMetricsPersistence: InputKubeMetricsPersistence,
): string {
  return JSON.stringify(
    InputKubeMetricsPersistence$outboundSchema.parse(
      inputKubeMetricsPersistence,
    ),
  );
}

/** @internal */
export type InputKubeMetricsPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  interval: number;
  rules?: Array<models.ItemsTypeRules$Outbound> | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  persistence?: InputKubeMetricsPersistence$Outbound | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputKubeMetricsPqEnabledTrueWithPqConstraint$outboundSchema:
  z.ZodType<
    InputKubeMetricsPqEnabledTrueWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputKubeMetricsPqEnabledTrueWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputKubeMetricsType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    interval: z.number().default(15),
    rules: z.array(models.ItemsTypeRules$outboundSchema).optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    persistence: z.lazy(() => InputKubeMetricsPersistence$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputKubeMetricsPqEnabledTrueWithPqConstraintToJSON(
  inputKubeMetricsPqEnabledTrueWithPqConstraint:
    InputKubeMetricsPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputKubeMetricsPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputKubeMetricsPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputKubeMetricsPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  interval: number;
  rules?: Array<models.ItemsTypeRules$Outbound> | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  persistence?: InputKubeMetricsPersistence$Outbound | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputKubeMetricsPqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputKubeMetricsPqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputKubeMetricsPqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputKubeMetricsType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    interval: z.number().default(15),
    rules: z.array(models.ItemsTypeRules$outboundSchema).optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    persistence: z.lazy(() => InputKubeMetricsPersistence$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputKubeMetricsPqEnabledFalseWithPqConstraintToJSON(
  inputKubeMetricsPqEnabledFalseWithPqConstraint:
    InputKubeMetricsPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputKubeMetricsPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputKubeMetricsPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputKubeMetricsSendToRoutesFalseWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    interval: number;
    rules?: Array<models.ItemsTypeRules$Outbound> | undefined;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    persistence?: InputKubeMetricsPersistence$Outbound | undefined;
    description?: string | undefined;
  };

/** @internal */
export const InputKubeMetricsSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputKubeMetricsSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputKubeMetricsSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputKubeMetricsType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    interval: z.number().default(15),
    rules: z.array(models.ItemsTypeRules$outboundSchema).optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    persistence: z.lazy(() => InputKubeMetricsPersistence$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputKubeMetricsSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputKubeMetricsSendToRoutesFalseWithConnectionsConstraint:
    InputKubeMetricsSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputKubeMetricsSendToRoutesFalseWithConnectionsConstraint$outboundSchema
      .parse(inputKubeMetricsSendToRoutesFalseWithConnectionsConstraint),
  );
}

/** @internal */
export type InputKubeMetricsSendToRoutesTrueWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    interval: number;
    rules?: Array<models.ItemsTypeRules$Outbound> | undefined;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    persistence?: InputKubeMetricsPersistence$Outbound | undefined;
    description?: string | undefined;
  };

/** @internal */
export const InputKubeMetricsSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputKubeMetricsSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputKubeMetricsSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputKubeMetricsType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    interval: z.number().default(15),
    rules: z.array(models.ItemsTypeRules$outboundSchema).optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    persistence: z.lazy(() => InputKubeMetricsPersistence$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputKubeMetricsSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputKubeMetricsSendToRoutesTrueWithConnectionsConstraint:
    InputKubeMetricsSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputKubeMetricsSendToRoutesTrueWithConnectionsConstraint$outboundSchema
      .parse(inputKubeMetricsSendToRoutesTrueWithConnectionsConstraint),
  );
}

/** @internal */
export type InputKubeMetrics$Outbound =
  | InputKubeMetricsSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputKubeMetricsSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputKubeMetricsPqEnabledFalseWithPqConstraint$Outbound
  | InputKubeMetricsPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputKubeMetrics$outboundSchema: z.ZodType<
  InputKubeMetrics$Outbound,
  z.ZodTypeDef,
  InputKubeMetrics
> = z.union([
  z.lazy(() =>
    InputKubeMetricsSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputKubeMetricsSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputKubeMetricsPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputKubeMetricsPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputKubeMetricsToJSON(
  inputKubeMetrics: InputKubeMetrics,
): string {
  return JSON.stringify(
    InputKubeMetrics$outboundSchema.parse(inputKubeMetrics),
  );
}

/** @internal */
export const InputSystemStateType$outboundSchema: z.ZodNativeEnum<
  typeof InputSystemStateType
> = z.nativeEnum(InputSystemStateType);

/** @internal */
export type HostsFile$Outbound = {
  enable: boolean;
};

/** @internal */
export const HostsFile$outboundSchema: z.ZodType<
  HostsFile$Outbound,
  z.ZodTypeDef,
  HostsFile
> = z.object({
  enable: z.boolean().default(true),
});

export function hostsFileToJSON(hostsFile: HostsFile): string {
  return JSON.stringify(HostsFile$outboundSchema.parse(hostsFile));
}

/** @internal */
export type Interfaces$Outbound = {
  enable: boolean;
};

/** @internal */
export const Interfaces$outboundSchema: z.ZodType<
  Interfaces$Outbound,
  z.ZodTypeDef,
  Interfaces
> = z.object({
  enable: z.boolean().default(true),
});

export function interfacesToJSON(interfaces: Interfaces): string {
  return JSON.stringify(Interfaces$outboundSchema.parse(interfaces));
}

/** @internal */
export type DisksAndFileSystems$Outbound = {
  enable: boolean;
};

/** @internal */
export const DisksAndFileSystems$outboundSchema: z.ZodType<
  DisksAndFileSystems$Outbound,
  z.ZodTypeDef,
  DisksAndFileSystems
> = z.object({
  enable: z.boolean().default(true),
});

export function disksAndFileSystemsToJSON(
  disksAndFileSystems: DisksAndFileSystems,
): string {
  return JSON.stringify(
    DisksAndFileSystems$outboundSchema.parse(disksAndFileSystems),
  );
}

/** @internal */
export type HostInfo$Outbound = {
  enable: boolean;
};

/** @internal */
export const HostInfo$outboundSchema: z.ZodType<
  HostInfo$Outbound,
  z.ZodTypeDef,
  HostInfo
> = z.object({
  enable: z.boolean().default(true),
});

export function hostInfoToJSON(hostInfo: HostInfo): string {
  return JSON.stringify(HostInfo$outboundSchema.parse(hostInfo));
}

/** @internal */
export type Routes$Outbound = {
  enable: boolean;
};

/** @internal */
export const Routes$outboundSchema: z.ZodType<
  Routes$Outbound,
  z.ZodTypeDef,
  Routes
> = z.object({
  enable: z.boolean().default(true),
});

export function routesToJSON(routes: Routes): string {
  return JSON.stringify(Routes$outboundSchema.parse(routes));
}

/** @internal */
export type Dns$Outbound = {
  enable: boolean;
};

/** @internal */
export const Dns$outboundSchema: z.ZodType<Dns$Outbound, z.ZodTypeDef, Dns> = z
  .object({
    enable: z.boolean().default(true),
  });

export function dnsToJSON(dns: Dns): string {
  return JSON.stringify(Dns$outboundSchema.parse(dns));
}

/** @internal */
export type UsersAndGroups$Outbound = {
  enable: boolean;
};

/** @internal */
export const UsersAndGroups$outboundSchema: z.ZodType<
  UsersAndGroups$Outbound,
  z.ZodTypeDef,
  UsersAndGroups
> = z.object({
  enable: z.boolean().default(true),
});

export function usersAndGroupsToJSON(usersAndGroups: UsersAndGroups): string {
  return JSON.stringify(UsersAndGroups$outboundSchema.parse(usersAndGroups));
}

/** @internal */
export type Firewall$Outbound = {
  enable: boolean;
};

/** @internal */
export const Firewall$outboundSchema: z.ZodType<
  Firewall$Outbound,
  z.ZodTypeDef,
  Firewall
> = z.object({
  enable: z.boolean().default(true),
});

export function firewallToJSON(firewall: Firewall): string {
  return JSON.stringify(Firewall$outboundSchema.parse(firewall));
}

/** @internal */
export type Services$Outbound = {
  enable: boolean;
};

/** @internal */
export const Services$outboundSchema: z.ZodType<
  Services$Outbound,
  z.ZodTypeDef,
  Services
> = z.object({
  enable: z.boolean().default(true),
});

export function servicesToJSON(services: Services): string {
  return JSON.stringify(Services$outboundSchema.parse(services));
}

/** @internal */
export type ListeningPorts$Outbound = {
  enable: boolean;
};

/** @internal */
export const ListeningPorts$outboundSchema: z.ZodType<
  ListeningPorts$Outbound,
  z.ZodTypeDef,
  ListeningPorts
> = z.object({
  enable: z.boolean().default(true),
});

export function listeningPortsToJSON(listeningPorts: ListeningPorts): string {
  return JSON.stringify(ListeningPorts$outboundSchema.parse(listeningPorts));
}

/** @internal */
export type LoggedInUsers$Outbound = {
  enable: boolean;
};

/** @internal */
export const LoggedInUsers$outboundSchema: z.ZodType<
  LoggedInUsers$Outbound,
  z.ZodTypeDef,
  LoggedInUsers
> = z.object({
  enable: z.boolean().default(true),
});

export function loggedInUsersToJSON(loggedInUsers: LoggedInUsers): string {
  return JSON.stringify(LoggedInUsers$outboundSchema.parse(loggedInUsers));
}

/** @internal */
export type Collectors$Outbound = {
  hostsfile?: HostsFile$Outbound | undefined;
  interfaces?: Interfaces$Outbound | undefined;
  disk?: DisksAndFileSystems$Outbound | undefined;
  metadata?: HostInfo$Outbound | undefined;
  routes?: Routes$Outbound | undefined;
  dns?: Dns$Outbound | undefined;
  user?: UsersAndGroups$Outbound | undefined;
  firewall?: Firewall$Outbound | undefined;
  services?: Services$Outbound | undefined;
  ports?: ListeningPorts$Outbound | undefined;
  loginUsers?: LoggedInUsers$Outbound | undefined;
};

/** @internal */
export const Collectors$outboundSchema: z.ZodType<
  Collectors$Outbound,
  z.ZodTypeDef,
  Collectors
> = z.object({
  hostsfile: z.lazy(() => HostsFile$outboundSchema).optional(),
  interfaces: z.lazy(() => Interfaces$outboundSchema).optional(),
  disk: z.lazy(() => DisksAndFileSystems$outboundSchema).optional(),
  metadata: z.lazy(() => HostInfo$outboundSchema).optional(),
  routes: z.lazy(() => Routes$outboundSchema).optional(),
  dns: z.lazy(() => Dns$outboundSchema).optional(),
  user: z.lazy(() => UsersAndGroups$outboundSchema).optional(),
  firewall: z.lazy(() => Firewall$outboundSchema).optional(),
  services: z.lazy(() => Services$outboundSchema).optional(),
  ports: z.lazy(() => ListeningPorts$outboundSchema).optional(),
  loginUsers: z.lazy(() => LoggedInUsers$outboundSchema).optional(),
});

export function collectorsToJSON(collectors: Collectors): string {
  return JSON.stringify(Collectors$outboundSchema.parse(collectors));
}

/** @internal */
export const DataCompressionFormat$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  DataCompressionFormat
> = openEnums.outboundSchema(DataCompressionFormat);

/** @internal */
export type InputSystemStatePersistence$Outbound = {
  enable: boolean;
  timeWindow: string;
  maxDataSize: string;
  maxDataTime: string;
  compress: string;
  destPath: string;
};

/** @internal */
export const InputSystemStatePersistence$outboundSchema: z.ZodType<
  InputSystemStatePersistence$Outbound,
  z.ZodTypeDef,
  InputSystemStatePersistence
> = z.object({
  enable: z.boolean().default(false),
  timeWindow: z.string().default("10m"),
  maxDataSize: z.string().default("1GB"),
  maxDataTime: z.string().default("24h"),
  compress: DataCompressionFormat$outboundSchema.default("none"),
  destPath: z.string().default("$CRIBL_HOME/state/system_state"),
});

export function inputSystemStatePersistenceToJSON(
  inputSystemStatePersistence: InputSystemStatePersistence,
): string {
  return JSON.stringify(
    InputSystemStatePersistence$outboundSchema.parse(
      inputSystemStatePersistence,
    ),
  );
}

/** @internal */
export type InputSystemStatePqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  interval: number;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  collectors?: Collectors$Outbound | undefined;
  persistence?: InputSystemStatePersistence$Outbound | undefined;
  disableNativeModule: boolean;
  description?: string | undefined;
};

/** @internal */
export const InputSystemStatePqEnabledTrueWithPqConstraint$outboundSchema:
  z.ZodType<
    InputSystemStatePqEnabledTrueWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputSystemStatePqEnabledTrueWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputSystemStateType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    interval: z.number().default(300),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    collectors: z.lazy(() => Collectors$outboundSchema).optional(),
    persistence: z.lazy(() => InputSystemStatePersistence$outboundSchema)
      .optional(),
    disableNativeModule: z.boolean().default(false),
    description: z.string().optional(),
  });

export function inputSystemStatePqEnabledTrueWithPqConstraintToJSON(
  inputSystemStatePqEnabledTrueWithPqConstraint:
    InputSystemStatePqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputSystemStatePqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputSystemStatePqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputSystemStatePqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  interval: number;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  collectors?: Collectors$Outbound | undefined;
  persistence?: InputSystemStatePersistence$Outbound | undefined;
  disableNativeModule: boolean;
  description?: string | undefined;
};

/** @internal */
export const InputSystemStatePqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputSystemStatePqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputSystemStatePqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputSystemStateType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    interval: z.number().default(300),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    collectors: z.lazy(() => Collectors$outboundSchema).optional(),
    persistence: z.lazy(() => InputSystemStatePersistence$outboundSchema)
      .optional(),
    disableNativeModule: z.boolean().default(false),
    description: z.string().optional(),
  });

export function inputSystemStatePqEnabledFalseWithPqConstraintToJSON(
  inputSystemStatePqEnabledFalseWithPqConstraint:
    InputSystemStatePqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputSystemStatePqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputSystemStatePqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputSystemStateSendToRoutesFalseWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    interval: number;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    collectors?: Collectors$Outbound | undefined;
    persistence?: InputSystemStatePersistence$Outbound | undefined;
    disableNativeModule: boolean;
    description?: string | undefined;
  };

/** @internal */
export const InputSystemStateSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputSystemStateSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputSystemStateSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputSystemStateType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    interval: z.number().default(300),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    collectors: z.lazy(() => Collectors$outboundSchema).optional(),
    persistence: z.lazy(() => InputSystemStatePersistence$outboundSchema)
      .optional(),
    disableNativeModule: z.boolean().default(false),
    description: z.string().optional(),
  });

export function inputSystemStateSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputSystemStateSendToRoutesFalseWithConnectionsConstraint:
    InputSystemStateSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputSystemStateSendToRoutesFalseWithConnectionsConstraint$outboundSchema
      .parse(inputSystemStateSendToRoutesFalseWithConnectionsConstraint),
  );
}

/** @internal */
export type InputSystemStateSendToRoutesTrueWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    interval: number;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    collectors?: Collectors$Outbound | undefined;
    persistence?: InputSystemStatePersistence$Outbound | undefined;
    disableNativeModule: boolean;
    description?: string | undefined;
  };

/** @internal */
export const InputSystemStateSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputSystemStateSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputSystemStateSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputSystemStateType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    interval: z.number().default(300),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    collectors: z.lazy(() => Collectors$outboundSchema).optional(),
    persistence: z.lazy(() => InputSystemStatePersistence$outboundSchema)
      .optional(),
    disableNativeModule: z.boolean().default(false),
    description: z.string().optional(),
  });

export function inputSystemStateSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputSystemStateSendToRoutesTrueWithConnectionsConstraint:
    InputSystemStateSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputSystemStateSendToRoutesTrueWithConnectionsConstraint$outboundSchema
      .parse(inputSystemStateSendToRoutesTrueWithConnectionsConstraint),
  );
}

/** @internal */
export type InputSystemState$Outbound =
  | InputSystemStateSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputSystemStateSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputSystemStatePqEnabledFalseWithPqConstraint$Outbound
  | InputSystemStatePqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputSystemState$outboundSchema: z.ZodType<
  InputSystemState$Outbound,
  z.ZodTypeDef,
  InputSystemState
> = z.union([
  z.lazy(() =>
    InputSystemStateSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputSystemStateSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputSystemStatePqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputSystemStatePqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputSystemStateToJSON(
  inputSystemState: InputSystemState,
): string {
  return JSON.stringify(
    InputSystemState$outboundSchema.parse(inputSystemState),
  );
}

/** @internal */
export const InputSystemMetricsType$outboundSchema: z.ZodNativeEnum<
  typeof InputSystemMetricsType
> = z.nativeEnum(InputSystemMetricsType);

/** @internal */
export const InputSystemMetricsSystemMode$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  InputSystemMetricsSystemMode
> = openEnums.outboundSchema(InputSystemMetricsSystemMode);

/** @internal */
export type InputSystemMetricsSystem$Outbound = {
  mode: string;
  processes: boolean;
};

/** @internal */
export const InputSystemMetricsSystem$outboundSchema: z.ZodType<
  InputSystemMetricsSystem$Outbound,
  z.ZodTypeDef,
  InputSystemMetricsSystem
> = z.object({
  mode: InputSystemMetricsSystemMode$outboundSchema.default("basic"),
  processes: z.boolean().default(false),
});

export function inputSystemMetricsSystemToJSON(
  inputSystemMetricsSystem: InputSystemMetricsSystem,
): string {
  return JSON.stringify(
    InputSystemMetricsSystem$outboundSchema.parse(inputSystemMetricsSystem),
  );
}

/** @internal */
export const InputSystemMetricsCpuMode$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  InputSystemMetricsCpuMode
> = openEnums.outboundSchema(InputSystemMetricsCpuMode);

/** @internal */
export type InputSystemMetricsCpu$Outbound = {
  mode: string;
  perCpu: boolean;
  detail: boolean;
  time: boolean;
};

/** @internal */
export const InputSystemMetricsCpu$outboundSchema: z.ZodType<
  InputSystemMetricsCpu$Outbound,
  z.ZodTypeDef,
  InputSystemMetricsCpu
> = z.object({
  mode: InputSystemMetricsCpuMode$outboundSchema.default("basic"),
  perCpu: z.boolean().default(false),
  detail: z.boolean().default(false),
  time: z.boolean().default(false),
});

export function inputSystemMetricsCpuToJSON(
  inputSystemMetricsCpu: InputSystemMetricsCpu,
): string {
  return JSON.stringify(
    InputSystemMetricsCpu$outboundSchema.parse(inputSystemMetricsCpu),
  );
}

/** @internal */
export const InputSystemMetricsMemoryMode$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  InputSystemMetricsMemoryMode
> = openEnums.outboundSchema(InputSystemMetricsMemoryMode);

/** @internal */
export type InputSystemMetricsMemory$Outbound = {
  mode: string;
  detail: boolean;
};

/** @internal */
export const InputSystemMetricsMemory$outboundSchema: z.ZodType<
  InputSystemMetricsMemory$Outbound,
  z.ZodTypeDef,
  InputSystemMetricsMemory
> = z.object({
  mode: InputSystemMetricsMemoryMode$outboundSchema.default("basic"),
  detail: z.boolean().default(false),
});

export function inputSystemMetricsMemoryToJSON(
  inputSystemMetricsMemory: InputSystemMetricsMemory,
): string {
  return JSON.stringify(
    InputSystemMetricsMemory$outboundSchema.parse(inputSystemMetricsMemory),
  );
}

/** @internal */
export const InputSystemMetricsNetworkMode$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  InputSystemMetricsNetworkMode
> = openEnums.outboundSchema(InputSystemMetricsNetworkMode);

/** @internal */
export type InputSystemMetricsNetwork$Outbound = {
  mode: string;
  detail: boolean;
  protocols: boolean;
  devices?: Array<string> | undefined;
  perInterface: boolean;
};

/** @internal */
export const InputSystemMetricsNetwork$outboundSchema: z.ZodType<
  InputSystemMetricsNetwork$Outbound,
  z.ZodTypeDef,
  InputSystemMetricsNetwork
> = z.object({
  mode: InputSystemMetricsNetworkMode$outboundSchema.default("basic"),
  detail: z.boolean().default(false),
  protocols: z.boolean().default(false),
  devices: z.array(z.string()).optional(),
  perInterface: z.boolean().default(false),
});

export function inputSystemMetricsNetworkToJSON(
  inputSystemMetricsNetwork: InputSystemMetricsNetwork,
): string {
  return JSON.stringify(
    InputSystemMetricsNetwork$outboundSchema.parse(inputSystemMetricsNetwork),
  );
}

/** @internal */
export const InputSystemMetricsDiskMode$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  InputSystemMetricsDiskMode
> = openEnums.outboundSchema(InputSystemMetricsDiskMode);

/** @internal */
export type InputSystemMetricsDisk$Outbound = {
  mode: string;
  detail: boolean;
  inodes: boolean;
  devices?: Array<string> | undefined;
  mountpoints?: Array<string> | undefined;
  fstypes?: Array<string> | undefined;
  perDevice: boolean;
};

/** @internal */
export const InputSystemMetricsDisk$outboundSchema: z.ZodType<
  InputSystemMetricsDisk$Outbound,
  z.ZodTypeDef,
  InputSystemMetricsDisk
> = z.object({
  mode: InputSystemMetricsDiskMode$outboundSchema.default("basic"),
  detail: z.boolean().default(false),
  inodes: z.boolean().default(false),
  devices: z.array(z.string()).optional(),
  mountpoints: z.array(z.string()).optional(),
  fstypes: z.array(z.string()).optional(),
  perDevice: z.boolean().default(false),
});

export function inputSystemMetricsDiskToJSON(
  inputSystemMetricsDisk: InputSystemMetricsDisk,
): string {
  return JSON.stringify(
    InputSystemMetricsDisk$outboundSchema.parse(inputSystemMetricsDisk),
  );
}

/** @internal */
export type InputSystemMetricsCustom$Outbound = {
  system?: InputSystemMetricsSystem$Outbound | undefined;
  cpu?: InputSystemMetricsCpu$Outbound | undefined;
  memory?: InputSystemMetricsMemory$Outbound | undefined;
  network?: InputSystemMetricsNetwork$Outbound | undefined;
  disk?: InputSystemMetricsDisk$Outbound | undefined;
};

/** @internal */
export const InputSystemMetricsCustom$outboundSchema: z.ZodType<
  InputSystemMetricsCustom$Outbound,
  z.ZodTypeDef,
  InputSystemMetricsCustom
> = z.object({
  system: z.lazy(() => InputSystemMetricsSystem$outboundSchema).optional(),
  cpu: z.lazy(() => InputSystemMetricsCpu$outboundSchema).optional(),
  memory: z.lazy(() => InputSystemMetricsMemory$outboundSchema).optional(),
  network: z.lazy(() => InputSystemMetricsNetwork$outboundSchema).optional(),
  disk: z.lazy(() => InputSystemMetricsDisk$outboundSchema).optional(),
});

export function inputSystemMetricsCustomToJSON(
  inputSystemMetricsCustom: InputSystemMetricsCustom,
): string {
  return JSON.stringify(
    InputSystemMetricsCustom$outboundSchema.parse(inputSystemMetricsCustom),
  );
}

/** @internal */
export type InputSystemMetricsHost$Outbound = {
  mode: string;
  custom?: InputSystemMetricsCustom$Outbound | undefined;
};

/** @internal */
export const InputSystemMetricsHost$outboundSchema: z.ZodType<
  InputSystemMetricsHost$Outbound,
  z.ZodTypeDef,
  InputSystemMetricsHost
> = z.object({
  mode: models.ModeOptionsHost$outboundSchema.default("basic"),
  custom: z.lazy(() => InputSystemMetricsCustom$outboundSchema).optional(),
});

export function inputSystemMetricsHostToJSON(
  inputSystemMetricsHost: InputSystemMetricsHost,
): string {
  return JSON.stringify(
    InputSystemMetricsHost$outboundSchema.parse(inputSystemMetricsHost),
  );
}

/** @internal */
export const ContainerMode$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  ContainerMode
> = openEnums.outboundSchema(ContainerMode);

/** @internal */
export type InputSystemMetricsFilter$Outbound = {
  expr: string;
};

/** @internal */
export const InputSystemMetricsFilter$outboundSchema: z.ZodType<
  InputSystemMetricsFilter$Outbound,
  z.ZodTypeDef,
  InputSystemMetricsFilter
> = z.object({
  expr: z.string(),
});

export function inputSystemMetricsFilterToJSON(
  inputSystemMetricsFilter: InputSystemMetricsFilter,
): string {
  return JSON.stringify(
    InputSystemMetricsFilter$outboundSchema.parse(inputSystemMetricsFilter),
  );
}

/** @internal */
export type Container$Outbound = {
  mode: string;
  dockerSocket?: Array<string> | undefined;
  dockerTimeout: number;
  filters?: Array<InputSystemMetricsFilter$Outbound> | undefined;
  allContainers: boolean;
  perDevice: boolean;
  detail: boolean;
};

/** @internal */
export const Container$outboundSchema: z.ZodType<
  Container$Outbound,
  z.ZodTypeDef,
  Container
> = z.object({
  mode: ContainerMode$outboundSchema.default("basic"),
  dockerSocket: z.array(z.string()).optional(),
  dockerTimeout: z.number().default(5),
  filters: z.array(z.lazy(() => InputSystemMetricsFilter$outboundSchema))
    .optional(),
  allContainers: z.boolean().default(false),
  perDevice: z.boolean().default(false),
  detail: z.boolean().default(false),
});

export function containerToJSON(container: Container): string {
  return JSON.stringify(Container$outboundSchema.parse(container));
}

/** @internal */
export type InputSystemMetricsPersistence$Outbound = {
  enable: boolean;
  timeWindow: string;
  maxDataSize: string;
  maxDataTime: string;
  compress: string;
  destPath: string;
};

/** @internal */
export const InputSystemMetricsPersistence$outboundSchema: z.ZodType<
  InputSystemMetricsPersistence$Outbound,
  z.ZodTypeDef,
  InputSystemMetricsPersistence
> = z.object({
  enable: z.boolean().default(false),
  timeWindow: z.string().default("10m"),
  maxDataSize: z.string().default("1GB"),
  maxDataTime: z.string().default("24h"),
  compress: models.DataCompressionFormatOptionsPersistence$outboundSchema
    .default("gzip"),
  destPath: z.string().default("$CRIBL_HOME/state/system_metrics"),
});

export function inputSystemMetricsPersistenceToJSON(
  inputSystemMetricsPersistence: InputSystemMetricsPersistence,
): string {
  return JSON.stringify(
    InputSystemMetricsPersistence$outboundSchema.parse(
      inputSystemMetricsPersistence,
    ),
  );
}

/** @internal */
export type InputSystemMetricsPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  interval: number;
  host?: InputSystemMetricsHost$Outbound | undefined;
  process?: models.ProcessType$Outbound | undefined;
  container?: Container$Outbound | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  persistence?: InputSystemMetricsPersistence$Outbound | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputSystemMetricsPqEnabledTrueWithPqConstraint$outboundSchema:
  z.ZodType<
    InputSystemMetricsPqEnabledTrueWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputSystemMetricsPqEnabledTrueWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputSystemMetricsType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    interval: z.number().default(10),
    host: z.lazy(() => InputSystemMetricsHost$outboundSchema).optional(),
    process: models.ProcessType$outboundSchema.optional(),
    container: z.lazy(() => Container$outboundSchema).optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    persistence: z.lazy(() => InputSystemMetricsPersistence$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputSystemMetricsPqEnabledTrueWithPqConstraintToJSON(
  inputSystemMetricsPqEnabledTrueWithPqConstraint:
    InputSystemMetricsPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputSystemMetricsPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputSystemMetricsPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputSystemMetricsPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  interval: number;
  host?: InputSystemMetricsHost$Outbound | undefined;
  process?: models.ProcessType$Outbound | undefined;
  container?: Container$Outbound | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  persistence?: InputSystemMetricsPersistence$Outbound | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputSystemMetricsPqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputSystemMetricsPqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputSystemMetricsPqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputSystemMetricsType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    interval: z.number().default(10),
    host: z.lazy(() => InputSystemMetricsHost$outboundSchema).optional(),
    process: models.ProcessType$outboundSchema.optional(),
    container: z.lazy(() => Container$outboundSchema).optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    persistence: z.lazy(() => InputSystemMetricsPersistence$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputSystemMetricsPqEnabledFalseWithPqConstraintToJSON(
  inputSystemMetricsPqEnabledFalseWithPqConstraint:
    InputSystemMetricsPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputSystemMetricsPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputSystemMetricsPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputSystemMetricsSendToRoutesFalseWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    interval: number;
    host?: InputSystemMetricsHost$Outbound | undefined;
    process?: models.ProcessType$Outbound | undefined;
    container?: Container$Outbound | undefined;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    persistence?: InputSystemMetricsPersistence$Outbound | undefined;
    description?: string | undefined;
  };

/** @internal */
export const InputSystemMetricsSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputSystemMetricsSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputSystemMetricsSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputSystemMetricsType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    interval: z.number().default(10),
    host: z.lazy(() => InputSystemMetricsHost$outboundSchema).optional(),
    process: models.ProcessType$outboundSchema.optional(),
    container: z.lazy(() => Container$outboundSchema).optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    persistence: z.lazy(() => InputSystemMetricsPersistence$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputSystemMetricsSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputSystemMetricsSendToRoutesFalseWithConnectionsConstraint:
    InputSystemMetricsSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputSystemMetricsSendToRoutesFalseWithConnectionsConstraint$outboundSchema
      .parse(inputSystemMetricsSendToRoutesFalseWithConnectionsConstraint),
  );
}

/** @internal */
export type InputSystemMetricsSendToRoutesTrueWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    interval: number;
    host?: InputSystemMetricsHost$Outbound | undefined;
    process?: models.ProcessType$Outbound | undefined;
    container?: Container$Outbound | undefined;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    persistence?: InputSystemMetricsPersistence$Outbound | undefined;
    description?: string | undefined;
  };

/** @internal */
export const InputSystemMetricsSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputSystemMetricsSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputSystemMetricsSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputSystemMetricsType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    interval: z.number().default(10),
    host: z.lazy(() => InputSystemMetricsHost$outboundSchema).optional(),
    process: models.ProcessType$outboundSchema.optional(),
    container: z.lazy(() => Container$outboundSchema).optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    persistence: z.lazy(() => InputSystemMetricsPersistence$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputSystemMetricsSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputSystemMetricsSendToRoutesTrueWithConnectionsConstraint:
    InputSystemMetricsSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputSystemMetricsSendToRoutesTrueWithConnectionsConstraint$outboundSchema
      .parse(inputSystemMetricsSendToRoutesTrueWithConnectionsConstraint),
  );
}

/** @internal */
export type InputSystemMetrics$Outbound =
  | InputSystemMetricsSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputSystemMetricsSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputSystemMetricsPqEnabledFalseWithPqConstraint$Outbound
  | InputSystemMetricsPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputSystemMetrics$outboundSchema: z.ZodType<
  InputSystemMetrics$Outbound,
  z.ZodTypeDef,
  InputSystemMetrics
> = z.union([
  z.lazy(() =>
    InputSystemMetricsSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputSystemMetricsSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputSystemMetricsPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputSystemMetricsPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputSystemMetricsToJSON(
  inputSystemMetrics: InputSystemMetrics,
): string {
  return JSON.stringify(
    InputSystemMetrics$outboundSchema.parse(inputSystemMetrics),
  );
}

/** @internal */
export const InputTcpjsonType$outboundSchema: z.ZodNativeEnum<
  typeof InputTcpjsonType
> = z.nativeEnum(InputTcpjsonType);

/** @internal */
export type InputTcpjsonPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  ipWhitelistRegex: string;
  maxActiveCxn: number;
  socketIdleTimeout: number;
  socketEndingMaxWait: number;
  socketMaxLifespan: number;
  enableProxyHeader: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  enableLoadBalancing: boolean;
  authType: string;
  description?: string | undefined;
  authToken: string;
  textSecret?: string | undefined;
};

/** @internal */
export const InputTcpjsonPqEnabledTrueWithPqConstraint$outboundSchema:
  z.ZodType<
    InputTcpjsonPqEnabledTrueWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputTcpjsonPqEnabledTrueWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputTcpjsonType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    ipWhitelistRegex: z.string().default("/.*/"),
    maxActiveCxn: z.number().default(1000),
    socketIdleTimeout: z.number().default(0),
    socketEndingMaxWait: z.number().default(30),
    socketMaxLifespan: z.number().default(0),
    enableProxyHeader: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    enableLoadBalancing: z.boolean().default(false),
    authType: models.AuthenticationMethodOptionsAuthTokensItems$outboundSchema
      .default("manual"),
    description: z.string().optional(),
    authToken: z.string().default(""),
    textSecret: z.string().optional(),
  });

export function inputTcpjsonPqEnabledTrueWithPqConstraintToJSON(
  inputTcpjsonPqEnabledTrueWithPqConstraint:
    InputTcpjsonPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputTcpjsonPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputTcpjsonPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputTcpjsonPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  ipWhitelistRegex: string;
  maxActiveCxn: number;
  socketIdleTimeout: number;
  socketEndingMaxWait: number;
  socketMaxLifespan: number;
  enableProxyHeader: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  enableLoadBalancing: boolean;
  authType: string;
  description?: string | undefined;
  authToken: string;
  textSecret?: string | undefined;
};

/** @internal */
export const InputTcpjsonPqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputTcpjsonPqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputTcpjsonPqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputTcpjsonType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    ipWhitelistRegex: z.string().default("/.*/"),
    maxActiveCxn: z.number().default(1000),
    socketIdleTimeout: z.number().default(0),
    socketEndingMaxWait: z.number().default(30),
    socketMaxLifespan: z.number().default(0),
    enableProxyHeader: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    enableLoadBalancing: z.boolean().default(false),
    authType: models.AuthenticationMethodOptionsAuthTokensItems$outboundSchema
      .default("manual"),
    description: z.string().optional(),
    authToken: z.string().default(""),
    textSecret: z.string().optional(),
  });

export function inputTcpjsonPqEnabledFalseWithPqConstraintToJSON(
  inputTcpjsonPqEnabledFalseWithPqConstraint:
    InputTcpjsonPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputTcpjsonPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputTcpjsonPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputTcpjsonSendToRoutesFalseWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  host: string;
  port: number;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  ipWhitelistRegex: string;
  maxActiveCxn: number;
  socketIdleTimeout: number;
  socketEndingMaxWait: number;
  socketMaxLifespan: number;
  enableProxyHeader: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  enableLoadBalancing: boolean;
  authType: string;
  description?: string | undefined;
  authToken: string;
  textSecret?: string | undefined;
};

/** @internal */
export const InputTcpjsonSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputTcpjsonSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputTcpjsonSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputTcpjsonType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    ipWhitelistRegex: z.string().default("/.*/"),
    maxActiveCxn: z.number().default(1000),
    socketIdleTimeout: z.number().default(0),
    socketEndingMaxWait: z.number().default(30),
    socketMaxLifespan: z.number().default(0),
    enableProxyHeader: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    enableLoadBalancing: z.boolean().default(false),
    authType: models.AuthenticationMethodOptionsAuthTokensItems$outboundSchema
      .default("manual"),
    description: z.string().optional(),
    authToken: z.string().default(""),
    textSecret: z.string().optional(),
  });

export function inputTcpjsonSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputTcpjsonSendToRoutesFalseWithConnectionsConstraint:
    InputTcpjsonSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputTcpjsonSendToRoutesFalseWithConnectionsConstraint$outboundSchema.parse(
      inputTcpjsonSendToRoutesFalseWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputTcpjsonSendToRoutesTrueWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  host: string;
  port: number;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  ipWhitelistRegex: string;
  maxActiveCxn: number;
  socketIdleTimeout: number;
  socketEndingMaxWait: number;
  socketMaxLifespan: number;
  enableProxyHeader: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  enableLoadBalancing: boolean;
  authType: string;
  description?: string | undefined;
  authToken: string;
  textSecret?: string | undefined;
};

/** @internal */
export const InputTcpjsonSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputTcpjsonSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputTcpjsonSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputTcpjsonType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    ipWhitelistRegex: z.string().default("/.*/"),
    maxActiveCxn: z.number().default(1000),
    socketIdleTimeout: z.number().default(0),
    socketEndingMaxWait: z.number().default(30),
    socketMaxLifespan: z.number().default(0),
    enableProxyHeader: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    enableLoadBalancing: z.boolean().default(false),
    authType: models.AuthenticationMethodOptionsAuthTokensItems$outboundSchema
      .default("manual"),
    description: z.string().optional(),
    authToken: z.string().default(""),
    textSecret: z.string().optional(),
  });

export function inputTcpjsonSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputTcpjsonSendToRoutesTrueWithConnectionsConstraint:
    InputTcpjsonSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputTcpjsonSendToRoutesTrueWithConnectionsConstraint$outboundSchema.parse(
      inputTcpjsonSendToRoutesTrueWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputTcpjson$Outbound =
  | InputTcpjsonSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputTcpjsonSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputTcpjsonPqEnabledFalseWithPqConstraint$Outbound
  | InputTcpjsonPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputTcpjson$outboundSchema: z.ZodType<
  InputTcpjson$Outbound,
  z.ZodTypeDef,
  InputTcpjson
> = z.union([
  z.lazy(() =>
    InputTcpjsonSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputTcpjsonSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputTcpjsonPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputTcpjsonPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputTcpjsonToJSON(inputTcpjson: InputTcpjson): string {
  return JSON.stringify(InputTcpjson$outboundSchema.parse(inputTcpjson));
}

/** @internal */
export const InputCriblLakeHttpType$outboundSchema: z.ZodNativeEnum<
  typeof InputCriblLakeHttpType
> = z.nativeEnum(InputCriblLakeHttpType);

/** @internal */
export type SplunkHecMetadata$Outbound = {
  enabled?: boolean | undefined;
  defaultDataset?: string | undefined;
  allowedIndexesAtToken?: Array<string> | undefined;
};

/** @internal */
export const SplunkHecMetadata$outboundSchema: z.ZodType<
  SplunkHecMetadata$Outbound,
  z.ZodTypeDef,
  SplunkHecMetadata
> = z.object({
  enabled: z.boolean().optional(),
  defaultDataset: z.string().optional(),
  allowedIndexesAtToken: z.array(z.string()).optional(),
});

export function splunkHecMetadataToJSON(
  splunkHecMetadata: SplunkHecMetadata,
): string {
  return JSON.stringify(
    SplunkHecMetadata$outboundSchema.parse(splunkHecMetadata),
  );
}

/** @internal */
export type ElasticsearchMetadata$Outbound = {
  enabled?: boolean | undefined;
  defaultDataset?: string | undefined;
};

/** @internal */
export const ElasticsearchMetadata$outboundSchema: z.ZodType<
  ElasticsearchMetadata$Outbound,
  z.ZodTypeDef,
  ElasticsearchMetadata
> = z.object({
  enabled: z.boolean().optional(),
  defaultDataset: z.string().optional(),
});

export function elasticsearchMetadataToJSON(
  elasticsearchMetadata: ElasticsearchMetadata,
): string {
  return JSON.stringify(
    ElasticsearchMetadata$outboundSchema.parse(elasticsearchMetadata),
  );
}

/** @internal */
export type AuthTokensExt$Outbound = {
  token: string;
  description?: string | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  splunkHecMetadata?: SplunkHecMetadata$Outbound | undefined;
  elasticsearchMetadata?: ElasticsearchMetadata$Outbound | undefined;
};

/** @internal */
export const AuthTokensExt$outboundSchema: z.ZodType<
  AuthTokensExt$Outbound,
  z.ZodTypeDef,
  AuthTokensExt
> = z.object({
  token: z.string(),
  description: z.string().optional(),
  metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
    .optional(),
  splunkHecMetadata: z.lazy(() => SplunkHecMetadata$outboundSchema).optional(),
  elasticsearchMetadata: z.lazy(() => ElasticsearchMetadata$outboundSchema)
    .optional(),
});

export function authTokensExtToJSON(authTokensExt: AuthTokensExt): string {
  return JSON.stringify(AuthTokensExt$outboundSchema.parse(authTokensExt));
}

/** @internal */
export type InputCriblLakeHttpPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  authTokens?: Array<string> | undefined;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  activityLogSampleRate: number;
  requestTimeout: number;
  socketTimeout: number;
  keepAliveTimeout: number;
  enableHealthCheck: boolean;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  criblAPI: string;
  elasticAPI: string;
  splunkHecAPI: string;
  splunkHecAcks: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  authTokensExt?: Array<AuthTokensExt$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputCriblLakeHttpPqEnabledTrueWithPqConstraint$outboundSchema:
  z.ZodType<
    InputCriblLakeHttpPqEnabledTrueWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputCriblLakeHttpPqEnabledTrueWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputCriblLakeHttpType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    authTokens: z.array(z.string()).optional(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    criblAPI: z.string().default("/cribl"),
    elasticAPI: z.string().default("/elastic"),
    splunkHecAPI: z.string().default("/services/collector"),
    splunkHecAcks: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    authTokensExt: z.array(z.lazy(() => AuthTokensExt$outboundSchema))
      .optional(),
    description: z.string().optional(),
  });

export function inputCriblLakeHttpPqEnabledTrueWithPqConstraintToJSON(
  inputCriblLakeHttpPqEnabledTrueWithPqConstraint:
    InputCriblLakeHttpPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputCriblLakeHttpPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputCriblLakeHttpPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputCriblLakeHttpPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  authTokens?: Array<string> | undefined;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  activityLogSampleRate: number;
  requestTimeout: number;
  socketTimeout: number;
  keepAliveTimeout: number;
  enableHealthCheck: boolean;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  criblAPI: string;
  elasticAPI: string;
  splunkHecAPI: string;
  splunkHecAcks: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  authTokensExt?: Array<AuthTokensExt$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputCriblLakeHttpPqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputCriblLakeHttpPqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputCriblLakeHttpPqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputCriblLakeHttpType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    authTokens: z.array(z.string()).optional(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    criblAPI: z.string().default("/cribl"),
    elasticAPI: z.string().default("/elastic"),
    splunkHecAPI: z.string().default("/services/collector"),
    splunkHecAcks: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    authTokensExt: z.array(z.lazy(() => AuthTokensExt$outboundSchema))
      .optional(),
    description: z.string().optional(),
  });

export function inputCriblLakeHttpPqEnabledFalseWithPqConstraintToJSON(
  inputCriblLakeHttpPqEnabledFalseWithPqConstraint:
    InputCriblLakeHttpPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputCriblLakeHttpPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputCriblLakeHttpPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputCriblLakeHttpSendToRoutesFalseWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    host: string;
    port: number;
    authTokens?: Array<string> | undefined;
    tls?: models.TlsSettingsServerSideType$Outbound | undefined;
    maxActiveReq: number;
    maxRequestsPerSocket: number;
    enableProxyHeader: boolean;
    captureHeaders: boolean;
    activityLogSampleRate: number;
    requestTimeout: number;
    socketTimeout: number;
    keepAliveTimeout: number;
    enableHealthCheck: boolean;
    ipAllowlistRegex: string;
    ipDenylistRegex: string;
    criblAPI: string;
    elasticAPI: string;
    splunkHecAPI: string;
    splunkHecAcks: boolean;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    authTokensExt?: Array<AuthTokensExt$Outbound> | undefined;
    description?: string | undefined;
  };

/** @internal */
export const InputCriblLakeHttpSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputCriblLakeHttpSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputCriblLakeHttpSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputCriblLakeHttpType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    authTokens: z.array(z.string()).optional(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    criblAPI: z.string().default("/cribl"),
    elasticAPI: z.string().default("/elastic"),
    splunkHecAPI: z.string().default("/services/collector"),
    splunkHecAcks: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    authTokensExt: z.array(z.lazy(() => AuthTokensExt$outboundSchema))
      .optional(),
    description: z.string().optional(),
  });

export function inputCriblLakeHttpSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputCriblLakeHttpSendToRoutesFalseWithConnectionsConstraint:
    InputCriblLakeHttpSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputCriblLakeHttpSendToRoutesFalseWithConnectionsConstraint$outboundSchema
      .parse(inputCriblLakeHttpSendToRoutesFalseWithConnectionsConstraint),
  );
}

/** @internal */
export type InputCriblLakeHttpSendToRoutesTrueWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    host: string;
    port: number;
    authTokens?: Array<string> | undefined;
    tls?: models.TlsSettingsServerSideType$Outbound | undefined;
    maxActiveReq: number;
    maxRequestsPerSocket: number;
    enableProxyHeader: boolean;
    captureHeaders: boolean;
    activityLogSampleRate: number;
    requestTimeout: number;
    socketTimeout: number;
    keepAliveTimeout: number;
    enableHealthCheck: boolean;
    ipAllowlistRegex: string;
    ipDenylistRegex: string;
    criblAPI: string;
    elasticAPI: string;
    splunkHecAPI: string;
    splunkHecAcks: boolean;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    authTokensExt?: Array<AuthTokensExt$Outbound> | undefined;
    description?: string | undefined;
  };

/** @internal */
export const InputCriblLakeHttpSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputCriblLakeHttpSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputCriblLakeHttpSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputCriblLakeHttpType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    authTokens: z.array(z.string()).optional(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    criblAPI: z.string().default("/cribl"),
    elasticAPI: z.string().default("/elastic"),
    splunkHecAPI: z.string().default("/services/collector"),
    splunkHecAcks: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    authTokensExt: z.array(z.lazy(() => AuthTokensExt$outboundSchema))
      .optional(),
    description: z.string().optional(),
  });

export function inputCriblLakeHttpSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputCriblLakeHttpSendToRoutesTrueWithConnectionsConstraint:
    InputCriblLakeHttpSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputCriblLakeHttpSendToRoutesTrueWithConnectionsConstraint$outboundSchema
      .parse(inputCriblLakeHttpSendToRoutesTrueWithConnectionsConstraint),
  );
}

/** @internal */
export type InputCriblLakeHttp$Outbound =
  | InputCriblLakeHttpSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputCriblLakeHttpSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputCriblLakeHttpPqEnabledFalseWithPqConstraint$Outbound
  | InputCriblLakeHttpPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputCriblLakeHttp$outboundSchema: z.ZodType<
  InputCriblLakeHttp$Outbound,
  z.ZodTypeDef,
  InputCriblLakeHttp
> = z.union([
  z.lazy(() =>
    InputCriblLakeHttpSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputCriblLakeHttpSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputCriblLakeHttpPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputCriblLakeHttpPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputCriblLakeHttpToJSON(
  inputCriblLakeHttp: InputCriblLakeHttp,
): string {
  return JSON.stringify(
    InputCriblLakeHttp$outboundSchema.parse(inputCriblLakeHttp),
  );
}

/** @internal */
export const InputCriblHttpType$outboundSchema: z.ZodNativeEnum<
  typeof InputCriblHttpType
> = z.nativeEnum(InputCriblHttpType);

/** @internal */
export type InputCriblHttpPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  authTokens?: Array<models.ItemsTypeAuthTokens$Outbound> | undefined;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  activityLogSampleRate: number;
  requestTimeout: number;
  socketTimeout: number;
  keepAliveTimeout: number;
  enableHealthCheck: boolean;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputCriblHttpPqEnabledTrueWithPqConstraint$outboundSchema:
  z.ZodType<
    InputCriblHttpPqEnabledTrueWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputCriblHttpPqEnabledTrueWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputCriblHttpType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    authTokens: z.array(models.ItemsTypeAuthTokens$outboundSchema).optional(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputCriblHttpPqEnabledTrueWithPqConstraintToJSON(
  inputCriblHttpPqEnabledTrueWithPqConstraint:
    InputCriblHttpPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputCriblHttpPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputCriblHttpPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputCriblHttpPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  authTokens?: Array<models.ItemsTypeAuthTokens$Outbound> | undefined;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  activityLogSampleRate: number;
  requestTimeout: number;
  socketTimeout: number;
  keepAliveTimeout: number;
  enableHealthCheck: boolean;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputCriblHttpPqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputCriblHttpPqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputCriblHttpPqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputCriblHttpType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    authTokens: z.array(models.ItemsTypeAuthTokens$outboundSchema).optional(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputCriblHttpPqEnabledFalseWithPqConstraintToJSON(
  inputCriblHttpPqEnabledFalseWithPqConstraint:
    InputCriblHttpPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputCriblHttpPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputCriblHttpPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputCriblHttpSendToRoutesFalseWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    host: string;
    port: number;
    authTokens?: Array<models.ItemsTypeAuthTokens$Outbound> | undefined;
    tls?: models.TlsSettingsServerSideType$Outbound | undefined;
    maxActiveReq: number;
    maxRequestsPerSocket: number;
    enableProxyHeader: boolean;
    captureHeaders: boolean;
    activityLogSampleRate: number;
    requestTimeout: number;
    socketTimeout: number;
    keepAliveTimeout: number;
    enableHealthCheck: boolean;
    ipAllowlistRegex: string;
    ipDenylistRegex: string;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    description?: string | undefined;
  };

/** @internal */
export const InputCriblHttpSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputCriblHttpSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputCriblHttpSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputCriblHttpType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    authTokens: z.array(models.ItemsTypeAuthTokens$outboundSchema).optional(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputCriblHttpSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputCriblHttpSendToRoutesFalseWithConnectionsConstraint:
    InputCriblHttpSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputCriblHttpSendToRoutesFalseWithConnectionsConstraint$outboundSchema
      .parse(inputCriblHttpSendToRoutesFalseWithConnectionsConstraint),
  );
}

/** @internal */
export type InputCriblHttpSendToRoutesTrueWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  host: string;
  port: number;
  authTokens?: Array<models.ItemsTypeAuthTokens$Outbound> | undefined;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  activityLogSampleRate: number;
  requestTimeout: number;
  socketTimeout: number;
  keepAliveTimeout: number;
  enableHealthCheck: boolean;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputCriblHttpSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputCriblHttpSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputCriblHttpSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputCriblHttpType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    authTokens: z.array(models.ItemsTypeAuthTokens$outboundSchema).optional(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputCriblHttpSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputCriblHttpSendToRoutesTrueWithConnectionsConstraint:
    InputCriblHttpSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputCriblHttpSendToRoutesTrueWithConnectionsConstraint$outboundSchema
      .parse(inputCriblHttpSendToRoutesTrueWithConnectionsConstraint),
  );
}

/** @internal */
export type InputCriblHttp$Outbound =
  | InputCriblHttpSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputCriblHttpSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputCriblHttpPqEnabledFalseWithPqConstraint$Outbound
  | InputCriblHttpPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputCriblHttp$outboundSchema: z.ZodType<
  InputCriblHttp$Outbound,
  z.ZodTypeDef,
  InputCriblHttp
> = z.union([
  z.lazy(() =>
    InputCriblHttpSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputCriblHttpSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputCriblHttpPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputCriblHttpPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputCriblHttpToJSON(inputCriblHttp: InputCriblHttp): string {
  return JSON.stringify(InputCriblHttp$outboundSchema.parse(inputCriblHttp));
}

/** @internal */
export const InputCriblTcpType$outboundSchema: z.ZodNativeEnum<
  typeof InputCriblTcpType
> = z.nativeEnum(InputCriblTcpType);

/** @internal */
export type InputCriblTcpPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveCxn: number;
  socketIdleTimeout: number;
  socketEndingMaxWait: number;
  socketMaxLifespan: number;
  enableProxyHeader: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  enableLoadBalancing: boolean;
  authTokens?: Array<models.ItemsTypeAuthTokens$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputCriblTcpPqEnabledTrueWithPqConstraint$outboundSchema:
  z.ZodType<
    InputCriblTcpPqEnabledTrueWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputCriblTcpPqEnabledTrueWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputCriblTcpType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveCxn: z.number().default(1000),
    socketIdleTimeout: z.number().default(0),
    socketEndingMaxWait: z.number().default(30),
    socketMaxLifespan: z.number().default(0),
    enableProxyHeader: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    enableLoadBalancing: z.boolean().default(false),
    authTokens: z.array(models.ItemsTypeAuthTokens$outboundSchema).optional(),
    description: z.string().optional(),
  });

export function inputCriblTcpPqEnabledTrueWithPqConstraintToJSON(
  inputCriblTcpPqEnabledTrueWithPqConstraint:
    InputCriblTcpPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputCriblTcpPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputCriblTcpPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputCriblTcpPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveCxn: number;
  socketIdleTimeout: number;
  socketEndingMaxWait: number;
  socketMaxLifespan: number;
  enableProxyHeader: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  enableLoadBalancing: boolean;
  authTokens?: Array<models.ItemsTypeAuthTokens$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputCriblTcpPqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputCriblTcpPqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputCriblTcpPqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputCriblTcpType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveCxn: z.number().default(1000),
    socketIdleTimeout: z.number().default(0),
    socketEndingMaxWait: z.number().default(30),
    socketMaxLifespan: z.number().default(0),
    enableProxyHeader: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    enableLoadBalancing: z.boolean().default(false),
    authTokens: z.array(models.ItemsTypeAuthTokens$outboundSchema).optional(),
    description: z.string().optional(),
  });

export function inputCriblTcpPqEnabledFalseWithPqConstraintToJSON(
  inputCriblTcpPqEnabledFalseWithPqConstraint:
    InputCriblTcpPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputCriblTcpPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputCriblTcpPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputCriblTcpSendToRoutesFalseWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  host: string;
  port: number;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveCxn: number;
  socketIdleTimeout: number;
  socketEndingMaxWait: number;
  socketMaxLifespan: number;
  enableProxyHeader: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  enableLoadBalancing: boolean;
  authTokens?: Array<models.ItemsTypeAuthTokens$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputCriblTcpSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputCriblTcpSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputCriblTcpSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputCriblTcpType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveCxn: z.number().default(1000),
    socketIdleTimeout: z.number().default(0),
    socketEndingMaxWait: z.number().default(30),
    socketMaxLifespan: z.number().default(0),
    enableProxyHeader: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    enableLoadBalancing: z.boolean().default(false),
    authTokens: z.array(models.ItemsTypeAuthTokens$outboundSchema).optional(),
    description: z.string().optional(),
  });

export function inputCriblTcpSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputCriblTcpSendToRoutesFalseWithConnectionsConstraint:
    InputCriblTcpSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputCriblTcpSendToRoutesFalseWithConnectionsConstraint$outboundSchema
      .parse(inputCriblTcpSendToRoutesFalseWithConnectionsConstraint),
  );
}

/** @internal */
export type InputCriblTcpSendToRoutesTrueWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  host: string;
  port: number;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveCxn: number;
  socketIdleTimeout: number;
  socketEndingMaxWait: number;
  socketMaxLifespan: number;
  enableProxyHeader: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  enableLoadBalancing: boolean;
  authTokens?: Array<models.ItemsTypeAuthTokens$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputCriblTcpSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputCriblTcpSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputCriblTcpSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputCriblTcpType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveCxn: z.number().default(1000),
    socketIdleTimeout: z.number().default(0),
    socketEndingMaxWait: z.number().default(30),
    socketMaxLifespan: z.number().default(0),
    enableProxyHeader: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    enableLoadBalancing: z.boolean().default(false),
    authTokens: z.array(models.ItemsTypeAuthTokens$outboundSchema).optional(),
    description: z.string().optional(),
  });

export function inputCriblTcpSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputCriblTcpSendToRoutesTrueWithConnectionsConstraint:
    InputCriblTcpSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputCriblTcpSendToRoutesTrueWithConnectionsConstraint$outboundSchema.parse(
      inputCriblTcpSendToRoutesTrueWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputCriblTcp$Outbound =
  | InputCriblTcpSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputCriblTcpSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputCriblTcpPqEnabledFalseWithPqConstraint$Outbound
  | InputCriblTcpPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputCriblTcp$outboundSchema: z.ZodType<
  InputCriblTcp$Outbound,
  z.ZodTypeDef,
  InputCriblTcp
> = z.union([
  z.lazy(() =>
    InputCriblTcpSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputCriblTcpSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputCriblTcpPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputCriblTcpPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputCriblTcpToJSON(inputCriblTcp: InputCriblTcp): string {
  return JSON.stringify(InputCriblTcp$outboundSchema.parse(inputCriblTcp));
}

/** @internal */
export const InputCriblType$outboundSchema: z.ZodNativeEnum<
  typeof InputCriblType
> = z.nativeEnum(InputCriblType);

/** @internal */
export type InputCriblPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  filter?: string | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputCriblPqEnabledTrueWithPqConstraint$outboundSchema: z.ZodType<
  InputCriblPqEnabledTrueWithPqConstraint$Outbound,
  z.ZodTypeDef,
  InputCriblPqEnabledTrueWithPqConstraint
> = z.object({
  pqEnabled: z.boolean().default(false),
  pq: models.PqType$outboundSchema.optional(),
  id: z.string(),
  type: InputCriblType$outboundSchema,
  disabled: z.boolean().default(false),
  pipeline: z.string().optional(),
  sendToRoutes: z.boolean().default(true),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
  filter: z.string().optional(),
  metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
    .optional(),
  description: z.string().optional(),
});

export function inputCriblPqEnabledTrueWithPqConstraintToJSON(
  inputCriblPqEnabledTrueWithPqConstraint:
    InputCriblPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputCriblPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputCriblPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputCriblPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  filter?: string | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputCriblPqEnabledFalseWithPqConstraint$outboundSchema: z.ZodType<
  InputCriblPqEnabledFalseWithPqConstraint$Outbound,
  z.ZodTypeDef,
  InputCriblPqEnabledFalseWithPqConstraint
> = z.object({
  pqEnabled: z.boolean().default(false),
  pq: models.PqType$outboundSchema.optional(),
  id: z.string(),
  type: InputCriblType$outboundSchema,
  disabled: z.boolean().default(false),
  pipeline: z.string().optional(),
  sendToRoutes: z.boolean().default(true),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
  filter: z.string().optional(),
  metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
    .optional(),
  description: z.string().optional(),
});

export function inputCriblPqEnabledFalseWithPqConstraintToJSON(
  inputCriblPqEnabledFalseWithPqConstraint:
    InputCriblPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputCriblPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputCriblPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputCriblSendToRoutesFalseWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  filter?: string | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputCriblSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputCriblSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputCriblSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputCriblType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    filter: z.string().optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputCriblSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputCriblSendToRoutesFalseWithConnectionsConstraint:
    InputCriblSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputCriblSendToRoutesFalseWithConnectionsConstraint$outboundSchema.parse(
      inputCriblSendToRoutesFalseWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputCriblSendToRoutesTrueWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  filter?: string | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputCriblSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputCriblSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputCriblSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputCriblType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    filter: z.string().optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputCriblSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputCriblSendToRoutesTrueWithConnectionsConstraint:
    InputCriblSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputCriblSendToRoutesTrueWithConnectionsConstraint$outboundSchema.parse(
      inputCriblSendToRoutesTrueWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputCribl$Outbound =
  | InputCriblSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputCriblSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputCriblPqEnabledFalseWithPqConstraint$Outbound
  | InputCriblPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputCribl$outboundSchema: z.ZodType<
  InputCribl$Outbound,
  z.ZodTypeDef,
  InputCribl
> = z.union([
  z.lazy(() =>
    InputCriblSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputCriblSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputCriblPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputCriblPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputCriblToJSON(inputCribl: InputCribl): string {
  return JSON.stringify(InputCribl$outboundSchema.parse(inputCribl));
}

/** @internal */
export const InputGooglePubsubType$outboundSchema: z.ZodNativeEnum<
  typeof InputGooglePubsubType
> = z.nativeEnum(InputGooglePubsubType);

/** @internal */
export type InputGooglePubsubPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  topicName: string;
  subscriptionName: string;
  monitorSubscription: boolean;
  createTopic: boolean;
  createSubscription: boolean;
  region?: string | undefined;
  googleAuthMethod: string;
  serviceAccountCredentials?: string | undefined;
  secret?: string | undefined;
  maxBacklog: number;
  concurrency: number;
  requestTimeout: number;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
  orderedDelivery: boolean;
};

/** @internal */
export const InputGooglePubsubPqEnabledTrueWithPqConstraint$outboundSchema:
  z.ZodType<
    InputGooglePubsubPqEnabledTrueWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputGooglePubsubPqEnabledTrueWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputGooglePubsubType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    topicName: z.string().default("cribl"),
    subscriptionName: z.string(),
    monitorSubscription: z.boolean().default(false),
    createTopic: z.boolean().default(false),
    createSubscription: z.boolean().default(true),
    region: z.string().optional(),
    googleAuthMethod: models.GoogleAuthenticationMethodOptions$outboundSchema
      .default("manual"),
    serviceAccountCredentials: z.string().optional(),
    secret: z.string().optional(),
    maxBacklog: z.number().default(1000),
    concurrency: z.number().default(5),
    requestTimeout: z.number().default(60000),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
    orderedDelivery: z.boolean().default(false),
  });

export function inputGooglePubsubPqEnabledTrueWithPqConstraintToJSON(
  inputGooglePubsubPqEnabledTrueWithPqConstraint:
    InputGooglePubsubPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputGooglePubsubPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputGooglePubsubPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputGooglePubsubPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  topicName: string;
  subscriptionName: string;
  monitorSubscription: boolean;
  createTopic: boolean;
  createSubscription: boolean;
  region?: string | undefined;
  googleAuthMethod: string;
  serviceAccountCredentials?: string | undefined;
  secret?: string | undefined;
  maxBacklog: number;
  concurrency: number;
  requestTimeout: number;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
  orderedDelivery: boolean;
};

/** @internal */
export const InputGooglePubsubPqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputGooglePubsubPqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputGooglePubsubPqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputGooglePubsubType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    topicName: z.string().default("cribl"),
    subscriptionName: z.string(),
    monitorSubscription: z.boolean().default(false),
    createTopic: z.boolean().default(false),
    createSubscription: z.boolean().default(true),
    region: z.string().optional(),
    googleAuthMethod: models.GoogleAuthenticationMethodOptions$outboundSchema
      .default("manual"),
    serviceAccountCredentials: z.string().optional(),
    secret: z.string().optional(),
    maxBacklog: z.number().default(1000),
    concurrency: z.number().default(5),
    requestTimeout: z.number().default(60000),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
    orderedDelivery: z.boolean().default(false),
  });

export function inputGooglePubsubPqEnabledFalseWithPqConstraintToJSON(
  inputGooglePubsubPqEnabledFalseWithPqConstraint:
    InputGooglePubsubPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputGooglePubsubPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputGooglePubsubPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputGooglePubsubSendToRoutesFalseWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    topicName: string;
    subscriptionName: string;
    monitorSubscription: boolean;
    createTopic: boolean;
    createSubscription: boolean;
    region?: string | undefined;
    googleAuthMethod: string;
    serviceAccountCredentials?: string | undefined;
    secret?: string | undefined;
    maxBacklog: number;
    concurrency: number;
    requestTimeout: number;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    description?: string | undefined;
    orderedDelivery: boolean;
  };

/** @internal */
export const InputGooglePubsubSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputGooglePubsubSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputGooglePubsubSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputGooglePubsubType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    topicName: z.string().default("cribl"),
    subscriptionName: z.string(),
    monitorSubscription: z.boolean().default(false),
    createTopic: z.boolean().default(false),
    createSubscription: z.boolean().default(true),
    region: z.string().optional(),
    googleAuthMethod: models.GoogleAuthenticationMethodOptions$outboundSchema
      .default("manual"),
    serviceAccountCredentials: z.string().optional(),
    secret: z.string().optional(),
    maxBacklog: z.number().default(1000),
    concurrency: z.number().default(5),
    requestTimeout: z.number().default(60000),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
    orderedDelivery: z.boolean().default(false),
  });

export function inputGooglePubsubSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputGooglePubsubSendToRoutesFalseWithConnectionsConstraint:
    InputGooglePubsubSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputGooglePubsubSendToRoutesFalseWithConnectionsConstraint$outboundSchema
      .parse(inputGooglePubsubSendToRoutesFalseWithConnectionsConstraint),
  );
}

/** @internal */
export type InputGooglePubsubSendToRoutesTrueWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    topicName: string;
    subscriptionName: string;
    monitorSubscription: boolean;
    createTopic: boolean;
    createSubscription: boolean;
    region?: string | undefined;
    googleAuthMethod: string;
    serviceAccountCredentials?: string | undefined;
    secret?: string | undefined;
    maxBacklog: number;
    concurrency: number;
    requestTimeout: number;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    description?: string | undefined;
    orderedDelivery: boolean;
  };

/** @internal */
export const InputGooglePubsubSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputGooglePubsubSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputGooglePubsubSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputGooglePubsubType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    topicName: z.string().default("cribl"),
    subscriptionName: z.string(),
    monitorSubscription: z.boolean().default(false),
    createTopic: z.boolean().default(false),
    createSubscription: z.boolean().default(true),
    region: z.string().optional(),
    googleAuthMethod: models.GoogleAuthenticationMethodOptions$outboundSchema
      .default("manual"),
    serviceAccountCredentials: z.string().optional(),
    secret: z.string().optional(),
    maxBacklog: z.number().default(1000),
    concurrency: z.number().default(5),
    requestTimeout: z.number().default(60000),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
    orderedDelivery: z.boolean().default(false),
  });

export function inputGooglePubsubSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputGooglePubsubSendToRoutesTrueWithConnectionsConstraint:
    InputGooglePubsubSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputGooglePubsubSendToRoutesTrueWithConnectionsConstraint$outboundSchema
      .parse(inputGooglePubsubSendToRoutesTrueWithConnectionsConstraint),
  );
}

/** @internal */
export type InputGooglePubsub$Outbound =
  | InputGooglePubsubSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputGooglePubsubSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputGooglePubsubPqEnabledFalseWithPqConstraint$Outbound
  | InputGooglePubsubPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputGooglePubsub$outboundSchema: z.ZodType<
  InputGooglePubsub$Outbound,
  z.ZodTypeDef,
  InputGooglePubsub
> = z.union([
  z.lazy(() =>
    InputGooglePubsubSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputGooglePubsubSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputGooglePubsubPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputGooglePubsubPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputGooglePubsubToJSON(
  inputGooglePubsub: InputGooglePubsub,
): string {
  return JSON.stringify(
    InputGooglePubsub$outboundSchema.parse(inputGooglePubsub),
  );
}

/** @internal */
export const InputFirehoseType$outboundSchema: z.ZodNativeEnum<
  typeof InputFirehoseType
> = z.nativeEnum(InputFirehoseType);

/** @internal */
export type InputFirehosePqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  authTokens?: Array<string> | undefined;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  activityLogSampleRate: number;
  requestTimeout: number;
  socketTimeout: number;
  keepAliveTimeout: number;
  enableHealthCheck: boolean;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputFirehosePqEnabledTrueWithPqConstraint$outboundSchema:
  z.ZodType<
    InputFirehosePqEnabledTrueWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputFirehosePqEnabledTrueWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputFirehoseType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    authTokens: z.array(z.string()).optional(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputFirehosePqEnabledTrueWithPqConstraintToJSON(
  inputFirehosePqEnabledTrueWithPqConstraint:
    InputFirehosePqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputFirehosePqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputFirehosePqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputFirehosePqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  authTokens?: Array<string> | undefined;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  activityLogSampleRate: number;
  requestTimeout: number;
  socketTimeout: number;
  keepAliveTimeout: number;
  enableHealthCheck: boolean;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputFirehosePqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputFirehosePqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputFirehosePqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputFirehoseType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    authTokens: z.array(z.string()).optional(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputFirehosePqEnabledFalseWithPqConstraintToJSON(
  inputFirehosePqEnabledFalseWithPqConstraint:
    InputFirehosePqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputFirehosePqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputFirehosePqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputFirehoseSendToRoutesFalseWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  host: string;
  port: number;
  authTokens?: Array<string> | undefined;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  activityLogSampleRate: number;
  requestTimeout: number;
  socketTimeout: number;
  keepAliveTimeout: number;
  enableHealthCheck: boolean;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputFirehoseSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputFirehoseSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputFirehoseSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputFirehoseType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    authTokens: z.array(z.string()).optional(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputFirehoseSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputFirehoseSendToRoutesFalseWithConnectionsConstraint:
    InputFirehoseSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputFirehoseSendToRoutesFalseWithConnectionsConstraint$outboundSchema
      .parse(inputFirehoseSendToRoutesFalseWithConnectionsConstraint),
  );
}

/** @internal */
export type InputFirehoseSendToRoutesTrueWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  host: string;
  port: number;
  authTokens?: Array<string> | undefined;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  activityLogSampleRate: number;
  requestTimeout: number;
  socketTimeout: number;
  keepAliveTimeout: number;
  enableHealthCheck: boolean;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputFirehoseSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputFirehoseSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputFirehoseSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputFirehoseType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    authTokens: z.array(z.string()).optional(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputFirehoseSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputFirehoseSendToRoutesTrueWithConnectionsConstraint:
    InputFirehoseSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputFirehoseSendToRoutesTrueWithConnectionsConstraint$outboundSchema.parse(
      inputFirehoseSendToRoutesTrueWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputFirehose$Outbound =
  | InputFirehoseSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputFirehoseSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputFirehosePqEnabledFalseWithPqConstraint$Outbound
  | InputFirehosePqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputFirehose$outboundSchema: z.ZodType<
  InputFirehose$Outbound,
  z.ZodTypeDef,
  InputFirehose
> = z.union([
  z.lazy(() =>
    InputFirehoseSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputFirehoseSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputFirehosePqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputFirehosePqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputFirehoseToJSON(inputFirehose: InputFirehose): string {
  return JSON.stringify(InputFirehose$outboundSchema.parse(inputFirehose));
}

/** @internal */
export const PqEnabledTrueWithPqConstraintInputExecType$outboundSchema:
  z.ZodNativeEnum<typeof PqEnabledTrueWithPqConstraintInputExecType> = z
    .nativeEnum(PqEnabledTrueWithPqConstraintInputExecType);

/** @internal */
export const PqEnabledTrueWithPqConstraintScheduleType$outboundSchema:
  z.ZodType<string, z.ZodTypeDef, PqEnabledTrueWithPqConstraintScheduleType> =
    openEnums.outboundSchema(PqEnabledTrueWithPqConstraintScheduleType);

/** @internal */
export type InputExecPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  command: string;
  retries: number;
  scheduleType: string;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
  interval: number;
  cronSchedule: string;
};

/** @internal */
export const InputExecPqEnabledTrueWithPqConstraint$outboundSchema: z.ZodType<
  InputExecPqEnabledTrueWithPqConstraint$Outbound,
  z.ZodTypeDef,
  InputExecPqEnabledTrueWithPqConstraint
> = z.object({
  pqEnabled: z.boolean().default(false),
  pq: models.PqType$outboundSchema.optional(),
  id: z.string(),
  type: PqEnabledTrueWithPqConstraintInputExecType$outboundSchema,
  disabled: z.boolean().default(false),
  pipeline: z.string().optional(),
  sendToRoutes: z.boolean().default(true),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
  command: z.string(),
  retries: z.number().default(10),
  scheduleType: PqEnabledTrueWithPqConstraintScheduleType$outboundSchema
    .default("interval"),
  breakerRulesets: z.array(z.string()).optional(),
  staleChannelFlushMs: z.number().default(10000),
  metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
    .optional(),
  description: z.string().optional(),
  interval: z.number().default(60),
  cronSchedule: z.string().default("* * * * *"),
});

export function inputExecPqEnabledTrueWithPqConstraintToJSON(
  inputExecPqEnabledTrueWithPqConstraint:
    InputExecPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputExecPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputExecPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export const PqEnabledFalseWithPqConstraintInputExecType$outboundSchema:
  z.ZodNativeEnum<typeof PqEnabledFalseWithPqConstraintInputExecType> = z
    .nativeEnum(PqEnabledFalseWithPqConstraintInputExecType);

/** @internal */
export const PqEnabledFalseWithPqConstraintScheduleType$outboundSchema:
  z.ZodType<string, z.ZodTypeDef, PqEnabledFalseWithPqConstraintScheduleType> =
    openEnums.outboundSchema(PqEnabledFalseWithPqConstraintScheduleType);

/** @internal */
export type InputExecPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  command: string;
  retries: number;
  scheduleType: string;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
  interval: number;
  cronSchedule: string;
};

/** @internal */
export const InputExecPqEnabledFalseWithPqConstraint$outboundSchema: z.ZodType<
  InputExecPqEnabledFalseWithPqConstraint$Outbound,
  z.ZodTypeDef,
  InputExecPqEnabledFalseWithPqConstraint
> = z.object({
  pqEnabled: z.boolean().default(false),
  pq: models.PqType$outboundSchema.optional(),
  id: z.string(),
  type: PqEnabledFalseWithPqConstraintInputExecType$outboundSchema,
  disabled: z.boolean().default(false),
  pipeline: z.string().optional(),
  sendToRoutes: z.boolean().default(true),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
  command: z.string(),
  retries: z.number().default(10),
  scheduleType: PqEnabledFalseWithPqConstraintScheduleType$outboundSchema
    .default("interval"),
  breakerRulesets: z.array(z.string()).optional(),
  staleChannelFlushMs: z.number().default(10000),
  metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
    .optional(),
  description: z.string().optional(),
  interval: z.number().default(60),
  cronSchedule: z.string().default("* * * * *"),
});

export function inputExecPqEnabledFalseWithPqConstraintToJSON(
  inputExecPqEnabledFalseWithPqConstraint:
    InputExecPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputExecPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputExecPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export const SendToRoutesFalseWithConnectionsConstraintInputExecType$outboundSchema:
  z.ZodNativeEnum<
    typeof SendToRoutesFalseWithConnectionsConstraintInputExecType
  > = z.nativeEnum(SendToRoutesFalseWithConnectionsConstraintInputExecType);

/** @internal */
export const SendToRoutesFalseWithConnectionsConstraintScheduleType$outboundSchema:
  z.ZodType<
    string,
    z.ZodTypeDef,
    SendToRoutesFalseWithConnectionsConstraintScheduleType
  > = openEnums.outboundSchema(
    SendToRoutesFalseWithConnectionsConstraintScheduleType,
  );

/** @internal */
export type InputExecSendToRoutesFalseWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  command: string;
  retries: number;
  scheduleType: string;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
  interval: number;
  cronSchedule: string;
};

/** @internal */
export const InputExecSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputExecSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputExecSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type:
      SendToRoutesFalseWithConnectionsConstraintInputExecType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    command: z.string(),
    retries: z.number().default(10),
    scheduleType:
      SendToRoutesFalseWithConnectionsConstraintScheduleType$outboundSchema
        .default("interval"),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
    interval: z.number().default(60),
    cronSchedule: z.string().default("* * * * *"),
  });

export function inputExecSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputExecSendToRoutesFalseWithConnectionsConstraint:
    InputExecSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputExecSendToRoutesFalseWithConnectionsConstraint$outboundSchema.parse(
      inputExecSendToRoutesFalseWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export const SendToRoutesTrueWithConnectionsConstraintInputExecType$outboundSchema:
  z.ZodNativeEnum<
    typeof SendToRoutesTrueWithConnectionsConstraintInputExecType
  > = z.nativeEnum(SendToRoutesTrueWithConnectionsConstraintInputExecType);

/** @internal */
export const SendToRoutesTrueWithConnectionsConstraintScheduleType$outboundSchema:
  z.ZodType<
    string,
    z.ZodTypeDef,
    SendToRoutesTrueWithConnectionsConstraintScheduleType
  > = openEnums.outboundSchema(
    SendToRoutesTrueWithConnectionsConstraintScheduleType,
  );

/** @internal */
export type InputExecSendToRoutesTrueWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  command: string;
  retries: number;
  scheduleType: string;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
  interval: number;
  cronSchedule: string;
};

/** @internal */
export const InputExecSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputExecSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputExecSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: SendToRoutesTrueWithConnectionsConstraintInputExecType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    command: z.string(),
    retries: z.number().default(10),
    scheduleType:
      SendToRoutesTrueWithConnectionsConstraintScheduleType$outboundSchema
        .default("interval"),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
    interval: z.number().default(60),
    cronSchedule: z.string().default("* * * * *"),
  });

export function inputExecSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputExecSendToRoutesTrueWithConnectionsConstraint:
    InputExecSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputExecSendToRoutesTrueWithConnectionsConstraint$outboundSchema.parse(
      inputExecSendToRoutesTrueWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputExec$Outbound =
  | InputExecSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputExecSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputExecPqEnabledFalseWithPqConstraint$Outbound
  | InputExecPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputExec$outboundSchema: z.ZodType<
  InputExec$Outbound,
  z.ZodTypeDef,
  InputExec
> = z.union([
  z.lazy(() =>
    InputExecSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputExecSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputExecPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputExecPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputExecToJSON(inputExec: InputExec): string {
  return JSON.stringify(InputExec$outboundSchema.parse(inputExec));
}

/** @internal */
export const InputEventhubType$outboundSchema: z.ZodNativeEnum<
  typeof InputEventhubType
> = z.nativeEnum(InputEventhubType);

/** @internal */
export type InputEventhubPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  brokers: Array<string>;
  topics: Array<string>;
  groupId: string;
  fromBeginning: boolean;
  connectionTimeout: number;
  requestTimeout: number;
  maxRetries: number;
  maxBackOff: number;
  initialBackoff: number;
  backoffRate: number;
  authenticationTimeout: number;
  reauthenticationThreshold: number;
  sasl?: models.AuthenticationType1$Outbound | undefined;
  tls?: models.TlsSettingsClientSideType$Outbound | undefined;
  sessionTimeout: number;
  rebalanceTimeout: number;
  heartbeatInterval: number;
  autoCommitInterval?: number | undefined;
  autoCommitThreshold?: number | undefined;
  maxBytesPerPartition: number;
  maxBytes: number;
  maxSocketErrors: number;
  minimizeDuplicates: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputEventhubPqEnabledTrueWithPqConstraint$outboundSchema:
  z.ZodType<
    InputEventhubPqEnabledTrueWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputEventhubPqEnabledTrueWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputEventhubType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    brokers: z.array(z.string()),
    topics: z.array(z.string()),
    groupId: z.string().default("Cribl"),
    fromBeginning: z.boolean().default(true),
    connectionTimeout: z.number().default(10000),
    requestTimeout: z.number().default(60000),
    maxRetries: z.number().default(5),
    maxBackOff: z.number().default(30000),
    initialBackoff: z.number().default(300),
    backoffRate: z.number().default(2),
    authenticationTimeout: z.number().default(10000),
    reauthenticationThreshold: z.number().default(10000),
    sasl: models.AuthenticationType1$outboundSchema.optional(),
    tls: models.TlsSettingsClientSideType$outboundSchema.optional(),
    sessionTimeout: z.number().default(30000),
    rebalanceTimeout: z.number().default(60000),
    heartbeatInterval: z.number().default(3000),
    autoCommitInterval: z.number().optional(),
    autoCommitThreshold: z.number().optional(),
    maxBytesPerPartition: z.number().default(1048576),
    maxBytes: z.number().default(10485760),
    maxSocketErrors: z.number().default(0),
    minimizeDuplicates: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputEventhubPqEnabledTrueWithPqConstraintToJSON(
  inputEventhubPqEnabledTrueWithPqConstraint:
    InputEventhubPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputEventhubPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputEventhubPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputEventhubPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  brokers: Array<string>;
  topics: Array<string>;
  groupId: string;
  fromBeginning: boolean;
  connectionTimeout: number;
  requestTimeout: number;
  maxRetries: number;
  maxBackOff: number;
  initialBackoff: number;
  backoffRate: number;
  authenticationTimeout: number;
  reauthenticationThreshold: number;
  sasl?: models.AuthenticationType1$Outbound | undefined;
  tls?: models.TlsSettingsClientSideType$Outbound | undefined;
  sessionTimeout: number;
  rebalanceTimeout: number;
  heartbeatInterval: number;
  autoCommitInterval?: number | undefined;
  autoCommitThreshold?: number | undefined;
  maxBytesPerPartition: number;
  maxBytes: number;
  maxSocketErrors: number;
  minimizeDuplicates: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputEventhubPqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputEventhubPqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputEventhubPqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputEventhubType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    brokers: z.array(z.string()),
    topics: z.array(z.string()),
    groupId: z.string().default("Cribl"),
    fromBeginning: z.boolean().default(true),
    connectionTimeout: z.number().default(10000),
    requestTimeout: z.number().default(60000),
    maxRetries: z.number().default(5),
    maxBackOff: z.number().default(30000),
    initialBackoff: z.number().default(300),
    backoffRate: z.number().default(2),
    authenticationTimeout: z.number().default(10000),
    reauthenticationThreshold: z.number().default(10000),
    sasl: models.AuthenticationType1$outboundSchema.optional(),
    tls: models.TlsSettingsClientSideType$outboundSchema.optional(),
    sessionTimeout: z.number().default(30000),
    rebalanceTimeout: z.number().default(60000),
    heartbeatInterval: z.number().default(3000),
    autoCommitInterval: z.number().optional(),
    autoCommitThreshold: z.number().optional(),
    maxBytesPerPartition: z.number().default(1048576),
    maxBytes: z.number().default(10485760),
    maxSocketErrors: z.number().default(0),
    minimizeDuplicates: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputEventhubPqEnabledFalseWithPqConstraintToJSON(
  inputEventhubPqEnabledFalseWithPqConstraint:
    InputEventhubPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputEventhubPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputEventhubPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputEventhubSendToRoutesFalseWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  brokers: Array<string>;
  topics: Array<string>;
  groupId: string;
  fromBeginning: boolean;
  connectionTimeout: number;
  requestTimeout: number;
  maxRetries: number;
  maxBackOff: number;
  initialBackoff: number;
  backoffRate: number;
  authenticationTimeout: number;
  reauthenticationThreshold: number;
  sasl?: models.AuthenticationType1$Outbound | undefined;
  tls?: models.TlsSettingsClientSideType$Outbound | undefined;
  sessionTimeout: number;
  rebalanceTimeout: number;
  heartbeatInterval: number;
  autoCommitInterval?: number | undefined;
  autoCommitThreshold?: number | undefined;
  maxBytesPerPartition: number;
  maxBytes: number;
  maxSocketErrors: number;
  minimizeDuplicates: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputEventhubSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputEventhubSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputEventhubSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputEventhubType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    brokers: z.array(z.string()),
    topics: z.array(z.string()),
    groupId: z.string().default("Cribl"),
    fromBeginning: z.boolean().default(true),
    connectionTimeout: z.number().default(10000),
    requestTimeout: z.number().default(60000),
    maxRetries: z.number().default(5),
    maxBackOff: z.number().default(30000),
    initialBackoff: z.number().default(300),
    backoffRate: z.number().default(2),
    authenticationTimeout: z.number().default(10000),
    reauthenticationThreshold: z.number().default(10000),
    sasl: models.AuthenticationType1$outboundSchema.optional(),
    tls: models.TlsSettingsClientSideType$outboundSchema.optional(),
    sessionTimeout: z.number().default(30000),
    rebalanceTimeout: z.number().default(60000),
    heartbeatInterval: z.number().default(3000),
    autoCommitInterval: z.number().optional(),
    autoCommitThreshold: z.number().optional(),
    maxBytesPerPartition: z.number().default(1048576),
    maxBytes: z.number().default(10485760),
    maxSocketErrors: z.number().default(0),
    minimizeDuplicates: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputEventhubSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputEventhubSendToRoutesFalseWithConnectionsConstraint:
    InputEventhubSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputEventhubSendToRoutesFalseWithConnectionsConstraint$outboundSchema
      .parse(inputEventhubSendToRoutesFalseWithConnectionsConstraint),
  );
}

/** @internal */
export type InputEventhubSendToRoutesTrueWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  brokers: Array<string>;
  topics: Array<string>;
  groupId: string;
  fromBeginning: boolean;
  connectionTimeout: number;
  requestTimeout: number;
  maxRetries: number;
  maxBackOff: number;
  initialBackoff: number;
  backoffRate: number;
  authenticationTimeout: number;
  reauthenticationThreshold: number;
  sasl?: models.AuthenticationType1$Outbound | undefined;
  tls?: models.TlsSettingsClientSideType$Outbound | undefined;
  sessionTimeout: number;
  rebalanceTimeout: number;
  heartbeatInterval: number;
  autoCommitInterval?: number | undefined;
  autoCommitThreshold?: number | undefined;
  maxBytesPerPartition: number;
  maxBytes: number;
  maxSocketErrors: number;
  minimizeDuplicates: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputEventhubSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputEventhubSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputEventhubSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputEventhubType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    brokers: z.array(z.string()),
    topics: z.array(z.string()),
    groupId: z.string().default("Cribl"),
    fromBeginning: z.boolean().default(true),
    connectionTimeout: z.number().default(10000),
    requestTimeout: z.number().default(60000),
    maxRetries: z.number().default(5),
    maxBackOff: z.number().default(30000),
    initialBackoff: z.number().default(300),
    backoffRate: z.number().default(2),
    authenticationTimeout: z.number().default(10000),
    reauthenticationThreshold: z.number().default(10000),
    sasl: models.AuthenticationType1$outboundSchema.optional(),
    tls: models.TlsSettingsClientSideType$outboundSchema.optional(),
    sessionTimeout: z.number().default(30000),
    rebalanceTimeout: z.number().default(60000),
    heartbeatInterval: z.number().default(3000),
    autoCommitInterval: z.number().optional(),
    autoCommitThreshold: z.number().optional(),
    maxBytesPerPartition: z.number().default(1048576),
    maxBytes: z.number().default(10485760),
    maxSocketErrors: z.number().default(0),
    minimizeDuplicates: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputEventhubSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputEventhubSendToRoutesTrueWithConnectionsConstraint:
    InputEventhubSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputEventhubSendToRoutesTrueWithConnectionsConstraint$outboundSchema.parse(
      inputEventhubSendToRoutesTrueWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputEventhub$Outbound =
  | InputEventhubSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputEventhubSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputEventhubPqEnabledFalseWithPqConstraint$Outbound
  | InputEventhubPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputEventhub$outboundSchema: z.ZodType<
  InputEventhub$Outbound,
  z.ZodTypeDef,
  InputEventhub
> = z.union([
  z.lazy(() =>
    InputEventhubSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputEventhubSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputEventhubPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputEventhubPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputEventhubToJSON(inputEventhub: InputEventhub): string {
  return JSON.stringify(InputEventhub$outboundSchema.parse(inputEventhub));
}

/** @internal */
export const InputOffice365MsgTraceType$outboundSchema: z.ZodNativeEnum<
  typeof InputOffice365MsgTraceType
> = z.nativeEnum(InputOffice365MsgTraceType);

/** @internal */
export const InputOffice365MsgTraceAuthenticationMethod$outboundSchema:
  z.ZodType<string, z.ZodTypeDef, InputOffice365MsgTraceAuthenticationMethod> =
    openEnums.outboundSchema(InputOffice365MsgTraceAuthenticationMethod);

/** @internal */
export const InputOffice365MsgTraceLogLevel$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  InputOffice365MsgTraceLogLevel
> = openEnums.outboundSchema(InputOffice365MsgTraceLogLevel);

/** @internal */
export type CertOptions$Outbound = {
  certificateName?: string | undefined;
  privKeyPath: string;
  passphrase?: string | undefined;
  certPath: string;
};

/** @internal */
export const CertOptions$outboundSchema: z.ZodType<
  CertOptions$Outbound,
  z.ZodTypeDef,
  CertOptions
> = z.object({
  certificateName: z.string().optional(),
  privKeyPath: z.string(),
  passphrase: z.string().optional(),
  certPath: z.string(),
});

export function certOptionsToJSON(certOptions: CertOptions): string {
  return JSON.stringify(CertOptions$outboundSchema.parse(certOptions));
}

/** @internal */
export type InputOffice365MsgTracePqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  url: string;
  interval: number;
  startDate?: string | undefined;
  endDate?: string | undefined;
  timeout: number;
  disableTimeFilter: boolean;
  authType: string;
  rescheduleDroppedTasks: boolean;
  maxTaskReschedule: number;
  logLevel: string;
  jobTimeout: string;
  keepAliveTime: number;
  maxMissedKeepAlives: number;
  ttl: string;
  ignoreGroupJobsLimit: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  retryRules?: models.RetryRulesType1$Outbound | undefined;
  description?: string | undefined;
  username?: string | undefined;
  password?: string | undefined;
  credentialsSecret?: string | undefined;
  clientSecret?: string | undefined;
  tenantId?: string | undefined;
  clientId?: string | undefined;
  resource: string;
  planType: string;
  textSecret?: string | undefined;
  certOptions?: CertOptions$Outbound | undefined;
};

/** @internal */
export const InputOffice365MsgTracePqEnabledTrueWithPqConstraint$outboundSchema:
  z.ZodType<
    InputOffice365MsgTracePqEnabledTrueWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputOffice365MsgTracePqEnabledTrueWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputOffice365MsgTraceType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    url: z.string().default(
      "https://reports.office365.com/ecp/reportingwebservice/reporting.svc/MessageTrace",
    ),
    interval: z.number().default(60),
    startDate: z.string().optional(),
    endDate: z.string().optional(),
    timeout: z.number().default(300),
    disableTimeFilter: z.boolean().default(true),
    authType: InputOffice365MsgTraceAuthenticationMethod$outboundSchema.default(
      "oauth",
    ),
    rescheduleDroppedTasks: z.boolean().default(true),
    maxTaskReschedule: z.number().default(1),
    logLevel: InputOffice365MsgTraceLogLevel$outboundSchema.default("info"),
    jobTimeout: z.string().default("0"),
    keepAliveTime: z.number().default(30),
    maxMissedKeepAlives: z.number().default(3),
    ttl: z.string().default("4h"),
    ignoreGroupJobsLimit: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    retryRules: models.RetryRulesType1$outboundSchema.optional(),
    description: z.string().optional(),
    username: z.string().optional(),
    password: z.string().optional(),
    credentialsSecret: z.string().optional(),
    clientSecret: z.string().optional(),
    tenantId: z.string().optional(),
    clientId: z.string().optional(),
    resource: z.string().default("https://outlook.office365.com"),
    planType: models.SubscriptionPlanOptions$outboundSchema.default(
      "enterprise_gcc",
    ),
    textSecret: z.string().optional(),
    certOptions: z.lazy(() => CertOptions$outboundSchema).optional(),
  });

export function inputOffice365MsgTracePqEnabledTrueWithPqConstraintToJSON(
  inputOffice365MsgTracePqEnabledTrueWithPqConstraint:
    InputOffice365MsgTracePqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputOffice365MsgTracePqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputOffice365MsgTracePqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputOffice365MsgTracePqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  url: string;
  interval: number;
  startDate?: string | undefined;
  endDate?: string | undefined;
  timeout: number;
  disableTimeFilter: boolean;
  authType: string;
  rescheduleDroppedTasks: boolean;
  maxTaskReschedule: number;
  logLevel: string;
  jobTimeout: string;
  keepAliveTime: number;
  maxMissedKeepAlives: number;
  ttl: string;
  ignoreGroupJobsLimit: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  retryRules?: models.RetryRulesType1$Outbound | undefined;
  description?: string | undefined;
  username?: string | undefined;
  password?: string | undefined;
  credentialsSecret?: string | undefined;
  clientSecret?: string | undefined;
  tenantId?: string | undefined;
  clientId?: string | undefined;
  resource: string;
  planType: string;
  textSecret?: string | undefined;
  certOptions?: CertOptions$Outbound | undefined;
};

/** @internal */
export const InputOffice365MsgTracePqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputOffice365MsgTracePqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputOffice365MsgTracePqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputOffice365MsgTraceType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    url: z.string().default(
      "https://reports.office365.com/ecp/reportingwebservice/reporting.svc/MessageTrace",
    ),
    interval: z.number().default(60),
    startDate: z.string().optional(),
    endDate: z.string().optional(),
    timeout: z.number().default(300),
    disableTimeFilter: z.boolean().default(true),
    authType: InputOffice365MsgTraceAuthenticationMethod$outboundSchema.default(
      "oauth",
    ),
    rescheduleDroppedTasks: z.boolean().default(true),
    maxTaskReschedule: z.number().default(1),
    logLevel: InputOffice365MsgTraceLogLevel$outboundSchema.default("info"),
    jobTimeout: z.string().default("0"),
    keepAliveTime: z.number().default(30),
    maxMissedKeepAlives: z.number().default(3),
    ttl: z.string().default("4h"),
    ignoreGroupJobsLimit: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    retryRules: models.RetryRulesType1$outboundSchema.optional(),
    description: z.string().optional(),
    username: z.string().optional(),
    password: z.string().optional(),
    credentialsSecret: z.string().optional(),
    clientSecret: z.string().optional(),
    tenantId: z.string().optional(),
    clientId: z.string().optional(),
    resource: z.string().default("https://outlook.office365.com"),
    planType: models.SubscriptionPlanOptions$outboundSchema.default(
      "enterprise_gcc",
    ),
    textSecret: z.string().optional(),
    certOptions: z.lazy(() => CertOptions$outboundSchema).optional(),
  });

export function inputOffice365MsgTracePqEnabledFalseWithPqConstraintToJSON(
  inputOffice365MsgTracePqEnabledFalseWithPqConstraint:
    InputOffice365MsgTracePqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputOffice365MsgTracePqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputOffice365MsgTracePqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputOffice365MsgTraceSendToRoutesFalseWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    url: string;
    interval: number;
    startDate?: string | undefined;
    endDate?: string | undefined;
    timeout: number;
    disableTimeFilter: boolean;
    authType: string;
    rescheduleDroppedTasks: boolean;
    maxTaskReschedule: number;
    logLevel: string;
    jobTimeout: string;
    keepAliveTime: number;
    maxMissedKeepAlives: number;
    ttl: string;
    ignoreGroupJobsLimit: boolean;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    retryRules?: models.RetryRulesType1$Outbound | undefined;
    description?: string | undefined;
    username?: string | undefined;
    password?: string | undefined;
    credentialsSecret?: string | undefined;
    clientSecret?: string | undefined;
    tenantId?: string | undefined;
    clientId?: string | undefined;
    resource: string;
    planType: string;
    textSecret?: string | undefined;
    certOptions?: CertOptions$Outbound | undefined;
  };

/** @internal */
export const InputOffice365MsgTraceSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputOffice365MsgTraceSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputOffice365MsgTraceSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputOffice365MsgTraceType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    url: z.string().default(
      "https://reports.office365.com/ecp/reportingwebservice/reporting.svc/MessageTrace",
    ),
    interval: z.number().default(60),
    startDate: z.string().optional(),
    endDate: z.string().optional(),
    timeout: z.number().default(300),
    disableTimeFilter: z.boolean().default(true),
    authType: InputOffice365MsgTraceAuthenticationMethod$outboundSchema.default(
      "oauth",
    ),
    rescheduleDroppedTasks: z.boolean().default(true),
    maxTaskReschedule: z.number().default(1),
    logLevel: InputOffice365MsgTraceLogLevel$outboundSchema.default("info"),
    jobTimeout: z.string().default("0"),
    keepAliveTime: z.number().default(30),
    maxMissedKeepAlives: z.number().default(3),
    ttl: z.string().default("4h"),
    ignoreGroupJobsLimit: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    retryRules: models.RetryRulesType1$outboundSchema.optional(),
    description: z.string().optional(),
    username: z.string().optional(),
    password: z.string().optional(),
    credentialsSecret: z.string().optional(),
    clientSecret: z.string().optional(),
    tenantId: z.string().optional(),
    clientId: z.string().optional(),
    resource: z.string().default("https://outlook.office365.com"),
    planType: models.SubscriptionPlanOptions$outboundSchema.default(
      "enterprise_gcc",
    ),
    textSecret: z.string().optional(),
    certOptions: z.lazy(() => CertOptions$outboundSchema).optional(),
  });

export function inputOffice365MsgTraceSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputOffice365MsgTraceSendToRoutesFalseWithConnectionsConstraint:
    InputOffice365MsgTraceSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputOffice365MsgTraceSendToRoutesFalseWithConnectionsConstraint$outboundSchema
      .parse(inputOffice365MsgTraceSendToRoutesFalseWithConnectionsConstraint),
  );
}

/** @internal */
export type InputOffice365MsgTraceSendToRoutesTrueWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    url: string;
    interval: number;
    startDate?: string | undefined;
    endDate?: string | undefined;
    timeout: number;
    disableTimeFilter: boolean;
    authType: string;
    rescheduleDroppedTasks: boolean;
    maxTaskReschedule: number;
    logLevel: string;
    jobTimeout: string;
    keepAliveTime: number;
    maxMissedKeepAlives: number;
    ttl: string;
    ignoreGroupJobsLimit: boolean;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    retryRules?: models.RetryRulesType1$Outbound | undefined;
    description?: string | undefined;
    username?: string | undefined;
    password?: string | undefined;
    credentialsSecret?: string | undefined;
    clientSecret?: string | undefined;
    tenantId?: string | undefined;
    clientId?: string | undefined;
    resource: string;
    planType: string;
    textSecret?: string | undefined;
    certOptions?: CertOptions$Outbound | undefined;
  };

/** @internal */
export const InputOffice365MsgTraceSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputOffice365MsgTraceSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputOffice365MsgTraceSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputOffice365MsgTraceType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    url: z.string().default(
      "https://reports.office365.com/ecp/reportingwebservice/reporting.svc/MessageTrace",
    ),
    interval: z.number().default(60),
    startDate: z.string().optional(),
    endDate: z.string().optional(),
    timeout: z.number().default(300),
    disableTimeFilter: z.boolean().default(true),
    authType: InputOffice365MsgTraceAuthenticationMethod$outboundSchema.default(
      "oauth",
    ),
    rescheduleDroppedTasks: z.boolean().default(true),
    maxTaskReschedule: z.number().default(1),
    logLevel: InputOffice365MsgTraceLogLevel$outboundSchema.default("info"),
    jobTimeout: z.string().default("0"),
    keepAliveTime: z.number().default(30),
    maxMissedKeepAlives: z.number().default(3),
    ttl: z.string().default("4h"),
    ignoreGroupJobsLimit: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    retryRules: models.RetryRulesType1$outboundSchema.optional(),
    description: z.string().optional(),
    username: z.string().optional(),
    password: z.string().optional(),
    credentialsSecret: z.string().optional(),
    clientSecret: z.string().optional(),
    tenantId: z.string().optional(),
    clientId: z.string().optional(),
    resource: z.string().default("https://outlook.office365.com"),
    planType: models.SubscriptionPlanOptions$outboundSchema.default(
      "enterprise_gcc",
    ),
    textSecret: z.string().optional(),
    certOptions: z.lazy(() => CertOptions$outboundSchema).optional(),
  });

export function inputOffice365MsgTraceSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputOffice365MsgTraceSendToRoutesTrueWithConnectionsConstraint:
    InputOffice365MsgTraceSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputOffice365MsgTraceSendToRoutesTrueWithConnectionsConstraint$outboundSchema
      .parse(inputOffice365MsgTraceSendToRoutesTrueWithConnectionsConstraint),
  );
}

/** @internal */
export type InputOffice365MsgTrace$Outbound =
  | InputOffice365MsgTraceSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputOffice365MsgTraceSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputOffice365MsgTracePqEnabledFalseWithPqConstraint$Outbound
  | InputOffice365MsgTracePqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputOffice365MsgTrace$outboundSchema: z.ZodType<
  InputOffice365MsgTrace$Outbound,
  z.ZodTypeDef,
  InputOffice365MsgTrace
> = z.union([
  z.lazy(() =>
    InputOffice365MsgTraceSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputOffice365MsgTraceSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputOffice365MsgTracePqEnabledFalseWithPqConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputOffice365MsgTracePqEnabledTrueWithPqConstraint$outboundSchema
  ),
]);

export function inputOffice365MsgTraceToJSON(
  inputOffice365MsgTrace: InputOffice365MsgTrace,
): string {
  return JSON.stringify(
    InputOffice365MsgTrace$outboundSchema.parse(inputOffice365MsgTrace),
  );
}

/** @internal */
export const InputOffice365ServiceType$outboundSchema: z.ZodNativeEnum<
  typeof InputOffice365ServiceType
> = z.nativeEnum(InputOffice365ServiceType);

/** @internal */
export type InputOffice365ServiceContentConfig$Outbound = {
  contentType?: string | undefined;
  description?: string | undefined;
  interval?: number | undefined;
  logLevel?: string | undefined;
  enabled?: boolean | undefined;
};

/** @internal */
export const InputOffice365ServiceContentConfig$outboundSchema: z.ZodType<
  InputOffice365ServiceContentConfig$Outbound,
  z.ZodTypeDef,
  InputOffice365ServiceContentConfig
> = z.object({
  contentType: z.string().optional(),
  description: z.string().optional(),
  interval: z.number().optional(),
  logLevel: models.LogLevelOptionsContentConfigItems$outboundSchema.optional(),
  enabled: z.boolean().optional(),
});

export function inputOffice365ServiceContentConfigToJSON(
  inputOffice365ServiceContentConfig: InputOffice365ServiceContentConfig,
): string {
  return JSON.stringify(
    InputOffice365ServiceContentConfig$outboundSchema.parse(
      inputOffice365ServiceContentConfig,
    ),
  );
}

/** @internal */
export type InputOffice365ServicePqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  planType: string;
  tenantId: string;
  appId: string;
  timeout: number;
  keepAliveTime: number;
  jobTimeout: string;
  maxMissedKeepAlives: number;
  ttl: string;
  ignoreGroupJobsLimit: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  contentConfig?:
    | Array<InputOffice365ServiceContentConfig$Outbound>
    | undefined;
  retryRules?: models.RetryRulesType1$Outbound | undefined;
  authType: string;
  description?: string | undefined;
  clientSecret?: string | undefined;
  textSecret?: string | undefined;
};

/** @internal */
export const InputOffice365ServicePqEnabledTrueWithPqConstraint$outboundSchema:
  z.ZodType<
    InputOffice365ServicePqEnabledTrueWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputOffice365ServicePqEnabledTrueWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputOffice365ServiceType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    planType: models.SubscriptionPlanOptions$outboundSchema.default(
      "enterprise_gcc",
    ),
    tenantId: z.string(),
    appId: z.string(),
    timeout: z.number().default(300),
    keepAliveTime: z.number().default(30),
    jobTimeout: z.string().default("0"),
    maxMissedKeepAlives: z.number().default(3),
    ttl: z.string().default("4h"),
    ignoreGroupJobsLimit: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    contentConfig: z.array(
      z.lazy(() => InputOffice365ServiceContentConfig$outboundSchema),
    ).optional(),
    retryRules: models.RetryRulesType1$outboundSchema.optional(),
    authType: models.AuthenticationMethodOptions1$outboundSchema.default(
      "manual",
    ),
    description: z.string().optional(),
    clientSecret: z.string().optional(),
    textSecret: z.string().optional(),
  });

export function inputOffice365ServicePqEnabledTrueWithPqConstraintToJSON(
  inputOffice365ServicePqEnabledTrueWithPqConstraint:
    InputOffice365ServicePqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputOffice365ServicePqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputOffice365ServicePqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputOffice365ServicePqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  planType: string;
  tenantId: string;
  appId: string;
  timeout: number;
  keepAliveTime: number;
  jobTimeout: string;
  maxMissedKeepAlives: number;
  ttl: string;
  ignoreGroupJobsLimit: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  contentConfig?:
    | Array<InputOffice365ServiceContentConfig$Outbound>
    | undefined;
  retryRules?: models.RetryRulesType1$Outbound | undefined;
  authType: string;
  description?: string | undefined;
  clientSecret?: string | undefined;
  textSecret?: string | undefined;
};

/** @internal */
export const InputOffice365ServicePqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputOffice365ServicePqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputOffice365ServicePqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputOffice365ServiceType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    planType: models.SubscriptionPlanOptions$outboundSchema.default(
      "enterprise_gcc",
    ),
    tenantId: z.string(),
    appId: z.string(),
    timeout: z.number().default(300),
    keepAliveTime: z.number().default(30),
    jobTimeout: z.string().default("0"),
    maxMissedKeepAlives: z.number().default(3),
    ttl: z.string().default("4h"),
    ignoreGroupJobsLimit: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    contentConfig: z.array(
      z.lazy(() => InputOffice365ServiceContentConfig$outboundSchema),
    ).optional(),
    retryRules: models.RetryRulesType1$outboundSchema.optional(),
    authType: models.AuthenticationMethodOptions1$outboundSchema.default(
      "manual",
    ),
    description: z.string().optional(),
    clientSecret: z.string().optional(),
    textSecret: z.string().optional(),
  });

export function inputOffice365ServicePqEnabledFalseWithPqConstraintToJSON(
  inputOffice365ServicePqEnabledFalseWithPqConstraint:
    InputOffice365ServicePqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputOffice365ServicePqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputOffice365ServicePqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputOffice365ServiceSendToRoutesFalseWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    planType: string;
    tenantId: string;
    appId: string;
    timeout: number;
    keepAliveTime: number;
    jobTimeout: string;
    maxMissedKeepAlives: number;
    ttl: string;
    ignoreGroupJobsLimit: boolean;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    contentConfig?:
      | Array<InputOffice365ServiceContentConfig$Outbound>
      | undefined;
    retryRules?: models.RetryRulesType1$Outbound | undefined;
    authType: string;
    description?: string | undefined;
    clientSecret?: string | undefined;
    textSecret?: string | undefined;
  };

/** @internal */
export const InputOffice365ServiceSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputOffice365ServiceSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputOffice365ServiceSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputOffice365ServiceType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    planType: models.SubscriptionPlanOptions$outboundSchema.default(
      "enterprise_gcc",
    ),
    tenantId: z.string(),
    appId: z.string(),
    timeout: z.number().default(300),
    keepAliveTime: z.number().default(30),
    jobTimeout: z.string().default("0"),
    maxMissedKeepAlives: z.number().default(3),
    ttl: z.string().default("4h"),
    ignoreGroupJobsLimit: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    contentConfig: z.array(
      z.lazy(() => InputOffice365ServiceContentConfig$outboundSchema),
    ).optional(),
    retryRules: models.RetryRulesType1$outboundSchema.optional(),
    authType: models.AuthenticationMethodOptions1$outboundSchema.default(
      "manual",
    ),
    description: z.string().optional(),
    clientSecret: z.string().optional(),
    textSecret: z.string().optional(),
  });

export function inputOffice365ServiceSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputOffice365ServiceSendToRoutesFalseWithConnectionsConstraint:
    InputOffice365ServiceSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputOffice365ServiceSendToRoutesFalseWithConnectionsConstraint$outboundSchema
      .parse(inputOffice365ServiceSendToRoutesFalseWithConnectionsConstraint),
  );
}

/** @internal */
export type InputOffice365ServiceSendToRoutesTrueWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    planType: string;
    tenantId: string;
    appId: string;
    timeout: number;
    keepAliveTime: number;
    jobTimeout: string;
    maxMissedKeepAlives: number;
    ttl: string;
    ignoreGroupJobsLimit: boolean;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    contentConfig?:
      | Array<InputOffice365ServiceContentConfig$Outbound>
      | undefined;
    retryRules?: models.RetryRulesType1$Outbound | undefined;
    authType: string;
    description?: string | undefined;
    clientSecret?: string | undefined;
    textSecret?: string | undefined;
  };

/** @internal */
export const InputOffice365ServiceSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputOffice365ServiceSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputOffice365ServiceSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputOffice365ServiceType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    planType: models.SubscriptionPlanOptions$outboundSchema.default(
      "enterprise_gcc",
    ),
    tenantId: z.string(),
    appId: z.string(),
    timeout: z.number().default(300),
    keepAliveTime: z.number().default(30),
    jobTimeout: z.string().default("0"),
    maxMissedKeepAlives: z.number().default(3),
    ttl: z.string().default("4h"),
    ignoreGroupJobsLimit: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    contentConfig: z.array(
      z.lazy(() => InputOffice365ServiceContentConfig$outboundSchema),
    ).optional(),
    retryRules: models.RetryRulesType1$outboundSchema.optional(),
    authType: models.AuthenticationMethodOptions1$outboundSchema.default(
      "manual",
    ),
    description: z.string().optional(),
    clientSecret: z.string().optional(),
    textSecret: z.string().optional(),
  });

export function inputOffice365ServiceSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputOffice365ServiceSendToRoutesTrueWithConnectionsConstraint:
    InputOffice365ServiceSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputOffice365ServiceSendToRoutesTrueWithConnectionsConstraint$outboundSchema
      .parse(inputOffice365ServiceSendToRoutesTrueWithConnectionsConstraint),
  );
}

/** @internal */
export type InputOffice365Service$Outbound =
  | InputOffice365ServiceSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputOffice365ServiceSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputOffice365ServicePqEnabledFalseWithPqConstraint$Outbound
  | InputOffice365ServicePqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputOffice365Service$outboundSchema: z.ZodType<
  InputOffice365Service$Outbound,
  z.ZodTypeDef,
  InputOffice365Service
> = z.union([
  z.lazy(() =>
    InputOffice365ServiceSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputOffice365ServiceSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputOffice365ServicePqEnabledFalseWithPqConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputOffice365ServicePqEnabledTrueWithPqConstraint$outboundSchema
  ),
]);

export function inputOffice365ServiceToJSON(
  inputOffice365Service: InputOffice365Service,
): string {
  return JSON.stringify(
    InputOffice365Service$outboundSchema.parse(inputOffice365Service),
  );
}

/** @internal */
export const InputOffice365MgmtType$outboundSchema: z.ZodNativeEnum<
  typeof InputOffice365MgmtType
> = z.nativeEnum(InputOffice365MgmtType);

/** @internal */
export type InputOffice365MgmtContentConfig$Outbound = {
  contentType?: string | undefined;
  description?: string | undefined;
  interval?: number | undefined;
  logLevel?: string | undefined;
  enabled?: boolean | undefined;
};

/** @internal */
export const InputOffice365MgmtContentConfig$outboundSchema: z.ZodType<
  InputOffice365MgmtContentConfig$Outbound,
  z.ZodTypeDef,
  InputOffice365MgmtContentConfig
> = z.object({
  contentType: z.string().optional(),
  description: z.string().optional(),
  interval: z.number().optional(),
  logLevel: models.LogLevelOptionsContentConfigItems$outboundSchema.optional(),
  enabled: z.boolean().optional(),
});

export function inputOffice365MgmtContentConfigToJSON(
  inputOffice365MgmtContentConfig: InputOffice365MgmtContentConfig,
): string {
  return JSON.stringify(
    InputOffice365MgmtContentConfig$outboundSchema.parse(
      inputOffice365MgmtContentConfig,
    ),
  );
}

/** @internal */
export type InputOffice365MgmtPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  planType: string;
  tenantId: string;
  appId: string;
  timeout: number;
  keepAliveTime: number;
  jobTimeout: string;
  maxMissedKeepAlives: number;
  ttl: string;
  ignoreGroupJobsLimit: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  publisherIdentifier?: string | undefined;
  contentConfig?: Array<InputOffice365MgmtContentConfig$Outbound> | undefined;
  ingestionLag: number;
  retryRules?: models.RetryRulesType1$Outbound | undefined;
  authType: string;
  description?: string | undefined;
  clientSecret?: string | undefined;
  textSecret?: string | undefined;
};

/** @internal */
export const InputOffice365MgmtPqEnabledTrueWithPqConstraint$outboundSchema:
  z.ZodType<
    InputOffice365MgmtPqEnabledTrueWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputOffice365MgmtPqEnabledTrueWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputOffice365MgmtType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    planType: models.SubscriptionPlanOptions$outboundSchema.default(
      "enterprise_gcc",
    ),
    tenantId: z.string(),
    appId: z.string(),
    timeout: z.number().default(300),
    keepAliveTime: z.number().default(30),
    jobTimeout: z.string().default("0"),
    maxMissedKeepAlives: z.number().default(3),
    ttl: z.string().default("4h"),
    ignoreGroupJobsLimit: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    publisherIdentifier: z.string().optional(),
    contentConfig: z.array(
      z.lazy(() => InputOffice365MgmtContentConfig$outboundSchema),
    ).optional(),
    ingestionLag: z.number().default(0),
    retryRules: models.RetryRulesType1$outboundSchema.optional(),
    authType: models.AuthenticationMethodOptions1$outboundSchema.default(
      "manual",
    ),
    description: z.string().optional(),
    clientSecret: z.string().optional(),
    textSecret: z.string().optional(),
  });

export function inputOffice365MgmtPqEnabledTrueWithPqConstraintToJSON(
  inputOffice365MgmtPqEnabledTrueWithPqConstraint:
    InputOffice365MgmtPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputOffice365MgmtPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputOffice365MgmtPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputOffice365MgmtPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  planType: string;
  tenantId: string;
  appId: string;
  timeout: number;
  keepAliveTime: number;
  jobTimeout: string;
  maxMissedKeepAlives: number;
  ttl: string;
  ignoreGroupJobsLimit: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  publisherIdentifier?: string | undefined;
  contentConfig?: Array<InputOffice365MgmtContentConfig$Outbound> | undefined;
  ingestionLag: number;
  retryRules?: models.RetryRulesType1$Outbound | undefined;
  authType: string;
  description?: string | undefined;
  clientSecret?: string | undefined;
  textSecret?: string | undefined;
};

/** @internal */
export const InputOffice365MgmtPqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputOffice365MgmtPqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputOffice365MgmtPqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputOffice365MgmtType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    planType: models.SubscriptionPlanOptions$outboundSchema.default(
      "enterprise_gcc",
    ),
    tenantId: z.string(),
    appId: z.string(),
    timeout: z.number().default(300),
    keepAliveTime: z.number().default(30),
    jobTimeout: z.string().default("0"),
    maxMissedKeepAlives: z.number().default(3),
    ttl: z.string().default("4h"),
    ignoreGroupJobsLimit: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    publisherIdentifier: z.string().optional(),
    contentConfig: z.array(
      z.lazy(() => InputOffice365MgmtContentConfig$outboundSchema),
    ).optional(),
    ingestionLag: z.number().default(0),
    retryRules: models.RetryRulesType1$outboundSchema.optional(),
    authType: models.AuthenticationMethodOptions1$outboundSchema.default(
      "manual",
    ),
    description: z.string().optional(),
    clientSecret: z.string().optional(),
    textSecret: z.string().optional(),
  });

export function inputOffice365MgmtPqEnabledFalseWithPqConstraintToJSON(
  inputOffice365MgmtPqEnabledFalseWithPqConstraint:
    InputOffice365MgmtPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputOffice365MgmtPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputOffice365MgmtPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputOffice365MgmtSendToRoutesFalseWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    planType: string;
    tenantId: string;
    appId: string;
    timeout: number;
    keepAliveTime: number;
    jobTimeout: string;
    maxMissedKeepAlives: number;
    ttl: string;
    ignoreGroupJobsLimit: boolean;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    publisherIdentifier?: string | undefined;
    contentConfig?: Array<InputOffice365MgmtContentConfig$Outbound> | undefined;
    ingestionLag: number;
    retryRules?: models.RetryRulesType1$Outbound | undefined;
    authType: string;
    description?: string | undefined;
    clientSecret?: string | undefined;
    textSecret?: string | undefined;
  };

/** @internal */
export const InputOffice365MgmtSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputOffice365MgmtSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputOffice365MgmtSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputOffice365MgmtType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    planType: models.SubscriptionPlanOptions$outboundSchema.default(
      "enterprise_gcc",
    ),
    tenantId: z.string(),
    appId: z.string(),
    timeout: z.number().default(300),
    keepAliveTime: z.number().default(30),
    jobTimeout: z.string().default("0"),
    maxMissedKeepAlives: z.number().default(3),
    ttl: z.string().default("4h"),
    ignoreGroupJobsLimit: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    publisherIdentifier: z.string().optional(),
    contentConfig: z.array(
      z.lazy(() => InputOffice365MgmtContentConfig$outboundSchema),
    ).optional(),
    ingestionLag: z.number().default(0),
    retryRules: models.RetryRulesType1$outboundSchema.optional(),
    authType: models.AuthenticationMethodOptions1$outboundSchema.default(
      "manual",
    ),
    description: z.string().optional(),
    clientSecret: z.string().optional(),
    textSecret: z.string().optional(),
  });

export function inputOffice365MgmtSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputOffice365MgmtSendToRoutesFalseWithConnectionsConstraint:
    InputOffice365MgmtSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputOffice365MgmtSendToRoutesFalseWithConnectionsConstraint$outboundSchema
      .parse(inputOffice365MgmtSendToRoutesFalseWithConnectionsConstraint),
  );
}

/** @internal */
export type InputOffice365MgmtSendToRoutesTrueWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    planType: string;
    tenantId: string;
    appId: string;
    timeout: number;
    keepAliveTime: number;
    jobTimeout: string;
    maxMissedKeepAlives: number;
    ttl: string;
    ignoreGroupJobsLimit: boolean;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    publisherIdentifier?: string | undefined;
    contentConfig?: Array<InputOffice365MgmtContentConfig$Outbound> | undefined;
    ingestionLag: number;
    retryRules?: models.RetryRulesType1$Outbound | undefined;
    authType: string;
    description?: string | undefined;
    clientSecret?: string | undefined;
    textSecret?: string | undefined;
  };

/** @internal */
export const InputOffice365MgmtSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputOffice365MgmtSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputOffice365MgmtSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputOffice365MgmtType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    planType: models.SubscriptionPlanOptions$outboundSchema.default(
      "enterprise_gcc",
    ),
    tenantId: z.string(),
    appId: z.string(),
    timeout: z.number().default(300),
    keepAliveTime: z.number().default(30),
    jobTimeout: z.string().default("0"),
    maxMissedKeepAlives: z.number().default(3),
    ttl: z.string().default("4h"),
    ignoreGroupJobsLimit: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    publisherIdentifier: z.string().optional(),
    contentConfig: z.array(
      z.lazy(() => InputOffice365MgmtContentConfig$outboundSchema),
    ).optional(),
    ingestionLag: z.number().default(0),
    retryRules: models.RetryRulesType1$outboundSchema.optional(),
    authType: models.AuthenticationMethodOptions1$outboundSchema.default(
      "manual",
    ),
    description: z.string().optional(),
    clientSecret: z.string().optional(),
    textSecret: z.string().optional(),
  });

export function inputOffice365MgmtSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputOffice365MgmtSendToRoutesTrueWithConnectionsConstraint:
    InputOffice365MgmtSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputOffice365MgmtSendToRoutesTrueWithConnectionsConstraint$outboundSchema
      .parse(inputOffice365MgmtSendToRoutesTrueWithConnectionsConstraint),
  );
}

/** @internal */
export type InputOffice365Mgmt$Outbound =
  | InputOffice365MgmtSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputOffice365MgmtSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputOffice365MgmtPqEnabledFalseWithPqConstraint$Outbound
  | InputOffice365MgmtPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputOffice365Mgmt$outboundSchema: z.ZodType<
  InputOffice365Mgmt$Outbound,
  z.ZodTypeDef,
  InputOffice365Mgmt
> = z.union([
  z.lazy(() =>
    InputOffice365MgmtSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputOffice365MgmtSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputOffice365MgmtPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputOffice365MgmtPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputOffice365MgmtToJSON(
  inputOffice365Mgmt: InputOffice365Mgmt,
): string {
  return JSON.stringify(
    InputOffice365Mgmt$outboundSchema.parse(inputOffice365Mgmt),
  );
}

/** @internal */
export const InputEdgePrometheusType$outboundSchema: z.ZodNativeEnum<
  typeof InputEdgePrometheusType
> = z.nativeEnum(InputEdgePrometheusType);

/** @internal */
export const InputEdgePrometheusDiscoveryType$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  InputEdgePrometheusDiscoveryType
> = openEnums.outboundSchema(InputEdgePrometheusDiscoveryType);

/** @internal */
export const InputEdgePrometheusAuthenticationMethod$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  InputEdgePrometheusAuthenticationMethod
> = openEnums.outboundSchema(InputEdgePrometheusAuthenticationMethod);

/** @internal */
export type Target$Outbound = {
  protocol: string;
  host: string;
  port: number;
  path: string;
};

/** @internal */
export const Target$outboundSchema: z.ZodType<
  Target$Outbound,
  z.ZodTypeDef,
  Target
> = z.object({
  protocol: models.ProtocolOptionsTargetsItems$outboundSchema.default("http"),
  host: z.string(),
  port: z.number().default(9090),
  path: z.string().default("/metrics"),
});

export function targetToJSON(target: Target): string {
  return JSON.stringify(Target$outboundSchema.parse(target));
}

/** @internal */
export type PodFilter$Outbound = {
  filter: string;
  description?: string | undefined;
};

/** @internal */
export const PodFilter$outboundSchema: z.ZodType<
  PodFilter$Outbound,
  z.ZodTypeDef,
  PodFilter
> = z.object({
  filter: z.string(),
  description: z.string().optional(),
});

export function podFilterToJSON(podFilter: PodFilter): string {
  return JSON.stringify(PodFilter$outboundSchema.parse(podFilter));
}

/** @internal */
export type InputEdgePrometheusPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  dimensionList?: Array<string> | undefined;
  discoveryType: string;
  interval: number;
  timeout: number;
  persistence?: models.DiskSpoolingType$Outbound | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  authType: string;
  description?: string | undefined;
  targets?: Array<Target$Outbound> | undefined;
  recordType: string;
  scrapePort: number;
  nameList?: Array<string> | undefined;
  scrapeProtocol: string;
  scrapePath: string;
  awsAuthenticationMethod: string;
  awsApiKey?: string | undefined;
  awsSecret?: string | undefined;
  usePublicIp: boolean;
  searchFilter?: Array<models.ItemsTypeSearchFilter$Outbound> | undefined;
  awsSecretKey?: string | undefined;
  region?: string | undefined;
  endpoint?: string | undefined;
  signatureVersion: string;
  reuseConnections: boolean;
  rejectUnauthorized: boolean;
  enableAssumeRole: boolean;
  assumeRoleArn?: string | undefined;
  assumeRoleExternalId?: string | undefined;
  durationSeconds: number;
  scrapeProtocolExpr: string;
  scrapePortExpr: string;
  scrapePathExpr: string;
  podFilter?: Array<PodFilter$Outbound> | undefined;
  username?: string | undefined;
  password?: string | undefined;
  credentialsSecret?: string | undefined;
};

/** @internal */
export const InputEdgePrometheusPqEnabledTrueWithPqConstraint$outboundSchema:
  z.ZodType<
    InputEdgePrometheusPqEnabledTrueWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputEdgePrometheusPqEnabledTrueWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputEdgePrometheusType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    dimensionList: z.array(z.string()).optional(),
    discoveryType: InputEdgePrometheusDiscoveryType$outboundSchema.default(
      "static",
    ),
    interval: z.number().default(15),
    timeout: z.number().default(5000),
    persistence: models.DiskSpoolingType$outboundSchema.optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    authType: InputEdgePrometheusAuthenticationMethod$outboundSchema.default(
      "manual",
    ),
    description: z.string().optional(),
    targets: z.array(z.lazy(() => Target$outboundSchema)).optional(),
    recordType: models.RecordTypeOptions$outboundSchema.default("SRV"),
    scrapePort: z.number().default(9090),
    nameList: z.array(z.string()).optional(),
    scrapeProtocol: models.ProtocolOptionsTargetsItems$outboundSchema.default(
      "http",
    ),
    scrapePath: z.string().default("/metrics"),
    awsAuthenticationMethod: z.string().default("auto"),
    awsApiKey: z.string().optional(),
    awsSecret: z.string().optional(),
    usePublicIp: z.boolean().default(true),
    searchFilter: z.array(models.ItemsTypeSearchFilter$outboundSchema)
      .optional(),
    awsSecretKey: z.string().optional(),
    region: z.string().optional(),
    endpoint: z.string().optional(),
    signatureVersion: models.SignatureVersionOptions1$outboundSchema.default(
      "v4",
    ),
    reuseConnections: z.boolean().default(true),
    rejectUnauthorized: z.boolean().default(true),
    enableAssumeRole: z.boolean().default(false),
    assumeRoleArn: z.string().optional(),
    assumeRoleExternalId: z.string().optional(),
    durationSeconds: z.number().default(3600),
    scrapeProtocolExpr: z.string().default(
      "metadata.annotations['prometheus.io/scheme'] || 'http'",
    ),
    scrapePortExpr: z.string().default(
      "metadata.annotations['prometheus.io/port'] || 9090",
    ),
    scrapePathExpr: z.string().default(
      "metadata.annotations['prometheus.io/path'] || '/metrics'",
    ),
    podFilter: z.array(z.lazy(() => PodFilter$outboundSchema)).optional(),
    username: z.string().optional(),
    password: z.string().optional(),
    credentialsSecret: z.string().optional(),
  });

export function inputEdgePrometheusPqEnabledTrueWithPqConstraintToJSON(
  inputEdgePrometheusPqEnabledTrueWithPqConstraint:
    InputEdgePrometheusPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputEdgePrometheusPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputEdgePrometheusPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputEdgePrometheusPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  dimensionList?: Array<string> | undefined;
  discoveryType: string;
  interval: number;
  timeout: number;
  persistence?: models.DiskSpoolingType$Outbound | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  authType: string;
  description?: string | undefined;
  targets?: Array<Target$Outbound> | undefined;
  recordType: string;
  scrapePort: number;
  nameList?: Array<string> | undefined;
  scrapeProtocol: string;
  scrapePath: string;
  awsAuthenticationMethod: string;
  awsApiKey?: string | undefined;
  awsSecret?: string | undefined;
  usePublicIp: boolean;
  searchFilter?: Array<models.ItemsTypeSearchFilter$Outbound> | undefined;
  awsSecretKey?: string | undefined;
  region?: string | undefined;
  endpoint?: string | undefined;
  signatureVersion: string;
  reuseConnections: boolean;
  rejectUnauthorized: boolean;
  enableAssumeRole: boolean;
  assumeRoleArn?: string | undefined;
  assumeRoleExternalId?: string | undefined;
  durationSeconds: number;
  scrapeProtocolExpr: string;
  scrapePortExpr: string;
  scrapePathExpr: string;
  podFilter?: Array<PodFilter$Outbound> | undefined;
  username?: string | undefined;
  password?: string | undefined;
  credentialsSecret?: string | undefined;
};

/** @internal */
export const InputEdgePrometheusPqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputEdgePrometheusPqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputEdgePrometheusPqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputEdgePrometheusType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    dimensionList: z.array(z.string()).optional(),
    discoveryType: InputEdgePrometheusDiscoveryType$outboundSchema.default(
      "static",
    ),
    interval: z.number().default(15),
    timeout: z.number().default(5000),
    persistence: models.DiskSpoolingType$outboundSchema.optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    authType: InputEdgePrometheusAuthenticationMethod$outboundSchema.default(
      "manual",
    ),
    description: z.string().optional(),
    targets: z.array(z.lazy(() => Target$outboundSchema)).optional(),
    recordType: models.RecordTypeOptions$outboundSchema.default("SRV"),
    scrapePort: z.number().default(9090),
    nameList: z.array(z.string()).optional(),
    scrapeProtocol: models.ProtocolOptionsTargetsItems$outboundSchema.default(
      "http",
    ),
    scrapePath: z.string().default("/metrics"),
    awsAuthenticationMethod: z.string().default("auto"),
    awsApiKey: z.string().optional(),
    awsSecret: z.string().optional(),
    usePublicIp: z.boolean().default(true),
    searchFilter: z.array(models.ItemsTypeSearchFilter$outboundSchema)
      .optional(),
    awsSecretKey: z.string().optional(),
    region: z.string().optional(),
    endpoint: z.string().optional(),
    signatureVersion: models.SignatureVersionOptions1$outboundSchema.default(
      "v4",
    ),
    reuseConnections: z.boolean().default(true),
    rejectUnauthorized: z.boolean().default(true),
    enableAssumeRole: z.boolean().default(false),
    assumeRoleArn: z.string().optional(),
    assumeRoleExternalId: z.string().optional(),
    durationSeconds: z.number().default(3600),
    scrapeProtocolExpr: z.string().default(
      "metadata.annotations['prometheus.io/scheme'] || 'http'",
    ),
    scrapePortExpr: z.string().default(
      "metadata.annotations['prometheus.io/port'] || 9090",
    ),
    scrapePathExpr: z.string().default(
      "metadata.annotations['prometheus.io/path'] || '/metrics'",
    ),
    podFilter: z.array(z.lazy(() => PodFilter$outboundSchema)).optional(),
    username: z.string().optional(),
    password: z.string().optional(),
    credentialsSecret: z.string().optional(),
  });

export function inputEdgePrometheusPqEnabledFalseWithPqConstraintToJSON(
  inputEdgePrometheusPqEnabledFalseWithPqConstraint:
    InputEdgePrometheusPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputEdgePrometheusPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputEdgePrometheusPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputEdgePrometheusSendToRoutesFalseWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    dimensionList?: Array<string> | undefined;
    discoveryType: string;
    interval: number;
    timeout: number;
    persistence?: models.DiskSpoolingType$Outbound | undefined;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    authType: string;
    description?: string | undefined;
    targets?: Array<Target$Outbound> | undefined;
    recordType: string;
    scrapePort: number;
    nameList?: Array<string> | undefined;
    scrapeProtocol: string;
    scrapePath: string;
    awsAuthenticationMethod: string;
    awsApiKey?: string | undefined;
    awsSecret?: string | undefined;
    usePublicIp: boolean;
    searchFilter?: Array<models.ItemsTypeSearchFilter$Outbound> | undefined;
    awsSecretKey?: string | undefined;
    region?: string | undefined;
    endpoint?: string | undefined;
    signatureVersion: string;
    reuseConnections: boolean;
    rejectUnauthorized: boolean;
    enableAssumeRole: boolean;
    assumeRoleArn?: string | undefined;
    assumeRoleExternalId?: string | undefined;
    durationSeconds: number;
    scrapeProtocolExpr: string;
    scrapePortExpr: string;
    scrapePathExpr: string;
    podFilter?: Array<PodFilter$Outbound> | undefined;
    username?: string | undefined;
    password?: string | undefined;
    credentialsSecret?: string | undefined;
  };

/** @internal */
export const InputEdgePrometheusSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputEdgePrometheusSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputEdgePrometheusSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputEdgePrometheusType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    dimensionList: z.array(z.string()).optional(),
    discoveryType: InputEdgePrometheusDiscoveryType$outboundSchema.default(
      "static",
    ),
    interval: z.number().default(15),
    timeout: z.number().default(5000),
    persistence: models.DiskSpoolingType$outboundSchema.optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    authType: InputEdgePrometheusAuthenticationMethod$outboundSchema.default(
      "manual",
    ),
    description: z.string().optional(),
    targets: z.array(z.lazy(() => Target$outboundSchema)).optional(),
    recordType: models.RecordTypeOptions$outboundSchema.default("SRV"),
    scrapePort: z.number().default(9090),
    nameList: z.array(z.string()).optional(),
    scrapeProtocol: models.ProtocolOptionsTargetsItems$outboundSchema.default(
      "http",
    ),
    scrapePath: z.string().default("/metrics"),
    awsAuthenticationMethod: z.string().default("auto"),
    awsApiKey: z.string().optional(),
    awsSecret: z.string().optional(),
    usePublicIp: z.boolean().default(true),
    searchFilter: z.array(models.ItemsTypeSearchFilter$outboundSchema)
      .optional(),
    awsSecretKey: z.string().optional(),
    region: z.string().optional(),
    endpoint: z.string().optional(),
    signatureVersion: models.SignatureVersionOptions1$outboundSchema.default(
      "v4",
    ),
    reuseConnections: z.boolean().default(true),
    rejectUnauthorized: z.boolean().default(true),
    enableAssumeRole: z.boolean().default(false),
    assumeRoleArn: z.string().optional(),
    assumeRoleExternalId: z.string().optional(),
    durationSeconds: z.number().default(3600),
    scrapeProtocolExpr: z.string().default(
      "metadata.annotations['prometheus.io/scheme'] || 'http'",
    ),
    scrapePortExpr: z.string().default(
      "metadata.annotations['prometheus.io/port'] || 9090",
    ),
    scrapePathExpr: z.string().default(
      "metadata.annotations['prometheus.io/path'] || '/metrics'",
    ),
    podFilter: z.array(z.lazy(() => PodFilter$outboundSchema)).optional(),
    username: z.string().optional(),
    password: z.string().optional(),
    credentialsSecret: z.string().optional(),
  });

export function inputEdgePrometheusSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputEdgePrometheusSendToRoutesFalseWithConnectionsConstraint:
    InputEdgePrometheusSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputEdgePrometheusSendToRoutesFalseWithConnectionsConstraint$outboundSchema
      .parse(inputEdgePrometheusSendToRoutesFalseWithConnectionsConstraint),
  );
}

/** @internal */
export type InputEdgePrometheusSendToRoutesTrueWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    dimensionList?: Array<string> | undefined;
    discoveryType: string;
    interval: number;
    timeout: number;
    persistence?: models.DiskSpoolingType$Outbound | undefined;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    authType: string;
    description?: string | undefined;
    targets?: Array<Target$Outbound> | undefined;
    recordType: string;
    scrapePort: number;
    nameList?: Array<string> | undefined;
    scrapeProtocol: string;
    scrapePath: string;
    awsAuthenticationMethod: string;
    awsApiKey?: string | undefined;
    awsSecret?: string | undefined;
    usePublicIp: boolean;
    searchFilter?: Array<models.ItemsTypeSearchFilter$Outbound> | undefined;
    awsSecretKey?: string | undefined;
    region?: string | undefined;
    endpoint?: string | undefined;
    signatureVersion: string;
    reuseConnections: boolean;
    rejectUnauthorized: boolean;
    enableAssumeRole: boolean;
    assumeRoleArn?: string | undefined;
    assumeRoleExternalId?: string | undefined;
    durationSeconds: number;
    scrapeProtocolExpr: string;
    scrapePortExpr: string;
    scrapePathExpr: string;
    podFilter?: Array<PodFilter$Outbound> | undefined;
    username?: string | undefined;
    password?: string | undefined;
    credentialsSecret?: string | undefined;
  };

/** @internal */
export const InputEdgePrometheusSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputEdgePrometheusSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputEdgePrometheusSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputEdgePrometheusType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    dimensionList: z.array(z.string()).optional(),
    discoveryType: InputEdgePrometheusDiscoveryType$outboundSchema.default(
      "static",
    ),
    interval: z.number().default(15),
    timeout: z.number().default(5000),
    persistence: models.DiskSpoolingType$outboundSchema.optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    authType: InputEdgePrometheusAuthenticationMethod$outboundSchema.default(
      "manual",
    ),
    description: z.string().optional(),
    targets: z.array(z.lazy(() => Target$outboundSchema)).optional(),
    recordType: models.RecordTypeOptions$outboundSchema.default("SRV"),
    scrapePort: z.number().default(9090),
    nameList: z.array(z.string()).optional(),
    scrapeProtocol: models.ProtocolOptionsTargetsItems$outboundSchema.default(
      "http",
    ),
    scrapePath: z.string().default("/metrics"),
    awsAuthenticationMethod: z.string().default("auto"),
    awsApiKey: z.string().optional(),
    awsSecret: z.string().optional(),
    usePublicIp: z.boolean().default(true),
    searchFilter: z.array(models.ItemsTypeSearchFilter$outboundSchema)
      .optional(),
    awsSecretKey: z.string().optional(),
    region: z.string().optional(),
    endpoint: z.string().optional(),
    signatureVersion: models.SignatureVersionOptions1$outboundSchema.default(
      "v4",
    ),
    reuseConnections: z.boolean().default(true),
    rejectUnauthorized: z.boolean().default(true),
    enableAssumeRole: z.boolean().default(false),
    assumeRoleArn: z.string().optional(),
    assumeRoleExternalId: z.string().optional(),
    durationSeconds: z.number().default(3600),
    scrapeProtocolExpr: z.string().default(
      "metadata.annotations['prometheus.io/scheme'] || 'http'",
    ),
    scrapePortExpr: z.string().default(
      "metadata.annotations['prometheus.io/port'] || 9090",
    ),
    scrapePathExpr: z.string().default(
      "metadata.annotations['prometheus.io/path'] || '/metrics'",
    ),
    podFilter: z.array(z.lazy(() => PodFilter$outboundSchema)).optional(),
    username: z.string().optional(),
    password: z.string().optional(),
    credentialsSecret: z.string().optional(),
  });

export function inputEdgePrometheusSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputEdgePrometheusSendToRoutesTrueWithConnectionsConstraint:
    InputEdgePrometheusSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputEdgePrometheusSendToRoutesTrueWithConnectionsConstraint$outboundSchema
      .parse(inputEdgePrometheusSendToRoutesTrueWithConnectionsConstraint),
  );
}

/** @internal */
export type InputEdgePrometheus$Outbound =
  | InputEdgePrometheusSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputEdgePrometheusSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputEdgePrometheusPqEnabledFalseWithPqConstraint$Outbound
  | InputEdgePrometheusPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputEdgePrometheus$outboundSchema: z.ZodType<
  InputEdgePrometheus$Outbound,
  z.ZodTypeDef,
  InputEdgePrometheus
> = z.union([
  z.lazy(() =>
    InputEdgePrometheusSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputEdgePrometheusSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputEdgePrometheusPqEnabledFalseWithPqConstraint$outboundSchema
  ),
  z.lazy(() => InputEdgePrometheusPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputEdgePrometheusToJSON(
  inputEdgePrometheus: InputEdgePrometheus,
): string {
  return JSON.stringify(
    InputEdgePrometheus$outboundSchema.parse(inputEdgePrometheus),
  );
}

/** @internal */
export const InputPrometheusType$outboundSchema: z.ZodNativeEnum<
  typeof InputPrometheusType
> = z.nativeEnum(InputPrometheusType);

/** @internal */
export const InputPrometheusDiscoveryType$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  InputPrometheusDiscoveryType
> = openEnums.outboundSchema(InputPrometheusDiscoveryType);

/** @internal */
export const InputPrometheusLogLevel$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  InputPrometheusLogLevel
> = openEnums.outboundSchema(InputPrometheusLogLevel);

/** @internal */
export const MetricsProtocol$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  MetricsProtocol
> = openEnums.outboundSchema(MetricsProtocol);

/** @internal */
export type InputPrometheusPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  dimensionList?: Array<string> | undefined;
  discoveryType: string;
  interval: number;
  logLevel: string;
  rejectUnauthorized: boolean;
  timeout: number;
  keepAliveTime: number;
  jobTimeout: string;
  maxMissedKeepAlives: number;
  ttl: string;
  ignoreGroupJobsLimit: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  authType: string;
  description?: string | undefined;
  targetList?: Array<string> | undefined;
  recordType: string;
  scrapePort: number;
  nameList?: Array<string> | undefined;
  scrapeProtocol: string;
  scrapePath: string;
  awsAuthenticationMethod: string;
  awsApiKey?: string | undefined;
  awsSecret?: string | undefined;
  usePublicIp: boolean;
  searchFilter?: Array<models.ItemsTypeSearchFilter$Outbound> | undefined;
  awsSecretKey?: string | undefined;
  region?: string | undefined;
  endpoint?: string | undefined;
  signatureVersion: string;
  reuseConnections: boolean;
  enableAssumeRole: boolean;
  assumeRoleArn?: string | undefined;
  assumeRoleExternalId?: string | undefined;
  durationSeconds: number;
  username?: string | undefined;
  password?: string | undefined;
  credentialsSecret?: string | undefined;
};

/** @internal */
export const InputPrometheusPqEnabledTrueWithPqConstraint$outboundSchema:
  z.ZodType<
    InputPrometheusPqEnabledTrueWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputPrometheusPqEnabledTrueWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputPrometheusType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    dimensionList: z.array(z.string()).optional(),
    discoveryType: InputPrometheusDiscoveryType$outboundSchema.default(
      "static",
    ),
    interval: z.number().default(15),
    logLevel: InputPrometheusLogLevel$outboundSchema.default("info"),
    rejectUnauthorized: z.boolean().default(true),
    timeout: z.number().default(0),
    keepAliveTime: z.number().default(30),
    jobTimeout: z.string().default("0"),
    maxMissedKeepAlives: z.number().default(3),
    ttl: z.string().default("4h"),
    ignoreGroupJobsLimit: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    authType: models.AuthenticationMethodOptionsSasl$outboundSchema.default(
      "manual",
    ),
    description: z.string().optional(),
    targetList: z.array(z.string()).optional(),
    recordType: models.RecordTypeOptions$outboundSchema.default("SRV"),
    scrapePort: z.number().default(9090),
    nameList: z.array(z.string()).optional(),
    scrapeProtocol: MetricsProtocol$outboundSchema.default("http"),
    scrapePath: z.string().default("/metrics"),
    awsAuthenticationMethod: z.string().default("auto"),
    awsApiKey: z.string().optional(),
    awsSecret: z.string().optional(),
    usePublicIp: z.boolean().default(true),
    searchFilter: z.array(models.ItemsTypeSearchFilter$outboundSchema)
      .optional(),
    awsSecretKey: z.string().optional(),
    region: z.string().optional(),
    endpoint: z.string().optional(),
    signatureVersion: models.SignatureVersionOptions1$outboundSchema.default(
      "v4",
    ),
    reuseConnections: z.boolean().default(true),
    enableAssumeRole: z.boolean().default(false),
    assumeRoleArn: z.string().optional(),
    assumeRoleExternalId: z.string().optional(),
    durationSeconds: z.number().default(3600),
    username: z.string().optional(),
    password: z.string().optional(),
    credentialsSecret: z.string().optional(),
  });

export function inputPrometheusPqEnabledTrueWithPqConstraintToJSON(
  inputPrometheusPqEnabledTrueWithPqConstraint:
    InputPrometheusPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputPrometheusPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputPrometheusPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputPrometheusPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  dimensionList?: Array<string> | undefined;
  discoveryType: string;
  interval: number;
  logLevel: string;
  rejectUnauthorized: boolean;
  timeout: number;
  keepAliveTime: number;
  jobTimeout: string;
  maxMissedKeepAlives: number;
  ttl: string;
  ignoreGroupJobsLimit: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  authType: string;
  description?: string | undefined;
  targetList?: Array<string> | undefined;
  recordType: string;
  scrapePort: number;
  nameList?: Array<string> | undefined;
  scrapeProtocol: string;
  scrapePath: string;
  awsAuthenticationMethod: string;
  awsApiKey?: string | undefined;
  awsSecret?: string | undefined;
  usePublicIp: boolean;
  searchFilter?: Array<models.ItemsTypeSearchFilter$Outbound> | undefined;
  awsSecretKey?: string | undefined;
  region?: string | undefined;
  endpoint?: string | undefined;
  signatureVersion: string;
  reuseConnections: boolean;
  enableAssumeRole: boolean;
  assumeRoleArn?: string | undefined;
  assumeRoleExternalId?: string | undefined;
  durationSeconds: number;
  username?: string | undefined;
  password?: string | undefined;
  credentialsSecret?: string | undefined;
};

/** @internal */
export const InputPrometheusPqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputPrometheusPqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputPrometheusPqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputPrometheusType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    dimensionList: z.array(z.string()).optional(),
    discoveryType: InputPrometheusDiscoveryType$outboundSchema.default(
      "static",
    ),
    interval: z.number().default(15),
    logLevel: InputPrometheusLogLevel$outboundSchema.default("info"),
    rejectUnauthorized: z.boolean().default(true),
    timeout: z.number().default(0),
    keepAliveTime: z.number().default(30),
    jobTimeout: z.string().default("0"),
    maxMissedKeepAlives: z.number().default(3),
    ttl: z.string().default("4h"),
    ignoreGroupJobsLimit: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    authType: models.AuthenticationMethodOptionsSasl$outboundSchema.default(
      "manual",
    ),
    description: z.string().optional(),
    targetList: z.array(z.string()).optional(),
    recordType: models.RecordTypeOptions$outboundSchema.default("SRV"),
    scrapePort: z.number().default(9090),
    nameList: z.array(z.string()).optional(),
    scrapeProtocol: MetricsProtocol$outboundSchema.default("http"),
    scrapePath: z.string().default("/metrics"),
    awsAuthenticationMethod: z.string().default("auto"),
    awsApiKey: z.string().optional(),
    awsSecret: z.string().optional(),
    usePublicIp: z.boolean().default(true),
    searchFilter: z.array(models.ItemsTypeSearchFilter$outboundSchema)
      .optional(),
    awsSecretKey: z.string().optional(),
    region: z.string().optional(),
    endpoint: z.string().optional(),
    signatureVersion: models.SignatureVersionOptions1$outboundSchema.default(
      "v4",
    ),
    reuseConnections: z.boolean().default(true),
    enableAssumeRole: z.boolean().default(false),
    assumeRoleArn: z.string().optional(),
    assumeRoleExternalId: z.string().optional(),
    durationSeconds: z.number().default(3600),
    username: z.string().optional(),
    password: z.string().optional(),
    credentialsSecret: z.string().optional(),
  });

export function inputPrometheusPqEnabledFalseWithPqConstraintToJSON(
  inputPrometheusPqEnabledFalseWithPqConstraint:
    InputPrometheusPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputPrometheusPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputPrometheusPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputPrometheusSendToRoutesFalseWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    dimensionList?: Array<string> | undefined;
    discoveryType: string;
    interval: number;
    logLevel: string;
    rejectUnauthorized: boolean;
    timeout: number;
    keepAliveTime: number;
    jobTimeout: string;
    maxMissedKeepAlives: number;
    ttl: string;
    ignoreGroupJobsLimit: boolean;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    authType: string;
    description?: string | undefined;
    targetList?: Array<string> | undefined;
    recordType: string;
    scrapePort: number;
    nameList?: Array<string> | undefined;
    scrapeProtocol: string;
    scrapePath: string;
    awsAuthenticationMethod: string;
    awsApiKey?: string | undefined;
    awsSecret?: string | undefined;
    usePublicIp: boolean;
    searchFilter?: Array<models.ItemsTypeSearchFilter$Outbound> | undefined;
    awsSecretKey?: string | undefined;
    region?: string | undefined;
    endpoint?: string | undefined;
    signatureVersion: string;
    reuseConnections: boolean;
    enableAssumeRole: boolean;
    assumeRoleArn?: string | undefined;
    assumeRoleExternalId?: string | undefined;
    durationSeconds: number;
    username?: string | undefined;
    password?: string | undefined;
    credentialsSecret?: string | undefined;
  };

/** @internal */
export const InputPrometheusSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputPrometheusSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputPrometheusSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputPrometheusType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    dimensionList: z.array(z.string()).optional(),
    discoveryType: InputPrometheusDiscoveryType$outboundSchema.default(
      "static",
    ),
    interval: z.number().default(15),
    logLevel: InputPrometheusLogLevel$outboundSchema.default("info"),
    rejectUnauthorized: z.boolean().default(true),
    timeout: z.number().default(0),
    keepAliveTime: z.number().default(30),
    jobTimeout: z.string().default("0"),
    maxMissedKeepAlives: z.number().default(3),
    ttl: z.string().default("4h"),
    ignoreGroupJobsLimit: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    authType: models.AuthenticationMethodOptionsSasl$outboundSchema.default(
      "manual",
    ),
    description: z.string().optional(),
    targetList: z.array(z.string()).optional(),
    recordType: models.RecordTypeOptions$outboundSchema.default("SRV"),
    scrapePort: z.number().default(9090),
    nameList: z.array(z.string()).optional(),
    scrapeProtocol: MetricsProtocol$outboundSchema.default("http"),
    scrapePath: z.string().default("/metrics"),
    awsAuthenticationMethod: z.string().default("auto"),
    awsApiKey: z.string().optional(),
    awsSecret: z.string().optional(),
    usePublicIp: z.boolean().default(true),
    searchFilter: z.array(models.ItemsTypeSearchFilter$outboundSchema)
      .optional(),
    awsSecretKey: z.string().optional(),
    region: z.string().optional(),
    endpoint: z.string().optional(),
    signatureVersion: models.SignatureVersionOptions1$outboundSchema.default(
      "v4",
    ),
    reuseConnections: z.boolean().default(true),
    enableAssumeRole: z.boolean().default(false),
    assumeRoleArn: z.string().optional(),
    assumeRoleExternalId: z.string().optional(),
    durationSeconds: z.number().default(3600),
    username: z.string().optional(),
    password: z.string().optional(),
    credentialsSecret: z.string().optional(),
  });

export function inputPrometheusSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputPrometheusSendToRoutesFalseWithConnectionsConstraint:
    InputPrometheusSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputPrometheusSendToRoutesFalseWithConnectionsConstraint$outboundSchema
      .parse(inputPrometheusSendToRoutesFalseWithConnectionsConstraint),
  );
}

/** @internal */
export type InputPrometheusSendToRoutesTrueWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    dimensionList?: Array<string> | undefined;
    discoveryType: string;
    interval: number;
    logLevel: string;
    rejectUnauthorized: boolean;
    timeout: number;
    keepAliveTime: number;
    jobTimeout: string;
    maxMissedKeepAlives: number;
    ttl: string;
    ignoreGroupJobsLimit: boolean;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    authType: string;
    description?: string | undefined;
    targetList?: Array<string> | undefined;
    recordType: string;
    scrapePort: number;
    nameList?: Array<string> | undefined;
    scrapeProtocol: string;
    scrapePath: string;
    awsAuthenticationMethod: string;
    awsApiKey?: string | undefined;
    awsSecret?: string | undefined;
    usePublicIp: boolean;
    searchFilter?: Array<models.ItemsTypeSearchFilter$Outbound> | undefined;
    awsSecretKey?: string | undefined;
    region?: string | undefined;
    endpoint?: string | undefined;
    signatureVersion: string;
    reuseConnections: boolean;
    enableAssumeRole: boolean;
    assumeRoleArn?: string | undefined;
    assumeRoleExternalId?: string | undefined;
    durationSeconds: number;
    username?: string | undefined;
    password?: string | undefined;
    credentialsSecret?: string | undefined;
  };

/** @internal */
export const InputPrometheusSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputPrometheusSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputPrometheusSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputPrometheusType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    dimensionList: z.array(z.string()).optional(),
    discoveryType: InputPrometheusDiscoveryType$outboundSchema.default(
      "static",
    ),
    interval: z.number().default(15),
    logLevel: InputPrometheusLogLevel$outboundSchema.default("info"),
    rejectUnauthorized: z.boolean().default(true),
    timeout: z.number().default(0),
    keepAliveTime: z.number().default(30),
    jobTimeout: z.string().default("0"),
    maxMissedKeepAlives: z.number().default(3),
    ttl: z.string().default("4h"),
    ignoreGroupJobsLimit: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    authType: models.AuthenticationMethodOptionsSasl$outboundSchema.default(
      "manual",
    ),
    description: z.string().optional(),
    targetList: z.array(z.string()).optional(),
    recordType: models.RecordTypeOptions$outboundSchema.default("SRV"),
    scrapePort: z.number().default(9090),
    nameList: z.array(z.string()).optional(),
    scrapeProtocol: MetricsProtocol$outboundSchema.default("http"),
    scrapePath: z.string().default("/metrics"),
    awsAuthenticationMethod: z.string().default("auto"),
    awsApiKey: z.string().optional(),
    awsSecret: z.string().optional(),
    usePublicIp: z.boolean().default(true),
    searchFilter: z.array(models.ItemsTypeSearchFilter$outboundSchema)
      .optional(),
    awsSecretKey: z.string().optional(),
    region: z.string().optional(),
    endpoint: z.string().optional(),
    signatureVersion: models.SignatureVersionOptions1$outboundSchema.default(
      "v4",
    ),
    reuseConnections: z.boolean().default(true),
    enableAssumeRole: z.boolean().default(false),
    assumeRoleArn: z.string().optional(),
    assumeRoleExternalId: z.string().optional(),
    durationSeconds: z.number().default(3600),
    username: z.string().optional(),
    password: z.string().optional(),
    credentialsSecret: z.string().optional(),
  });

export function inputPrometheusSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputPrometheusSendToRoutesTrueWithConnectionsConstraint:
    InputPrometheusSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputPrometheusSendToRoutesTrueWithConnectionsConstraint$outboundSchema
      .parse(inputPrometheusSendToRoutesTrueWithConnectionsConstraint),
  );
}

/** @internal */
export type InputPrometheus$Outbound =
  | InputPrometheusSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputPrometheusSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputPrometheusPqEnabledFalseWithPqConstraint$Outbound
  | InputPrometheusPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputPrometheus$outboundSchema: z.ZodType<
  InputPrometheus$Outbound,
  z.ZodTypeDef,
  InputPrometheus
> = z.union([
  z.lazy(() =>
    InputPrometheusSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputPrometheusSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputPrometheusPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputPrometheusPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputPrometheusToJSON(
  inputPrometheus: InputPrometheus,
): string {
  return JSON.stringify(InputPrometheus$outboundSchema.parse(inputPrometheus));
}

/** @internal */
export const InputPrometheusRwType$outboundSchema: z.ZodNativeEnum<
  typeof InputPrometheusRwType
> = z.nativeEnum(InputPrometheusRwType);

/** @internal */
export type InputPrometheusRwPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  activityLogSampleRate: number;
  requestTimeout: number;
  socketTimeout: number;
  keepAliveTimeout: number;
  enableHealthCheck: boolean;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  prometheusAPI: string;
  authType: string;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
  username?: string | undefined;
  password?: string | undefined;
  token?: string | undefined;
  credentialsSecret?: string | undefined;
  textSecret?: string | undefined;
  loginUrl?: string | undefined;
  secretParamName?: string | undefined;
  secret?: string | undefined;
  tokenAttributeName?: string | undefined;
  authHeaderExpr: string;
  tokenTimeoutSecs: number;
  oauthParams?: Array<models.ItemsTypeOauthParams$Outbound> | undefined;
  oauthHeaders?: Array<models.ItemsTypeOauthHeaders$Outbound> | undefined;
};

/** @internal */
export const InputPrometheusRwPqEnabledTrueWithPqConstraint$outboundSchema:
  z.ZodType<
    InputPrometheusRwPqEnabledTrueWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputPrometheusRwPqEnabledTrueWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputPrometheusRwType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    prometheusAPI: z.string().default("/write"),
    authType: models.AuthenticationTypeOptionsPrometheusAuth$outboundSchema
      .default("none"),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
    username: z.string().optional(),
    password: z.string().optional(),
    token: z.string().optional(),
    credentialsSecret: z.string().optional(),
    textSecret: z.string().optional(),
    loginUrl: z.string().optional(),
    secretParamName: z.string().optional(),
    secret: z.string().optional(),
    tokenAttributeName: z.string().optional(),
    authHeaderExpr: z.string().default("`Bearer ${token}`"),
    tokenTimeoutSecs: z.number().default(3600),
    oauthParams: z.array(models.ItemsTypeOauthParams$outboundSchema).optional(),
    oauthHeaders: z.array(models.ItemsTypeOauthHeaders$outboundSchema)
      .optional(),
  });

export function inputPrometheusRwPqEnabledTrueWithPqConstraintToJSON(
  inputPrometheusRwPqEnabledTrueWithPqConstraint:
    InputPrometheusRwPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputPrometheusRwPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputPrometheusRwPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputPrometheusRwPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  activityLogSampleRate: number;
  requestTimeout: number;
  socketTimeout: number;
  keepAliveTimeout: number;
  enableHealthCheck: boolean;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  prometheusAPI: string;
  authType: string;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
  username?: string | undefined;
  password?: string | undefined;
  token?: string | undefined;
  credentialsSecret?: string | undefined;
  textSecret?: string | undefined;
  loginUrl?: string | undefined;
  secretParamName?: string | undefined;
  secret?: string | undefined;
  tokenAttributeName?: string | undefined;
  authHeaderExpr: string;
  tokenTimeoutSecs: number;
  oauthParams?: Array<models.ItemsTypeOauthParams$Outbound> | undefined;
  oauthHeaders?: Array<models.ItemsTypeOauthHeaders$Outbound> | undefined;
};

/** @internal */
export const InputPrometheusRwPqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputPrometheusRwPqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputPrometheusRwPqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputPrometheusRwType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    prometheusAPI: z.string().default("/write"),
    authType: models.AuthenticationTypeOptionsPrometheusAuth$outboundSchema
      .default("none"),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
    username: z.string().optional(),
    password: z.string().optional(),
    token: z.string().optional(),
    credentialsSecret: z.string().optional(),
    textSecret: z.string().optional(),
    loginUrl: z.string().optional(),
    secretParamName: z.string().optional(),
    secret: z.string().optional(),
    tokenAttributeName: z.string().optional(),
    authHeaderExpr: z.string().default("`Bearer ${token}`"),
    tokenTimeoutSecs: z.number().default(3600),
    oauthParams: z.array(models.ItemsTypeOauthParams$outboundSchema).optional(),
    oauthHeaders: z.array(models.ItemsTypeOauthHeaders$outboundSchema)
      .optional(),
  });

export function inputPrometheusRwPqEnabledFalseWithPqConstraintToJSON(
  inputPrometheusRwPqEnabledFalseWithPqConstraint:
    InputPrometheusRwPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputPrometheusRwPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputPrometheusRwPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputPrometheusRwSendToRoutesFalseWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    host: string;
    port: number;
    tls?: models.TlsSettingsServerSideType$Outbound | undefined;
    maxActiveReq: number;
    maxRequestsPerSocket: number;
    enableProxyHeader: boolean;
    captureHeaders: boolean;
    activityLogSampleRate: number;
    requestTimeout: number;
    socketTimeout: number;
    keepAliveTimeout: number;
    enableHealthCheck: boolean;
    ipAllowlistRegex: string;
    ipDenylistRegex: string;
    prometheusAPI: string;
    authType: string;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    description?: string | undefined;
    username?: string | undefined;
    password?: string | undefined;
    token?: string | undefined;
    credentialsSecret?: string | undefined;
    textSecret?: string | undefined;
    loginUrl?: string | undefined;
    secretParamName?: string | undefined;
    secret?: string | undefined;
    tokenAttributeName?: string | undefined;
    authHeaderExpr: string;
    tokenTimeoutSecs: number;
    oauthParams?: Array<models.ItemsTypeOauthParams$Outbound> | undefined;
    oauthHeaders?: Array<models.ItemsTypeOauthHeaders$Outbound> | undefined;
  };

/** @internal */
export const InputPrometheusRwSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputPrometheusRwSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputPrometheusRwSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputPrometheusRwType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    prometheusAPI: z.string().default("/write"),
    authType: models.AuthenticationTypeOptionsPrometheusAuth$outboundSchema
      .default("none"),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
    username: z.string().optional(),
    password: z.string().optional(),
    token: z.string().optional(),
    credentialsSecret: z.string().optional(),
    textSecret: z.string().optional(),
    loginUrl: z.string().optional(),
    secretParamName: z.string().optional(),
    secret: z.string().optional(),
    tokenAttributeName: z.string().optional(),
    authHeaderExpr: z.string().default("`Bearer ${token}`"),
    tokenTimeoutSecs: z.number().default(3600),
    oauthParams: z.array(models.ItemsTypeOauthParams$outboundSchema).optional(),
    oauthHeaders: z.array(models.ItemsTypeOauthHeaders$outboundSchema)
      .optional(),
  });

export function inputPrometheusRwSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputPrometheusRwSendToRoutesFalseWithConnectionsConstraint:
    InputPrometheusRwSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputPrometheusRwSendToRoutesFalseWithConnectionsConstraint$outboundSchema
      .parse(inputPrometheusRwSendToRoutesFalseWithConnectionsConstraint),
  );
}

/** @internal */
export type InputPrometheusRwSendToRoutesTrueWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    host: string;
    port: number;
    tls?: models.TlsSettingsServerSideType$Outbound | undefined;
    maxActiveReq: number;
    maxRequestsPerSocket: number;
    enableProxyHeader: boolean;
    captureHeaders: boolean;
    activityLogSampleRate: number;
    requestTimeout: number;
    socketTimeout: number;
    keepAliveTimeout: number;
    enableHealthCheck: boolean;
    ipAllowlistRegex: string;
    ipDenylistRegex: string;
    prometheusAPI: string;
    authType: string;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    description?: string | undefined;
    username?: string | undefined;
    password?: string | undefined;
    token?: string | undefined;
    credentialsSecret?: string | undefined;
    textSecret?: string | undefined;
    loginUrl?: string | undefined;
    secretParamName?: string | undefined;
    secret?: string | undefined;
    tokenAttributeName?: string | undefined;
    authHeaderExpr: string;
    tokenTimeoutSecs: number;
    oauthParams?: Array<models.ItemsTypeOauthParams$Outbound> | undefined;
    oauthHeaders?: Array<models.ItemsTypeOauthHeaders$Outbound> | undefined;
  };

/** @internal */
export const InputPrometheusRwSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputPrometheusRwSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputPrometheusRwSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputPrometheusRwType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    prometheusAPI: z.string().default("/write"),
    authType: models.AuthenticationTypeOptionsPrometheusAuth$outboundSchema
      .default("none"),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
    username: z.string().optional(),
    password: z.string().optional(),
    token: z.string().optional(),
    credentialsSecret: z.string().optional(),
    textSecret: z.string().optional(),
    loginUrl: z.string().optional(),
    secretParamName: z.string().optional(),
    secret: z.string().optional(),
    tokenAttributeName: z.string().optional(),
    authHeaderExpr: z.string().default("`Bearer ${token}`"),
    tokenTimeoutSecs: z.number().default(3600),
    oauthParams: z.array(models.ItemsTypeOauthParams$outboundSchema).optional(),
    oauthHeaders: z.array(models.ItemsTypeOauthHeaders$outboundSchema)
      .optional(),
  });

export function inputPrometheusRwSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputPrometheusRwSendToRoutesTrueWithConnectionsConstraint:
    InputPrometheusRwSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputPrometheusRwSendToRoutesTrueWithConnectionsConstraint$outboundSchema
      .parse(inputPrometheusRwSendToRoutesTrueWithConnectionsConstraint),
  );
}

/** @internal */
export type InputPrometheusRw$Outbound =
  | InputPrometheusRwSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputPrometheusRwSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputPrometheusRwPqEnabledFalseWithPqConstraint$Outbound
  | InputPrometheusRwPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputPrometheusRw$outboundSchema: z.ZodType<
  InputPrometheusRw$Outbound,
  z.ZodTypeDef,
  InputPrometheusRw
> = z.union([
  z.lazy(() =>
    InputPrometheusRwSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputPrometheusRwSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputPrometheusRwPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputPrometheusRwPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputPrometheusRwToJSON(
  inputPrometheusRw: InputPrometheusRw,
): string {
  return JSON.stringify(
    InputPrometheusRw$outboundSchema.parse(inputPrometheusRw),
  );
}

/** @internal */
export const InputLokiType$outboundSchema: z.ZodNativeEnum<
  typeof InputLokiType
> = z.nativeEnum(InputLokiType);

/** @internal */
export type InputLokiPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  activityLogSampleRate: number;
  requestTimeout: number;
  socketTimeout: number;
  keepAliveTimeout: number;
  enableHealthCheck: boolean;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  lokiAPI: string;
  authType: string;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
  username?: string | undefined;
  password?: string | undefined;
  token?: string | undefined;
  credentialsSecret?: string | undefined;
  textSecret?: string | undefined;
  loginUrl?: string | undefined;
  secretParamName?: string | undefined;
  secret?: string | undefined;
  tokenAttributeName?: string | undefined;
  authHeaderExpr: string;
  tokenTimeoutSecs: number;
  oauthParams?: Array<models.ItemsTypeOauthParams$Outbound> | undefined;
  oauthHeaders?: Array<models.ItemsTypeOauthHeaders$Outbound> | undefined;
};

/** @internal */
export const InputLokiPqEnabledTrueWithPqConstraint$outboundSchema: z.ZodType<
  InputLokiPqEnabledTrueWithPqConstraint$Outbound,
  z.ZodTypeDef,
  InputLokiPqEnabledTrueWithPqConstraint
> = z.object({
  pqEnabled: z.boolean().default(false),
  pq: models.PqType$outboundSchema.optional(),
  id: z.string(),
  type: InputLokiType$outboundSchema,
  disabled: z.boolean().default(false),
  pipeline: z.string().optional(),
  sendToRoutes: z.boolean().default(true),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
  host: z.string().default("0.0.0.0"),
  port: z.number(),
  tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
  maxActiveReq: z.number().default(256),
  maxRequestsPerSocket: z.number().int().default(0),
  enableProxyHeader: z.boolean().default(false),
  captureHeaders: z.boolean().default(false),
  activityLogSampleRate: z.number().default(100),
  requestTimeout: z.number().default(0),
  socketTimeout: z.number().default(0),
  keepAliveTimeout: z.number().default(5),
  enableHealthCheck: z.boolean().default(false),
  ipAllowlistRegex: z.string().default("/.*/"),
  ipDenylistRegex: z.string().default("/^$/"),
  lokiAPI: z.string().default("/loki/api/v1/push"),
  authType: models.AuthenticationTypeOptionsLokiAuth$outboundSchema.default(
    "none",
  ),
  metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
    .optional(),
  description: z.string().optional(),
  username: z.string().optional(),
  password: z.string().optional(),
  token: z.string().optional(),
  credentialsSecret: z.string().optional(),
  textSecret: z.string().optional(),
  loginUrl: z.string().optional(),
  secretParamName: z.string().optional(),
  secret: z.string().optional(),
  tokenAttributeName: z.string().optional(),
  authHeaderExpr: z.string().default("`Bearer ${token}`"),
  tokenTimeoutSecs: z.number().default(3600),
  oauthParams: z.array(models.ItemsTypeOauthParams$outboundSchema).optional(),
  oauthHeaders: z.array(models.ItemsTypeOauthHeaders$outboundSchema).optional(),
});

export function inputLokiPqEnabledTrueWithPqConstraintToJSON(
  inputLokiPqEnabledTrueWithPqConstraint:
    InputLokiPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputLokiPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputLokiPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputLokiPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  activityLogSampleRate: number;
  requestTimeout: number;
  socketTimeout: number;
  keepAliveTimeout: number;
  enableHealthCheck: boolean;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  lokiAPI: string;
  authType: string;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
  username?: string | undefined;
  password?: string | undefined;
  token?: string | undefined;
  credentialsSecret?: string | undefined;
  textSecret?: string | undefined;
  loginUrl?: string | undefined;
  secretParamName?: string | undefined;
  secret?: string | undefined;
  tokenAttributeName?: string | undefined;
  authHeaderExpr: string;
  tokenTimeoutSecs: number;
  oauthParams?: Array<models.ItemsTypeOauthParams$Outbound> | undefined;
  oauthHeaders?: Array<models.ItemsTypeOauthHeaders$Outbound> | undefined;
};

/** @internal */
export const InputLokiPqEnabledFalseWithPqConstraint$outboundSchema: z.ZodType<
  InputLokiPqEnabledFalseWithPqConstraint$Outbound,
  z.ZodTypeDef,
  InputLokiPqEnabledFalseWithPqConstraint
> = z.object({
  pqEnabled: z.boolean().default(false),
  pq: models.PqType$outboundSchema.optional(),
  id: z.string(),
  type: InputLokiType$outboundSchema,
  disabled: z.boolean().default(false),
  pipeline: z.string().optional(),
  sendToRoutes: z.boolean().default(true),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
  host: z.string().default("0.0.0.0"),
  port: z.number(),
  tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
  maxActiveReq: z.number().default(256),
  maxRequestsPerSocket: z.number().int().default(0),
  enableProxyHeader: z.boolean().default(false),
  captureHeaders: z.boolean().default(false),
  activityLogSampleRate: z.number().default(100),
  requestTimeout: z.number().default(0),
  socketTimeout: z.number().default(0),
  keepAliveTimeout: z.number().default(5),
  enableHealthCheck: z.boolean().default(false),
  ipAllowlistRegex: z.string().default("/.*/"),
  ipDenylistRegex: z.string().default("/^$/"),
  lokiAPI: z.string().default("/loki/api/v1/push"),
  authType: models.AuthenticationTypeOptionsLokiAuth$outboundSchema.default(
    "none",
  ),
  metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
    .optional(),
  description: z.string().optional(),
  username: z.string().optional(),
  password: z.string().optional(),
  token: z.string().optional(),
  credentialsSecret: z.string().optional(),
  textSecret: z.string().optional(),
  loginUrl: z.string().optional(),
  secretParamName: z.string().optional(),
  secret: z.string().optional(),
  tokenAttributeName: z.string().optional(),
  authHeaderExpr: z.string().default("`Bearer ${token}`"),
  tokenTimeoutSecs: z.number().default(3600),
  oauthParams: z.array(models.ItemsTypeOauthParams$outboundSchema).optional(),
  oauthHeaders: z.array(models.ItemsTypeOauthHeaders$outboundSchema).optional(),
});

export function inputLokiPqEnabledFalseWithPqConstraintToJSON(
  inputLokiPqEnabledFalseWithPqConstraint:
    InputLokiPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputLokiPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputLokiPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputLokiSendToRoutesFalseWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  host: string;
  port: number;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  activityLogSampleRate: number;
  requestTimeout: number;
  socketTimeout: number;
  keepAliveTimeout: number;
  enableHealthCheck: boolean;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  lokiAPI: string;
  authType: string;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
  username?: string | undefined;
  password?: string | undefined;
  token?: string | undefined;
  credentialsSecret?: string | undefined;
  textSecret?: string | undefined;
  loginUrl?: string | undefined;
  secretParamName?: string | undefined;
  secret?: string | undefined;
  tokenAttributeName?: string | undefined;
  authHeaderExpr: string;
  tokenTimeoutSecs: number;
  oauthParams?: Array<models.ItemsTypeOauthParams$Outbound> | undefined;
  oauthHeaders?: Array<models.ItemsTypeOauthHeaders$Outbound> | undefined;
};

/** @internal */
export const InputLokiSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputLokiSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputLokiSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputLokiType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    lokiAPI: z.string().default("/loki/api/v1/push"),
    authType: models.AuthenticationTypeOptionsLokiAuth$outboundSchema.default(
      "none",
    ),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
    username: z.string().optional(),
    password: z.string().optional(),
    token: z.string().optional(),
    credentialsSecret: z.string().optional(),
    textSecret: z.string().optional(),
    loginUrl: z.string().optional(),
    secretParamName: z.string().optional(),
    secret: z.string().optional(),
    tokenAttributeName: z.string().optional(),
    authHeaderExpr: z.string().default("`Bearer ${token}`"),
    tokenTimeoutSecs: z.number().default(3600),
    oauthParams: z.array(models.ItemsTypeOauthParams$outboundSchema).optional(),
    oauthHeaders: z.array(models.ItemsTypeOauthHeaders$outboundSchema)
      .optional(),
  });

export function inputLokiSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputLokiSendToRoutesFalseWithConnectionsConstraint:
    InputLokiSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputLokiSendToRoutesFalseWithConnectionsConstraint$outboundSchema.parse(
      inputLokiSendToRoutesFalseWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputLokiSendToRoutesTrueWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  host: string;
  port: number;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  activityLogSampleRate: number;
  requestTimeout: number;
  socketTimeout: number;
  keepAliveTimeout: number;
  enableHealthCheck: boolean;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  lokiAPI: string;
  authType: string;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
  username?: string | undefined;
  password?: string | undefined;
  token?: string | undefined;
  credentialsSecret?: string | undefined;
  textSecret?: string | undefined;
  loginUrl?: string | undefined;
  secretParamName?: string | undefined;
  secret?: string | undefined;
  tokenAttributeName?: string | undefined;
  authHeaderExpr: string;
  tokenTimeoutSecs: number;
  oauthParams?: Array<models.ItemsTypeOauthParams$Outbound> | undefined;
  oauthHeaders?: Array<models.ItemsTypeOauthHeaders$Outbound> | undefined;
};

/** @internal */
export const InputLokiSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputLokiSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputLokiSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputLokiType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    lokiAPI: z.string().default("/loki/api/v1/push"),
    authType: models.AuthenticationTypeOptionsLokiAuth$outboundSchema.default(
      "none",
    ),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
    username: z.string().optional(),
    password: z.string().optional(),
    token: z.string().optional(),
    credentialsSecret: z.string().optional(),
    textSecret: z.string().optional(),
    loginUrl: z.string().optional(),
    secretParamName: z.string().optional(),
    secret: z.string().optional(),
    tokenAttributeName: z.string().optional(),
    authHeaderExpr: z.string().default("`Bearer ${token}`"),
    tokenTimeoutSecs: z.number().default(3600),
    oauthParams: z.array(models.ItemsTypeOauthParams$outboundSchema).optional(),
    oauthHeaders: z.array(models.ItemsTypeOauthHeaders$outboundSchema)
      .optional(),
  });

export function inputLokiSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputLokiSendToRoutesTrueWithConnectionsConstraint:
    InputLokiSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputLokiSendToRoutesTrueWithConnectionsConstraint$outboundSchema.parse(
      inputLokiSendToRoutesTrueWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputLoki$Outbound =
  | InputLokiSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputLokiSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputLokiPqEnabledFalseWithPqConstraint$Outbound
  | InputLokiPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputLoki$outboundSchema: z.ZodType<
  InputLoki$Outbound,
  z.ZodTypeDef,
  InputLoki
> = z.union([
  z.lazy(() =>
    InputLokiSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputLokiSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputLokiPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputLokiPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputLokiToJSON(inputLoki: InputLoki): string {
  return JSON.stringify(InputLoki$outboundSchema.parse(inputLoki));
}

/** @internal */
export const InputGrafanaType2$outboundSchema: z.ZodNativeEnum<
  typeof InputGrafanaType2
> = z.nativeEnum(InputGrafanaType2);

/** @internal */
export type PrometheusAuth2$Outbound = {
  authType: string;
  username?: string | undefined;
  password?: string | undefined;
  token?: string | undefined;
  credentialsSecret?: string | undefined;
  textSecret?: string | undefined;
  loginUrl?: string | undefined;
  secretParamName?: string | undefined;
  secret?: string | undefined;
  tokenAttributeName?: string | undefined;
  authHeaderExpr: string;
  tokenTimeoutSecs: number;
  oauthParams?: Array<models.ItemsTypeOauthParams$Outbound> | undefined;
  oauthHeaders?: Array<models.ItemsTypeOauthHeaders$Outbound> | undefined;
};

/** @internal */
export const PrometheusAuth2$outboundSchema: z.ZodType<
  PrometheusAuth2$Outbound,
  z.ZodTypeDef,
  PrometheusAuth2
> = z.object({
  authType: models.AuthenticationTypeOptionsPrometheusAuth$outboundSchema
    .default("none"),
  username: z.string().optional(),
  password: z.string().optional(),
  token: z.string().optional(),
  credentialsSecret: z.string().optional(),
  textSecret: z.string().optional(),
  loginUrl: z.string().optional(),
  secretParamName: z.string().optional(),
  secret: z.string().optional(),
  tokenAttributeName: z.string().optional(),
  authHeaderExpr: z.string().default("`Bearer ${token}`"),
  tokenTimeoutSecs: z.number().default(3600),
  oauthParams: z.array(models.ItemsTypeOauthParams$outboundSchema).optional(),
  oauthHeaders: z.array(models.ItemsTypeOauthHeaders$outboundSchema).optional(),
});

export function prometheusAuth2ToJSON(
  prometheusAuth2: PrometheusAuth2,
): string {
  return JSON.stringify(PrometheusAuth2$outboundSchema.parse(prometheusAuth2));
}

/** @internal */
export type LokiAuth2$Outbound = {
  authType: string;
  username?: string | undefined;
  password?: string | undefined;
  token?: string | undefined;
  credentialsSecret?: string | undefined;
  textSecret?: string | undefined;
  loginUrl?: string | undefined;
  secretParamName?: string | undefined;
  secret?: string | undefined;
  tokenAttributeName?: string | undefined;
  authHeaderExpr: string;
  tokenTimeoutSecs: number;
  oauthParams?: Array<models.ItemsTypeOauthParams$Outbound> | undefined;
  oauthHeaders?: Array<models.ItemsTypeOauthHeaders$Outbound> | undefined;
};

/** @internal */
export const LokiAuth2$outboundSchema: z.ZodType<
  LokiAuth2$Outbound,
  z.ZodTypeDef,
  LokiAuth2
> = z.object({
  authType: models.AuthenticationTypeOptionsLokiAuth$outboundSchema.default(
    "none",
  ),
  username: z.string().optional(),
  password: z.string().optional(),
  token: z.string().optional(),
  credentialsSecret: z.string().optional(),
  textSecret: z.string().optional(),
  loginUrl: z.string().optional(),
  secretParamName: z.string().optional(),
  secret: z.string().optional(),
  tokenAttributeName: z.string().optional(),
  authHeaderExpr: z.string().default("`Bearer ${token}`"),
  tokenTimeoutSecs: z.number().default(3600),
  oauthParams: z.array(models.ItemsTypeOauthParams$outboundSchema).optional(),
  oauthHeaders: z.array(models.ItemsTypeOauthHeaders$outboundSchema).optional(),
});

export function lokiAuth2ToJSON(lokiAuth2: LokiAuth2): string {
  return JSON.stringify(LokiAuth2$outboundSchema.parse(lokiAuth2));
}

/** @internal */
export type InputGrafanaGrafana2$Outbound = {
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  pq?: models.PqType$Outbound | undefined;
  host: string;
  port: number;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  activityLogSampleRate: number;
  requestTimeout: number;
  socketTimeout: number;
  keepAliveTimeout: number;
  enableHealthCheck: boolean;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  prometheusAPI: string;
  lokiAPI: string;
  prometheusAuth?: PrometheusAuth2$Outbound | undefined;
  lokiAuth?: LokiAuth2$Outbound | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputGrafanaGrafana2$outboundSchema: z.ZodType<
  InputGrafanaGrafana2$Outbound,
  z.ZodTypeDef,
  InputGrafanaGrafana2
> = z.object({
  id: z.string(),
  type: InputGrafanaType2$outboundSchema,
  disabled: z.boolean().default(false),
  pipeline: z.string().optional(),
  sendToRoutes: z.boolean().default(true),
  environment: z.string().optional(),
  pqEnabled: z.boolean().default(false),
  streamtags: z.array(z.string()).optional(),
  connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
  pq: models.PqType$outboundSchema.optional(),
  host: z.string().default("0.0.0.0"),
  port: z.number(),
  tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
  maxActiveReq: z.number().default(256),
  maxRequestsPerSocket: z.number().int().default(0),
  enableProxyHeader: z.boolean().default(false),
  captureHeaders: z.boolean().default(false),
  activityLogSampleRate: z.number().default(100),
  requestTimeout: z.number().default(0),
  socketTimeout: z.number().default(0),
  keepAliveTimeout: z.number().default(5),
  enableHealthCheck: z.boolean().default(false),
  ipAllowlistRegex: z.string().default("/.*/"),
  ipDenylistRegex: z.string().default("/^$/"),
  prometheusAPI: z.string().default("/api/prom/push"),
  lokiAPI: z.string().default("/loki/api/v1/push"),
  prometheusAuth: z.lazy(() => PrometheusAuth2$outboundSchema).optional(),
  lokiAuth: z.lazy(() => LokiAuth2$outboundSchema).optional(),
  metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
    .optional(),
  description: z.string().optional(),
});

export function inputGrafanaGrafana2ToJSON(
  inputGrafanaGrafana2: InputGrafanaGrafana2,
): string {
  return JSON.stringify(
    InputGrafanaGrafana2$outboundSchema.parse(inputGrafanaGrafana2),
  );
}

/** @internal */
export const InputGrafanaType1$outboundSchema: z.ZodNativeEnum<
  typeof InputGrafanaType1
> = z.nativeEnum(InputGrafanaType1);

/** @internal */
export type PrometheusAuth1$Outbound = {
  authType: string;
  username?: string | undefined;
  password?: string | undefined;
  token?: string | undefined;
  credentialsSecret?: string | undefined;
  textSecret?: string | undefined;
  loginUrl?: string | undefined;
  secretParamName?: string | undefined;
  secret?: string | undefined;
  tokenAttributeName?: string | undefined;
  authHeaderExpr: string;
  tokenTimeoutSecs: number;
  oauthParams?: Array<models.ItemsTypeOauthParams$Outbound> | undefined;
  oauthHeaders?: Array<models.ItemsTypeOauthHeaders$Outbound> | undefined;
};

/** @internal */
export const PrometheusAuth1$outboundSchema: z.ZodType<
  PrometheusAuth1$Outbound,
  z.ZodTypeDef,
  PrometheusAuth1
> = z.object({
  authType: models.AuthenticationTypeOptionsPrometheusAuth$outboundSchema
    .default("none"),
  username: z.string().optional(),
  password: z.string().optional(),
  token: z.string().optional(),
  credentialsSecret: z.string().optional(),
  textSecret: z.string().optional(),
  loginUrl: z.string().optional(),
  secretParamName: z.string().optional(),
  secret: z.string().optional(),
  tokenAttributeName: z.string().optional(),
  authHeaderExpr: z.string().default("`Bearer ${token}`"),
  tokenTimeoutSecs: z.number().default(3600),
  oauthParams: z.array(models.ItemsTypeOauthParams$outboundSchema).optional(),
  oauthHeaders: z.array(models.ItemsTypeOauthHeaders$outboundSchema).optional(),
});

export function prometheusAuth1ToJSON(
  prometheusAuth1: PrometheusAuth1,
): string {
  return JSON.stringify(PrometheusAuth1$outboundSchema.parse(prometheusAuth1));
}

/** @internal */
export type LokiAuth1$Outbound = {
  authType: string;
  username?: string | undefined;
  password?: string | undefined;
  token?: string | undefined;
  credentialsSecret?: string | undefined;
  textSecret?: string | undefined;
  loginUrl?: string | undefined;
  secretParamName?: string | undefined;
  secret?: string | undefined;
  tokenAttributeName?: string | undefined;
  authHeaderExpr: string;
  tokenTimeoutSecs: number;
  oauthParams?: Array<models.ItemsTypeOauthParams$Outbound> | undefined;
  oauthHeaders?: Array<models.ItemsTypeOauthHeaders$Outbound> | undefined;
};

/** @internal */
export const LokiAuth1$outboundSchema: z.ZodType<
  LokiAuth1$Outbound,
  z.ZodTypeDef,
  LokiAuth1
> = z.object({
  authType: models.AuthenticationTypeOptionsLokiAuth$outboundSchema.default(
    "none",
  ),
  username: z.string().optional(),
  password: z.string().optional(),
  token: z.string().optional(),
  credentialsSecret: z.string().optional(),
  textSecret: z.string().optional(),
  loginUrl: z.string().optional(),
  secretParamName: z.string().optional(),
  secret: z.string().optional(),
  tokenAttributeName: z.string().optional(),
  authHeaderExpr: z.string().default("`Bearer ${token}`"),
  tokenTimeoutSecs: z.number().default(3600),
  oauthParams: z.array(models.ItemsTypeOauthParams$outboundSchema).optional(),
  oauthHeaders: z.array(models.ItemsTypeOauthHeaders$outboundSchema).optional(),
});

export function lokiAuth1ToJSON(lokiAuth1: LokiAuth1): string {
  return JSON.stringify(LokiAuth1$outboundSchema.parse(lokiAuth1));
}

/** @internal */
export type InputGrafanaGrafana1$Outbound = {
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  pq?: models.PqType$Outbound | undefined;
  host: string;
  port: number;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  activityLogSampleRate: number;
  requestTimeout: number;
  socketTimeout: number;
  keepAliveTimeout: number;
  enableHealthCheck: boolean;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  prometheusAPI: string;
  lokiAPI: string;
  prometheusAuth?: PrometheusAuth1$Outbound | undefined;
  lokiAuth?: LokiAuth1$Outbound | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputGrafanaGrafana1$outboundSchema: z.ZodType<
  InputGrafanaGrafana1$Outbound,
  z.ZodTypeDef,
  InputGrafanaGrafana1
> = z.object({
  id: z.string(),
  type: InputGrafanaType1$outboundSchema,
  disabled: z.boolean().default(false),
  pipeline: z.string().optional(),
  sendToRoutes: z.boolean().default(true),
  environment: z.string().optional(),
  pqEnabled: z.boolean().default(false),
  streamtags: z.array(z.string()).optional(),
  connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
  pq: models.PqType$outboundSchema.optional(),
  host: z.string().default("0.0.0.0"),
  port: z.number(),
  tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
  maxActiveReq: z.number().default(256),
  maxRequestsPerSocket: z.number().int().default(0),
  enableProxyHeader: z.boolean().default(false),
  captureHeaders: z.boolean().default(false),
  activityLogSampleRate: z.number().default(100),
  requestTimeout: z.number().default(0),
  socketTimeout: z.number().default(0),
  keepAliveTimeout: z.number().default(5),
  enableHealthCheck: z.boolean().default(false),
  ipAllowlistRegex: z.string().default("/.*/"),
  ipDenylistRegex: z.string().default("/^$/"),
  prometheusAPI: z.string().default("/api/prom/push"),
  lokiAPI: z.string().default("/loki/api/v1/push"),
  prometheusAuth: z.lazy(() => PrometheusAuth1$outboundSchema).optional(),
  lokiAuth: z.lazy(() => LokiAuth1$outboundSchema).optional(),
  metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
    .optional(),
  description: z.string().optional(),
});

export function inputGrafanaGrafana1ToJSON(
  inputGrafanaGrafana1: InputGrafanaGrafana1,
): string {
  return JSON.stringify(
    InputGrafanaGrafana1$outboundSchema.parse(inputGrafanaGrafana1),
  );
}

/** @internal */
export type InputGrafana$Outbound =
  | InputGrafanaGrafana1$Outbound
  | InputGrafanaGrafana2$Outbound;

/** @internal */
export const InputGrafana$outboundSchema: z.ZodType<
  InputGrafana$Outbound,
  z.ZodTypeDef,
  InputGrafana
> = z.union([
  z.lazy(() => InputGrafanaGrafana1$outboundSchema),
  z.lazy(() => InputGrafanaGrafana2$outboundSchema),
]);

export function inputGrafanaToJSON(inputGrafana: InputGrafana): string {
  return JSON.stringify(InputGrafana$outboundSchema.parse(inputGrafana));
}

/** @internal */
export const InputConfluentCloudType$outboundSchema: z.ZodNativeEnum<
  typeof InputConfluentCloudType
> = z.nativeEnum(InputConfluentCloudType);

/** @internal */
export type InputConfluentCloudPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  brokers: Array<string>;
  tls?: models.TlsSettingsClientSideType1$Outbound | undefined;
  topics: Array<string>;
  groupId: string;
  fromBeginning: boolean;
  kafkaSchemaRegistry?:
    | models.KafkaSchemaRegistryAuthenticationType$Outbound
    | undefined;
  connectionTimeout: number;
  requestTimeout: number;
  maxRetries: number;
  maxBackOff: number;
  initialBackoff: number;
  backoffRate: number;
  authenticationTimeout: number;
  reauthenticationThreshold: number;
  sasl?: models.AuthenticationType$Outbound | undefined;
  sessionTimeout: number;
  rebalanceTimeout: number;
  heartbeatInterval: number;
  autoCommitInterval?: number | undefined;
  autoCommitThreshold?: number | undefined;
  maxBytesPerPartition: number;
  maxBytes: number;
  maxSocketErrors: number;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputConfluentCloudPqEnabledTrueWithPqConstraint$outboundSchema:
  z.ZodType<
    InputConfluentCloudPqEnabledTrueWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputConfluentCloudPqEnabledTrueWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputConfluentCloudType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    brokers: z.array(z.string()),
    tls: models.TlsSettingsClientSideType1$outboundSchema.optional(),
    topics: z.array(z.string()),
    groupId: z.string().default("Cribl"),
    fromBeginning: z.boolean().default(true),
    kafkaSchemaRegistry: models
      .KafkaSchemaRegistryAuthenticationType$outboundSchema.optional(),
    connectionTimeout: z.number().default(10000),
    requestTimeout: z.number().default(60000),
    maxRetries: z.number().default(5),
    maxBackOff: z.number().default(30000),
    initialBackoff: z.number().default(300),
    backoffRate: z.number().default(2),
    authenticationTimeout: z.number().default(10000),
    reauthenticationThreshold: z.number().default(10000),
    sasl: models.AuthenticationType$outboundSchema.optional(),
    sessionTimeout: z.number().default(30000),
    rebalanceTimeout: z.number().default(60000),
    heartbeatInterval: z.number().default(3000),
    autoCommitInterval: z.number().optional(),
    autoCommitThreshold: z.number().optional(),
    maxBytesPerPartition: z.number().default(1048576),
    maxBytes: z.number().default(10485760),
    maxSocketErrors: z.number().default(0),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputConfluentCloudPqEnabledTrueWithPqConstraintToJSON(
  inputConfluentCloudPqEnabledTrueWithPqConstraint:
    InputConfluentCloudPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputConfluentCloudPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputConfluentCloudPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputConfluentCloudPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  brokers: Array<string>;
  tls?: models.TlsSettingsClientSideType1$Outbound | undefined;
  topics: Array<string>;
  groupId: string;
  fromBeginning: boolean;
  kafkaSchemaRegistry?:
    | models.KafkaSchemaRegistryAuthenticationType$Outbound
    | undefined;
  connectionTimeout: number;
  requestTimeout: number;
  maxRetries: number;
  maxBackOff: number;
  initialBackoff: number;
  backoffRate: number;
  authenticationTimeout: number;
  reauthenticationThreshold: number;
  sasl?: models.AuthenticationType$Outbound | undefined;
  sessionTimeout: number;
  rebalanceTimeout: number;
  heartbeatInterval: number;
  autoCommitInterval?: number | undefined;
  autoCommitThreshold?: number | undefined;
  maxBytesPerPartition: number;
  maxBytes: number;
  maxSocketErrors: number;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputConfluentCloudPqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputConfluentCloudPqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputConfluentCloudPqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputConfluentCloudType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    brokers: z.array(z.string()),
    tls: models.TlsSettingsClientSideType1$outboundSchema.optional(),
    topics: z.array(z.string()),
    groupId: z.string().default("Cribl"),
    fromBeginning: z.boolean().default(true),
    kafkaSchemaRegistry: models
      .KafkaSchemaRegistryAuthenticationType$outboundSchema.optional(),
    connectionTimeout: z.number().default(10000),
    requestTimeout: z.number().default(60000),
    maxRetries: z.number().default(5),
    maxBackOff: z.number().default(30000),
    initialBackoff: z.number().default(300),
    backoffRate: z.number().default(2),
    authenticationTimeout: z.number().default(10000),
    reauthenticationThreshold: z.number().default(10000),
    sasl: models.AuthenticationType$outboundSchema.optional(),
    sessionTimeout: z.number().default(30000),
    rebalanceTimeout: z.number().default(60000),
    heartbeatInterval: z.number().default(3000),
    autoCommitInterval: z.number().optional(),
    autoCommitThreshold: z.number().optional(),
    maxBytesPerPartition: z.number().default(1048576),
    maxBytes: z.number().default(10485760),
    maxSocketErrors: z.number().default(0),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputConfluentCloudPqEnabledFalseWithPqConstraintToJSON(
  inputConfluentCloudPqEnabledFalseWithPqConstraint:
    InputConfluentCloudPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputConfluentCloudPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputConfluentCloudPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputConfluentCloudSendToRoutesFalseWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    brokers: Array<string>;
    tls?: models.TlsSettingsClientSideType1$Outbound | undefined;
    topics: Array<string>;
    groupId: string;
    fromBeginning: boolean;
    kafkaSchemaRegistry?:
      | models.KafkaSchemaRegistryAuthenticationType$Outbound
      | undefined;
    connectionTimeout: number;
    requestTimeout: number;
    maxRetries: number;
    maxBackOff: number;
    initialBackoff: number;
    backoffRate: number;
    authenticationTimeout: number;
    reauthenticationThreshold: number;
    sasl?: models.AuthenticationType$Outbound | undefined;
    sessionTimeout: number;
    rebalanceTimeout: number;
    heartbeatInterval: number;
    autoCommitInterval?: number | undefined;
    autoCommitThreshold?: number | undefined;
    maxBytesPerPartition: number;
    maxBytes: number;
    maxSocketErrors: number;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    description?: string | undefined;
  };

/** @internal */
export const InputConfluentCloudSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputConfluentCloudSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputConfluentCloudSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputConfluentCloudType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    brokers: z.array(z.string()),
    tls: models.TlsSettingsClientSideType1$outboundSchema.optional(),
    topics: z.array(z.string()),
    groupId: z.string().default("Cribl"),
    fromBeginning: z.boolean().default(true),
    kafkaSchemaRegistry: models
      .KafkaSchemaRegistryAuthenticationType$outboundSchema.optional(),
    connectionTimeout: z.number().default(10000),
    requestTimeout: z.number().default(60000),
    maxRetries: z.number().default(5),
    maxBackOff: z.number().default(30000),
    initialBackoff: z.number().default(300),
    backoffRate: z.number().default(2),
    authenticationTimeout: z.number().default(10000),
    reauthenticationThreshold: z.number().default(10000),
    sasl: models.AuthenticationType$outboundSchema.optional(),
    sessionTimeout: z.number().default(30000),
    rebalanceTimeout: z.number().default(60000),
    heartbeatInterval: z.number().default(3000),
    autoCommitInterval: z.number().optional(),
    autoCommitThreshold: z.number().optional(),
    maxBytesPerPartition: z.number().default(1048576),
    maxBytes: z.number().default(10485760),
    maxSocketErrors: z.number().default(0),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputConfluentCloudSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputConfluentCloudSendToRoutesFalseWithConnectionsConstraint:
    InputConfluentCloudSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputConfluentCloudSendToRoutesFalseWithConnectionsConstraint$outboundSchema
      .parse(inputConfluentCloudSendToRoutesFalseWithConnectionsConstraint),
  );
}

/** @internal */
export type InputConfluentCloudSendToRoutesTrueWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    brokers: Array<string>;
    tls?: models.TlsSettingsClientSideType1$Outbound | undefined;
    topics: Array<string>;
    groupId: string;
    fromBeginning: boolean;
    kafkaSchemaRegistry?:
      | models.KafkaSchemaRegistryAuthenticationType$Outbound
      | undefined;
    connectionTimeout: number;
    requestTimeout: number;
    maxRetries: number;
    maxBackOff: number;
    initialBackoff: number;
    backoffRate: number;
    authenticationTimeout: number;
    reauthenticationThreshold: number;
    sasl?: models.AuthenticationType$Outbound | undefined;
    sessionTimeout: number;
    rebalanceTimeout: number;
    heartbeatInterval: number;
    autoCommitInterval?: number | undefined;
    autoCommitThreshold?: number | undefined;
    maxBytesPerPartition: number;
    maxBytes: number;
    maxSocketErrors: number;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    description?: string | undefined;
  };

/** @internal */
export const InputConfluentCloudSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputConfluentCloudSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputConfluentCloudSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputConfluentCloudType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    brokers: z.array(z.string()),
    tls: models.TlsSettingsClientSideType1$outboundSchema.optional(),
    topics: z.array(z.string()),
    groupId: z.string().default("Cribl"),
    fromBeginning: z.boolean().default(true),
    kafkaSchemaRegistry: models
      .KafkaSchemaRegistryAuthenticationType$outboundSchema.optional(),
    connectionTimeout: z.number().default(10000),
    requestTimeout: z.number().default(60000),
    maxRetries: z.number().default(5),
    maxBackOff: z.number().default(30000),
    initialBackoff: z.number().default(300),
    backoffRate: z.number().default(2),
    authenticationTimeout: z.number().default(10000),
    reauthenticationThreshold: z.number().default(10000),
    sasl: models.AuthenticationType$outboundSchema.optional(),
    sessionTimeout: z.number().default(30000),
    rebalanceTimeout: z.number().default(60000),
    heartbeatInterval: z.number().default(3000),
    autoCommitInterval: z.number().optional(),
    autoCommitThreshold: z.number().optional(),
    maxBytesPerPartition: z.number().default(1048576),
    maxBytes: z.number().default(10485760),
    maxSocketErrors: z.number().default(0),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputConfluentCloudSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputConfluentCloudSendToRoutesTrueWithConnectionsConstraint:
    InputConfluentCloudSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputConfluentCloudSendToRoutesTrueWithConnectionsConstraint$outboundSchema
      .parse(inputConfluentCloudSendToRoutesTrueWithConnectionsConstraint),
  );
}

/** @internal */
export type InputConfluentCloud$Outbound =
  | InputConfluentCloudSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputConfluentCloudSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputConfluentCloudPqEnabledFalseWithPqConstraint$Outbound
  | InputConfluentCloudPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputConfluentCloud$outboundSchema: z.ZodType<
  InputConfluentCloud$Outbound,
  z.ZodTypeDef,
  InputConfluentCloud
> = z.union([
  z.lazy(() =>
    InputConfluentCloudSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputConfluentCloudSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputConfluentCloudPqEnabledFalseWithPqConstraint$outboundSchema
  ),
  z.lazy(() => InputConfluentCloudPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputConfluentCloudToJSON(
  inputConfluentCloud: InputConfluentCloud,
): string {
  return JSON.stringify(
    InputConfluentCloud$outboundSchema.parse(inputConfluentCloud),
  );
}

/** @internal */
export const InputElasticType$outboundSchema: z.ZodNativeEnum<
  typeof InputElasticType
> = z.nativeEnum(InputElasticType);

/** @internal */
export const InputElasticAuthenticationType$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  InputElasticAuthenticationType
> = openEnums.outboundSchema(InputElasticAuthenticationType);

/** @internal */
export const CreateInputAPIVersion$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  CreateInputAPIVersion
> = openEnums.outboundSchema(CreateInputAPIVersion);

/** @internal */
export const InputElasticAuthenticationMethod$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  InputElasticAuthenticationMethod
> = openEnums.outboundSchema(InputElasticAuthenticationMethod);

/** @internal */
export type InputElasticProxyMode$Outbound = {
  enabled: boolean;
  authType: string;
  username?: string | undefined;
  password?: string | undefined;
  credentialsSecret?: string | undefined;
  url?: string | undefined;
  rejectUnauthorized: boolean;
  removeHeaders?: Array<string> | undefined;
  timeoutSec: number;
};

/** @internal */
export const InputElasticProxyMode$outboundSchema: z.ZodType<
  InputElasticProxyMode$Outbound,
  z.ZodTypeDef,
  InputElasticProxyMode
> = z.object({
  enabled: z.boolean().default(false),
  authType: InputElasticAuthenticationMethod$outboundSchema.default("none"),
  username: z.string().optional(),
  password: z.string().optional(),
  credentialsSecret: z.string().optional(),
  url: z.string().optional(),
  rejectUnauthorized: z.boolean().default(false),
  removeHeaders: z.array(z.string()).optional(),
  timeoutSec: z.number().default(60),
});

export function inputElasticProxyModeToJSON(
  inputElasticProxyMode: InputElasticProxyMode,
): string {
  return JSON.stringify(
    InputElasticProxyMode$outboundSchema.parse(inputElasticProxyMode),
  );
}

/** @internal */
export type InputElasticPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  activityLogSampleRate: number;
  requestTimeout: number;
  socketTimeout: number;
  keepAliveTimeout: number;
  enableHealthCheck: boolean;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  elasticAPI: string;
  authType: string;
  apiVersion: string;
  extraHttpHeaders?:
    | Array<models.ItemsTypeExtraHttpHeaders$Outbound>
    | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  proxyMode?: InputElasticProxyMode$Outbound | undefined;
  description?: string | undefined;
  username?: string | undefined;
  password?: string | undefined;
  credentialsSecret?: string | undefined;
  authTokens?: Array<string> | undefined;
  customAPIVersion: string;
};

/** @internal */
export const InputElasticPqEnabledTrueWithPqConstraint$outboundSchema:
  z.ZodType<
    InputElasticPqEnabledTrueWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputElasticPqEnabledTrueWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputElasticType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    elasticAPI: z.string().default("/"),
    authType: InputElasticAuthenticationType$outboundSchema.default("none"),
    apiVersion: CreateInputAPIVersion$outboundSchema.default("8.3.2"),
    extraHttpHeaders: z.array(models.ItemsTypeExtraHttpHeaders$outboundSchema)
      .optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    proxyMode: z.lazy(() => InputElasticProxyMode$outboundSchema).optional(),
    description: z.string().optional(),
    username: z.string().optional(),
    password: z.string().optional(),
    credentialsSecret: z.string().optional(),
    authTokens: z.array(z.string()).optional(),
    customAPIVersion: z.string().default(
      "{\n    \"name\": \"AzU84iL\",\n    \"cluster_name\": \"cribl\",\n    \"cluster_uuid\": \"Js6_Z2VKS3KbfRSxPmPbaw\",\n    \"version\": {\n        \"number\": \"8.3.2\",\n        \"build_type\": \"tar\",\n        \"build_hash\": \"bca0c8d\",\n        \"build_date\": \"2019-10-16T06:19:49.319352Z\",\n        \"build_snapshot\": false,\n        \"lucene_version\": \"9.7.2\",\n        \"minimum_wire_compatibility_version\": \"7.17.0\",\n        \"minimum_index_compatibility_version\": \"7.0.0\"\n    },\n    \"tagline\": \"You Know, for Search\"\n}",
    ),
  });

export function inputElasticPqEnabledTrueWithPqConstraintToJSON(
  inputElasticPqEnabledTrueWithPqConstraint:
    InputElasticPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputElasticPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputElasticPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputElasticPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  activityLogSampleRate: number;
  requestTimeout: number;
  socketTimeout: number;
  keepAliveTimeout: number;
  enableHealthCheck: boolean;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  elasticAPI: string;
  authType: string;
  apiVersion: string;
  extraHttpHeaders?:
    | Array<models.ItemsTypeExtraHttpHeaders$Outbound>
    | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  proxyMode?: InputElasticProxyMode$Outbound | undefined;
  description?: string | undefined;
  username?: string | undefined;
  password?: string | undefined;
  credentialsSecret?: string | undefined;
  authTokens?: Array<string> | undefined;
  customAPIVersion: string;
};

/** @internal */
export const InputElasticPqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputElasticPqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputElasticPqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputElasticType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    elasticAPI: z.string().default("/"),
    authType: InputElasticAuthenticationType$outboundSchema.default("none"),
    apiVersion: CreateInputAPIVersion$outboundSchema.default("8.3.2"),
    extraHttpHeaders: z.array(models.ItemsTypeExtraHttpHeaders$outboundSchema)
      .optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    proxyMode: z.lazy(() => InputElasticProxyMode$outboundSchema).optional(),
    description: z.string().optional(),
    username: z.string().optional(),
    password: z.string().optional(),
    credentialsSecret: z.string().optional(),
    authTokens: z.array(z.string()).optional(),
    customAPIVersion: z.string().default(
      "{\n    \"name\": \"AzU84iL\",\n    \"cluster_name\": \"cribl\",\n    \"cluster_uuid\": \"Js6_Z2VKS3KbfRSxPmPbaw\",\n    \"version\": {\n        \"number\": \"8.3.2\",\n        \"build_type\": \"tar\",\n        \"build_hash\": \"bca0c8d\",\n        \"build_date\": \"2019-10-16T06:19:49.319352Z\",\n        \"build_snapshot\": false,\n        \"lucene_version\": \"9.7.2\",\n        \"minimum_wire_compatibility_version\": \"7.17.0\",\n        \"minimum_index_compatibility_version\": \"7.0.0\"\n    },\n    \"tagline\": \"You Know, for Search\"\n}",
    ),
  });

export function inputElasticPqEnabledFalseWithPqConstraintToJSON(
  inputElasticPqEnabledFalseWithPqConstraint:
    InputElasticPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputElasticPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputElasticPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputElasticSendToRoutesFalseWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  host: string;
  port: number;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  activityLogSampleRate: number;
  requestTimeout: number;
  socketTimeout: number;
  keepAliveTimeout: number;
  enableHealthCheck: boolean;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  elasticAPI: string;
  authType: string;
  apiVersion: string;
  extraHttpHeaders?:
    | Array<models.ItemsTypeExtraHttpHeaders$Outbound>
    | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  proxyMode?: InputElasticProxyMode$Outbound | undefined;
  description?: string | undefined;
  username?: string | undefined;
  password?: string | undefined;
  credentialsSecret?: string | undefined;
  authTokens?: Array<string> | undefined;
  customAPIVersion: string;
};

/** @internal */
export const InputElasticSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputElasticSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputElasticSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputElasticType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    elasticAPI: z.string().default("/"),
    authType: InputElasticAuthenticationType$outboundSchema.default("none"),
    apiVersion: CreateInputAPIVersion$outboundSchema.default("8.3.2"),
    extraHttpHeaders: z.array(models.ItemsTypeExtraHttpHeaders$outboundSchema)
      .optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    proxyMode: z.lazy(() => InputElasticProxyMode$outboundSchema).optional(),
    description: z.string().optional(),
    username: z.string().optional(),
    password: z.string().optional(),
    credentialsSecret: z.string().optional(),
    authTokens: z.array(z.string()).optional(),
    customAPIVersion: z.string().default(
      "{\n    \"name\": \"AzU84iL\",\n    \"cluster_name\": \"cribl\",\n    \"cluster_uuid\": \"Js6_Z2VKS3KbfRSxPmPbaw\",\n    \"version\": {\n        \"number\": \"8.3.2\",\n        \"build_type\": \"tar\",\n        \"build_hash\": \"bca0c8d\",\n        \"build_date\": \"2019-10-16T06:19:49.319352Z\",\n        \"build_snapshot\": false,\n        \"lucene_version\": \"9.7.2\",\n        \"minimum_wire_compatibility_version\": \"7.17.0\",\n        \"minimum_index_compatibility_version\": \"7.0.0\"\n    },\n    \"tagline\": \"You Know, for Search\"\n}",
    ),
  });

export function inputElasticSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputElasticSendToRoutesFalseWithConnectionsConstraint:
    InputElasticSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputElasticSendToRoutesFalseWithConnectionsConstraint$outboundSchema.parse(
      inputElasticSendToRoutesFalseWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputElasticSendToRoutesTrueWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  host: string;
  port: number;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  activityLogSampleRate: number;
  requestTimeout: number;
  socketTimeout: number;
  keepAliveTimeout: number;
  enableHealthCheck: boolean;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  elasticAPI: string;
  authType: string;
  apiVersion: string;
  extraHttpHeaders?:
    | Array<models.ItemsTypeExtraHttpHeaders$Outbound>
    | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  proxyMode?: InputElasticProxyMode$Outbound | undefined;
  description?: string | undefined;
  username?: string | undefined;
  password?: string | undefined;
  credentialsSecret?: string | undefined;
  authTokens?: Array<string> | undefined;
  customAPIVersion: string;
};

/** @internal */
export const InputElasticSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputElasticSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputElasticSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputElasticType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    elasticAPI: z.string().default("/"),
    authType: InputElasticAuthenticationType$outboundSchema.default("none"),
    apiVersion: CreateInputAPIVersion$outboundSchema.default("8.3.2"),
    extraHttpHeaders: z.array(models.ItemsTypeExtraHttpHeaders$outboundSchema)
      .optional(),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    proxyMode: z.lazy(() => InputElasticProxyMode$outboundSchema).optional(),
    description: z.string().optional(),
    username: z.string().optional(),
    password: z.string().optional(),
    credentialsSecret: z.string().optional(),
    authTokens: z.array(z.string()).optional(),
    customAPIVersion: z.string().default(
      "{\n    \"name\": \"AzU84iL\",\n    \"cluster_name\": \"cribl\",\n    \"cluster_uuid\": \"Js6_Z2VKS3KbfRSxPmPbaw\",\n    \"version\": {\n        \"number\": \"8.3.2\",\n        \"build_type\": \"tar\",\n        \"build_hash\": \"bca0c8d\",\n        \"build_date\": \"2019-10-16T06:19:49.319352Z\",\n        \"build_snapshot\": false,\n        \"lucene_version\": \"9.7.2\",\n        \"minimum_wire_compatibility_version\": \"7.17.0\",\n        \"minimum_index_compatibility_version\": \"7.0.0\"\n    },\n    \"tagline\": \"You Know, for Search\"\n}",
    ),
  });

export function inputElasticSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputElasticSendToRoutesTrueWithConnectionsConstraint:
    InputElasticSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputElasticSendToRoutesTrueWithConnectionsConstraint$outboundSchema.parse(
      inputElasticSendToRoutesTrueWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputElastic$Outbound =
  | InputElasticSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputElasticSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputElasticPqEnabledFalseWithPqConstraint$Outbound
  | InputElasticPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputElastic$outboundSchema: z.ZodType<
  InputElastic$Outbound,
  z.ZodTypeDef,
  InputElastic
> = z.union([
  z.lazy(() =>
    InputElasticSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputElasticSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputElasticPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputElasticPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputElasticToJSON(inputElastic: InputElastic): string {
  return JSON.stringify(InputElastic$outboundSchema.parse(inputElastic));
}

/** @internal */
export const InputAzureBlobType$outboundSchema: z.ZodNativeEnum<
  typeof InputAzureBlobType
> = z.nativeEnum(InputAzureBlobType);

/** @internal */
export type InputAzureBlobPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  queueName: string;
  fileFilter: string;
  visibilityTimeout: number;
  numReceivers: number;
  maxMessages: number;
  servicePeriodSecs: number;
  skipOnError: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  parquetChunkSizeMB: number;
  parquetChunkDownloadTimeout: number;
  authType: string;
  description?: string | undefined;
  connectionString?: string | undefined;
  textSecret?: string | undefined;
  storageAccountName?: string | undefined;
  tenantId?: string | undefined;
  clientId?: string | undefined;
  azureCloud?: string | undefined;
  endpointSuffix?: string | undefined;
  clientTextSecret?: string | undefined;
  certificate?:
    | models.CertificateTypeAzureBlobAuthTypeClientCert$Outbound
    | undefined;
};

/** @internal */
export const InputAzureBlobPqEnabledTrueWithPqConstraint$outboundSchema:
  z.ZodType<
    InputAzureBlobPqEnabledTrueWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputAzureBlobPqEnabledTrueWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputAzureBlobType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    queueName: z.string(),
    fileFilter: z.string().default("/.*/"),
    visibilityTimeout: z.number().default(600),
    numReceivers: z.number().default(1),
    maxMessages: z.number().default(1),
    servicePeriodSecs: z.number().default(5),
    skipOnError: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    parquetChunkSizeMB: z.number().default(5),
    parquetChunkDownloadTimeout: z.number().default(600),
    authType: models.AuthenticationMethodOptions$outboundSchema.default(
      "manual",
    ),
    description: z.string().optional(),
    connectionString: z.string().optional(),
    textSecret: z.string().optional(),
    storageAccountName: z.string().optional(),
    tenantId: z.string().optional(),
    clientId: z.string().optional(),
    azureCloud: z.string().optional(),
    endpointSuffix: z.string().optional(),
    clientTextSecret: z.string().optional(),
    certificate: models
      .CertificateTypeAzureBlobAuthTypeClientCert$outboundSchema.optional(),
  });

export function inputAzureBlobPqEnabledTrueWithPqConstraintToJSON(
  inputAzureBlobPqEnabledTrueWithPqConstraint:
    InputAzureBlobPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputAzureBlobPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputAzureBlobPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputAzureBlobPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  queueName: string;
  fileFilter: string;
  visibilityTimeout: number;
  numReceivers: number;
  maxMessages: number;
  servicePeriodSecs: number;
  skipOnError: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  parquetChunkSizeMB: number;
  parquetChunkDownloadTimeout: number;
  authType: string;
  description?: string | undefined;
  connectionString?: string | undefined;
  textSecret?: string | undefined;
  storageAccountName?: string | undefined;
  tenantId?: string | undefined;
  clientId?: string | undefined;
  azureCloud?: string | undefined;
  endpointSuffix?: string | undefined;
  clientTextSecret?: string | undefined;
  certificate?:
    | models.CertificateTypeAzureBlobAuthTypeClientCert$Outbound
    | undefined;
};

/** @internal */
export const InputAzureBlobPqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputAzureBlobPqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputAzureBlobPqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputAzureBlobType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    queueName: z.string(),
    fileFilter: z.string().default("/.*/"),
    visibilityTimeout: z.number().default(600),
    numReceivers: z.number().default(1),
    maxMessages: z.number().default(1),
    servicePeriodSecs: z.number().default(5),
    skipOnError: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    parquetChunkSizeMB: z.number().default(5),
    parquetChunkDownloadTimeout: z.number().default(600),
    authType: models.AuthenticationMethodOptions$outboundSchema.default(
      "manual",
    ),
    description: z.string().optional(),
    connectionString: z.string().optional(),
    textSecret: z.string().optional(),
    storageAccountName: z.string().optional(),
    tenantId: z.string().optional(),
    clientId: z.string().optional(),
    azureCloud: z.string().optional(),
    endpointSuffix: z.string().optional(),
    clientTextSecret: z.string().optional(),
    certificate: models
      .CertificateTypeAzureBlobAuthTypeClientCert$outboundSchema.optional(),
  });

export function inputAzureBlobPqEnabledFalseWithPqConstraintToJSON(
  inputAzureBlobPqEnabledFalseWithPqConstraint:
    InputAzureBlobPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputAzureBlobPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputAzureBlobPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputAzureBlobSendToRoutesFalseWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    queueName: string;
    fileFilter: string;
    visibilityTimeout: number;
    numReceivers: number;
    maxMessages: number;
    servicePeriodSecs: number;
    skipOnError: boolean;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    breakerRulesets?: Array<string> | undefined;
    staleChannelFlushMs: number;
    parquetChunkSizeMB: number;
    parquetChunkDownloadTimeout: number;
    authType: string;
    description?: string | undefined;
    connectionString?: string | undefined;
    textSecret?: string | undefined;
    storageAccountName?: string | undefined;
    tenantId?: string | undefined;
    clientId?: string | undefined;
    azureCloud?: string | undefined;
    endpointSuffix?: string | undefined;
    clientTextSecret?: string | undefined;
    certificate?:
      | models.CertificateTypeAzureBlobAuthTypeClientCert$Outbound
      | undefined;
  };

/** @internal */
export const InputAzureBlobSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputAzureBlobSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputAzureBlobSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputAzureBlobType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    queueName: z.string(),
    fileFilter: z.string().default("/.*/"),
    visibilityTimeout: z.number().default(600),
    numReceivers: z.number().default(1),
    maxMessages: z.number().default(1),
    servicePeriodSecs: z.number().default(5),
    skipOnError: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    parquetChunkSizeMB: z.number().default(5),
    parquetChunkDownloadTimeout: z.number().default(600),
    authType: models.AuthenticationMethodOptions$outboundSchema.default(
      "manual",
    ),
    description: z.string().optional(),
    connectionString: z.string().optional(),
    textSecret: z.string().optional(),
    storageAccountName: z.string().optional(),
    tenantId: z.string().optional(),
    clientId: z.string().optional(),
    azureCloud: z.string().optional(),
    endpointSuffix: z.string().optional(),
    clientTextSecret: z.string().optional(),
    certificate: models
      .CertificateTypeAzureBlobAuthTypeClientCert$outboundSchema.optional(),
  });

export function inputAzureBlobSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputAzureBlobSendToRoutesFalseWithConnectionsConstraint:
    InputAzureBlobSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputAzureBlobSendToRoutesFalseWithConnectionsConstraint$outboundSchema
      .parse(inputAzureBlobSendToRoutesFalseWithConnectionsConstraint),
  );
}

/** @internal */
export type InputAzureBlobSendToRoutesTrueWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  queueName: string;
  fileFilter: string;
  visibilityTimeout: number;
  numReceivers: number;
  maxMessages: number;
  servicePeriodSecs: number;
  skipOnError: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  parquetChunkSizeMB: number;
  parquetChunkDownloadTimeout: number;
  authType: string;
  description?: string | undefined;
  connectionString?: string | undefined;
  textSecret?: string | undefined;
  storageAccountName?: string | undefined;
  tenantId?: string | undefined;
  clientId?: string | undefined;
  azureCloud?: string | undefined;
  endpointSuffix?: string | undefined;
  clientTextSecret?: string | undefined;
  certificate?:
    | models.CertificateTypeAzureBlobAuthTypeClientCert$Outbound
    | undefined;
};

/** @internal */
export const InputAzureBlobSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputAzureBlobSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputAzureBlobSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputAzureBlobType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    queueName: z.string(),
    fileFilter: z.string().default("/.*/"),
    visibilityTimeout: z.number().default(600),
    numReceivers: z.number().default(1),
    maxMessages: z.number().default(1),
    servicePeriodSecs: z.number().default(5),
    skipOnError: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    parquetChunkSizeMB: z.number().default(5),
    parquetChunkDownloadTimeout: z.number().default(600),
    authType: models.AuthenticationMethodOptions$outboundSchema.default(
      "manual",
    ),
    description: z.string().optional(),
    connectionString: z.string().optional(),
    textSecret: z.string().optional(),
    storageAccountName: z.string().optional(),
    tenantId: z.string().optional(),
    clientId: z.string().optional(),
    azureCloud: z.string().optional(),
    endpointSuffix: z.string().optional(),
    clientTextSecret: z.string().optional(),
    certificate: models
      .CertificateTypeAzureBlobAuthTypeClientCert$outboundSchema.optional(),
  });

export function inputAzureBlobSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputAzureBlobSendToRoutesTrueWithConnectionsConstraint:
    InputAzureBlobSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputAzureBlobSendToRoutesTrueWithConnectionsConstraint$outboundSchema
      .parse(inputAzureBlobSendToRoutesTrueWithConnectionsConstraint),
  );
}

/** @internal */
export type InputAzureBlob$Outbound =
  | InputAzureBlobSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputAzureBlobSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputAzureBlobPqEnabledFalseWithPqConstraint$Outbound
  | InputAzureBlobPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputAzureBlob$outboundSchema: z.ZodType<
  InputAzureBlob$Outbound,
  z.ZodTypeDef,
  InputAzureBlob
> = z.union([
  z.lazy(() =>
    InputAzureBlobSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputAzureBlobSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputAzureBlobPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputAzureBlobPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputAzureBlobToJSON(inputAzureBlob: InputAzureBlob): string {
  return JSON.stringify(InputAzureBlob$outboundSchema.parse(inputAzureBlob));
}

/** @internal */
export const InputSplunkHecType$outboundSchema: z.ZodNativeEnum<
  typeof InputSplunkHecType
> = z.nativeEnum(InputSplunkHecType);

/** @internal */
export type InputSplunkHecAuthToken$Outbound = {
  authType: string;
  tokenSecret?: string | undefined;
  token: string;
  enabled: boolean;
  description?: string | undefined;
  allowedIndexesAtToken?: Array<string> | undefined;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
};

/** @internal */
export const InputSplunkHecAuthToken$outboundSchema: z.ZodType<
  InputSplunkHecAuthToken$Outbound,
  z.ZodTypeDef,
  InputSplunkHecAuthToken
> = z.object({
  authType: models.AuthenticationMethodOptionsAuthTokensItems$outboundSchema
    .default("manual"),
  tokenSecret: z.string().optional(),
  token: z.string(),
  enabled: z.boolean().default(true),
  description: z.string().optional(),
  allowedIndexesAtToken: z.array(z.string()).optional(),
  metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
    .optional(),
});

export function inputSplunkHecAuthTokenToJSON(
  inputSplunkHecAuthToken: InputSplunkHecAuthToken,
): string {
  return JSON.stringify(
    InputSplunkHecAuthToken$outboundSchema.parse(inputSplunkHecAuthToken),
  );
}

/** @internal */
export type InputSplunkHecPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  authTokens?: Array<InputSplunkHecAuthToken$Outbound> | undefined;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  activityLogSampleRate: number;
  requestTimeout: number;
  socketTimeout: number;
  keepAliveTimeout: number;
  enableHealthCheck?: any | undefined;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  splunkHecAPI: string;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  allowedIndexes?: Array<string> | undefined;
  splunkHecAcks: boolean;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  useFwdTimezone: boolean;
  dropControlFields: boolean;
  extractMetrics: boolean;
  accessControlAllowOrigin?: Array<string> | undefined;
  accessControlAllowHeaders?: Array<string> | undefined;
  emitTokenMetrics: boolean;
  description?: string | undefined;
};

/** @internal */
export const InputSplunkHecPqEnabledTrueWithPqConstraint$outboundSchema:
  z.ZodType<
    InputSplunkHecPqEnabledTrueWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputSplunkHecPqEnabledTrueWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputSplunkHecType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    authTokens: z.array(z.lazy(() => InputSplunkHecAuthToken$outboundSchema))
      .optional(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.any().optional(),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    splunkHecAPI: z.string().default("/services/collector"),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    allowedIndexes: z.array(z.string()).optional(),
    splunkHecAcks: z.boolean().default(false),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    useFwdTimezone: z.boolean().default(true),
    dropControlFields: z.boolean().default(true),
    extractMetrics: z.boolean().default(false),
    accessControlAllowOrigin: z.array(z.string()).optional(),
    accessControlAllowHeaders: z.array(z.string()).optional(),
    emitTokenMetrics: z.boolean().default(false),
    description: z.string().optional(),
  });

export function inputSplunkHecPqEnabledTrueWithPqConstraintToJSON(
  inputSplunkHecPqEnabledTrueWithPqConstraint:
    InputSplunkHecPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputSplunkHecPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputSplunkHecPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputSplunkHecPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  authTokens?: Array<InputSplunkHecAuthToken$Outbound> | undefined;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  activityLogSampleRate: number;
  requestTimeout: number;
  socketTimeout: number;
  keepAliveTimeout: number;
  enableHealthCheck?: any | undefined;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  splunkHecAPI: string;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  allowedIndexes?: Array<string> | undefined;
  splunkHecAcks: boolean;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  useFwdTimezone: boolean;
  dropControlFields: boolean;
  extractMetrics: boolean;
  accessControlAllowOrigin?: Array<string> | undefined;
  accessControlAllowHeaders?: Array<string> | undefined;
  emitTokenMetrics: boolean;
  description?: string | undefined;
};

/** @internal */
export const InputSplunkHecPqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputSplunkHecPqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputSplunkHecPqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputSplunkHecType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    authTokens: z.array(z.lazy(() => InputSplunkHecAuthToken$outboundSchema))
      .optional(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.any().optional(),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    splunkHecAPI: z.string().default("/services/collector"),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    allowedIndexes: z.array(z.string()).optional(),
    splunkHecAcks: z.boolean().default(false),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    useFwdTimezone: z.boolean().default(true),
    dropControlFields: z.boolean().default(true),
    extractMetrics: z.boolean().default(false),
    accessControlAllowOrigin: z.array(z.string()).optional(),
    accessControlAllowHeaders: z.array(z.string()).optional(),
    emitTokenMetrics: z.boolean().default(false),
    description: z.string().optional(),
  });

export function inputSplunkHecPqEnabledFalseWithPqConstraintToJSON(
  inputSplunkHecPqEnabledFalseWithPqConstraint:
    InputSplunkHecPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputSplunkHecPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputSplunkHecPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputSplunkHecSendToRoutesFalseWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    host: string;
    port: number;
    authTokens?: Array<InputSplunkHecAuthToken$Outbound> | undefined;
    tls?: models.TlsSettingsServerSideType$Outbound | undefined;
    maxActiveReq: number;
    maxRequestsPerSocket: number;
    enableProxyHeader: boolean;
    captureHeaders: boolean;
    activityLogSampleRate: number;
    requestTimeout: number;
    socketTimeout: number;
    keepAliveTimeout: number;
    enableHealthCheck?: any | undefined;
    ipAllowlistRegex: string;
    ipDenylistRegex: string;
    splunkHecAPI: string;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    allowedIndexes?: Array<string> | undefined;
    splunkHecAcks: boolean;
    breakerRulesets?: Array<string> | undefined;
    staleChannelFlushMs: number;
    useFwdTimezone: boolean;
    dropControlFields: boolean;
    extractMetrics: boolean;
    accessControlAllowOrigin?: Array<string> | undefined;
    accessControlAllowHeaders?: Array<string> | undefined;
    emitTokenMetrics: boolean;
    description?: string | undefined;
  };

/** @internal */
export const InputSplunkHecSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputSplunkHecSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputSplunkHecSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputSplunkHecType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    authTokens: z.array(z.lazy(() => InputSplunkHecAuthToken$outboundSchema))
      .optional(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.any().optional(),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    splunkHecAPI: z.string().default("/services/collector"),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    allowedIndexes: z.array(z.string()).optional(),
    splunkHecAcks: z.boolean().default(false),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    useFwdTimezone: z.boolean().default(true),
    dropControlFields: z.boolean().default(true),
    extractMetrics: z.boolean().default(false),
    accessControlAllowOrigin: z.array(z.string()).optional(),
    accessControlAllowHeaders: z.array(z.string()).optional(),
    emitTokenMetrics: z.boolean().default(false),
    description: z.string().optional(),
  });

export function inputSplunkHecSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputSplunkHecSendToRoutesFalseWithConnectionsConstraint:
    InputSplunkHecSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputSplunkHecSendToRoutesFalseWithConnectionsConstraint$outboundSchema
      .parse(inputSplunkHecSendToRoutesFalseWithConnectionsConstraint),
  );
}

/** @internal */
export type InputSplunkHecSendToRoutesTrueWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  host: string;
  port: number;
  authTokens?: Array<InputSplunkHecAuthToken$Outbound> | undefined;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  activityLogSampleRate: number;
  requestTimeout: number;
  socketTimeout: number;
  keepAliveTimeout: number;
  enableHealthCheck?: any | undefined;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  splunkHecAPI: string;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  allowedIndexes?: Array<string> | undefined;
  splunkHecAcks: boolean;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  useFwdTimezone: boolean;
  dropControlFields: boolean;
  extractMetrics: boolean;
  accessControlAllowOrigin?: Array<string> | undefined;
  accessControlAllowHeaders?: Array<string> | undefined;
  emitTokenMetrics: boolean;
  description?: string | undefined;
};

/** @internal */
export const InputSplunkHecSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputSplunkHecSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputSplunkHecSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputSplunkHecType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    authTokens: z.array(z.lazy(() => InputSplunkHecAuthToken$outboundSchema))
      .optional(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.any().optional(),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    splunkHecAPI: z.string().default("/services/collector"),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    allowedIndexes: z.array(z.string()).optional(),
    splunkHecAcks: z.boolean().default(false),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    useFwdTimezone: z.boolean().default(true),
    dropControlFields: z.boolean().default(true),
    extractMetrics: z.boolean().default(false),
    accessControlAllowOrigin: z.array(z.string()).optional(),
    accessControlAllowHeaders: z.array(z.string()).optional(),
    emitTokenMetrics: z.boolean().default(false),
    description: z.string().optional(),
  });

export function inputSplunkHecSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputSplunkHecSendToRoutesTrueWithConnectionsConstraint:
    InputSplunkHecSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputSplunkHecSendToRoutesTrueWithConnectionsConstraint$outboundSchema
      .parse(inputSplunkHecSendToRoutesTrueWithConnectionsConstraint),
  );
}

/** @internal */
export type InputSplunkHec$Outbound =
  | InputSplunkHecSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputSplunkHecSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputSplunkHecPqEnabledFalseWithPqConstraint$Outbound
  | InputSplunkHecPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputSplunkHec$outboundSchema: z.ZodType<
  InputSplunkHec$Outbound,
  z.ZodTypeDef,
  InputSplunkHec
> = z.union([
  z.lazy(() =>
    InputSplunkHecSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputSplunkHecSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputSplunkHecPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputSplunkHecPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputSplunkHecToJSON(inputSplunkHec: InputSplunkHec): string {
  return JSON.stringify(InputSplunkHec$outboundSchema.parse(inputSplunkHec));
}

/** @internal */
export const InputSplunkSearchType$outboundSchema: z.ZodNativeEnum<
  typeof InputSplunkSearchType
> = z.nativeEnum(InputSplunkSearchType);

/** @internal */
export type EndpointParam$Outbound = {
  name: string;
  value: string;
};

/** @internal */
export const EndpointParam$outboundSchema: z.ZodType<
  EndpointParam$Outbound,
  z.ZodTypeDef,
  EndpointParam
> = z.object({
  name: z.string(),
  value: z.string(),
});

export function endpointParamToJSON(endpointParam: EndpointParam): string {
  return JSON.stringify(EndpointParam$outboundSchema.parse(endpointParam));
}

/** @internal */
export type EndpointHeader$Outbound = {
  name: string;
  value: string;
};

/** @internal */
export const EndpointHeader$outboundSchema: z.ZodType<
  EndpointHeader$Outbound,
  z.ZodTypeDef,
  EndpointHeader
> = z.object({
  name: z.string(),
  value: z.string(),
});

export function endpointHeaderToJSON(endpointHeader: EndpointHeader): string {
  return JSON.stringify(EndpointHeader$outboundSchema.parse(endpointHeader));
}

/** @internal */
export const InputSplunkSearchLogLevel$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  InputSplunkSearchLogLevel
> = openEnums.outboundSchema(InputSplunkSearchLogLevel);

/** @internal */
export const InputSplunkSearchAuthenticationType$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  InputSplunkSearchAuthenticationType
> = openEnums.outboundSchema(InputSplunkSearchAuthenticationType);

/** @internal */
export type InputSplunkSearchPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  searchHead: string;
  search: string;
  earliest: string;
  latest: string;
  cronSchedule: string;
  endpoint: string;
  outputMode: string;
  endpointParams?: Array<EndpointParam$Outbound> | undefined;
  endpointHeaders?: Array<EndpointHeader$Outbound> | undefined;
  logLevel?: string | undefined;
  requestTimeout: number;
  useRoundRobinDns: boolean;
  rejectUnauthorized: boolean;
  encoding?: string | undefined;
  keepAliveTime: number;
  jobTimeout: string;
  maxMissedKeepAlives: number;
  ttl: string;
  ignoreGroupJobsLimit: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  retryRules?: models.RetryRulesType$Outbound | undefined;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  authType: string;
  description?: string | undefined;
  username?: string | undefined;
  password?: string | undefined;
  token?: string | undefined;
  credentialsSecret?: string | undefined;
  textSecret?: string | undefined;
  loginUrl?: string | undefined;
  secretParamName?: string | undefined;
  secret?: string | undefined;
  tokenAttributeName?: string | undefined;
  authHeaderExpr: string;
  tokenTimeoutSecs: number;
  oauthParams?: Array<models.ItemsTypeOauthParams$Outbound> | undefined;
  oauthHeaders?: Array<models.ItemsTypeOauthHeaders$Outbound> | undefined;
};

/** @internal */
export const InputSplunkSearchPqEnabledTrueWithPqConstraint$outboundSchema:
  z.ZodType<
    InputSplunkSearchPqEnabledTrueWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputSplunkSearchPqEnabledTrueWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputSplunkSearchType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    searchHead: z.string().default("https://localhost:8089"),
    search: z.string(),
    earliest: z.string().default("-16m@m"),
    latest: z.string().default("-1m@m"),
    cronSchedule: z.string().default("*/15 * * * *"),
    endpoint: z.string().default("/services/search/v2/jobs/export"),
    outputMode: models.OutputModeOptionsSplunkCollectorConf$outboundSchema
      .default("json"),
    endpointParams: z.array(z.lazy(() => EndpointParam$outboundSchema))
      .optional(),
    endpointHeaders: z.array(z.lazy(() => EndpointHeader$outboundSchema))
      .optional(),
    logLevel: InputSplunkSearchLogLevel$outboundSchema.optional(),
    requestTimeout: z.number().default(0),
    useRoundRobinDns: z.boolean().default(false),
    rejectUnauthorized: z.boolean().default(false),
    encoding: z.string().optional(),
    keepAliveTime: z.number().default(30),
    jobTimeout: z.string().default("0"),
    maxMissedKeepAlives: z.number().default(3),
    ttl: z.string().default("4h"),
    ignoreGroupJobsLimit: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    retryRules: models.RetryRulesType$outboundSchema.optional(),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    authType: InputSplunkSearchAuthenticationType$outboundSchema.default(
      "basic",
    ),
    description: z.string().optional(),
    username: z.string().optional(),
    password: z.string().optional(),
    token: z.string().optional(),
    credentialsSecret: z.string().optional(),
    textSecret: z.string().optional(),
    loginUrl: z.string().optional(),
    secretParamName: z.string().optional(),
    secret: z.string().optional(),
    tokenAttributeName: z.string().optional(),
    authHeaderExpr: z.string().default("`Bearer ${token}`"),
    tokenTimeoutSecs: z.number().default(3600),
    oauthParams: z.array(models.ItemsTypeOauthParams$outboundSchema).optional(),
    oauthHeaders: z.array(models.ItemsTypeOauthHeaders$outboundSchema)
      .optional(),
  });

export function inputSplunkSearchPqEnabledTrueWithPqConstraintToJSON(
  inputSplunkSearchPqEnabledTrueWithPqConstraint:
    InputSplunkSearchPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputSplunkSearchPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputSplunkSearchPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputSplunkSearchPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  searchHead: string;
  search: string;
  earliest: string;
  latest: string;
  cronSchedule: string;
  endpoint: string;
  outputMode: string;
  endpointParams?: Array<EndpointParam$Outbound> | undefined;
  endpointHeaders?: Array<EndpointHeader$Outbound> | undefined;
  logLevel?: string | undefined;
  requestTimeout: number;
  useRoundRobinDns: boolean;
  rejectUnauthorized: boolean;
  encoding?: string | undefined;
  keepAliveTime: number;
  jobTimeout: string;
  maxMissedKeepAlives: number;
  ttl: string;
  ignoreGroupJobsLimit: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  retryRules?: models.RetryRulesType$Outbound | undefined;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  authType: string;
  description?: string | undefined;
  username?: string | undefined;
  password?: string | undefined;
  token?: string | undefined;
  credentialsSecret?: string | undefined;
  textSecret?: string | undefined;
  loginUrl?: string | undefined;
  secretParamName?: string | undefined;
  secret?: string | undefined;
  tokenAttributeName?: string | undefined;
  authHeaderExpr: string;
  tokenTimeoutSecs: number;
  oauthParams?: Array<models.ItemsTypeOauthParams$Outbound> | undefined;
  oauthHeaders?: Array<models.ItemsTypeOauthHeaders$Outbound> | undefined;
};

/** @internal */
export const InputSplunkSearchPqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputSplunkSearchPqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputSplunkSearchPqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputSplunkSearchType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    searchHead: z.string().default("https://localhost:8089"),
    search: z.string(),
    earliest: z.string().default("-16m@m"),
    latest: z.string().default("-1m@m"),
    cronSchedule: z.string().default("*/15 * * * *"),
    endpoint: z.string().default("/services/search/v2/jobs/export"),
    outputMode: models.OutputModeOptionsSplunkCollectorConf$outboundSchema
      .default("json"),
    endpointParams: z.array(z.lazy(() => EndpointParam$outboundSchema))
      .optional(),
    endpointHeaders: z.array(z.lazy(() => EndpointHeader$outboundSchema))
      .optional(),
    logLevel: InputSplunkSearchLogLevel$outboundSchema.optional(),
    requestTimeout: z.number().default(0),
    useRoundRobinDns: z.boolean().default(false),
    rejectUnauthorized: z.boolean().default(false),
    encoding: z.string().optional(),
    keepAliveTime: z.number().default(30),
    jobTimeout: z.string().default("0"),
    maxMissedKeepAlives: z.number().default(3),
    ttl: z.string().default("4h"),
    ignoreGroupJobsLimit: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    retryRules: models.RetryRulesType$outboundSchema.optional(),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    authType: InputSplunkSearchAuthenticationType$outboundSchema.default(
      "basic",
    ),
    description: z.string().optional(),
    username: z.string().optional(),
    password: z.string().optional(),
    token: z.string().optional(),
    credentialsSecret: z.string().optional(),
    textSecret: z.string().optional(),
    loginUrl: z.string().optional(),
    secretParamName: z.string().optional(),
    secret: z.string().optional(),
    tokenAttributeName: z.string().optional(),
    authHeaderExpr: z.string().default("`Bearer ${token}`"),
    tokenTimeoutSecs: z.number().default(3600),
    oauthParams: z.array(models.ItemsTypeOauthParams$outboundSchema).optional(),
    oauthHeaders: z.array(models.ItemsTypeOauthHeaders$outboundSchema)
      .optional(),
  });

export function inputSplunkSearchPqEnabledFalseWithPqConstraintToJSON(
  inputSplunkSearchPqEnabledFalseWithPqConstraint:
    InputSplunkSearchPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputSplunkSearchPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputSplunkSearchPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputSplunkSearchSendToRoutesFalseWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    searchHead: string;
    search: string;
    earliest: string;
    latest: string;
    cronSchedule: string;
    endpoint: string;
    outputMode: string;
    endpointParams?: Array<EndpointParam$Outbound> | undefined;
    endpointHeaders?: Array<EndpointHeader$Outbound> | undefined;
    logLevel?: string | undefined;
    requestTimeout: number;
    useRoundRobinDns: boolean;
    rejectUnauthorized: boolean;
    encoding?: string | undefined;
    keepAliveTime: number;
    jobTimeout: string;
    maxMissedKeepAlives: number;
    ttl: string;
    ignoreGroupJobsLimit: boolean;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    retryRules?: models.RetryRulesType$Outbound | undefined;
    breakerRulesets?: Array<string> | undefined;
    staleChannelFlushMs: number;
    authType: string;
    description?: string | undefined;
    username?: string | undefined;
    password?: string | undefined;
    token?: string | undefined;
    credentialsSecret?: string | undefined;
    textSecret?: string | undefined;
    loginUrl?: string | undefined;
    secretParamName?: string | undefined;
    secret?: string | undefined;
    tokenAttributeName?: string | undefined;
    authHeaderExpr: string;
    tokenTimeoutSecs: number;
    oauthParams?: Array<models.ItemsTypeOauthParams$Outbound> | undefined;
    oauthHeaders?: Array<models.ItemsTypeOauthHeaders$Outbound> | undefined;
  };

/** @internal */
export const InputSplunkSearchSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputSplunkSearchSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputSplunkSearchSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputSplunkSearchType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    searchHead: z.string().default("https://localhost:8089"),
    search: z.string(),
    earliest: z.string().default("-16m@m"),
    latest: z.string().default("-1m@m"),
    cronSchedule: z.string().default("*/15 * * * *"),
    endpoint: z.string().default("/services/search/v2/jobs/export"),
    outputMode: models.OutputModeOptionsSplunkCollectorConf$outboundSchema
      .default("json"),
    endpointParams: z.array(z.lazy(() => EndpointParam$outboundSchema))
      .optional(),
    endpointHeaders: z.array(z.lazy(() => EndpointHeader$outboundSchema))
      .optional(),
    logLevel: InputSplunkSearchLogLevel$outboundSchema.optional(),
    requestTimeout: z.number().default(0),
    useRoundRobinDns: z.boolean().default(false),
    rejectUnauthorized: z.boolean().default(false),
    encoding: z.string().optional(),
    keepAliveTime: z.number().default(30),
    jobTimeout: z.string().default("0"),
    maxMissedKeepAlives: z.number().default(3),
    ttl: z.string().default("4h"),
    ignoreGroupJobsLimit: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    retryRules: models.RetryRulesType$outboundSchema.optional(),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    authType: InputSplunkSearchAuthenticationType$outboundSchema.default(
      "basic",
    ),
    description: z.string().optional(),
    username: z.string().optional(),
    password: z.string().optional(),
    token: z.string().optional(),
    credentialsSecret: z.string().optional(),
    textSecret: z.string().optional(),
    loginUrl: z.string().optional(),
    secretParamName: z.string().optional(),
    secret: z.string().optional(),
    tokenAttributeName: z.string().optional(),
    authHeaderExpr: z.string().default("`Bearer ${token}`"),
    tokenTimeoutSecs: z.number().default(3600),
    oauthParams: z.array(models.ItemsTypeOauthParams$outboundSchema).optional(),
    oauthHeaders: z.array(models.ItemsTypeOauthHeaders$outboundSchema)
      .optional(),
  });

export function inputSplunkSearchSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputSplunkSearchSendToRoutesFalseWithConnectionsConstraint:
    InputSplunkSearchSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputSplunkSearchSendToRoutesFalseWithConnectionsConstraint$outboundSchema
      .parse(inputSplunkSearchSendToRoutesFalseWithConnectionsConstraint),
  );
}

/** @internal */
export type InputSplunkSearchSendToRoutesTrueWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    searchHead: string;
    search: string;
    earliest: string;
    latest: string;
    cronSchedule: string;
    endpoint: string;
    outputMode: string;
    endpointParams?: Array<EndpointParam$Outbound> | undefined;
    endpointHeaders?: Array<EndpointHeader$Outbound> | undefined;
    logLevel?: string | undefined;
    requestTimeout: number;
    useRoundRobinDns: boolean;
    rejectUnauthorized: boolean;
    encoding?: string | undefined;
    keepAliveTime: number;
    jobTimeout: string;
    maxMissedKeepAlives: number;
    ttl: string;
    ignoreGroupJobsLimit: boolean;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    retryRules?: models.RetryRulesType$Outbound | undefined;
    breakerRulesets?: Array<string> | undefined;
    staleChannelFlushMs: number;
    authType: string;
    description?: string | undefined;
    username?: string | undefined;
    password?: string | undefined;
    token?: string | undefined;
    credentialsSecret?: string | undefined;
    textSecret?: string | undefined;
    loginUrl?: string | undefined;
    secretParamName?: string | undefined;
    secret?: string | undefined;
    tokenAttributeName?: string | undefined;
    authHeaderExpr: string;
    tokenTimeoutSecs: number;
    oauthParams?: Array<models.ItemsTypeOauthParams$Outbound> | undefined;
    oauthHeaders?: Array<models.ItemsTypeOauthHeaders$Outbound> | undefined;
  };

/** @internal */
export const InputSplunkSearchSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputSplunkSearchSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputSplunkSearchSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputSplunkSearchType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    searchHead: z.string().default("https://localhost:8089"),
    search: z.string(),
    earliest: z.string().default("-16m@m"),
    latest: z.string().default("-1m@m"),
    cronSchedule: z.string().default("*/15 * * * *"),
    endpoint: z.string().default("/services/search/v2/jobs/export"),
    outputMode: models.OutputModeOptionsSplunkCollectorConf$outboundSchema
      .default("json"),
    endpointParams: z.array(z.lazy(() => EndpointParam$outboundSchema))
      .optional(),
    endpointHeaders: z.array(z.lazy(() => EndpointHeader$outboundSchema))
      .optional(),
    logLevel: InputSplunkSearchLogLevel$outboundSchema.optional(),
    requestTimeout: z.number().default(0),
    useRoundRobinDns: z.boolean().default(false),
    rejectUnauthorized: z.boolean().default(false),
    encoding: z.string().optional(),
    keepAliveTime: z.number().default(30),
    jobTimeout: z.string().default("0"),
    maxMissedKeepAlives: z.number().default(3),
    ttl: z.string().default("4h"),
    ignoreGroupJobsLimit: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    retryRules: models.RetryRulesType$outboundSchema.optional(),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    authType: InputSplunkSearchAuthenticationType$outboundSchema.default(
      "basic",
    ),
    description: z.string().optional(),
    username: z.string().optional(),
    password: z.string().optional(),
    token: z.string().optional(),
    credentialsSecret: z.string().optional(),
    textSecret: z.string().optional(),
    loginUrl: z.string().optional(),
    secretParamName: z.string().optional(),
    secret: z.string().optional(),
    tokenAttributeName: z.string().optional(),
    authHeaderExpr: z.string().default("`Bearer ${token}`"),
    tokenTimeoutSecs: z.number().default(3600),
    oauthParams: z.array(models.ItemsTypeOauthParams$outboundSchema).optional(),
    oauthHeaders: z.array(models.ItemsTypeOauthHeaders$outboundSchema)
      .optional(),
  });

export function inputSplunkSearchSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputSplunkSearchSendToRoutesTrueWithConnectionsConstraint:
    InputSplunkSearchSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputSplunkSearchSendToRoutesTrueWithConnectionsConstraint$outboundSchema
      .parse(inputSplunkSearchSendToRoutesTrueWithConnectionsConstraint),
  );
}

/** @internal */
export type InputSplunkSearch$Outbound =
  | InputSplunkSearchSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputSplunkSearchSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputSplunkSearchPqEnabledFalseWithPqConstraint$Outbound
  | InputSplunkSearchPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputSplunkSearch$outboundSchema: z.ZodType<
  InputSplunkSearch$Outbound,
  z.ZodTypeDef,
  InputSplunkSearch
> = z.union([
  z.lazy(() =>
    InputSplunkSearchSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputSplunkSearchSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputSplunkSearchPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputSplunkSearchPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputSplunkSearchToJSON(
  inputSplunkSearch: InputSplunkSearch,
): string {
  return JSON.stringify(
    InputSplunkSearch$outboundSchema.parse(inputSplunkSearch),
  );
}

/** @internal */
export const InputSplunkType$outboundSchema: z.ZodNativeEnum<
  typeof InputSplunkType
> = z.nativeEnum(InputSplunkType);

/** @internal */
export type InputSplunkAuthToken$Outbound = {
  token: string;
  description?: string | undefined;
};

/** @internal */
export const InputSplunkAuthToken$outboundSchema: z.ZodType<
  InputSplunkAuthToken$Outbound,
  z.ZodTypeDef,
  InputSplunkAuthToken
> = z.object({
  token: z.string(),
  description: z.string().optional(),
});

export function inputSplunkAuthTokenToJSON(
  inputSplunkAuthToken: InputSplunkAuthToken,
): string {
  return JSON.stringify(
    InputSplunkAuthToken$outboundSchema.parse(inputSplunkAuthToken),
  );
}

/** @internal */
export const MaxS2SVersion$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  MaxS2SVersion
> = openEnums.outboundSchema(MaxS2SVersion);

/** @internal */
export const CreateInputCompression$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  CreateInputCompression
> = openEnums.outboundSchema(CreateInputCompression);

/** @internal */
export type InputSplunkPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  ipWhitelistRegex: string;
  maxActiveCxn: number;
  socketIdleTimeout: number;
  socketEndingMaxWait: number;
  socketMaxLifespan: number;
  enableProxyHeader: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  authTokens?: Array<InputSplunkAuthToken$Outbound> | undefined;
  maxS2Sversion: string;
  description?: string | undefined;
  useFwdTimezone: boolean;
  dropControlFields: boolean;
  extractMetrics: boolean;
  compress: string;
};

/** @internal */
export const InputSplunkPqEnabledTrueWithPqConstraint$outboundSchema: z.ZodType<
  InputSplunkPqEnabledTrueWithPqConstraint$Outbound,
  z.ZodTypeDef,
  InputSplunkPqEnabledTrueWithPqConstraint
> = z.object({
  pqEnabled: z.boolean().default(false),
  pq: models.PqType$outboundSchema.optional(),
  id: z.string(),
  type: InputSplunkType$outboundSchema,
  disabled: z.boolean().default(false),
  pipeline: z.string().optional(),
  sendToRoutes: z.boolean().default(true),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
  host: z.string().default("0.0.0.0"),
  port: z.number(),
  tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
  ipWhitelistRegex: z.string().default("/.*/"),
  maxActiveCxn: z.number().default(1000),
  socketIdleTimeout: z.number().default(0),
  socketEndingMaxWait: z.number().default(30),
  socketMaxLifespan: z.number().default(0),
  enableProxyHeader: z.boolean().default(false),
  metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
    .optional(),
  breakerRulesets: z.array(z.string()).optional(),
  staleChannelFlushMs: z.number().default(10000),
  authTokens: z.array(z.lazy(() => InputSplunkAuthToken$outboundSchema))
    .optional(),
  maxS2Sversion: MaxS2SVersion$outboundSchema.default("v3"),
  description: z.string().optional(),
  useFwdTimezone: z.boolean().default(true),
  dropControlFields: z.boolean().default(true),
  extractMetrics: z.boolean().default(false),
  compress: CreateInputCompression$outboundSchema.default("disabled"),
});

export function inputSplunkPqEnabledTrueWithPqConstraintToJSON(
  inputSplunkPqEnabledTrueWithPqConstraint:
    InputSplunkPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputSplunkPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputSplunkPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputSplunkPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  ipWhitelistRegex: string;
  maxActiveCxn: number;
  socketIdleTimeout: number;
  socketEndingMaxWait: number;
  socketMaxLifespan: number;
  enableProxyHeader: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  authTokens?: Array<InputSplunkAuthToken$Outbound> | undefined;
  maxS2Sversion: string;
  description?: string | undefined;
  useFwdTimezone: boolean;
  dropControlFields: boolean;
  extractMetrics: boolean;
  compress: string;
};

/** @internal */
export const InputSplunkPqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputSplunkPqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputSplunkPqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputSplunkType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    ipWhitelistRegex: z.string().default("/.*/"),
    maxActiveCxn: z.number().default(1000),
    socketIdleTimeout: z.number().default(0),
    socketEndingMaxWait: z.number().default(30),
    socketMaxLifespan: z.number().default(0),
    enableProxyHeader: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    authTokens: z.array(z.lazy(() => InputSplunkAuthToken$outboundSchema))
      .optional(),
    maxS2Sversion: MaxS2SVersion$outboundSchema.default("v3"),
    description: z.string().optional(),
    useFwdTimezone: z.boolean().default(true),
    dropControlFields: z.boolean().default(true),
    extractMetrics: z.boolean().default(false),
    compress: CreateInputCompression$outboundSchema.default("disabled"),
  });

export function inputSplunkPqEnabledFalseWithPqConstraintToJSON(
  inputSplunkPqEnabledFalseWithPqConstraint:
    InputSplunkPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputSplunkPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputSplunkPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputSplunkSendToRoutesFalseWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  host: string;
  port: number;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  ipWhitelistRegex: string;
  maxActiveCxn: number;
  socketIdleTimeout: number;
  socketEndingMaxWait: number;
  socketMaxLifespan: number;
  enableProxyHeader: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  authTokens?: Array<InputSplunkAuthToken$Outbound> | undefined;
  maxS2Sversion: string;
  description?: string | undefined;
  useFwdTimezone: boolean;
  dropControlFields: boolean;
  extractMetrics: boolean;
  compress: string;
};

/** @internal */
export const InputSplunkSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputSplunkSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputSplunkSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputSplunkType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    ipWhitelistRegex: z.string().default("/.*/"),
    maxActiveCxn: z.number().default(1000),
    socketIdleTimeout: z.number().default(0),
    socketEndingMaxWait: z.number().default(30),
    socketMaxLifespan: z.number().default(0),
    enableProxyHeader: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    authTokens: z.array(z.lazy(() => InputSplunkAuthToken$outboundSchema))
      .optional(),
    maxS2Sversion: MaxS2SVersion$outboundSchema.default("v3"),
    description: z.string().optional(),
    useFwdTimezone: z.boolean().default(true),
    dropControlFields: z.boolean().default(true),
    extractMetrics: z.boolean().default(false),
    compress: CreateInputCompression$outboundSchema.default("disabled"),
  });

export function inputSplunkSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputSplunkSendToRoutesFalseWithConnectionsConstraint:
    InputSplunkSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputSplunkSendToRoutesFalseWithConnectionsConstraint$outboundSchema.parse(
      inputSplunkSendToRoutesFalseWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputSplunkSendToRoutesTrueWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  host: string;
  port: number;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  ipWhitelistRegex: string;
  maxActiveCxn: number;
  socketIdleTimeout: number;
  socketEndingMaxWait: number;
  socketMaxLifespan: number;
  enableProxyHeader: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  authTokens?: Array<InputSplunkAuthToken$Outbound> | undefined;
  maxS2Sversion: string;
  description?: string | undefined;
  useFwdTimezone: boolean;
  dropControlFields: boolean;
  extractMetrics: boolean;
  compress: string;
};

/** @internal */
export const InputSplunkSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputSplunkSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputSplunkSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputSplunkType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    ipWhitelistRegex: z.string().default("/.*/"),
    maxActiveCxn: z.number().default(1000),
    socketIdleTimeout: z.number().default(0),
    socketEndingMaxWait: z.number().default(30),
    socketMaxLifespan: z.number().default(0),
    enableProxyHeader: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    authTokens: z.array(z.lazy(() => InputSplunkAuthToken$outboundSchema))
      .optional(),
    maxS2Sversion: MaxS2SVersion$outboundSchema.default("v3"),
    description: z.string().optional(),
    useFwdTimezone: z.boolean().default(true),
    dropControlFields: z.boolean().default(true),
    extractMetrics: z.boolean().default(false),
    compress: CreateInputCompression$outboundSchema.default("disabled"),
  });

export function inputSplunkSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputSplunkSendToRoutesTrueWithConnectionsConstraint:
    InputSplunkSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputSplunkSendToRoutesTrueWithConnectionsConstraint$outboundSchema.parse(
      inputSplunkSendToRoutesTrueWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputSplunk$Outbound =
  | InputSplunkSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputSplunkSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputSplunkPqEnabledFalseWithPqConstraint$Outbound
  | InputSplunkPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputSplunk$outboundSchema: z.ZodType<
  InputSplunk$Outbound,
  z.ZodTypeDef,
  InputSplunk
> = z.union([
  z.lazy(() =>
    InputSplunkSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputSplunkSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputSplunkPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputSplunkPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputSplunkToJSON(inputSplunk: InputSplunk): string {
  return JSON.stringify(InputSplunk$outboundSchema.parse(inputSplunk));
}

/** @internal */
export const InputHttpType$outboundSchema: z.ZodNativeEnum<
  typeof InputHttpType
> = z.nativeEnum(InputHttpType);

/** @internal */
export type InputHttpPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  authTokens?: Array<string> | undefined;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  activityLogSampleRate: number;
  requestTimeout: number;
  socketTimeout: number;
  keepAliveTimeout: number;
  enableHealthCheck: boolean;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  criblAPI: string;
  elasticAPI: string;
  splunkHecAPI: string;
  splunkHecAcks: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  authTokensExt?: Array<models.ItemsTypeAuthTokensExt$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputHttpPqEnabledTrueWithPqConstraint$outboundSchema: z.ZodType<
  InputHttpPqEnabledTrueWithPqConstraint$Outbound,
  z.ZodTypeDef,
  InputHttpPqEnabledTrueWithPqConstraint
> = z.object({
  pqEnabled: z.boolean().default(false),
  pq: models.PqType$outboundSchema.optional(),
  id: z.string(),
  type: InputHttpType$outboundSchema,
  disabled: z.boolean().default(false),
  pipeline: z.string().optional(),
  sendToRoutes: z.boolean().default(true),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
  host: z.string().default("0.0.0.0"),
  port: z.number(),
  authTokens: z.array(z.string()).optional(),
  tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
  maxActiveReq: z.number().default(256),
  maxRequestsPerSocket: z.number().int().default(0),
  enableProxyHeader: z.boolean().default(false),
  captureHeaders: z.boolean().default(false),
  activityLogSampleRate: z.number().default(100),
  requestTimeout: z.number().default(0),
  socketTimeout: z.number().default(0),
  keepAliveTimeout: z.number().default(5),
  enableHealthCheck: z.boolean().default(false),
  ipAllowlistRegex: z.string().default("/.*/"),
  ipDenylistRegex: z.string().default("/^$/"),
  criblAPI: z.string().default("/cribl"),
  elasticAPI: z.string().default("/elastic"),
  splunkHecAPI: z.string().default("/services/collector"),
  splunkHecAcks: z.boolean().default(false),
  metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
    .optional(),
  authTokensExt: z.array(models.ItemsTypeAuthTokensExt$outboundSchema)
    .optional(),
  description: z.string().optional(),
});

export function inputHttpPqEnabledTrueWithPqConstraintToJSON(
  inputHttpPqEnabledTrueWithPqConstraint:
    InputHttpPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputHttpPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputHttpPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputHttpPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  host: string;
  port: number;
  authTokens?: Array<string> | undefined;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  activityLogSampleRate: number;
  requestTimeout: number;
  socketTimeout: number;
  keepAliveTimeout: number;
  enableHealthCheck: boolean;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  criblAPI: string;
  elasticAPI: string;
  splunkHecAPI: string;
  splunkHecAcks: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  authTokensExt?: Array<models.ItemsTypeAuthTokensExt$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputHttpPqEnabledFalseWithPqConstraint$outboundSchema: z.ZodType<
  InputHttpPqEnabledFalseWithPqConstraint$Outbound,
  z.ZodTypeDef,
  InputHttpPqEnabledFalseWithPqConstraint
> = z.object({
  pqEnabled: z.boolean().default(false),
  pq: models.PqType$outboundSchema.optional(),
  id: z.string(),
  type: InputHttpType$outboundSchema,
  disabled: z.boolean().default(false),
  pipeline: z.string().optional(),
  sendToRoutes: z.boolean().default(true),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
  host: z.string().default("0.0.0.0"),
  port: z.number(),
  authTokens: z.array(z.string()).optional(),
  tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
  maxActiveReq: z.number().default(256),
  maxRequestsPerSocket: z.number().int().default(0),
  enableProxyHeader: z.boolean().default(false),
  captureHeaders: z.boolean().default(false),
  activityLogSampleRate: z.number().default(100),
  requestTimeout: z.number().default(0),
  socketTimeout: z.number().default(0),
  keepAliveTimeout: z.number().default(5),
  enableHealthCheck: z.boolean().default(false),
  ipAllowlistRegex: z.string().default("/.*/"),
  ipDenylistRegex: z.string().default("/^$/"),
  criblAPI: z.string().default("/cribl"),
  elasticAPI: z.string().default("/elastic"),
  splunkHecAPI: z.string().default("/services/collector"),
  splunkHecAcks: z.boolean().default(false),
  metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
    .optional(),
  authTokensExt: z.array(models.ItemsTypeAuthTokensExt$outboundSchema)
    .optional(),
  description: z.string().optional(),
});

export function inputHttpPqEnabledFalseWithPqConstraintToJSON(
  inputHttpPqEnabledFalseWithPqConstraint:
    InputHttpPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputHttpPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputHttpPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputHttpSendToRoutesFalseWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  host: string;
  port: number;
  authTokens?: Array<string> | undefined;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  activityLogSampleRate: number;
  requestTimeout: number;
  socketTimeout: number;
  keepAliveTimeout: number;
  enableHealthCheck: boolean;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  criblAPI: string;
  elasticAPI: string;
  splunkHecAPI: string;
  splunkHecAcks: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  authTokensExt?: Array<models.ItemsTypeAuthTokensExt$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputHttpSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputHttpSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputHttpSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputHttpType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    authTokens: z.array(z.string()).optional(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    criblAPI: z.string().default("/cribl"),
    elasticAPI: z.string().default("/elastic"),
    splunkHecAPI: z.string().default("/services/collector"),
    splunkHecAcks: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    authTokensExt: z.array(models.ItemsTypeAuthTokensExt$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputHttpSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputHttpSendToRoutesFalseWithConnectionsConstraint:
    InputHttpSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputHttpSendToRoutesFalseWithConnectionsConstraint$outboundSchema.parse(
      inputHttpSendToRoutesFalseWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputHttpSendToRoutesTrueWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  host: string;
  port: number;
  authTokens?: Array<string> | undefined;
  tls?: models.TlsSettingsServerSideType$Outbound | undefined;
  maxActiveReq: number;
  maxRequestsPerSocket: number;
  enableProxyHeader: boolean;
  captureHeaders: boolean;
  activityLogSampleRate: number;
  requestTimeout: number;
  socketTimeout: number;
  keepAliveTimeout: number;
  enableHealthCheck: boolean;
  ipAllowlistRegex: string;
  ipDenylistRegex: string;
  criblAPI: string;
  elasticAPI: string;
  splunkHecAPI: string;
  splunkHecAcks: boolean;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  authTokensExt?: Array<models.ItemsTypeAuthTokensExt$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputHttpSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputHttpSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputHttpSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputHttpType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    host: z.string().default("0.0.0.0"),
    port: z.number(),
    authTokens: z.array(z.string()).optional(),
    tls: models.TlsSettingsServerSideType$outboundSchema.optional(),
    maxActiveReq: z.number().default(256),
    maxRequestsPerSocket: z.number().int().default(0),
    enableProxyHeader: z.boolean().default(false),
    captureHeaders: z.boolean().default(false),
    activityLogSampleRate: z.number().default(100),
    requestTimeout: z.number().default(0),
    socketTimeout: z.number().default(0),
    keepAliveTimeout: z.number().default(5),
    enableHealthCheck: z.boolean().default(false),
    ipAllowlistRegex: z.string().default("/.*/"),
    ipDenylistRegex: z.string().default("/^$/"),
    criblAPI: z.string().default("/cribl"),
    elasticAPI: z.string().default("/elastic"),
    splunkHecAPI: z.string().default("/services/collector"),
    splunkHecAcks: z.boolean().default(false),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    authTokensExt: z.array(models.ItemsTypeAuthTokensExt$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputHttpSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputHttpSendToRoutesTrueWithConnectionsConstraint:
    InputHttpSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputHttpSendToRoutesTrueWithConnectionsConstraint$outboundSchema.parse(
      inputHttpSendToRoutesTrueWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputHttp$Outbound =
  | InputHttpSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputHttpSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputHttpPqEnabledFalseWithPqConstraint$Outbound
  | InputHttpPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputHttp$outboundSchema: z.ZodType<
  InputHttp$Outbound,
  z.ZodTypeDef,
  InputHttp
> = z.union([
  z.lazy(() =>
    InputHttpSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputHttpSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputHttpPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputHttpPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputHttpToJSON(inputHttp: InputHttp): string {
  return JSON.stringify(InputHttp$outboundSchema.parse(inputHttp));
}

/** @internal */
export const InputMskType$outboundSchema: z.ZodNativeEnum<typeof InputMskType> =
  z.nativeEnum(InputMskType);

/** @internal */
export type InputMskPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  brokers: Array<string>;
  topics: Array<string>;
  groupId: string;
  fromBeginning: boolean;
  sessionTimeout: number;
  rebalanceTimeout: number;
  heartbeatInterval: number;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  kafkaSchemaRegistry?:
    | models.KafkaSchemaRegistryAuthenticationType$Outbound
    | undefined;
  connectionTimeout: number;
  requestTimeout: number;
  maxRetries: number;
  maxBackOff: number;
  initialBackoff: number;
  backoffRate: number;
  authenticationTimeout: number;
  reauthenticationThreshold: number;
  awsAuthenticationMethod: string;
  awsSecretKey?: string | undefined;
  region: string;
  endpoint?: string | undefined;
  signatureVersion: string;
  reuseConnections: boolean;
  rejectUnauthorized: boolean;
  enableAssumeRole: boolean;
  assumeRoleArn?: string | undefined;
  assumeRoleExternalId?: string | undefined;
  durationSeconds: number;
  tls?: models.TlsSettingsClientSideType1$Outbound | undefined;
  autoCommitInterval?: number | undefined;
  autoCommitThreshold?: number | undefined;
  maxBytesPerPartition: number;
  maxBytes: number;
  maxSocketErrors: number;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  awsSecret?: string | undefined;
};

/** @internal */
export const InputMskPqEnabledTrueWithPqConstraint$outboundSchema: z.ZodType<
  InputMskPqEnabledTrueWithPqConstraint$Outbound,
  z.ZodTypeDef,
  InputMskPqEnabledTrueWithPqConstraint
> = z.object({
  pqEnabled: z.boolean().default(false),
  pq: models.PqType$outboundSchema.optional(),
  id: z.string(),
  type: InputMskType$outboundSchema,
  disabled: z.boolean().default(false),
  pipeline: z.string().optional(),
  sendToRoutes: z.boolean().default(true),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
  brokers: z.array(z.string()),
  topics: z.array(z.string()),
  groupId: z.string().default("Cribl"),
  fromBeginning: z.boolean().default(true),
  sessionTimeout: z.number().default(30000),
  rebalanceTimeout: z.number().default(60000),
  heartbeatInterval: z.number().default(3000),
  metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
    .optional(),
  kafkaSchemaRegistry: models
    .KafkaSchemaRegistryAuthenticationType$outboundSchema.optional(),
  connectionTimeout: z.number().default(10000),
  requestTimeout: z.number().default(60000),
  maxRetries: z.number().default(5),
  maxBackOff: z.number().default(30000),
  initialBackoff: z.number().default(300),
  backoffRate: z.number().default(2),
  authenticationTimeout: z.number().default(10000),
  reauthenticationThreshold: z.number().default(10000),
  awsAuthenticationMethod: z.string().default("auto"),
  awsSecretKey: z.string().optional(),
  region: z.string(),
  endpoint: z.string().optional(),
  signatureVersion: models.SignatureVersionOptions$outboundSchema.default("v4"),
  reuseConnections: z.boolean().default(true),
  rejectUnauthorized: z.boolean().default(true),
  enableAssumeRole: z.boolean().default(false),
  assumeRoleArn: z.string().optional(),
  assumeRoleExternalId: z.string().optional(),
  durationSeconds: z.number().default(3600),
  tls: models.TlsSettingsClientSideType1$outboundSchema.optional(),
  autoCommitInterval: z.number().optional(),
  autoCommitThreshold: z.number().optional(),
  maxBytesPerPartition: z.number().default(1048576),
  maxBytes: z.number().default(10485760),
  maxSocketErrors: z.number().default(0),
  description: z.string().optional(),
  awsApiKey: z.string().optional(),
  awsSecret: z.string().optional(),
});

export function inputMskPqEnabledTrueWithPqConstraintToJSON(
  inputMskPqEnabledTrueWithPqConstraint: InputMskPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputMskPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputMskPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputMskPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  brokers: Array<string>;
  topics: Array<string>;
  groupId: string;
  fromBeginning: boolean;
  sessionTimeout: number;
  rebalanceTimeout: number;
  heartbeatInterval: number;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  kafkaSchemaRegistry?:
    | models.KafkaSchemaRegistryAuthenticationType$Outbound
    | undefined;
  connectionTimeout: number;
  requestTimeout: number;
  maxRetries: number;
  maxBackOff: number;
  initialBackoff: number;
  backoffRate: number;
  authenticationTimeout: number;
  reauthenticationThreshold: number;
  awsAuthenticationMethod: string;
  awsSecretKey?: string | undefined;
  region: string;
  endpoint?: string | undefined;
  signatureVersion: string;
  reuseConnections: boolean;
  rejectUnauthorized: boolean;
  enableAssumeRole: boolean;
  assumeRoleArn?: string | undefined;
  assumeRoleExternalId?: string | undefined;
  durationSeconds: number;
  tls?: models.TlsSettingsClientSideType1$Outbound | undefined;
  autoCommitInterval?: number | undefined;
  autoCommitThreshold?: number | undefined;
  maxBytesPerPartition: number;
  maxBytes: number;
  maxSocketErrors: number;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  awsSecret?: string | undefined;
};

/** @internal */
export const InputMskPqEnabledFalseWithPqConstraint$outboundSchema: z.ZodType<
  InputMskPqEnabledFalseWithPqConstraint$Outbound,
  z.ZodTypeDef,
  InputMskPqEnabledFalseWithPqConstraint
> = z.object({
  pqEnabled: z.boolean().default(false),
  pq: models.PqType$outboundSchema.optional(),
  id: z.string(),
  type: InputMskType$outboundSchema,
  disabled: z.boolean().default(false),
  pipeline: z.string().optional(),
  sendToRoutes: z.boolean().default(true),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
  brokers: z.array(z.string()),
  topics: z.array(z.string()),
  groupId: z.string().default("Cribl"),
  fromBeginning: z.boolean().default(true),
  sessionTimeout: z.number().default(30000),
  rebalanceTimeout: z.number().default(60000),
  heartbeatInterval: z.number().default(3000),
  metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
    .optional(),
  kafkaSchemaRegistry: models
    .KafkaSchemaRegistryAuthenticationType$outboundSchema.optional(),
  connectionTimeout: z.number().default(10000),
  requestTimeout: z.number().default(60000),
  maxRetries: z.number().default(5),
  maxBackOff: z.number().default(30000),
  initialBackoff: z.number().default(300),
  backoffRate: z.number().default(2),
  authenticationTimeout: z.number().default(10000),
  reauthenticationThreshold: z.number().default(10000),
  awsAuthenticationMethod: z.string().default("auto"),
  awsSecretKey: z.string().optional(),
  region: z.string(),
  endpoint: z.string().optional(),
  signatureVersion: models.SignatureVersionOptions$outboundSchema.default("v4"),
  reuseConnections: z.boolean().default(true),
  rejectUnauthorized: z.boolean().default(true),
  enableAssumeRole: z.boolean().default(false),
  assumeRoleArn: z.string().optional(),
  assumeRoleExternalId: z.string().optional(),
  durationSeconds: z.number().default(3600),
  tls: models.TlsSettingsClientSideType1$outboundSchema.optional(),
  autoCommitInterval: z.number().optional(),
  autoCommitThreshold: z.number().optional(),
  maxBytesPerPartition: z.number().default(1048576),
  maxBytes: z.number().default(10485760),
  maxSocketErrors: z.number().default(0),
  description: z.string().optional(),
  awsApiKey: z.string().optional(),
  awsSecret: z.string().optional(),
});

export function inputMskPqEnabledFalseWithPqConstraintToJSON(
  inputMskPqEnabledFalseWithPqConstraint:
    InputMskPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputMskPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputMskPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputMskSendToRoutesFalseWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  brokers: Array<string>;
  topics: Array<string>;
  groupId: string;
  fromBeginning: boolean;
  sessionTimeout: number;
  rebalanceTimeout: number;
  heartbeatInterval: number;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  kafkaSchemaRegistry?:
    | models.KafkaSchemaRegistryAuthenticationType$Outbound
    | undefined;
  connectionTimeout: number;
  requestTimeout: number;
  maxRetries: number;
  maxBackOff: number;
  initialBackoff: number;
  backoffRate: number;
  authenticationTimeout: number;
  reauthenticationThreshold: number;
  awsAuthenticationMethod: string;
  awsSecretKey?: string | undefined;
  region: string;
  endpoint?: string | undefined;
  signatureVersion: string;
  reuseConnections: boolean;
  rejectUnauthorized: boolean;
  enableAssumeRole: boolean;
  assumeRoleArn?: string | undefined;
  assumeRoleExternalId?: string | undefined;
  durationSeconds: number;
  tls?: models.TlsSettingsClientSideType1$Outbound | undefined;
  autoCommitInterval?: number | undefined;
  autoCommitThreshold?: number | undefined;
  maxBytesPerPartition: number;
  maxBytes: number;
  maxSocketErrors: number;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  awsSecret?: string | undefined;
};

/** @internal */
export const InputMskSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputMskSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputMskSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputMskType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    brokers: z.array(z.string()),
    topics: z.array(z.string()),
    groupId: z.string().default("Cribl"),
    fromBeginning: z.boolean().default(true),
    sessionTimeout: z.number().default(30000),
    rebalanceTimeout: z.number().default(60000),
    heartbeatInterval: z.number().default(3000),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    kafkaSchemaRegistry: models
      .KafkaSchemaRegistryAuthenticationType$outboundSchema.optional(),
    connectionTimeout: z.number().default(10000),
    requestTimeout: z.number().default(60000),
    maxRetries: z.number().default(5),
    maxBackOff: z.number().default(30000),
    initialBackoff: z.number().default(300),
    backoffRate: z.number().default(2),
    authenticationTimeout: z.number().default(10000),
    reauthenticationThreshold: z.number().default(10000),
    awsAuthenticationMethod: z.string().default("auto"),
    awsSecretKey: z.string().optional(),
    region: z.string(),
    endpoint: z.string().optional(),
    signatureVersion: models.SignatureVersionOptions$outboundSchema.default(
      "v4",
    ),
    reuseConnections: z.boolean().default(true),
    rejectUnauthorized: z.boolean().default(true),
    enableAssumeRole: z.boolean().default(false),
    assumeRoleArn: z.string().optional(),
    assumeRoleExternalId: z.string().optional(),
    durationSeconds: z.number().default(3600),
    tls: models.TlsSettingsClientSideType1$outboundSchema.optional(),
    autoCommitInterval: z.number().optional(),
    autoCommitThreshold: z.number().optional(),
    maxBytesPerPartition: z.number().default(1048576),
    maxBytes: z.number().default(10485760),
    maxSocketErrors: z.number().default(0),
    description: z.string().optional(),
    awsApiKey: z.string().optional(),
    awsSecret: z.string().optional(),
  });

export function inputMskSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputMskSendToRoutesFalseWithConnectionsConstraint:
    InputMskSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputMskSendToRoutesFalseWithConnectionsConstraint$outboundSchema.parse(
      inputMskSendToRoutesFalseWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputMskSendToRoutesTrueWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  brokers: Array<string>;
  topics: Array<string>;
  groupId: string;
  fromBeginning: boolean;
  sessionTimeout: number;
  rebalanceTimeout: number;
  heartbeatInterval: number;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  kafkaSchemaRegistry?:
    | models.KafkaSchemaRegistryAuthenticationType$Outbound
    | undefined;
  connectionTimeout: number;
  requestTimeout: number;
  maxRetries: number;
  maxBackOff: number;
  initialBackoff: number;
  backoffRate: number;
  authenticationTimeout: number;
  reauthenticationThreshold: number;
  awsAuthenticationMethod: string;
  awsSecretKey?: string | undefined;
  region: string;
  endpoint?: string | undefined;
  signatureVersion: string;
  reuseConnections: boolean;
  rejectUnauthorized: boolean;
  enableAssumeRole: boolean;
  assumeRoleArn?: string | undefined;
  assumeRoleExternalId?: string | undefined;
  durationSeconds: number;
  tls?: models.TlsSettingsClientSideType1$Outbound | undefined;
  autoCommitInterval?: number | undefined;
  autoCommitThreshold?: number | undefined;
  maxBytesPerPartition: number;
  maxBytes: number;
  maxSocketErrors: number;
  description?: string | undefined;
  awsApiKey?: string | undefined;
  awsSecret?: string | undefined;
};

/** @internal */
export const InputMskSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputMskSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputMskSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputMskType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    brokers: z.array(z.string()),
    topics: z.array(z.string()),
    groupId: z.string().default("Cribl"),
    fromBeginning: z.boolean().default(true),
    sessionTimeout: z.number().default(30000),
    rebalanceTimeout: z.number().default(60000),
    heartbeatInterval: z.number().default(3000),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    kafkaSchemaRegistry: models
      .KafkaSchemaRegistryAuthenticationType$outboundSchema.optional(),
    connectionTimeout: z.number().default(10000),
    requestTimeout: z.number().default(60000),
    maxRetries: z.number().default(5),
    maxBackOff: z.number().default(30000),
    initialBackoff: z.number().default(300),
    backoffRate: z.number().default(2),
    authenticationTimeout: z.number().default(10000),
    reauthenticationThreshold: z.number().default(10000),
    awsAuthenticationMethod: z.string().default("auto"),
    awsSecretKey: z.string().optional(),
    region: z.string(),
    endpoint: z.string().optional(),
    signatureVersion: models.SignatureVersionOptions$outboundSchema.default(
      "v4",
    ),
    reuseConnections: z.boolean().default(true),
    rejectUnauthorized: z.boolean().default(true),
    enableAssumeRole: z.boolean().default(false),
    assumeRoleArn: z.string().optional(),
    assumeRoleExternalId: z.string().optional(),
    durationSeconds: z.number().default(3600),
    tls: models.TlsSettingsClientSideType1$outboundSchema.optional(),
    autoCommitInterval: z.number().optional(),
    autoCommitThreshold: z.number().optional(),
    maxBytesPerPartition: z.number().default(1048576),
    maxBytes: z.number().default(10485760),
    maxSocketErrors: z.number().default(0),
    description: z.string().optional(),
    awsApiKey: z.string().optional(),
    awsSecret: z.string().optional(),
  });

export function inputMskSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputMskSendToRoutesTrueWithConnectionsConstraint:
    InputMskSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputMskSendToRoutesTrueWithConnectionsConstraint$outboundSchema.parse(
      inputMskSendToRoutesTrueWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputMsk$Outbound =
  | InputMskSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputMskSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputMskPqEnabledFalseWithPqConstraint$Outbound
  | InputMskPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputMsk$outboundSchema: z.ZodType<
  InputMsk$Outbound,
  z.ZodTypeDef,
  InputMsk
> = z.union([
  z.lazy(() =>
    InputMskSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputMskSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputMskPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputMskPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputMskToJSON(inputMsk: InputMsk): string {
  return JSON.stringify(InputMsk$outboundSchema.parse(inputMsk));
}

/** @internal */
export const InputKafkaType$outboundSchema: z.ZodNativeEnum<
  typeof InputKafkaType
> = z.nativeEnum(InputKafkaType);

/** @internal */
export type InputKafkaPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  brokers: Array<string>;
  topics: Array<string>;
  groupId: string;
  fromBeginning: boolean;
  kafkaSchemaRegistry?:
    | models.KafkaSchemaRegistryAuthenticationType$Outbound
    | undefined;
  connectionTimeout: number;
  requestTimeout: number;
  maxRetries: number;
  maxBackOff: number;
  initialBackoff: number;
  backoffRate: number;
  authenticationTimeout: number;
  reauthenticationThreshold: number;
  sasl?: models.AuthenticationType$Outbound | undefined;
  tls?:
    | models.TlsSettingsClientSideTypeKafkaSchemaRegistry$Outbound
    | undefined;
  sessionTimeout: number;
  rebalanceTimeout: number;
  heartbeatInterval: number;
  autoCommitInterval?: number | undefined;
  autoCommitThreshold?: number | undefined;
  maxBytesPerPartition: number;
  maxBytes: number;
  maxSocketErrors: number;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputKafkaPqEnabledTrueWithPqConstraint$outboundSchema: z.ZodType<
  InputKafkaPqEnabledTrueWithPqConstraint$Outbound,
  z.ZodTypeDef,
  InputKafkaPqEnabledTrueWithPqConstraint
> = z.object({
  pqEnabled: z.boolean().default(false),
  pq: models.PqType$outboundSchema.optional(),
  id: z.string(),
  type: InputKafkaType$outboundSchema,
  disabled: z.boolean().default(false),
  pipeline: z.string().optional(),
  sendToRoutes: z.boolean().default(true),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
  brokers: z.array(z.string()),
  topics: z.array(z.string()),
  groupId: z.string().default("Cribl"),
  fromBeginning: z.boolean().default(true),
  kafkaSchemaRegistry: models
    .KafkaSchemaRegistryAuthenticationType$outboundSchema.optional(),
  connectionTimeout: z.number().default(10000),
  requestTimeout: z.number().default(60000),
  maxRetries: z.number().default(5),
  maxBackOff: z.number().default(30000),
  initialBackoff: z.number().default(300),
  backoffRate: z.number().default(2),
  authenticationTimeout: z.number().default(10000),
  reauthenticationThreshold: z.number().default(10000),
  sasl: models.AuthenticationType$outboundSchema.optional(),
  tls: models.TlsSettingsClientSideTypeKafkaSchemaRegistry$outboundSchema
    .optional(),
  sessionTimeout: z.number().default(30000),
  rebalanceTimeout: z.number().default(60000),
  heartbeatInterval: z.number().default(3000),
  autoCommitInterval: z.number().optional(),
  autoCommitThreshold: z.number().optional(),
  maxBytesPerPartition: z.number().default(1048576),
  maxBytes: z.number().default(10485760),
  maxSocketErrors: z.number().default(0),
  metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
    .optional(),
  description: z.string().optional(),
});

export function inputKafkaPqEnabledTrueWithPqConstraintToJSON(
  inputKafkaPqEnabledTrueWithPqConstraint:
    InputKafkaPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputKafkaPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputKafkaPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputKafkaPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  brokers: Array<string>;
  topics: Array<string>;
  groupId: string;
  fromBeginning: boolean;
  kafkaSchemaRegistry?:
    | models.KafkaSchemaRegistryAuthenticationType$Outbound
    | undefined;
  connectionTimeout: number;
  requestTimeout: number;
  maxRetries: number;
  maxBackOff: number;
  initialBackoff: number;
  backoffRate: number;
  authenticationTimeout: number;
  reauthenticationThreshold: number;
  sasl?: models.AuthenticationType$Outbound | undefined;
  tls?:
    | models.TlsSettingsClientSideTypeKafkaSchemaRegistry$Outbound
    | undefined;
  sessionTimeout: number;
  rebalanceTimeout: number;
  heartbeatInterval: number;
  autoCommitInterval?: number | undefined;
  autoCommitThreshold?: number | undefined;
  maxBytesPerPartition: number;
  maxBytes: number;
  maxSocketErrors: number;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputKafkaPqEnabledFalseWithPqConstraint$outboundSchema: z.ZodType<
  InputKafkaPqEnabledFalseWithPqConstraint$Outbound,
  z.ZodTypeDef,
  InputKafkaPqEnabledFalseWithPqConstraint
> = z.object({
  pqEnabled: z.boolean().default(false),
  pq: models.PqType$outboundSchema.optional(),
  id: z.string(),
  type: InputKafkaType$outboundSchema,
  disabled: z.boolean().default(false),
  pipeline: z.string().optional(),
  sendToRoutes: z.boolean().default(true),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
  brokers: z.array(z.string()),
  topics: z.array(z.string()),
  groupId: z.string().default("Cribl"),
  fromBeginning: z.boolean().default(true),
  kafkaSchemaRegistry: models
    .KafkaSchemaRegistryAuthenticationType$outboundSchema.optional(),
  connectionTimeout: z.number().default(10000),
  requestTimeout: z.number().default(60000),
  maxRetries: z.number().default(5),
  maxBackOff: z.number().default(30000),
  initialBackoff: z.number().default(300),
  backoffRate: z.number().default(2),
  authenticationTimeout: z.number().default(10000),
  reauthenticationThreshold: z.number().default(10000),
  sasl: models.AuthenticationType$outboundSchema.optional(),
  tls: models.TlsSettingsClientSideTypeKafkaSchemaRegistry$outboundSchema
    .optional(),
  sessionTimeout: z.number().default(30000),
  rebalanceTimeout: z.number().default(60000),
  heartbeatInterval: z.number().default(3000),
  autoCommitInterval: z.number().optional(),
  autoCommitThreshold: z.number().optional(),
  maxBytesPerPartition: z.number().default(1048576),
  maxBytes: z.number().default(10485760),
  maxSocketErrors: z.number().default(0),
  metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
    .optional(),
  description: z.string().optional(),
});

export function inputKafkaPqEnabledFalseWithPqConstraintToJSON(
  inputKafkaPqEnabledFalseWithPqConstraint:
    InputKafkaPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputKafkaPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputKafkaPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputKafkaSendToRoutesFalseWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  brokers: Array<string>;
  topics: Array<string>;
  groupId: string;
  fromBeginning: boolean;
  kafkaSchemaRegistry?:
    | models.KafkaSchemaRegistryAuthenticationType$Outbound
    | undefined;
  connectionTimeout: number;
  requestTimeout: number;
  maxRetries: number;
  maxBackOff: number;
  initialBackoff: number;
  backoffRate: number;
  authenticationTimeout: number;
  reauthenticationThreshold: number;
  sasl?: models.AuthenticationType$Outbound | undefined;
  tls?:
    | models.TlsSettingsClientSideTypeKafkaSchemaRegistry$Outbound
    | undefined;
  sessionTimeout: number;
  rebalanceTimeout: number;
  heartbeatInterval: number;
  autoCommitInterval?: number | undefined;
  autoCommitThreshold?: number | undefined;
  maxBytesPerPartition: number;
  maxBytes: number;
  maxSocketErrors: number;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputKafkaSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputKafkaSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputKafkaSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputKafkaType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    brokers: z.array(z.string()),
    topics: z.array(z.string()),
    groupId: z.string().default("Cribl"),
    fromBeginning: z.boolean().default(true),
    kafkaSchemaRegistry: models
      .KafkaSchemaRegistryAuthenticationType$outboundSchema.optional(),
    connectionTimeout: z.number().default(10000),
    requestTimeout: z.number().default(60000),
    maxRetries: z.number().default(5),
    maxBackOff: z.number().default(30000),
    initialBackoff: z.number().default(300),
    backoffRate: z.number().default(2),
    authenticationTimeout: z.number().default(10000),
    reauthenticationThreshold: z.number().default(10000),
    sasl: models.AuthenticationType$outboundSchema.optional(),
    tls: models.TlsSettingsClientSideTypeKafkaSchemaRegistry$outboundSchema
      .optional(),
    sessionTimeout: z.number().default(30000),
    rebalanceTimeout: z.number().default(60000),
    heartbeatInterval: z.number().default(3000),
    autoCommitInterval: z.number().optional(),
    autoCommitThreshold: z.number().optional(),
    maxBytesPerPartition: z.number().default(1048576),
    maxBytes: z.number().default(10485760),
    maxSocketErrors: z.number().default(0),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputKafkaSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputKafkaSendToRoutesFalseWithConnectionsConstraint:
    InputKafkaSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputKafkaSendToRoutesFalseWithConnectionsConstraint$outboundSchema.parse(
      inputKafkaSendToRoutesFalseWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputKafkaSendToRoutesTrueWithConnectionsConstraint$Outbound = {
  sendToRoutes: boolean;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  environment?: string | undefined;
  pqEnabled: boolean;
  streamtags?: Array<string> | undefined;
  pq?: models.PqType$Outbound | undefined;
  brokers: Array<string>;
  topics: Array<string>;
  groupId: string;
  fromBeginning: boolean;
  kafkaSchemaRegistry?:
    | models.KafkaSchemaRegistryAuthenticationType$Outbound
    | undefined;
  connectionTimeout: number;
  requestTimeout: number;
  maxRetries: number;
  maxBackOff: number;
  initialBackoff: number;
  backoffRate: number;
  authenticationTimeout: number;
  reauthenticationThreshold: number;
  sasl?: models.AuthenticationType$Outbound | undefined;
  tls?:
    | models.TlsSettingsClientSideTypeKafkaSchemaRegistry$Outbound
    | undefined;
  sessionTimeout: number;
  rebalanceTimeout: number;
  heartbeatInterval: number;
  autoCommitInterval?: number | undefined;
  autoCommitThreshold?: number | undefined;
  maxBytesPerPartition: number;
  maxBytes: number;
  maxSocketErrors: number;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  description?: string | undefined;
};

/** @internal */
export const InputKafkaSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputKafkaSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputKafkaSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputKafkaType$outboundSchema,
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    brokers: z.array(z.string()),
    topics: z.array(z.string()),
    groupId: z.string().default("Cribl"),
    fromBeginning: z.boolean().default(true),
    kafkaSchemaRegistry: models
      .KafkaSchemaRegistryAuthenticationType$outboundSchema.optional(),
    connectionTimeout: z.number().default(10000),
    requestTimeout: z.number().default(60000),
    maxRetries: z.number().default(5),
    maxBackOff: z.number().default(30000),
    initialBackoff: z.number().default(300),
    backoffRate: z.number().default(2),
    authenticationTimeout: z.number().default(10000),
    reauthenticationThreshold: z.number().default(10000),
    sasl: models.AuthenticationType$outboundSchema.optional(),
    tls: models.TlsSettingsClientSideTypeKafkaSchemaRegistry$outboundSchema
      .optional(),
    sessionTimeout: z.number().default(30000),
    rebalanceTimeout: z.number().default(60000),
    heartbeatInterval: z.number().default(3000),
    autoCommitInterval: z.number().optional(),
    autoCommitThreshold: z.number().optional(),
    maxBytesPerPartition: z.number().default(1048576),
    maxBytes: z.number().default(10485760),
    maxSocketErrors: z.number().default(0),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    description: z.string().optional(),
  });

export function inputKafkaSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputKafkaSendToRoutesTrueWithConnectionsConstraint:
    InputKafkaSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputKafkaSendToRoutesTrueWithConnectionsConstraint$outboundSchema.parse(
      inputKafkaSendToRoutesTrueWithConnectionsConstraint,
    ),
  );
}

/** @internal */
export type InputKafka$Outbound =
  | InputKafkaSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputKafkaSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputKafkaPqEnabledFalseWithPqConstraint$Outbound
  | InputKafkaPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputKafka$outboundSchema: z.ZodType<
  InputKafka$Outbound,
  z.ZodTypeDef,
  InputKafka
> = z.union([
  z.lazy(() =>
    InputKafkaSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputKafkaSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputKafkaPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputKafkaPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputKafkaToJSON(inputKafka: InputKafka): string {
  return JSON.stringify(InputKafka$outboundSchema.parse(inputKafka));
}

/** @internal */
export const InputCollectionType$outboundSchema: z.ZodNativeEnum<
  typeof InputCollectionType
> = z.nativeEnum(InputCollectionType);

/** @internal */
export type InputCollectionPqEnabledTrueWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  preprocess?:
    | models.PreprocessTypeSavedJobCollectionInput$Outbound
    | undefined;
  throttleRatePerSec: string;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  output?: string | undefined;
};

/** @internal */
export const InputCollectionPqEnabledTrueWithPqConstraint$outboundSchema:
  z.ZodType<
    InputCollectionPqEnabledTrueWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputCollectionPqEnabledTrueWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputCollectionType$outboundSchema.default("collection"),
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    preprocess: models.PreprocessTypeSavedJobCollectionInput$outboundSchema
      .optional(),
    throttleRatePerSec: z.string().default("0"),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    output: z.string().optional(),
  });

export function inputCollectionPqEnabledTrueWithPqConstraintToJSON(
  inputCollectionPqEnabledTrueWithPqConstraint:
    InputCollectionPqEnabledTrueWithPqConstraint,
): string {
  return JSON.stringify(
    InputCollectionPqEnabledTrueWithPqConstraint$outboundSchema.parse(
      inputCollectionPqEnabledTrueWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputCollectionPqEnabledFalseWithPqConstraint$Outbound = {
  pqEnabled: boolean;
  pq?: models.PqType$Outbound | undefined;
  id: string;
  type: string;
  disabled: boolean;
  pipeline?: string | undefined;
  sendToRoutes: boolean;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
  breakerRulesets?: Array<string> | undefined;
  staleChannelFlushMs: number;
  preprocess?:
    | models.PreprocessTypeSavedJobCollectionInput$Outbound
    | undefined;
  throttleRatePerSec: string;
  metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
  output?: string | undefined;
};

/** @internal */
export const InputCollectionPqEnabledFalseWithPqConstraint$outboundSchema:
  z.ZodType<
    InputCollectionPqEnabledFalseWithPqConstraint$Outbound,
    z.ZodTypeDef,
    InputCollectionPqEnabledFalseWithPqConstraint
  > = z.object({
    pqEnabled: z.boolean().default(false),
    pq: models.PqType$outboundSchema.optional(),
    id: z.string(),
    type: InputCollectionType$outboundSchema.default("collection"),
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    sendToRoutes: z.boolean().default(true),
    environment: z.string().optional(),
    streamtags: z.array(z.string()).optional(),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    preprocess: models.PreprocessTypeSavedJobCollectionInput$outboundSchema
      .optional(),
    throttleRatePerSec: z.string().default("0"),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    output: z.string().optional(),
  });

export function inputCollectionPqEnabledFalseWithPqConstraintToJSON(
  inputCollectionPqEnabledFalseWithPqConstraint:
    InputCollectionPqEnabledFalseWithPqConstraint,
): string {
  return JSON.stringify(
    InputCollectionPqEnabledFalseWithPqConstraint$outboundSchema.parse(
      inputCollectionPqEnabledFalseWithPqConstraint,
    ),
  );
}

/** @internal */
export type InputCollectionSendToRoutesFalseWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    breakerRulesets?: Array<string> | undefined;
    staleChannelFlushMs: number;
    preprocess?:
      | models.PreprocessTypeSavedJobCollectionInput$Outbound
      | undefined;
    throttleRatePerSec: string;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    output?: string | undefined;
  };

/** @internal */
export const InputCollectionSendToRoutesFalseWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputCollectionSendToRoutesFalseWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputCollectionSendToRoutesFalseWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputCollectionType$outboundSchema.default("collection"),
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    preprocess: models.PreprocessTypeSavedJobCollectionInput$outboundSchema
      .optional(),
    throttleRatePerSec: z.string().default("0"),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    output: z.string().optional(),
  });

export function inputCollectionSendToRoutesFalseWithConnectionsConstraintToJSON(
  inputCollectionSendToRoutesFalseWithConnectionsConstraint:
    InputCollectionSendToRoutesFalseWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputCollectionSendToRoutesFalseWithConnectionsConstraint$outboundSchema
      .parse(inputCollectionSendToRoutesFalseWithConnectionsConstraint),
  );
}

/** @internal */
export type InputCollectionSendToRoutesTrueWithConnectionsConstraint$Outbound =
  {
    sendToRoutes: boolean;
    connections?: Array<models.ItemsTypeConnections$Outbound> | undefined;
    id: string;
    type: string;
    disabled: boolean;
    pipeline?: string | undefined;
    environment?: string | undefined;
    pqEnabled: boolean;
    streamtags?: Array<string> | undefined;
    pq?: models.PqType$Outbound | undefined;
    breakerRulesets?: Array<string> | undefined;
    staleChannelFlushMs: number;
    preprocess?:
      | models.PreprocessTypeSavedJobCollectionInput$Outbound
      | undefined;
    throttleRatePerSec: string;
    metadata?: Array<models.ItemsTypeNotificationMetadata$Outbound> | undefined;
    output?: string | undefined;
  };

/** @internal */
export const InputCollectionSendToRoutesTrueWithConnectionsConstraint$outboundSchema:
  z.ZodType<
    InputCollectionSendToRoutesTrueWithConnectionsConstraint$Outbound,
    z.ZodTypeDef,
    InputCollectionSendToRoutesTrueWithConnectionsConstraint
  > = z.object({
    sendToRoutes: z.boolean().default(true),
    connections: z.array(models.ItemsTypeConnections$outboundSchema).optional(),
    id: z.string(),
    type: InputCollectionType$outboundSchema.default("collection"),
    disabled: z.boolean().default(false),
    pipeline: z.string().optional(),
    environment: z.string().optional(),
    pqEnabled: z.boolean().default(false),
    streamtags: z.array(z.string()).optional(),
    pq: models.PqType$outboundSchema.optional(),
    breakerRulesets: z.array(z.string()).optional(),
    staleChannelFlushMs: z.number().default(10000),
    preprocess: models.PreprocessTypeSavedJobCollectionInput$outboundSchema
      .optional(),
    throttleRatePerSec: z.string().default("0"),
    metadata: z.array(models.ItemsTypeNotificationMetadata$outboundSchema)
      .optional(),
    output: z.string().optional(),
  });

export function inputCollectionSendToRoutesTrueWithConnectionsConstraintToJSON(
  inputCollectionSendToRoutesTrueWithConnectionsConstraint:
    InputCollectionSendToRoutesTrueWithConnectionsConstraint,
): string {
  return JSON.stringify(
    InputCollectionSendToRoutesTrueWithConnectionsConstraint$outboundSchema
      .parse(inputCollectionSendToRoutesTrueWithConnectionsConstraint),
  );
}

/** @internal */
export type InputCollection$Outbound =
  | InputCollectionSendToRoutesTrueWithConnectionsConstraint$Outbound
  | InputCollectionSendToRoutesFalseWithConnectionsConstraint$Outbound
  | InputCollectionPqEnabledFalseWithPqConstraint$Outbound
  | InputCollectionPqEnabledTrueWithPqConstraint$Outbound;

/** @internal */
export const InputCollection$outboundSchema: z.ZodType<
  InputCollection$Outbound,
  z.ZodTypeDef,
  InputCollection
> = z.union([
  z.lazy(() =>
    InputCollectionSendToRoutesTrueWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() =>
    InputCollectionSendToRoutesFalseWithConnectionsConstraint$outboundSchema
  ),
  z.lazy(() => InputCollectionPqEnabledFalseWithPqConstraint$outboundSchema),
  z.lazy(() => InputCollectionPqEnabledTrueWithPqConstraint$outboundSchema),
]);

export function inputCollectionToJSON(
  inputCollection: InputCollection,
): string {
  return JSON.stringify(InputCollection$outboundSchema.parse(inputCollection));
}

/** @internal */
export type CreateInputRequest$Outbound =
  | (
    | InputCollectionSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputCollectionSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputCollectionPqEnabledFalseWithPqConstraint$Outbound
    | InputCollectionPqEnabledTrueWithPqConstraint$Outbound & {
      type: "collection";
    }
  )
  | (
    | InputKafkaSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputKafkaSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputKafkaPqEnabledFalseWithPqConstraint$Outbound
    | InputKafkaPqEnabledTrueWithPqConstraint$Outbound & { type: "kafka" }
  )
  | (
    | InputMskSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputMskSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputMskPqEnabledFalseWithPqConstraint$Outbound
    | InputMskPqEnabledTrueWithPqConstraint$Outbound & { type: "msk" }
  )
  | (
    | InputHttpSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputHttpSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputHttpPqEnabledFalseWithPqConstraint$Outbound
    | InputHttpPqEnabledTrueWithPqConstraint$Outbound & { type: "http" }
  )
  | (
    | InputSplunkSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputSplunkSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputSplunkPqEnabledFalseWithPqConstraint$Outbound
    | InputSplunkPqEnabledTrueWithPqConstraint$Outbound & { type: "splunk" }
  )
  | (
    | InputSplunkSearchSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputSplunkSearchSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputSplunkSearchPqEnabledFalseWithPqConstraint$Outbound
    | InputSplunkSearchPqEnabledTrueWithPqConstraint$Outbound & {
      type: "splunk_search";
    }
  )
  | (
    | InputSplunkHecSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputSplunkHecSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputSplunkHecPqEnabledFalseWithPqConstraint$Outbound
    | InputSplunkHecPqEnabledTrueWithPqConstraint$Outbound & {
      type: "splunk_hec";
    }
  )
  | (
    | InputAzureBlobSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputAzureBlobSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputAzureBlobPqEnabledFalseWithPqConstraint$Outbound
    | InputAzureBlobPqEnabledTrueWithPqConstraint$Outbound & {
      type: "azure_blob";
    }
  )
  | (
    | InputElasticSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputElasticSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputElasticPqEnabledFalseWithPqConstraint$Outbound
    | InputElasticPqEnabledTrueWithPqConstraint$Outbound & { type: "elastic" }
  )
  | (
    | InputConfluentCloudSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputConfluentCloudSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputConfluentCloudPqEnabledFalseWithPqConstraint$Outbound
    | InputConfluentCloudPqEnabledTrueWithPqConstraint$Outbound & {
      type: "confluent_cloud";
    }
  )
  | (
    | InputGrafanaGrafana1$Outbound
    | InputGrafanaGrafana2$Outbound & { type: "grafana" }
  )
  | (
    | InputLokiSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputLokiSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputLokiPqEnabledFalseWithPqConstraint$Outbound
    | InputLokiPqEnabledTrueWithPqConstraint$Outbound & { type: "loki" }
  )
  | (
    | InputPrometheusRwSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputPrometheusRwSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputPrometheusRwPqEnabledFalseWithPqConstraint$Outbound
    | InputPrometheusRwPqEnabledTrueWithPqConstraint$Outbound & {
      type: "prometheus_rw";
    }
  )
  | (
    | InputPrometheusSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputPrometheusSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputPrometheusPqEnabledFalseWithPqConstraint$Outbound
    | InputPrometheusPqEnabledTrueWithPqConstraint$Outbound & {
      type: "prometheus";
    }
  )
  | (
    | InputEdgePrometheusSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputEdgePrometheusSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputEdgePrometheusPqEnabledFalseWithPqConstraint$Outbound
    | InputEdgePrometheusPqEnabledTrueWithPqConstraint$Outbound & {
      type: "edge_prometheus";
    }
  )
  | (
    | InputOffice365MgmtSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputOffice365MgmtSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputOffice365MgmtPqEnabledFalseWithPqConstraint$Outbound
    | InputOffice365MgmtPqEnabledTrueWithPqConstraint$Outbound & {
      type: "office365_mgmt";
    }
  )
  | (
    | InputOffice365ServiceSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputOffice365ServiceSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputOffice365ServicePqEnabledFalseWithPqConstraint$Outbound
    | InputOffice365ServicePqEnabledTrueWithPqConstraint$Outbound & {
      type: "office365_service";
    }
  )
  | (
    | InputOffice365MsgTraceSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputOffice365MsgTraceSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputOffice365MsgTracePqEnabledFalseWithPqConstraint$Outbound
    | InputOffice365MsgTracePqEnabledTrueWithPqConstraint$Outbound & {
      type: "office365_msg_trace";
    }
  )
  | (
    | InputEventhubSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputEventhubSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputEventhubPqEnabledFalseWithPqConstraint$Outbound
    | InputEventhubPqEnabledTrueWithPqConstraint$Outbound & { type: "eventhub" }
  )
  | (
    | InputExecSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputExecSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputExecPqEnabledFalseWithPqConstraint$Outbound
    | InputExecPqEnabledTrueWithPqConstraint$Outbound & { type: "exec" }
  )
  | (
    | InputFirehoseSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputFirehoseSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputFirehosePqEnabledFalseWithPqConstraint$Outbound
    | InputFirehosePqEnabledTrueWithPqConstraint$Outbound & { type: "firehose" }
  )
  | (
    | InputGooglePubsubSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputGooglePubsubSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputGooglePubsubPqEnabledFalseWithPqConstraint$Outbound
    | InputGooglePubsubPqEnabledTrueWithPqConstraint$Outbound & {
      type: "google_pubsub";
    }
  )
  | (
    | InputCriblSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputCriblSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputCriblPqEnabledFalseWithPqConstraint$Outbound
    | InputCriblPqEnabledTrueWithPqConstraint$Outbound & { type: "cribl" }
  )
  | (
    | InputCriblTcpSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputCriblTcpSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputCriblTcpPqEnabledFalseWithPqConstraint$Outbound
    | InputCriblTcpPqEnabledTrueWithPqConstraint$Outbound & {
      type: "cribl_tcp";
    }
  )
  | (
    | InputCriblHttpSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputCriblHttpSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputCriblHttpPqEnabledFalseWithPqConstraint$Outbound
    | InputCriblHttpPqEnabledTrueWithPqConstraint$Outbound & {
      type: "cribl_http";
    }
  )
  | (
    | InputCriblLakeHttpSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputCriblLakeHttpSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputCriblLakeHttpPqEnabledFalseWithPqConstraint$Outbound
    | InputCriblLakeHttpPqEnabledTrueWithPqConstraint$Outbound & {
      type: "cribl_lake_http";
    }
  )
  | (
    | InputTcpjsonSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputTcpjsonSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputTcpjsonPqEnabledFalseWithPqConstraint$Outbound
    | InputTcpjsonPqEnabledTrueWithPqConstraint$Outbound & { type: "tcpjson" }
  )
  | (
    | InputSystemMetricsSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputSystemMetricsSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputSystemMetricsPqEnabledFalseWithPqConstraint$Outbound
    | InputSystemMetricsPqEnabledTrueWithPqConstraint$Outbound & {
      type: "system_metrics";
    }
  )
  | (
    | InputSystemStateSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputSystemStateSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputSystemStatePqEnabledFalseWithPqConstraint$Outbound
    | InputSystemStatePqEnabledTrueWithPqConstraint$Outbound & {
      type: "system_state";
    }
  )
  | (
    | InputKubeMetricsSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputKubeMetricsSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputKubeMetricsPqEnabledFalseWithPqConstraint$Outbound
    | InputKubeMetricsPqEnabledTrueWithPqConstraint$Outbound & {
      type: "kube_metrics";
    }
  )
  | (
    | InputKubeLogsSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputKubeLogsSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputKubeLogsPqEnabledFalseWithPqConstraint$Outbound
    | InputKubeLogsPqEnabledTrueWithPqConstraint$Outbound & {
      type: "kube_logs";
    }
  )
  | (
    | InputKubeEventsSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputKubeEventsSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputKubeEventsPqEnabledFalseWithPqConstraint$Outbound
    | InputKubeEventsPqEnabledTrueWithPqConstraint$Outbound & {
      type: "kube_events";
    }
  )
  | (
    | InputWindowsMetricsSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputWindowsMetricsSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputWindowsMetricsPqEnabledFalseWithPqConstraint$Outbound
    | InputWindowsMetricsPqEnabledTrueWithPqConstraint$Outbound & {
      type: "windows_metrics";
    }
  )
  | (
    | InputCrowdstrikeSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputCrowdstrikeSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputCrowdstrikePqEnabledFalseWithPqConstraint$Outbound
    | InputCrowdstrikePqEnabledTrueWithPqConstraint$Outbound & {
      type: "crowdstrike";
    }
  )
  | (
    | InputDatadogAgentSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputDatadogAgentSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputDatadogAgentPqEnabledFalseWithPqConstraint$Outbound
    | InputDatadogAgentPqEnabledTrueWithPqConstraint$Outbound & {
      type: "datadog_agent";
    }
  )
  | (
    | InputDatagenSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputDatagenSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputDatagenPqEnabledFalseWithPqConstraint$Outbound
    | InputDatagenPqEnabledTrueWithPqConstraint$Outbound & { type: "datagen" }
  )
  | (
    | InputHttpRawSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputHttpRawSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputHttpRawPqEnabledFalseWithPqConstraint$Outbound
    | InputHttpRawPqEnabledTrueWithPqConstraint$Outbound & { type: "http_raw" }
  )
  | (
    | InputKinesisSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputKinesisSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputKinesisPqEnabledFalseWithPqConstraint$Outbound
    | InputKinesisPqEnabledTrueWithPqConstraint$Outbound & { type: "kinesis" }
  )
  | (
    | InputCriblmetricsSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputCriblmetricsSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputCriblmetricsPqEnabledFalseWithPqConstraint$Outbound
    | InputCriblmetricsPqEnabledTrueWithPqConstraint$Outbound & {
      type: "criblmetrics";
    }
  )
  | (
    | InputMetricsSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputMetricsSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputMetricsPqEnabledFalseWithPqConstraint$Outbound
    | InputMetricsPqEnabledTrueWithPqConstraint$Outbound & { type: "metrics" }
  )
  | (
    | InputS3SendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputS3SendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputS3PqEnabledFalseWithPqConstraint$Outbound
    | InputS3PqEnabledTrueWithPqConstraint$Outbound & { type: "s3" }
  )
  | (
    | InputS3InventorySendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputS3InventorySendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputS3InventoryPqEnabledFalseWithPqConstraint$Outbound
    | InputS3InventoryPqEnabledTrueWithPqConstraint$Outbound & {
      type: "s3_inventory";
    }
  )
  | (
    | InputSnmpSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputSnmpSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputSnmpPqEnabledFalseWithPqConstraint$Outbound
    | InputSnmpPqEnabledTrueWithPqConstraint$Outbound & { type: "snmp" }
  )
  | (
    | InputOpenTelemetrySendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputOpenTelemetrySendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputOpenTelemetryPqEnabledFalseWithPqConstraint$Outbound
    | InputOpenTelemetryPqEnabledTrueWithPqConstraint$Outbound & {
      type: "open_telemetry";
    }
  )
  | (
    | InputModelDrivenTelemetrySendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputModelDrivenTelemetrySendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputModelDrivenTelemetryPqEnabledFalseWithPqConstraint$Outbound
    | InputModelDrivenTelemetryPqEnabledTrueWithPqConstraint$Outbound & {
      type: "model_driven_telemetry";
    }
  )
  | (
    | InputSqsSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputSqsSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputSqsPqEnabledFalseWithPqConstraint$Outbound
    | InputSqsPqEnabledTrueWithPqConstraint$Outbound & { type: "sqs" }
  )
  | (
    | InputSyslogSyslog1$Outbound
    | InputSyslogSyslog2$Outbound & { type: "syslog" }
  )
  | (
    | InputFileSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputFileSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputFilePqEnabledFalseWithPqConstraint$Outbound
    | InputFilePqEnabledTrueWithPqConstraint$Outbound & { type: "file" }
  )
  | (
    | InputTcpSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputTcpSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputTcpPqEnabledFalseWithPqConstraint$Outbound
    | InputTcpPqEnabledTrueWithPqConstraint$Outbound & { type: "tcp" }
  )
  | (
    | InputAppscopeSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputAppscopeSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputAppscopePqEnabledFalseWithPqConstraint$Outbound
    | InputAppscopePqEnabledTrueWithPqConstraint$Outbound & { type: "appscope" }
  )
  | (
    | InputWefSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputWefSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputWefPqEnabledFalseWithPqConstraint$Outbound
    | InputWefPqEnabledTrueWithPqConstraint$Outbound & { type: "wef" }
  )
  | (
    | InputWinEventLogsSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputWinEventLogsSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputWinEventLogsPqEnabledFalseWithPqConstraint$Outbound
    | InputWinEventLogsPqEnabledTrueWithPqConstraint$Outbound & {
      type: "win_event_logs";
    }
  )
  | (
    | InputRawUdpSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputRawUdpSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputRawUdpPqEnabledFalseWithPqConstraint$Outbound
    | InputRawUdpPqEnabledTrueWithPqConstraint$Outbound & { type: "raw_udp" }
  )
  | (
    | InputJournalFilesSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputJournalFilesSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputJournalFilesPqEnabledFalseWithPqConstraint$Outbound
    | InputJournalFilesPqEnabledTrueWithPqConstraint$Outbound & {
      type: "journal_files";
    }
  )
  | (
    | InputWizSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputWizSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputWizPqEnabledFalseWithPqConstraint$Outbound
    | InputWizPqEnabledTrueWithPqConstraint$Outbound & { type: "wiz" }
  )
  | (
    | InputWizWebhookSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputWizWebhookSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputWizWebhookPqEnabledFalseWithPqConstraint$Outbound
    | InputWizWebhookPqEnabledTrueWithPqConstraint$Outbound & {
      type: "wiz_webhook";
    }
  )
  | (
    | InputNetflowSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputNetflowSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputNetflowPqEnabledFalseWithPqConstraint$Outbound
    | InputNetflowPqEnabledTrueWithPqConstraint$Outbound & { type: "netflow" }
  )
  | (
    | InputSecurityLakeSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputSecurityLakeSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputSecurityLakePqEnabledFalseWithPqConstraint$Outbound
    | InputSecurityLakePqEnabledTrueWithPqConstraint$Outbound & {
      type: "security_lake";
    }
  )
  | (
    | InputZscalerHecSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputZscalerHecSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputZscalerHecPqEnabledFalseWithPqConstraint$Outbound
    | InputZscalerHecPqEnabledTrueWithPqConstraint$Outbound & {
      type: "zscaler_hec";
    }
  )
  | (
    | InputCloudflareHecSendToRoutesTrueWithConnectionsConstraint$Outbound
    | InputCloudflareHecSendToRoutesFalseWithConnectionsConstraint$Outbound
    | InputCloudflareHecPqEnabledFalseWithPqConstraint$Outbound
    | InputCloudflareHecPqEnabledTrueWithPqConstraint$Outbound & {
      type: "cloudflare_hec";
    }
  );

/** @internal */
export const CreateInputRequest$outboundSchema: z.ZodType<
  CreateInputRequest$Outbound,
  z.ZodTypeDef,
  CreateInputRequest
> = z.union([
  z.union([
    z.lazy(() =>
      InputCollectionSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputCollectionSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() => InputCollectionPqEnabledFalseWithPqConstraint$outboundSchema),
    z.lazy(() => InputCollectionPqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("collection") })),
  z.union([
    z.lazy(() =>
      InputKafkaSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputKafkaSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() => InputKafkaPqEnabledFalseWithPqConstraint$outboundSchema),
    z.lazy(() => InputKafkaPqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("kafka") })),
  z.union([
    z.lazy(() =>
      InputMskSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputMskSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() => InputMskPqEnabledFalseWithPqConstraint$outboundSchema),
    z.lazy(() => InputMskPqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("msk") })),
  z.union([
    z.lazy(() =>
      InputHttpSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputHttpSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() => InputHttpPqEnabledFalseWithPqConstraint$outboundSchema),
    z.lazy(() => InputHttpPqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("http") })),
  z.union([
    z.lazy(() =>
      InputSplunkSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputSplunkSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() => InputSplunkPqEnabledFalseWithPqConstraint$outboundSchema),
    z.lazy(() => InputSplunkPqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("splunk") })),
  z.union([
    z.lazy(() =>
      InputSplunkSearchSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputSplunkSearchSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputSplunkSearchPqEnabledFalseWithPqConstraint$outboundSchema
    ),
    z.lazy(() => InputSplunkSearchPqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("splunk_search") })),
  z.union([
    z.lazy(() =>
      InputSplunkHecSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputSplunkHecSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() => InputSplunkHecPqEnabledFalseWithPqConstraint$outboundSchema),
    z.lazy(() => InputSplunkHecPqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("splunk_hec") })),
  z.union([
    z.lazy(() =>
      InputAzureBlobSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputAzureBlobSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() => InputAzureBlobPqEnabledFalseWithPqConstraint$outboundSchema),
    z.lazy(() => InputAzureBlobPqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("azure_blob") })),
  z.union([
    z.lazy(() =>
      InputElasticSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputElasticSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() => InputElasticPqEnabledFalseWithPqConstraint$outboundSchema),
    z.lazy(() => InputElasticPqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("elastic") })),
  z.union([
    z.lazy(() =>
      InputConfluentCloudSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputConfluentCloudSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputConfluentCloudPqEnabledFalseWithPqConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputConfluentCloudPqEnabledTrueWithPqConstraint$outboundSchema
    ),
  ]).and(z.object({ type: z.literal("confluent_cloud") })),
  z.union([
    z.lazy(() => InputGrafanaGrafana1$outboundSchema),
    z.lazy(() => InputGrafanaGrafana2$outboundSchema),
  ]).and(z.object({ type: z.literal("grafana") })),
  z.union([
    z.lazy(() =>
      InputLokiSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputLokiSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() => InputLokiPqEnabledFalseWithPqConstraint$outboundSchema),
    z.lazy(() => InputLokiPqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("loki") })),
  z.union([
    z.lazy(() =>
      InputPrometheusRwSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputPrometheusRwSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputPrometheusRwPqEnabledFalseWithPqConstraint$outboundSchema
    ),
    z.lazy(() => InputPrometheusRwPqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("prometheus_rw") })),
  z.union([
    z.lazy(() =>
      InputPrometheusSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputPrometheusSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() => InputPrometheusPqEnabledFalseWithPqConstraint$outboundSchema),
    z.lazy(() => InputPrometheusPqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("prometheus") })),
  z.union([
    z.lazy(() =>
      InputEdgePrometheusSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputEdgePrometheusSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputEdgePrometheusPqEnabledFalseWithPqConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputEdgePrometheusPqEnabledTrueWithPqConstraint$outboundSchema
    ),
  ]).and(z.object({ type: z.literal("edge_prometheus") })),
  z.union([
    z.lazy(() =>
      InputOffice365MgmtSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputOffice365MgmtSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputOffice365MgmtPqEnabledFalseWithPqConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputOffice365MgmtPqEnabledTrueWithPqConstraint$outboundSchema
    ),
  ]).and(z.object({ type: z.literal("office365_mgmt") })),
  z.union([
    z.lazy(() =>
      InputOffice365ServiceSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputOffice365ServiceSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputOffice365ServicePqEnabledFalseWithPqConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputOffice365ServicePqEnabledTrueWithPqConstraint$outboundSchema
    ),
  ]).and(z.object({ type: z.literal("office365_service") })),
  z.union([
    z.lazy(() =>
      InputOffice365MsgTraceSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputOffice365MsgTraceSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputOffice365MsgTracePqEnabledFalseWithPqConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputOffice365MsgTracePqEnabledTrueWithPqConstraint$outboundSchema
    ),
  ]).and(z.object({ type: z.literal("office365_msg_trace") })),
  z.union([
    z.lazy(() =>
      InputEventhubSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputEventhubSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() => InputEventhubPqEnabledFalseWithPqConstraint$outboundSchema),
    z.lazy(() => InputEventhubPqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("eventhub") })),
  z.union([
    z.lazy(() =>
      InputExecSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputExecSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() => InputExecPqEnabledFalseWithPqConstraint$outboundSchema),
    z.lazy(() => InputExecPqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("exec") })),
  z.union([
    z.lazy(() =>
      InputFirehoseSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputFirehoseSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() => InputFirehosePqEnabledFalseWithPqConstraint$outboundSchema),
    z.lazy(() => InputFirehosePqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("firehose") })),
  z.union([
    z.lazy(() =>
      InputGooglePubsubSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputGooglePubsubSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputGooglePubsubPqEnabledFalseWithPqConstraint$outboundSchema
    ),
    z.lazy(() => InputGooglePubsubPqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("google_pubsub") })),
  z.union([
    z.lazy(() =>
      InputCriblSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputCriblSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() => InputCriblPqEnabledFalseWithPqConstraint$outboundSchema),
    z.lazy(() => InputCriblPqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("cribl") })),
  z.union([
    z.lazy(() =>
      InputCriblTcpSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputCriblTcpSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() => InputCriblTcpPqEnabledFalseWithPqConstraint$outboundSchema),
    z.lazy(() => InputCriblTcpPqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("cribl_tcp") })),
  z.union([
    z.lazy(() =>
      InputCriblHttpSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputCriblHttpSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() => InputCriblHttpPqEnabledFalseWithPqConstraint$outboundSchema),
    z.lazy(() => InputCriblHttpPqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("cribl_http") })),
  z.union([
    z.lazy(() =>
      InputCriblLakeHttpSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputCriblLakeHttpSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputCriblLakeHttpPqEnabledFalseWithPqConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputCriblLakeHttpPqEnabledTrueWithPqConstraint$outboundSchema
    ),
  ]).and(z.object({ type: z.literal("cribl_lake_http") })),
  z.union([
    z.lazy(() =>
      InputTcpjsonSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputTcpjsonSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() => InputTcpjsonPqEnabledFalseWithPqConstraint$outboundSchema),
    z.lazy(() => InputTcpjsonPqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("tcpjson") })),
  z.union([
    z.lazy(() =>
      InputSystemMetricsSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputSystemMetricsSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputSystemMetricsPqEnabledFalseWithPqConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputSystemMetricsPqEnabledTrueWithPqConstraint$outboundSchema
    ),
  ]).and(z.object({ type: z.literal("system_metrics") })),
  z.union([
    z.lazy(() =>
      InputSystemStateSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputSystemStateSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() => InputSystemStatePqEnabledFalseWithPqConstraint$outboundSchema),
    z.lazy(() => InputSystemStatePqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("system_state") })),
  z.union([
    z.lazy(() =>
      InputKubeMetricsSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputKubeMetricsSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() => InputKubeMetricsPqEnabledFalseWithPqConstraint$outboundSchema),
    z.lazy(() => InputKubeMetricsPqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("kube_metrics") })),
  z.union([
    z.lazy(() =>
      InputKubeLogsSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputKubeLogsSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() => InputKubeLogsPqEnabledFalseWithPqConstraint$outboundSchema),
    z.lazy(() => InputKubeLogsPqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("kube_logs") })),
  z.union([
    z.lazy(() =>
      InputKubeEventsSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputKubeEventsSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() => InputKubeEventsPqEnabledFalseWithPqConstraint$outboundSchema),
    z.lazy(() => InputKubeEventsPqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("kube_events") })),
  z.union([
    z.lazy(() =>
      InputWindowsMetricsSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputWindowsMetricsSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputWindowsMetricsPqEnabledFalseWithPqConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputWindowsMetricsPqEnabledTrueWithPqConstraint$outboundSchema
    ),
  ]).and(z.object({ type: z.literal("windows_metrics") })),
  z.union([
    z.lazy(() =>
      InputCrowdstrikeSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputCrowdstrikeSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() => InputCrowdstrikePqEnabledFalseWithPqConstraint$outboundSchema),
    z.lazy(() => InputCrowdstrikePqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("crowdstrike") })),
  z.union([
    z.lazy(() =>
      InputDatadogAgentSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputDatadogAgentSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputDatadogAgentPqEnabledFalseWithPqConstraint$outboundSchema
    ),
    z.lazy(() => InputDatadogAgentPqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("datadog_agent") })),
  z.union([
    z.lazy(() =>
      InputDatagenSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputDatagenSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() => InputDatagenPqEnabledFalseWithPqConstraint$outboundSchema),
    z.lazy(() => InputDatagenPqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("datagen") })),
  z.union([
    z.lazy(() =>
      InputHttpRawSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputHttpRawSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() => InputHttpRawPqEnabledFalseWithPqConstraint$outboundSchema),
    z.lazy(() => InputHttpRawPqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("http_raw") })),
  z.union([
    z.lazy(() =>
      InputKinesisSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputKinesisSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() => InputKinesisPqEnabledFalseWithPqConstraint$outboundSchema),
    z.lazy(() => InputKinesisPqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("kinesis") })),
  z.union([
    z.lazy(() =>
      InputCriblmetricsSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputCriblmetricsSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputCriblmetricsPqEnabledFalseWithPqConstraint$outboundSchema
    ),
    z.lazy(() => InputCriblmetricsPqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("criblmetrics") })),
  z.union([
    z.lazy(() =>
      InputMetricsSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputMetricsSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() => InputMetricsPqEnabledFalseWithPqConstraint$outboundSchema),
    z.lazy(() => InputMetricsPqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("metrics") })),
  z.union([
    z.lazy(() =>
      InputS3SendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputS3SendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() => InputS3PqEnabledFalseWithPqConstraint$outboundSchema),
    z.lazy(() => InputS3PqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("s3") })),
  z.union([
    z.lazy(() =>
      InputS3InventorySendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputS3InventorySendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() => InputS3InventoryPqEnabledFalseWithPqConstraint$outboundSchema),
    z.lazy(() => InputS3InventoryPqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("s3_inventory") })),
  z.union([
    z.lazy(() =>
      InputSnmpSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputSnmpSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() => InputSnmpPqEnabledFalseWithPqConstraint$outboundSchema),
    z.lazy(() => InputSnmpPqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("snmp") })),
  z.union([
    z.lazy(() =>
      InputOpenTelemetrySendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputOpenTelemetrySendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputOpenTelemetryPqEnabledFalseWithPqConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputOpenTelemetryPqEnabledTrueWithPqConstraint$outboundSchema
    ),
  ]).and(z.object({ type: z.literal("open_telemetry") })),
  z.union([
    z.lazy(() =>
      InputModelDrivenTelemetrySendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputModelDrivenTelemetrySendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputModelDrivenTelemetryPqEnabledFalseWithPqConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputModelDrivenTelemetryPqEnabledTrueWithPqConstraint$outboundSchema
    ),
  ]).and(z.object({ type: z.literal("model_driven_telemetry") })),
  z.union([
    z.lazy(() =>
      InputSqsSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputSqsSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() => InputSqsPqEnabledFalseWithPqConstraint$outboundSchema),
    z.lazy(() => InputSqsPqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("sqs") })),
  z.union([
    z.lazy(() => InputSyslogSyslog1$outboundSchema),
    z.lazy(() => InputSyslogSyslog2$outboundSchema),
  ]).and(z.object({ type: z.literal("syslog") })),
  z.union([
    z.lazy(() =>
      InputFileSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputFileSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() => InputFilePqEnabledFalseWithPqConstraint$outboundSchema),
    z.lazy(() => InputFilePqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("file") })),
  z.union([
    z.lazy(() =>
      InputTcpSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputTcpSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() => InputTcpPqEnabledFalseWithPqConstraint$outboundSchema),
    z.lazy(() => InputTcpPqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("tcp") })),
  z.union([
    z.lazy(() =>
      InputAppscopeSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputAppscopeSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() => InputAppscopePqEnabledFalseWithPqConstraint$outboundSchema),
    z.lazy(() => InputAppscopePqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("appscope") })),
  z.union([
    z.lazy(() =>
      InputWefSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputWefSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() => InputWefPqEnabledFalseWithPqConstraint$outboundSchema),
    z.lazy(() => InputWefPqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("wef") })),
  z.union([
    z.lazy(() =>
      InputWinEventLogsSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputWinEventLogsSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputWinEventLogsPqEnabledFalseWithPqConstraint$outboundSchema
    ),
    z.lazy(() => InputWinEventLogsPqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("win_event_logs") })),
  z.union([
    z.lazy(() =>
      InputRawUdpSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputRawUdpSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() => InputRawUdpPqEnabledFalseWithPqConstraint$outboundSchema),
    z.lazy(() => InputRawUdpPqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("raw_udp") })),
  z.union([
    z.lazy(() =>
      InputJournalFilesSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputJournalFilesSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputJournalFilesPqEnabledFalseWithPqConstraint$outboundSchema
    ),
    z.lazy(() => InputJournalFilesPqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("journal_files") })),
  z.union([
    z.lazy(() =>
      InputWizSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputWizSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() => InputWizPqEnabledFalseWithPqConstraint$outboundSchema),
    z.lazy(() => InputWizPqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("wiz") })),
  z.union([
    z.lazy(() =>
      InputWizWebhookSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputWizWebhookSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() => InputWizWebhookPqEnabledFalseWithPqConstraint$outboundSchema),
    z.lazy(() => InputWizWebhookPqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("wiz_webhook") })),
  z.union([
    z.lazy(() =>
      InputNetflowSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputNetflowSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() => InputNetflowPqEnabledFalseWithPqConstraint$outboundSchema),
    z.lazy(() => InputNetflowPqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("netflow") })),
  z.union([
    z.lazy(() =>
      InputSecurityLakeSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputSecurityLakeSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputSecurityLakePqEnabledFalseWithPqConstraint$outboundSchema
    ),
    z.lazy(() => InputSecurityLakePqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("security_lake") })),
  z.union([
    z.lazy(() =>
      InputZscalerHecSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputZscalerHecSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() => InputZscalerHecPqEnabledFalseWithPqConstraint$outboundSchema),
    z.lazy(() => InputZscalerHecPqEnabledTrueWithPqConstraint$outboundSchema),
  ]).and(z.object({ type: z.literal("zscaler_hec") })),
  z.union([
    z.lazy(() =>
      InputCloudflareHecSendToRoutesTrueWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputCloudflareHecSendToRoutesFalseWithConnectionsConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputCloudflareHecPqEnabledFalseWithPqConstraint$outboundSchema
    ),
    z.lazy(() =>
      InputCloudflareHecPqEnabledTrueWithPqConstraint$outboundSchema
    ),
  ]).and(z.object({ type: z.literal("cloudflare_hec") })),
]);

export function createInputRequestToJSON(
  createInputRequest: CreateInputRequest,
): string {
  return JSON.stringify(
    CreateInputRequest$outboundSchema.parse(createInputRequest),
  );
}
