/*
 * Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT.
 */

import * as z from "zod/v3";
import { safeParse } from "../lib/schemas.js";
import * as openEnums from "../types/enums.js";
import { ClosedEnum, OpenEnum } from "../types/enums.js";
import { Result as SafeParseResult } from "../types/fp.js";
import { SDKValidationError } from "./errors/sdkvalidationerror.js";

export const OutputDatabricksType = {
  Databricks: "databricks",
} as const;
export type OutputDatabricksType = ClosedEnum<typeof OutputDatabricksType>;

/**
 * Format of the output data
 */
export const OutputDatabricksDataFormat = {
  /**
   * JSON
   */
  Json: "json",
  /**
   * Raw
   */
  Raw: "raw",
  /**
   * Parquet
   */
  Parquet: "parquet",
} as const;
/**
 * Format of the output data
 */
export type OutputDatabricksDataFormat = OpenEnum<
  typeof OutputDatabricksDataFormat
>;

/**
 * How to handle events when all receivers are exerting backpressure
 */
export const OutputDatabricksBackpressureBehavior = {
  /**
   * Block
   */
  Block: "block",
  /**
   * Drop
   */
  Drop: "drop",
} as const;
/**
 * How to handle events when all receivers are exerting backpressure
 */
export type OutputDatabricksBackpressureBehavior = OpenEnum<
  typeof OutputDatabricksBackpressureBehavior
>;

/**
 * How to handle events when disk space is below the global 'Min free disk space' limit
 */
export const OutputDatabricksDiskSpaceProtection = {
  /**
   * Block
   */
  Block: "block",
  /**
   * Drop
   */
  Drop: "drop",
} as const;
/**
 * How to handle events when disk space is below the global 'Min free disk space' limit
 */
export type OutputDatabricksDiskSpaceProtection = OpenEnum<
  typeof OutputDatabricksDiskSpaceProtection
>;

/**
 * Data compression format to apply to HTTP content before it is delivered
 */
export const OutputDatabricksCompression = {
  None: "none",
  Gzip: "gzip",
} as const;
/**
 * Data compression format to apply to HTTP content before it is delivered
 */
export type OutputDatabricksCompression = OpenEnum<
  typeof OutputDatabricksCompression
>;

/**
 * Compression level to apply before moving files to final destination
 */
export const OutputDatabricksCompressionLevel = {
  /**
   * Best Speed
   */
  BestSpeed: "best_speed",
  /**
   * Normal
   */
  Normal: "normal",
  /**
   * Best Compression
   */
  BestCompression: "best_compression",
} as const;
/**
 * Compression level to apply before moving files to final destination
 */
export type OutputDatabricksCompressionLevel = OpenEnum<
  typeof OutputDatabricksCompressionLevel
>;

/**
 * Determines which data types are supported and how they are represented
 */
export const OutputDatabricksParquetVersion = {
  /**
   * 1.0
   */
  Parquet10: "PARQUET_1_0",
  /**
   * 2.4
   */
  Parquet24: "PARQUET_2_4",
  /**
   * 2.6
   */
  Parquet26: "PARQUET_2_6",
} as const;
/**
 * Determines which data types are supported and how they are represented
 */
export type OutputDatabricksParquetVersion = OpenEnum<
  typeof OutputDatabricksParquetVersion
>;

/**
 * Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
 */
export const OutputDatabricksDataPageVersion = {
  /**
   * V1
   */
  DataPageV1: "DATA_PAGE_V1",
  /**
   * V2
   */
  DataPageV2: "DATA_PAGE_V2",
} as const;
/**
 * Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
 */
export type OutputDatabricksDataPageVersion = OpenEnum<
  typeof OutputDatabricksDataPageVersion
>;

export type OutputDatabricksKeyValueMetadatum = {
  key?: string | undefined;
  value: string;
};

export type OutputDatabricks = {
  /**
   * Unique ID for this output
   */
  id?: string | undefined;
  type: OutputDatabricksType;
  /**
   * Pipeline to process data before sending out to this output
   */
  pipeline?: string | undefined;
  /**
   * Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
   */
  systemFields?: Array<string> | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Optional path to prepend to files before uploading.
   */
  destPath?: string | undefined;
  /**
   * Filesystem location in which to buffer files before compressing and moving to final destination. Use performant, stable storage.
   */
  stagePath?: string | undefined;
  /**
   * Add the Output ID value to staging location
   */
  addIdToStagePath?: boolean | undefined;
  /**
   * Remove empty staging directories after moving files
   */
  removeEmptyDirs?: boolean | undefined;
  /**
   * JavaScript expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.
   */
  partitionExpr?: string | undefined;
  /**
   * Format of the output data
   */
  format?: OutputDatabricksDataFormat | undefined;
  /**
   * JavaScript expression to define the output filename prefix (can be constant)
   */
  baseFileName?: string | undefined;
  /**
   * JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
   */
  fileNameSuffix?: string | undefined;
  /**
   * Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
   */
  maxFileSizeMB?: number | undefined;
  /**
   * Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
   */
  maxFileOpenTimeSec?: number | undefined;
  /**
   * Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
   */
  maxFileIdleTimeSec?: number | undefined;
  /**
   * Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
   */
  maxOpenFiles?: number | undefined;
  /**
   * If set, this line will be written to the beginning of each output file
   */
  headerLine?: string | undefined;
  /**
   * Buffer size used to write to a file
   */
  writeHighWaterMark?: number | undefined;
  /**
   * How to handle events when all receivers are exerting backpressure
   */
  onBackpressure?: OutputDatabricksBackpressureBehavior | undefined;
  /**
   * If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
   */
  deadletterEnabled?: boolean | undefined;
  /**
   * How to handle events when disk space is below the global 'Min free disk space' limit
   */
  onDiskFullBackpressure?: OutputDatabricksDiskSpaceProtection | undefined;
  /**
   * Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.
   */
  forceCloseOnShutdown?: boolean | undefined;
  /**
   * Databricks workspace ID
   */
  workspaceId: string;
  /**
   * OAuth scope for Unity Catalog authentication
   */
  scope?: string | undefined;
  /**
   * OAuth client ID for Unity Catalog authentication
   */
  clientId: string;
  /**
   * Name of the catalog to use for the output
   */
  catalog?: string | undefined;
  /**
   * Name of the catalog schema to use for the output
   */
  schema?: string | undefined;
  /**
   * Name of the events volume in Databricks
   */
  eventsVolumeName?: string | undefined;
  /**
   * OAuth client secret for Unity Catalog authentication
   */
  clientTextSecret: string;
  /**
   * Amount of time, in seconds, to wait for a request to complete before canceling it
   */
  timeoutSec?: number | undefined;
  description?: string | undefined;
  /**
   * Data compression format to apply to HTTP content before it is delivered
   */
  compress?: OutputDatabricksCompression | undefined;
  /**
   * Compression level to apply before moving files to final destination
   */
  compressionLevel?: OutputDatabricksCompressionLevel | undefined;
  /**
   * Automatically calculate the schema based on the events of each Parquet file generated
   */
  automaticSchema?: boolean | undefined;
  /**
   * To add a new schema, navigate to Processing > Knowledge > Parquet Schemas
   */
  parquetSchema?: string | undefined;
  /**
   * Determines which data types are supported and how they are represented
   */
  parquetVersion?: OutputDatabricksParquetVersion | undefined;
  /**
   * Serialization format of data pages. Note that some reader implementations use Data page V2's attributes to work more efficiently, while others ignore it.
   */
  parquetDataPageVersion?: OutputDatabricksDataPageVersion | undefined;
  /**
   * The number of rows that every group will contain. The final group can contain a smaller number of rows.
   */
  parquetRowGroupLength?: number | undefined;
  /**
   * Target memory size for page segments, such as 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression.
   */
  parquetPageSize?: string | undefined;
  /**
   * Log up to 3 rows that @{product} skips due to data mismatch
   */
  shouldLogInvalidRows?: boolean | undefined;
  /**
   * The metadata of files the Destination writes will include the properties you add here as key-value pairs. Useful for tagging. Examples: "key":"OCSF Event Class", "value":"9001"
   */
  keyValueMetadata?: Array<OutputDatabricksKeyValueMetadatum> | undefined;
  /**
   * Statistics profile an entire file in terms of minimum/maximum values within data, numbers of nulls, etc. You can use Parquet tools to view statistics.
   */
  enableStatistics?: boolean | undefined;
  /**
   * One page index contains statistics for one data page. Parquet readers use statistics to enable page skipping.
   */
  enableWritePageIndex?: boolean | undefined;
  /**
   * Parquet tools can use the checksum of a Parquet page to verify data integrity
   */
  enablePageChecksum?: boolean | undefined;
  /**
   * How frequently, in seconds, to clean up empty directories
   */
  emptyDirCleanupSec?: number | undefined;
  /**
   * Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.
   */
  directoryBatchSize?: number | undefined;
  /**
   * Storage location for files that fail to reach their final destination after maximum retries are exceeded
   */
  deadletterPath?: string | undefined;
  /**
   * The maximum number of times a file will attempt to move to its final destination before being dead-lettered
   */
  maxRetryNum?: number | undefined;
};

/** @internal */
export const OutputDatabricksType$inboundSchema: z.ZodNativeEnum<
  typeof OutputDatabricksType
> = z.nativeEnum(OutputDatabricksType);
/** @internal */
export const OutputDatabricksType$outboundSchema: z.ZodNativeEnum<
  typeof OutputDatabricksType
> = OutputDatabricksType$inboundSchema;

/** @internal */
export const OutputDatabricksDataFormat$inboundSchema: z.ZodType<
  OutputDatabricksDataFormat,
  z.ZodTypeDef,
  unknown
> = openEnums.inboundSchema(OutputDatabricksDataFormat);
/** @internal */
export const OutputDatabricksDataFormat$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  OutputDatabricksDataFormat
> = openEnums.outboundSchema(OutputDatabricksDataFormat);

/** @internal */
export const OutputDatabricksBackpressureBehavior$inboundSchema: z.ZodType<
  OutputDatabricksBackpressureBehavior,
  z.ZodTypeDef,
  unknown
> = openEnums.inboundSchema(OutputDatabricksBackpressureBehavior);
/** @internal */
export const OutputDatabricksBackpressureBehavior$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  OutputDatabricksBackpressureBehavior
> = openEnums.outboundSchema(OutputDatabricksBackpressureBehavior);

/** @internal */
export const OutputDatabricksDiskSpaceProtection$inboundSchema: z.ZodType<
  OutputDatabricksDiskSpaceProtection,
  z.ZodTypeDef,
  unknown
> = openEnums.inboundSchema(OutputDatabricksDiskSpaceProtection);
/** @internal */
export const OutputDatabricksDiskSpaceProtection$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  OutputDatabricksDiskSpaceProtection
> = openEnums.outboundSchema(OutputDatabricksDiskSpaceProtection);

/** @internal */
export const OutputDatabricksCompression$inboundSchema: z.ZodType<
  OutputDatabricksCompression,
  z.ZodTypeDef,
  unknown
> = openEnums.inboundSchema(OutputDatabricksCompression);
/** @internal */
export const OutputDatabricksCompression$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  OutputDatabricksCompression
> = openEnums.outboundSchema(OutputDatabricksCompression);

/** @internal */
export const OutputDatabricksCompressionLevel$inboundSchema: z.ZodType<
  OutputDatabricksCompressionLevel,
  z.ZodTypeDef,
  unknown
> = openEnums.inboundSchema(OutputDatabricksCompressionLevel);
/** @internal */
export const OutputDatabricksCompressionLevel$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  OutputDatabricksCompressionLevel
> = openEnums.outboundSchema(OutputDatabricksCompressionLevel);

/** @internal */
export const OutputDatabricksParquetVersion$inboundSchema: z.ZodType<
  OutputDatabricksParquetVersion,
  z.ZodTypeDef,
  unknown
> = openEnums.inboundSchema(OutputDatabricksParquetVersion);
/** @internal */
export const OutputDatabricksParquetVersion$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  OutputDatabricksParquetVersion
> = openEnums.outboundSchema(OutputDatabricksParquetVersion);

/** @internal */
export const OutputDatabricksDataPageVersion$inboundSchema: z.ZodType<
  OutputDatabricksDataPageVersion,
  z.ZodTypeDef,
  unknown
> = openEnums.inboundSchema(OutputDatabricksDataPageVersion);
/** @internal */
export const OutputDatabricksDataPageVersion$outboundSchema: z.ZodType<
  string,
  z.ZodTypeDef,
  OutputDatabricksDataPageVersion
> = openEnums.outboundSchema(OutputDatabricksDataPageVersion);

/** @internal */
export const OutputDatabricksKeyValueMetadatum$inboundSchema: z.ZodType<
  OutputDatabricksKeyValueMetadatum,
  z.ZodTypeDef,
  unknown
> = z.object({
  key: z.string().default(""),
  value: z.string(),
});
/** @internal */
export type OutputDatabricksKeyValueMetadatum$Outbound = {
  key: string;
  value: string;
};

/** @internal */
export const OutputDatabricksKeyValueMetadatum$outboundSchema: z.ZodType<
  OutputDatabricksKeyValueMetadatum$Outbound,
  z.ZodTypeDef,
  OutputDatabricksKeyValueMetadatum
> = z.object({
  key: z.string().default(""),
  value: z.string(),
});

export function outputDatabricksKeyValueMetadatumToJSON(
  outputDatabricksKeyValueMetadatum: OutputDatabricksKeyValueMetadatum,
): string {
  return JSON.stringify(
    OutputDatabricksKeyValueMetadatum$outboundSchema.parse(
      outputDatabricksKeyValueMetadatum,
    ),
  );
}
export function outputDatabricksKeyValueMetadatumFromJSON(
  jsonString: string,
): SafeParseResult<OutputDatabricksKeyValueMetadatum, SDKValidationError> {
  return safeParse(
    jsonString,
    (x) => OutputDatabricksKeyValueMetadatum$inboundSchema.parse(JSON.parse(x)),
    `Failed to parse 'OutputDatabricksKeyValueMetadatum' from JSON`,
  );
}

/** @internal */
export const OutputDatabricks$inboundSchema: z.ZodType<
  OutputDatabricks,
  z.ZodTypeDef,
  unknown
> = z.object({
  id: z.string().optional(),
  type: OutputDatabricksType$inboundSchema,
  pipeline: z.string().optional(),
  systemFields: z.array(z.string()).optional(),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  destPath: z.string().default(""),
  stagePath: z.string().default("$CRIBL_HOME/state/outputs/staging"),
  addIdToStagePath: z.boolean().default(true),
  removeEmptyDirs: z.boolean().default(true),
  partitionExpr: z.string().default(
    "C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')",
  ),
  format: OutputDatabricksDataFormat$inboundSchema.default("json"),
  baseFileName: z.string().default("`CriblOut`"),
  fileNameSuffix: z.string().default(
    "`.${C.env[\"CRIBL_WORKER_ID\"]}.${__format}${__compression === \"gzip\" ? \".gz\" : \"\"}`",
  ),
  maxFileSizeMB: z.number().default(32),
  maxFileOpenTimeSec: z.number().default(300),
  maxFileIdleTimeSec: z.number().default(30),
  maxOpenFiles: z.number().default(100),
  headerLine: z.string().default(""),
  writeHighWaterMark: z.number().default(64),
  onBackpressure: OutputDatabricksBackpressureBehavior$inboundSchema.default(
    "block",
  ),
  deadletterEnabled: z.boolean().default(false),
  onDiskFullBackpressure: OutputDatabricksDiskSpaceProtection$inboundSchema
    .default("block"),
  forceCloseOnShutdown: z.boolean().default(false),
  workspaceId: z.string(),
  scope: z.string().default("all-apis"),
  clientId: z.string(),
  catalog: z.string().default("main"),
  schema: z.string().default("external"),
  eventsVolumeName: z.string().default("events"),
  clientTextSecret: z.string(),
  timeoutSec: z.number().default(60),
  description: z.string().optional(),
  compress: OutputDatabricksCompression$inboundSchema.default("gzip"),
  compressionLevel: OutputDatabricksCompressionLevel$inboundSchema.default(
    "best_speed",
  ),
  automaticSchema: z.boolean().default(false),
  parquetSchema: z.string().optional(),
  parquetVersion: OutputDatabricksParquetVersion$inboundSchema.default(
    "PARQUET_2_6",
  ),
  parquetDataPageVersion: OutputDatabricksDataPageVersion$inboundSchema.default(
    "DATA_PAGE_V2",
  ),
  parquetRowGroupLength: z.number().default(10000),
  parquetPageSize: z.string().default("1MB"),
  shouldLogInvalidRows: z.boolean().optional(),
  keyValueMetadata: z.array(
    z.lazy(() => OutputDatabricksKeyValueMetadatum$inboundSchema),
  ).optional(),
  enableStatistics: z.boolean().default(true),
  enableWritePageIndex: z.boolean().default(true),
  enablePageChecksum: z.boolean().default(false),
  emptyDirCleanupSec: z.number().default(300),
  directoryBatchSize: z.number().default(1000),
  deadletterPath: z.string().default("$CRIBL_HOME/state/outputs/dead-letter"),
  maxRetryNum: z.number().default(20),
});
/** @internal */
export type OutputDatabricks$Outbound = {
  id?: string | undefined;
  type: string;
  pipeline?: string | undefined;
  systemFields?: Array<string> | undefined;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  destPath: string;
  stagePath: string;
  addIdToStagePath: boolean;
  removeEmptyDirs: boolean;
  partitionExpr: string;
  format: string;
  baseFileName: string;
  fileNameSuffix: string;
  maxFileSizeMB: number;
  maxFileOpenTimeSec: number;
  maxFileIdleTimeSec: number;
  maxOpenFiles: number;
  headerLine: string;
  writeHighWaterMark: number;
  onBackpressure: string;
  deadletterEnabled: boolean;
  onDiskFullBackpressure: string;
  forceCloseOnShutdown: boolean;
  workspaceId: string;
  scope: string;
  clientId: string;
  catalog: string;
  schema: string;
  eventsVolumeName: string;
  clientTextSecret: string;
  timeoutSec: number;
  description?: string | undefined;
  compress: string;
  compressionLevel: string;
  automaticSchema: boolean;
  parquetSchema?: string | undefined;
  parquetVersion: string;
  parquetDataPageVersion: string;
  parquetRowGroupLength: number;
  parquetPageSize: string;
  shouldLogInvalidRows?: boolean | undefined;
  keyValueMetadata?:
    | Array<OutputDatabricksKeyValueMetadatum$Outbound>
    | undefined;
  enableStatistics: boolean;
  enableWritePageIndex: boolean;
  enablePageChecksum: boolean;
  emptyDirCleanupSec: number;
  directoryBatchSize: number;
  deadletterPath: string;
  maxRetryNum: number;
};

/** @internal */
export const OutputDatabricks$outboundSchema: z.ZodType<
  OutputDatabricks$Outbound,
  z.ZodTypeDef,
  OutputDatabricks
> = z.object({
  id: z.string().optional(),
  type: OutputDatabricksType$outboundSchema,
  pipeline: z.string().optional(),
  systemFields: z.array(z.string()).optional(),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  destPath: z.string().default(""),
  stagePath: z.string().default("$CRIBL_HOME/state/outputs/staging"),
  addIdToStagePath: z.boolean().default(true),
  removeEmptyDirs: z.boolean().default(true),
  partitionExpr: z.string().default(
    "C.Time.strftime(_time ? _time : Date.now()/1000, '%Y/%m/%d')",
  ),
  format: OutputDatabricksDataFormat$outboundSchema.default("json"),
  baseFileName: z.string().default("`CriblOut`"),
  fileNameSuffix: z.string().default(
    "`.${C.env[\"CRIBL_WORKER_ID\"]}.${__format}${__compression === \"gzip\" ? \".gz\" : \"\"}`",
  ),
  maxFileSizeMB: z.number().default(32),
  maxFileOpenTimeSec: z.number().default(300),
  maxFileIdleTimeSec: z.number().default(30),
  maxOpenFiles: z.number().default(100),
  headerLine: z.string().default(""),
  writeHighWaterMark: z.number().default(64),
  onBackpressure: OutputDatabricksBackpressureBehavior$outboundSchema.default(
    "block",
  ),
  deadletterEnabled: z.boolean().default(false),
  onDiskFullBackpressure: OutputDatabricksDiskSpaceProtection$outboundSchema
    .default("block"),
  forceCloseOnShutdown: z.boolean().default(false),
  workspaceId: z.string(),
  scope: z.string().default("all-apis"),
  clientId: z.string(),
  catalog: z.string().default("main"),
  schema: z.string().default("external"),
  eventsVolumeName: z.string().default("events"),
  clientTextSecret: z.string(),
  timeoutSec: z.number().default(60),
  description: z.string().optional(),
  compress: OutputDatabricksCompression$outboundSchema.default("gzip"),
  compressionLevel: OutputDatabricksCompressionLevel$outboundSchema.default(
    "best_speed",
  ),
  automaticSchema: z.boolean().default(false),
  parquetSchema: z.string().optional(),
  parquetVersion: OutputDatabricksParquetVersion$outboundSchema.default(
    "PARQUET_2_6",
  ),
  parquetDataPageVersion: OutputDatabricksDataPageVersion$outboundSchema
    .default("DATA_PAGE_V2"),
  parquetRowGroupLength: z.number().default(10000),
  parquetPageSize: z.string().default("1MB"),
  shouldLogInvalidRows: z.boolean().optional(),
  keyValueMetadata: z.array(
    z.lazy(() => OutputDatabricksKeyValueMetadatum$outboundSchema),
  ).optional(),
  enableStatistics: z.boolean().default(true),
  enableWritePageIndex: z.boolean().default(true),
  enablePageChecksum: z.boolean().default(false),
  emptyDirCleanupSec: z.number().default(300),
  directoryBatchSize: z.number().default(1000),
  deadletterPath: z.string().default("$CRIBL_HOME/state/outputs/dead-letter"),
  maxRetryNum: z.number().default(20),
});

export function outputDatabricksToJSON(
  outputDatabricks: OutputDatabricks,
): string {
  return JSON.stringify(
    OutputDatabricks$outboundSchema.parse(outputDatabricks),
  );
}
export function outputDatabricksFromJSON(
  jsonString: string,
): SafeParseResult<OutputDatabricks, SDKValidationError> {
  return safeParse(
    jsonString,
    (x) => OutputDatabricks$inboundSchema.parse(JSON.parse(x)),
    `Failed to parse 'OutputDatabricks' from JSON`,
  );
}
