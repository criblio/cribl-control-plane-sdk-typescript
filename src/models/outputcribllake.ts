/*
 * Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT.
 */

import * as z from "zod/v3";
import { safeParse } from "../lib/schemas.js";
import { Result as SafeParseResult } from "../types/fp.js";
import {
  BackpressureBehaviorOptions1,
  BackpressureBehaviorOptions1$inboundSchema,
  BackpressureBehaviorOptions1$outboundSchema,
} from "./backpressurebehavioroptions1.js";
import {
  DiskSpaceProtectionOptions,
  DiskSpaceProtectionOptions$inboundSchema,
  DiskSpaceProtectionOptions$outboundSchema,
} from "./diskspaceprotectionoptions.js";
import { SDKValidationError } from "./errors/sdkvalidationerror.js";
import {
  FormatOptionsCriblLakeDataset,
  FormatOptionsCriblLakeDataset$inboundSchema,
  FormatOptionsCriblLakeDataset$outboundSchema,
} from "./formatoptionscribllakedataset.js";
import {
  MethodOptionsCredentials,
  MethodOptionsCredentials$inboundSchema,
  MethodOptionsCredentials$outboundSchema,
} from "./methodoptionscredentials.js";
import {
  ObjectAclOptions,
  ObjectAclOptions$inboundSchema,
  ObjectAclOptions$outboundSchema,
} from "./objectacloptions.js";
import {
  RetrySettingsType,
  RetrySettingsType$inboundSchema,
  RetrySettingsType$Outbound,
  RetrySettingsType$outboundSchema,
} from "./retrysettingstype.js";
import {
  ServerSideEncryptionForUploadedObjectsOptions,
  ServerSideEncryptionForUploadedObjectsOptions$inboundSchema,
  ServerSideEncryptionForUploadedObjectsOptions$outboundSchema,
} from "./serversideencryptionforuploadedobjectsoptions.js";
import {
  SignatureVersionOptionsS3CollectorConf,
  SignatureVersionOptionsS3CollectorConf$inboundSchema,
  SignatureVersionOptionsS3CollectorConf$outboundSchema,
} from "./signatureversionoptionss3collectorconf.js";
import {
  StorageClassOptions,
  StorageClassOptions$inboundSchema,
  StorageClassOptions$outboundSchema,
} from "./storageclassoptions.js";

export type OutputCriblLake = {
  /**
   * Unique ID for this output
   */
  id?: string | undefined;
  type: "cribl_lake";
  /**
   * Pipeline to process data before sending out to this output
   */
  pipeline?: string | undefined;
  /**
   * Fields to automatically add to events, such as cribl_pipe. Supports wildcards.
   */
  systemFields?: Array<string> | undefined;
  /**
   * Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
   */
  environment?: string | undefined;
  /**
   * Tags for filtering and grouping in @{product}
   */
  streamtags?: Array<string> | undefined;
  /**
   * Name of the destination S3 bucket. Must be a JavaScript expression (which can evaluate to a constant value), enclosed in quotes or backticks. Can be evaluated only at initialization time. Example referencing a Global Variable: `myBucket-${C.vars.myVar}`
   */
  bucket?: string | undefined;
  /**
   * Region where the S3 bucket is located
   */
  region?: string | undefined;
  /**
   * Secret key. This value can be a constant or a JavaScript expression. Example: `${C.env.SOME_SECRET}`)
   */
  awsSecretKey?: string | undefined;
  /**
   * S3 service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to S3-compatible endpoint.
   */
  endpoint?: string | undefined;
  /**
   * Signature version to use for signing S3 requests
   */
  signatureVersion?: SignatureVersionOptionsS3CollectorConf | undefined;
  /**
   * Reuse connections between requests, which can improve performance
   */
  reuseConnections?: boolean | undefined;
  /**
   * Reject certificates that cannot be verified against a valid CA, such as self-signed certificates
   */
  rejectUnauthorized?: boolean | undefined;
  /**
   * Use Assume Role credentials to access S3
   */
  enableAssumeRole?: boolean | undefined;
  /**
   * Amazon Resource Name (ARN) of the role to assume
   */
  assumeRoleArn?: string | undefined;
  /**
   * External ID to use when assuming role
   */
  assumeRoleExternalId?: string | undefined;
  /**
   * Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours).
   */
  durationSeconds?: number | undefined;
  /**
   * Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant and stable storage.
   */
  stagePath?: string | undefined;
  /**
   * Add the Output ID value to staging location
   */
  addIdToStagePath?: boolean | undefined;
  /**
   * Lake dataset to send the data to.
   */
  destPath?: string | undefined;
  /**
   * Object ACL to assign to uploaded objects
   */
  objectACL?: ObjectAclOptions | undefined;
  /**
   * Storage class to select for uploaded objects
   */
  storageClass?: StorageClassOptions | undefined;
  serverSideEncryption?:
    | ServerSideEncryptionForUploadedObjectsOptions
    | undefined;
  /**
   * ID or ARN of the KMS customer-managed key to use for encryption
   */
  kmsKeyId?: string | undefined;
  /**
   * Remove empty staging directories after moving files
   */
  removeEmptyDirs?: boolean | undefined;
  /**
   * JavaScript expression to define the output filename prefix (can be constant)
   */
  baseFileName?: string | undefined;
  /**
   * JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`).
   */
  fileNameSuffix?: string | undefined;
  /**
   * Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
   */
  maxFileSizeMB?: number | undefined;
  /**
   * Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
   */
  maxOpenFiles?: number | undefined;
  /**
   * If set, this line will be written to the beginning of each output file
   */
  headerLine?: string | undefined;
  /**
   * Buffer size used to write to a file
   */
  writeHighWaterMark?: number | undefined;
  /**
   * How to handle events when all receivers are exerting backpressure
   */
  onBackpressure?: BackpressureBehaviorOptions1 | undefined;
  /**
   * If a file fails to move to its final destination after the maximum number of retries, move it to a designated directory to prevent further errors
   */
  deadletterEnabled?: boolean | undefined;
  /**
   * How to handle events when disk space is below the global 'Min free disk space' limit
   */
  onDiskFullBackpressure?: DiskSpaceProtectionOptions | undefined;
  /**
   * Force all staged files to close during an orderly Node shutdown. This triggers immediate upload of in-progress data — regardless of idle time, file age, or size thresholds — to minimize data loss.
   */
  forceCloseOnShutdown?: boolean | undefined;
  retrySettings?: RetrySettingsType | undefined;
  /**
   * Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
   */
  maxFileOpenTimeSec?: number | undefined;
  /**
   * Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
   */
  maxFileIdleTimeSec?: number | undefined;
  /**
   * Disable if you can access files within the bucket but not the bucket itself
   */
  verifyPermissions?: boolean | undefined;
  /**
   * Maximum number of files that can be waiting for upload before backpressure is applied
   */
  maxClosingFilesToBackpressure?: number | undefined;
  awsAuthenticationMethod?: MethodOptionsCredentials | undefined;
  format?: FormatOptionsCriblLakeDataset | undefined;
  /**
   * Maximum number of parts to upload in parallel per file. Minimum part size is 5MB.
   */
  maxConcurrentFileParts?: number | undefined;
  description?: string | undefined;
  /**
   * How frequently, in seconds, to clean up empty directories
   */
  emptyDirCleanupSec?: number | undefined;
  /**
   * Number of directories to process in each batch during cleanup of empty directories. Minimum is 10, maximum is 10000. Higher values may require more memory.
   */
  directoryBatchSize?: number | undefined;
  /**
   * Storage location for files that fail to reach their final destination after maximum retries are exceeded
   */
  deadletterPath?: string | undefined;
  /**
   * The maximum number of times a file will attempt to move to its final destination before being dead-lettered
   */
  maxRetryNum?: number | undefined;
};

/** @internal */
export const OutputCriblLake$inboundSchema: z.ZodType<
  OutputCriblLake,
  z.ZodTypeDef,
  unknown
> = z.object({
  id: z.string().optional(),
  type: z.literal("cribl_lake"),
  pipeline: z.string().optional(),
  systemFields: z.array(z.string()).optional(),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  bucket: z.string().optional(),
  region: z.string().optional(),
  awsSecretKey: z.string().optional(),
  endpoint: z.string().optional(),
  signatureVersion: SignatureVersionOptionsS3CollectorConf$inboundSchema
    .optional(),
  reuseConnections: z.boolean().optional(),
  rejectUnauthorized: z.boolean().optional(),
  enableAssumeRole: z.boolean().optional(),
  assumeRoleArn: z.string().optional(),
  assumeRoleExternalId: z.string().optional(),
  durationSeconds: z.number().optional(),
  stagePath: z.string().optional(),
  addIdToStagePath: z.boolean().optional(),
  destPath: z.string().optional(),
  objectACL: ObjectAclOptions$inboundSchema.optional(),
  storageClass: StorageClassOptions$inboundSchema.optional(),
  serverSideEncryption:
    ServerSideEncryptionForUploadedObjectsOptions$inboundSchema.optional(),
  kmsKeyId: z.string().optional(),
  removeEmptyDirs: z.boolean().optional(),
  baseFileName: z.string().optional(),
  fileNameSuffix: z.string().optional(),
  maxFileSizeMB: z.number().optional(),
  maxOpenFiles: z.number().optional(),
  headerLine: z.string().optional(),
  writeHighWaterMark: z.number().optional(),
  onBackpressure: BackpressureBehaviorOptions1$inboundSchema.optional(),
  deadletterEnabled: z.boolean().optional(),
  onDiskFullBackpressure: DiskSpaceProtectionOptions$inboundSchema.optional(),
  forceCloseOnShutdown: z.boolean().optional(),
  retrySettings: RetrySettingsType$inboundSchema.optional(),
  maxFileOpenTimeSec: z.number().optional(),
  maxFileIdleTimeSec: z.number().optional(),
  verifyPermissions: z.boolean().optional(),
  maxClosingFilesToBackpressure: z.number().optional(),
  awsAuthenticationMethod: MethodOptionsCredentials$inboundSchema.optional(),
  format: FormatOptionsCriblLakeDataset$inboundSchema.optional(),
  maxConcurrentFileParts: z.number().optional(),
  description: z.string().optional(),
  emptyDirCleanupSec: z.number().optional(),
  directoryBatchSize: z.number().optional(),
  deadletterPath: z.string().optional(),
  maxRetryNum: z.number().optional(),
});
/** @internal */
export type OutputCriblLake$Outbound = {
  id?: string | undefined;
  type: "cribl_lake";
  pipeline?: string | undefined;
  systemFields?: Array<string> | undefined;
  environment?: string | undefined;
  streamtags?: Array<string> | undefined;
  bucket?: string | undefined;
  region?: string | undefined;
  awsSecretKey?: string | undefined;
  endpoint?: string | undefined;
  signatureVersion?: string | undefined;
  reuseConnections?: boolean | undefined;
  rejectUnauthorized?: boolean | undefined;
  enableAssumeRole?: boolean | undefined;
  assumeRoleArn?: string | undefined;
  assumeRoleExternalId?: string | undefined;
  durationSeconds?: number | undefined;
  stagePath?: string | undefined;
  addIdToStagePath?: boolean | undefined;
  destPath?: string | undefined;
  objectACL?: string | undefined;
  storageClass?: string | undefined;
  serverSideEncryption?: string | undefined;
  kmsKeyId?: string | undefined;
  removeEmptyDirs?: boolean | undefined;
  baseFileName?: string | undefined;
  fileNameSuffix?: string | undefined;
  maxFileSizeMB?: number | undefined;
  maxOpenFiles?: number | undefined;
  headerLine?: string | undefined;
  writeHighWaterMark?: number | undefined;
  onBackpressure?: string | undefined;
  deadletterEnabled?: boolean | undefined;
  onDiskFullBackpressure?: string | undefined;
  forceCloseOnShutdown?: boolean | undefined;
  retrySettings?: RetrySettingsType$Outbound | undefined;
  maxFileOpenTimeSec?: number | undefined;
  maxFileIdleTimeSec?: number | undefined;
  verifyPermissions?: boolean | undefined;
  maxClosingFilesToBackpressure?: number | undefined;
  awsAuthenticationMethod?: string | undefined;
  format?: string | undefined;
  maxConcurrentFileParts?: number | undefined;
  description?: string | undefined;
  emptyDirCleanupSec?: number | undefined;
  directoryBatchSize?: number | undefined;
  deadletterPath?: string | undefined;
  maxRetryNum?: number | undefined;
};

/** @internal */
export const OutputCriblLake$outboundSchema: z.ZodType<
  OutputCriblLake$Outbound,
  z.ZodTypeDef,
  OutputCriblLake
> = z.object({
  id: z.string().optional(),
  type: z.literal("cribl_lake"),
  pipeline: z.string().optional(),
  systemFields: z.array(z.string()).optional(),
  environment: z.string().optional(),
  streamtags: z.array(z.string()).optional(),
  bucket: z.string().optional(),
  region: z.string().optional(),
  awsSecretKey: z.string().optional(),
  endpoint: z.string().optional(),
  signatureVersion: SignatureVersionOptionsS3CollectorConf$outboundSchema
    .optional(),
  reuseConnections: z.boolean().optional(),
  rejectUnauthorized: z.boolean().optional(),
  enableAssumeRole: z.boolean().optional(),
  assumeRoleArn: z.string().optional(),
  assumeRoleExternalId: z.string().optional(),
  durationSeconds: z.number().optional(),
  stagePath: z.string().optional(),
  addIdToStagePath: z.boolean().optional(),
  destPath: z.string().optional(),
  objectACL: ObjectAclOptions$outboundSchema.optional(),
  storageClass: StorageClassOptions$outboundSchema.optional(),
  serverSideEncryption:
    ServerSideEncryptionForUploadedObjectsOptions$outboundSchema.optional(),
  kmsKeyId: z.string().optional(),
  removeEmptyDirs: z.boolean().optional(),
  baseFileName: z.string().optional(),
  fileNameSuffix: z.string().optional(),
  maxFileSizeMB: z.number().optional(),
  maxOpenFiles: z.number().optional(),
  headerLine: z.string().optional(),
  writeHighWaterMark: z.number().optional(),
  onBackpressure: BackpressureBehaviorOptions1$outboundSchema.optional(),
  deadletterEnabled: z.boolean().optional(),
  onDiskFullBackpressure: DiskSpaceProtectionOptions$outboundSchema.optional(),
  forceCloseOnShutdown: z.boolean().optional(),
  retrySettings: RetrySettingsType$outboundSchema.optional(),
  maxFileOpenTimeSec: z.number().optional(),
  maxFileIdleTimeSec: z.number().optional(),
  verifyPermissions: z.boolean().optional(),
  maxClosingFilesToBackpressure: z.number().optional(),
  awsAuthenticationMethod: MethodOptionsCredentials$outboundSchema.optional(),
  format: FormatOptionsCriblLakeDataset$outboundSchema.optional(),
  maxConcurrentFileParts: z.number().optional(),
  description: z.string().optional(),
  emptyDirCleanupSec: z.number().optional(),
  directoryBatchSize: z.number().optional(),
  deadletterPath: z.string().optional(),
  maxRetryNum: z.number().optional(),
});

export function outputCriblLakeToJSON(
  outputCriblLake: OutputCriblLake,
): string {
  return JSON.stringify(OutputCriblLake$outboundSchema.parse(outputCriblLake));
}
export function outputCriblLakeFromJSON(
  jsonString: string,
): SafeParseResult<OutputCriblLake, SDKValidationError> {
  return safeParse(
    jsonString,
    (x) => OutputCriblLake$inboundSchema.parse(JSON.parse(x)),
    `Failed to parse 'OutputCriblLake' from JSON`,
  );
}
